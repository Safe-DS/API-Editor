{
    "sklearn.feature_extraction.text.TfidfVectorizer": {
        "input": {
            "refined_type": {
                "kind": "EnumType",
                "values": ["file", "filename", "content"]
            },
            "docstring": {
                "type": "{'filename', 'file', 'content'}, default='content'",
                "description": "- If `'filename'`, the sequence passed as an argument to fit is\n  expected to be a list of filenames that need reading to fetch\n  the raw content to analyze.\n\n- If `'file'`, the sequence items must have a 'read' method (file-like\n  object) that is called to fetch the bytes in memory.\n\n- If `'content'`, the input is expected to be a sequence of items that\n  can be of type string or byte."
            }
        },
        "decode_error": {
            "refined_type": {
                "kind": "EnumType",
                "values": ["ignore", "replace", "strict"]
            },
            "docstring": {
                "type": "{'strict', 'ignore', 'replace'}, default='strict'",
                "description": "Instruction on what to do if a byte sequence is given to analyze that\ncontains characters not of the given `encoding`. By default, it is\n'strict', meaning that a UnicodeDecodeError will be raised. Other\nvalues are 'ignore' and 'replace'."
            }
        },
        "strip_accents": {
            "refined_type": {
                "kind": "EnumType",
                "values": ["ascii", "unicode"]
            },
            "docstring": {
                "type": "{'ascii', 'unicode'}, default=None",
                "description": "Remove accents and perform other character normalization\nduring the preprocessing step.\n'ascii' is a fast method that only works on characters that have\nan direct ASCII mapping.\n'unicode' is a slightly slower method that works on any characters.\nNone (default) does nothing.\n\nBoth 'ascii' and 'unicode' use NFKD normalization from\n:func:`unicodedata.normalize`."
            }
        },
        "analyzer": {
            "refined_type": {
                "kind": "UnionType",
                "types": [
                    {
                        "kind": "EnumType",
                        "values": ["char_wb", "char", "word"]
                    },
                    {
                        "kind": "NamedType",
                        "name": "callable"
                    }
                ]
            },
            "docstring": {
                "type": "{'word', 'char', 'char_wb'} or callable, default='word'",
                "description": "Whether the feature should be made of word or character n-grams.\nOption 'char_wb' creates character n-grams only from text inside\nword boundaries; n-grams at the edges of words are padded with space.\n\nIf a callable is passed it is used to extract the sequence of features\nout of the raw, unprocessed input.\n\n.. versionchanged:: 0.21\n    Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n    is first read from the file and then passed to the given callable\n    analyzer."
            }
        },
        "stop_words": {
            "refined_type": {
                "kind": "UnionType",
                "types": [
                    {
                        "kind": "EnumType",
                        "values": ["english"]
                    },
                    {
                        "kind": "NamedType",
                        "name": "list"
                    }
                ]
            },
            "docstring": {
                "type": "{'english'}, list, default=None",
                "description": "If a string, it is passed to _check_stop_list and the appropriate stop\nlist is returned. 'english' is currently the only supported string\nvalue.\nThere are several known issues with 'english' and you should\nconsider an alternative (see :ref:`stop_words`).\n\nIf a list, that list is assumed to contain stop words, all of which\nwill be removed from the resulting tokens.\nOnly applies if ``analyzer == 'word'``.\n\nIf None, no stop words will be used. max_df can be set to a value\nin the range [0.7, 1.0) to automatically detect and filter stop\nwords based on intra corpus document frequency of terms."
            }
        },
        "max_df": {
            "refined_type": {
                "kind": "UnionType",
                "types": [
                    {
                        "kind": "BoundaryType",
                        "baseType": "float",
                        "min": 0,
                        "minInclusive": true,
                        "max": 1,
                        "maxInclusive": true
                    },
                    {
                        "kind": "NamedType",
                        "name": "int"
                    }
                ]
            },
            "docstring": {
                "type": "float or int, default=1.0",
                "description": "When building the vocabulary ignore terms that have a document\nfrequency strictly higher than the given threshold (corpus-specific\nstop words).\nIf float in range [0.0, 1.0], the parameter represents a proportion of\ndocuments, integer absolute counts.\nThis parameter is ignored if vocabulary is not None."
            }
        },
        "min_df": {
            "refined_type": {
                "kind": "UnionType",
                "types": [
                    {
                        "kind": "BoundaryType",
                        "baseType": "float",
                        "min": 0,
                        "minInclusive": true,
                        "max": 1,
                        "maxInclusive": true
                    },
                    {
                        "kind": "NamedType",
                        "name": "int"
                    }
                ]
            },
            "docstring": {
                "type": "float or int, default=1",
                "description": "When building the vocabulary ignore terms that have a document\nfrequency strictly lower than the given threshold. This value is also\ncalled cut-off in the literature.\nIf float in range of [0.0, 1.0], the parameter represents a proportion\nof documents, integer absolute counts.\nThis parameter is ignored if vocabulary is not None."
            }
        },
        "norm": {
            "refined_type": {
                "kind": "EnumType",
                "values": ["l1", "l2"]
            },
            "docstring": {
                "type": "{'l1', 'l2'}, default='l2'",
                "description": "Each output row will have unit norm, either:\n\n- 'l2': Sum of squares of vector elements is 1. The cosine\n  similarity between two vectors is their dot product when l2 norm has\n  been applied.\n- 'l1': Sum of absolute values of vector elements is 1.\n  See :func:`preprocessing.normalize`."
            }
        }
    }
}
