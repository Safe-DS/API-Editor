{
  "name": "sklearn",
  "modules": [
    {
      "name": "sklearn",
      "imports": [
        {
          "module": "logging",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "random",
          "alias": null
        },
        {
          "module": "sys",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "",
          "declaration": "__check_build",
          "alias": null
        },
        {
          "module": "",
          "declaration": "_distributor_init",
          "alias": null
        },
        {
          "module": "_config",
          "declaration": "config_context",
          "alias": null
        },
        {
          "module": "_config",
          "declaration": "get_config",
          "alias": null
        },
        {
          "module": "_config",
          "declaration": "set_config",
          "alias": null
        },
        {
          "module": "base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "utils._show_versions",
          "declaration": "show_versions",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "setup_module",
          "decorators": [],
          "parameters": [
            {
              "name": "module",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Fixture for the tests to assure globally controllable seeding of RNGs"
        }
      ]
    },
    {
      "name": "sklearn.__check_build",
      "imports": [
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn._check_build",
          "declaration": "check_build",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "raise_build_error",
          "decorators": [],
          "parameters": [
            {
              "name": "e",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.__check_build.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn._build_utils",
      "imports": [
        {
          "module": "contextlib",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "sklearn",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "_min_dependencies",
          "declaration": "CYTHON_MIN_VERSION",
          "alias": null
        },
        {
          "module": "distutils.version",
          "declaration": "LooseVersion",
          "alias": null
        },
        {
          "module": "sklearn.openmp_helpers",
          "declaration": "check_openmp_support",
          "alias": null
        },
        {
          "module": "sklearn.pre_build_helpers",
          "declaration": "basic_check_build",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "cythonize_extensions",
          "decorators": [],
          "parameters": [
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "config",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check that a recent Cython is available and cythonize extensions"
        },
        {
          "name": "gen_from_templates",
          "decorators": [],
          "parameters": [
            {
              "name": "templates",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate cython files from a list of templates"
        }
      ]
    },
    {
      "name": "sklearn._build_utils.openmp_helpers",
      "imports": [
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "subprocess",
          "alias": null
        },
        {
          "module": "sys",
          "alias": null
        },
        {
          "module": "textwrap",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "distutils.errors",
          "declaration": "CompileError",
          "alias": null
        },
        {
          "module": "distutils.errors",
          "declaration": "LinkError",
          "alias": null
        },
        {
          "module": "sklearn._build_utils.pre_build_helpers",
          "declaration": "compile_test_program",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "check_openmp_support",
          "decorators": [],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check whether OpenMP test code can be compiled and run"
        },
        {
          "name": "get_openmp_flag",
          "decorators": [],
          "parameters": [
            {
              "name": "compiler",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn._build_utils.pre_build_helpers",
      "imports": [
        {
          "module": "glob",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "setuptools",
          "alias": null
        },
        {
          "module": "subprocess",
          "alias": null
        },
        {
          "module": "sys",
          "alias": null
        },
        {
          "module": "tempfile",
          "alias": null
        },
        {
          "module": "textwrap",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "distutils.dist",
          "declaration": "Distribution",
          "alias": null
        },
        {
          "module": "distutils.sysconfig",
          "declaration": "customize_compiler",
          "alias": null
        },
        {
          "module": "numpy.distutils.ccompiler",
          "declaration": "new_compiler",
          "alias": null
        },
        {
          "module": "numpy.distutils.command.config_compiler",
          "declaration": "config_cc",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "basic_check_build",
          "decorators": [],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check basic compilation and linking of C code"
        },
        {
          "name": "compile_test_program",
          "decorators": [],
          "parameters": [
            {
              "name": "code",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "extra_preargs",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "extra_postargs",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check that some C code can be compiled and run"
        }
      ]
    },
    {
      "name": "sklearn._config",
      "imports": [
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "contextlib",
          "declaration": "contextmanager",
          "alias": "contextmanager"
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "config_context",
          "decorators": [
            "contextmanager"
          ],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Context manager for global scikit-learn configuration\n\nParameters\n----------\nassume_finite : bool, default=False\n    If True, validation for finiteness will be skipped,\n    saving time, but leading to potential crashes. If\n    False, validation for finiteness will be performed,\n    avoiding error.  Global default: False.\n\nworking_memory : int, default=1024\n    If set, scikit-learn will attempt to limit the size of temporary arrays\n    to this number of MiB (per job when parallelised), often saving both\n    computation time and memory on expensive operations that can be\n    performed in chunks. Global default: 1024.\n\nprint_changed_only : bool, default=True\n    If True, only the parameters that were set to non-default\n    values will be printed when printing an estimator. For example,\n    ``print(SVC())`` while True will only print 'SVC()', but would print\n    'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n    when False. Default is True.\n\n    .. versionchanged:: 0.23\n       Default changed from False to True.\n\ndisplay : {'text', 'diagram'}, default='text'\n    If 'diagram', estimators will be displayed as a diagram in a Jupyter\n    lab or notebook context. If 'text', estimators will be displayed as\n    text. Default is 'text'.\n\n    .. versionadded:: 0.23\n\nNotes\n-----\nAll settings, not just those presently modified, will be returned to\ntheir previous values when the context manager is exited. This is not\nthread-safe.\n\nExamples\n--------\n>>> import sklearn\n>>> from sklearn.utils.validation import assert_all_finite\n>>> with sklearn.config_context(assume_finite=True):\n...     assert_all_finite([float('nan')])\n>>> with sklearn.config_context(assume_finite=True):\n...     with sklearn.config_context(assume_finite=False):\n...         assert_all_finite([float('nan')])\nTraceback (most recent call last):\n...\nValueError: Input contains NaN, ...\n\nSee Also\n--------\nset_config : Set global scikit-learn configuration.\nget_config : Retrieve current values of the global configuration."
        },
        {
          "name": "get_config",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "config",
              "type": "Dict",
              "description": "Keys are parameter names that can be passed to :func:`set_config`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Retrieve current values for configuration set by :func:`set_config`\n\nReturns\n-------\nconfig : dict\n    Keys are parameter names that can be passed to :func:`set_config`.\n\nSee Also\n--------\nconfig_context : Context manager for global scikit-learn configuration.\nset_config : Set global scikit-learn configuration."
        },
        {
          "name": "set_config",
          "decorators": [],
          "parameters": [
            {
              "name": "assume_finite",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If True, validation for finiteness will be skipped,\nsaving time, but leading to potential crashes. If\nFalse, validation for finiteness will be performed,\navoiding error.  Global default: False.\n\n.. versionadded:: 0.19"
            },
            {
              "name": "working_memory",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If set, scikit-learn will attempt to limit the size of temporary arrays\nto this number of MiB (per job when parallelised), often saving both\ncomputation time and memory on expensive operations that can be\nperformed in chunks. Global default: 1024.\n\n.. versionadded:: 0.20"
            },
            {
              "name": "print_changed_only",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If True, only the parameters that were set to non-default\nvalues will be printed when printing an estimator. For example,\n``print(SVC())`` while True will only print 'SVC()' while the default\nbehaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\nall the non-changed parameters.\n\n.. versionadded:: 0.21"
            },
            {
              "name": "display",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If 'diagram', estimators will be displayed as a diagram in a Jupyter\nlab or notebook context. If 'text', estimators will be displayed as\ntext. Default is 'text'.\n\n.. versionadded:: 0.23"
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Set global scikit-learn configuration\n\n.. versionadded:: 0.19\n\nParameters\n----------\nassume_finite : bool, default=None\n    If True, validation for finiteness will be skipped,\n    saving time, but leading to potential crashes. If\n    False, validation for finiteness will be performed,\n    avoiding error.  Global default: False.\n\n    .. versionadded:: 0.19\n\nworking_memory : int, default=None\n    If set, scikit-learn will attempt to limit the size of temporary arrays\n    to this number of MiB (per job when parallelised), often saving both\n    computation time and memory on expensive operations that can be\n    performed in chunks. Global default: 1024.\n\n    .. versionadded:: 0.20\n\nprint_changed_only : bool, default=None\n    If True, only the parameters that were set to non-default\n    values will be printed when printing an estimator. For example,\n    ``print(SVC())`` while True will only print 'SVC()' while the default\n    behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n    all the non-changed parameters.\n\n    .. versionadded:: 0.21\n\ndisplay : {'text', 'diagram'}, default=None\n    If 'diagram', estimators will be displayed as a diagram in a Jupyter\n    lab or notebook context. If 'text', estimators will be displayed as\n    text. Default is 'text'.\n\n    .. versionadded:: 0.23\n\nSee Also\n--------\nconfig_context : Context manager for global scikit-learn configuration.\nget_config : Retrieve current values of the global configuration."
        }
      ]
    },
    {
      "name": "sklearn._distributor_init",
      "imports": [
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "os.path",
          "alias": "op"
        }
      ],
      "fromImports": [
        {
          "module": "ctypes",
          "declaration": "WinDLL",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn._loss",
      "imports": [],
      "fromImports": [],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn._loss.glm_distribution",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "collections",
          "declaration": "namedtuple",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "xlogy",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "ExponentialDispersionModel",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "in_y_range",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns ``True`` if y is in the valid range of Y~EDM.\n\nParameters\n----------\ny : array of shape (n_samples,)\n    Target values."
            },
            {
              "name": "unit_variance",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Predicted mean."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the unit variance function.\n\nThe unit variance :math:`v(y_\\textrm{pred})` determines the variance as\na function of the mean :math:`y_\\textrm{pred}` by\n:math:`\\mathrm{Var}[Y_i] = \\phi/s_i*v(y_\\textrm{pred}_i)`.\nIt can also be derived from the unit deviance\n:math:`d(y,y_\\textrm{pred})` as\n\n.. math:: v(y_\\textrm{pred}) = \\frac{2}{\n    \\frac{\\partial^2 d(y,y_\\textrm{pred})}{\n    \\partialy_\\textrm{pred}^2}}\\big|_{y=y_\\textrm{pred}}\n\nSee also :func:`variance`.\n\nParameters\n----------\ny_pred : array of shape (n_samples,)\n    Predicted mean."
            },
            {
              "name": "unit_deviance",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Predicted mean."
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True raise an exception on invalid y or y_pred values, otherwise\nthey will be propagated as NaN."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the unit deviance.\n\nThe unit_deviance :math:`d(y,y_\\textrm{pred})` can be defined by the\nlog-likelihood as\n:math:`d(y,y_\\textrm{pred}) = -2\\phi\\cdot\n\\left(loglike(y,y_\\textrm{pred},\\phi) - loglike(y,y,\\phi)\\right).`\n\nParameters\n----------\ny : array of shape (n_samples,)\n    Target values.\n\ny_pred : array of shape (n_samples,)\n    Predicted mean.\n\ncheck_input : bool, default=False\n    If True raise an exception on invalid y or y_pred values, otherwise\n    they will be propagated as NaN.\nReturns\n-------\ndeviance: array of shape (n_samples,)\n    Computed deviance"
            },
            {
              "name": "unit_deviance_derivative",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Predicted mean."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the derivative of the unit deviance w.r.t. y_pred.\n\nThe derivative of the unit deviance is given by\n:math:`\\frac{\\partial}{\\partialy_\\textrm{pred}}d(y,y_\\textrm{pred})\n     = -2\\frac{y-y_\\textrm{pred}}{v(y_\\textrm{pred})}`\nwith unit variance :math:`v(y_\\textrm{pred})`.\n\nParameters\n----------\ny : array of shape (n_samples,)\n    Target values.\n\ny_pred : array of shape (n_samples,)\n    Predicted mean."
            },
            {
              "name": "deviance",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Predicted mean."
                },
                {
                  "name": "weights",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "1",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights or exposure to which variance is inverse proportional."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the deviance.\n\nThe deviance is a weighted sum of the per sample unit deviances,\n:math:`D = \\sum_i s_i \\cdot d(y_i, y_\\textrm{pred}_i)`\nwith weights :math:`s_i` and unit deviance\n:math:`d(y,y_\\textrm{pred})`.\nIn terms of the log-likelihood it is :math:`D = -2\\phi\\cdot\n\\left(loglike(y,y_\\textrm{pred},\\frac{phi}{s})\n- loglike(y,y,\\frac{phi}{s})\\right)`.\n\nParameters\n----------\ny : array of shape (n_samples,)\n    Target values.\n\ny_pred : array of shape (n_samples,)\n    Predicted mean.\n\nweights : {int, array of shape (n_samples,)}, default=1\n    Weights or exposure to which variance is inverse proportional."
            },
            {
              "name": "deviance_derivative",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Predicted mean."
                },
                {
                  "name": "weights",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "1",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights or exposure to which variance is inverse proportional."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the derivative of the deviance w.r.t. y_pred.\n\nIt gives :math:`\\frac{\\partial}{\\partial y_\\textrm{pred}}\nD(y, \\y_\\textrm{pred}; weights)`.\n\nParameters\n----------\ny : array, shape (n_samples,)\n    Target values.\n\ny_pred : array, shape (n_samples,)\n    Predicted mean.\n\nweights : {int, array of shape (n_samples,)}, default=1\n    Weights or exposure to which variance is inverse proportional."
            }
          ],
          "fullDocstring": "Base class for reproductive Exponential Dispersion Models (EDM).\n\nThe pdf of :math:`Y\\sim \\mathrm{EDM}(y_\\textrm{pred}, \\phi)` is given by\n\n.. math:: p(y| \\theta, \\phi) = c(y, \\phi)\n    \\exp\\left(\\frac{\\theta y-A(\\theta)}{\\phi}\\right)\n    = \\tilde{c}(y, \\phi)\n        \\exp\\left(-\\frac{d(y, y_\\textrm{pred})}{2\\phi}\\right)\n\nwith mean :math:`\\mathrm{E}[Y] = A'(\\theta) = y_\\textrm{pred}`,\nvariance :math:`\\mathrm{Var}[Y] = \\phi \\cdot v(y_\\textrm{pred})`,\nunit variance :math:`v(y_\\textrm{pred})` and\nunit deviance :math:`d(y,y_\\textrm{pred})`.\n\nMethods\n-------\ndeviance\ndeviance_derivative\nin_y_range\nunit_deviance\nunit_deviance_derivative\nunit_variance\n\nReferences\n----------\nhttps://en.wikipedia.org/wiki/Exponential_dispersion_model."
        },
        {
          "name": "GammaDistribution",
          "decorators": [],
          "superclasses": [
            "TweedieDistribution"
          ],
          "methods": [],
          "fullDocstring": "Class for the Gamma distribution."
        },
        {
          "name": "InverseGaussianDistribution",
          "decorators": [],
          "superclasses": [
            "TweedieDistribution"
          ],
          "methods": [],
          "fullDocstring": "Class for the scaled InverseGaussianDistribution distribution."
        },
        {
          "name": "NormalDistribution",
          "decorators": [],
          "superclasses": [
            "TweedieDistribution"
          ],
          "methods": [],
          "fullDocstring": "Class for the Normal (aka Gaussian) distribution."
        },
        {
          "name": "PoissonDistribution",
          "decorators": [],
          "superclasses": [
            "TweedieDistribution"
          ],
          "methods": [],
          "fullDocstring": "Class for the scaled Poisson distribution."
        },
        {
          "name": "TweedieDistribution",
          "decorators": [],
          "superclasses": [
            "ExponentialDispersionModel"
          ],
          "methods": [
            {
              "name": "power",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "power",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "unit_variance",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Predicted mean."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the unit variance of a Tweedie distribution\nv(y_    extrm{pred})=y_ extrm{pred}**power.\n\nParameters\n----------\ny_pred : array of shape (n_samples,)\n    Predicted mean."
            },
            {
              "name": "unit_deviance",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Predicted mean."
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True raise an exception on invalid y or y_pred values, otherwise\nthey will be propagated as NaN."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the unit deviance.\n\nThe unit_deviance :math:`d(y,y_\\textrm{pred})` can be defined by the\nlog-likelihood as\n:math:`d(y,y_\\textrm{pred}) = -2\\phi\\cdot\n\\left(loglike(y,y_\\textrm{pred},\\phi) - loglike(y,y,\\phi)\\right).`\n\nParameters\n----------\ny : array of shape (n_samples,)\n    Target values.\n\ny_pred : array of shape (n_samples,)\n    Predicted mean.\n\ncheck_input : bool, default=False\n    If True raise an exception on invalid y or y_pred values, otherwise\n    they will be propagated as NaN.\nReturns\n-------\ndeviance: array of shape (n_samples,)\n    Computed deviance"
            }
          ],
          "fullDocstring": "A class for the Tweedie distribution.\n\nA Tweedie distribution with mean :math:`y_\\textrm{pred}=\\mathrm{E}[Y]`\nis uniquely defined by it's mean-variance relationship\n:math:`\\mathrm{Var}[Y] \\propto y_\\textrm{pred}^power`.\n\nSpecial cases are:\n\n===== ================\nPower Distribution\n===== ================\n0     Normal\n1     Poisson\n(1,2) Compound Poisson\n2     Gamma\n3     Inverse Gaussian\n\nParameters\n----------\npower : float, default=0\n        The variance power of the `unit_variance`\n        :math:`v(y_\\textrm{pred}) = y_\\textrm{pred}^{power}`.\n        For ``0<power<1``, no distribution exists."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn._min_dependencies",
      "imports": [
        {
          "module": "argparse",
          "alias": null
        },
        {
          "module": "platform",
          "alias": null
        }
      ],
      "fromImports": [],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.base",
      "imports": [
        {
          "module": "copy",
          "alias": null
        },
        {
          "module": "inspect",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "platform",
          "alias": null
        },
        {
          "module": "re",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "defaultdict",
          "alias": null
        },
        {
          "module": "sklearn",
          "declaration": "__version__",
          "alias": null
        },
        {
          "module": "sklearn._config",
          "declaration": "get_config",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_IS_32BIT",
          "alias": null
        },
        {
          "module": "sklearn.utils._estimator_html_repr",
          "declaration": "estimator_html_repr",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_DEFAULT_TAGS",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_safe_tags",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseEstimator",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
                }
              ],
              "results": [
                {
                  "name": "params",
                  "type": "Dict",
                  "description": "Parameter names mapped to their values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get parameters for this estimator.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : dict\n    Parameter names mapped to their values."
            },
            {
              "name": "set_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Estimator instance."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as :class:`~sklearn.pipeline.Pipeline`). The latter have\nparameters of the form ``<component>__<parameter>`` so that it's\npossible to update each component of a nested object.\n\nParameters\n----------\n**params : dict\n    Estimator parameters.\n\nReturns\n-------\nself : estimator instance\n    Estimator instance."
            }
          ],
          "fullDocstring": "Base class for all estimators in scikit-learn.\n\nNotes\n-----\nAll estimators should specify all the parameters that can be set\nat the class level in their ``__init__`` as explicit keyword\narguments (no ``*args`` or ``**kwargs``)."
        },
        {
          "name": "BiclusterMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "biclusters_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Convenient way to get row and column indicators together.\n\nReturns the ``rows_`` and ``columns_`` members."
            },
            {
              "name": "get_indices",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "i",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The index of the cluster."
                }
              ],
              "results": [
                {
                  "name": "row_ind",
                  "type": null,
                  "description": "Indices of rows in the dataset that belong to the bicluster."
                },
                {
                  "name": "col_ind",
                  "type": null,
                  "description": "Indices of columns in the dataset that belong to the bicluster."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Row and column indices of the `i`'th bicluster.\n\nOnly works if ``rows_`` and ``columns_`` attributes exist.\n\nParameters\n----------\ni : int\n    The index of the cluster.\n\nReturns\n-------\nrow_ind : ndarray, dtype=np.intp\n    Indices of rows in the dataset that belong to the bicluster.\ncol_ind : ndarray, dtype=np.intp\n    Indices of columns in the dataset that belong to the bicluster."
            },
            {
              "name": "get_shape",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "i",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The index of the cluster."
                }
              ],
              "results": [
                {
                  "name": "n_rows",
                  "type": "int",
                  "description": "Number of rows in the bicluster."
                },
                {
                  "name": "n_cols",
                  "type": "int",
                  "description": "Number of columns in the bicluster."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Shape of the `i`'th bicluster.\n\nParameters\n----------\ni : int\n    The index of the cluster.\n\nReturns\n-------\nn_rows : int\n    Number of rows in the bicluster.\n\nn_cols : int\n    Number of columns in the bicluster."
            },
            {
              "name": "get_submatrix",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "i",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The index of the cluster."
                },
                {
                  "name": "data",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data."
                }
              ],
              "results": [
                {
                  "name": "submatrix",
                  "type": null,
                  "description": "The submatrix corresponding to bicluster `i`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the submatrix corresponding to bicluster `i`.\n\nParameters\n----------\ni : int\n    The index of the cluster.\ndata : array-like of shape (n_samples, n_features)\n    The data.\n\nReturns\n-------\nsubmatrix : ndarray of shape (n_rows, n_cols)\n    The submatrix corresponding to bicluster `i`.\n\nNotes\n-----\nWorks with sparse matrices. Only works if ``rows_`` and\n``columns_`` attributes exist."
            }
          ],
          "fullDocstring": "Mixin class for all bicluster estimators in scikit-learn."
        },
        {
          "name": "ClassifierMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True labels for `X`."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "Mean accuracy of ``self.predict(X)`` wrt. `y`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True labels for `X`.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    Mean accuracy of ``self.predict(X)`` wrt. `y`."
            }
          ],
          "fullDocstring": "Mixin class for all classifiers in scikit-learn."
        },
        {
          "name": "ClusterMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "fit_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Cluster labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform clustering on `X` and returns cluster labels.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input data.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,), dtype=np.int64\n    Cluster labels."
            }
          ],
          "fullDocstring": "Mixin class for all cluster estimators in scikit-learn."
        },
        {
          "name": "DensityMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the score of the model on the data `X`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nscore : float"
            }
          ],
          "fullDocstring": "Mixin class for all density estimators in scikit-learn."
        },
        {
          "name": "MetaEstimatorMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": null
        },
        {
          "name": "MultiOutputMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "Mixin to mark estimators that support multioutput."
        },
        {
          "name": "OutlierMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "fit_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "1 for inliers, -1 for outliers."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform fit on X and returns labels for X.\n\nReturns -1 for outliers and 1 for inliers.\n\nParameters\n----------\nX : {array-like, sparse matrix, dataframe} of shape             (n_samples, n_features)\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    1 for inliers, -1 for outliers."
            }
          ],
          "fullDocstring": "Mixin class for all outlier detection estimators in scikit-learn."
        },
        {
          "name": "RegressorMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples. For some estimators this may be a precomputed\nkernel matrix or a list of generic objects instead with shape\n``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\nis the number of samples used in the fitting for the estimator."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True values for `X`."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": ":math:`R^2` of ``self.predict(X)`` wrt. `y`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the coefficient of determination :math:`R^2` of the\nprediction.\n\nThe coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\nwhere :math:`u` is the residual sum of squares ``((y_true - y_pred)\n** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\ny_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\ncan be negative (because the model can be arbitrarily worse). A\nconstant model that always predicts the expected value of `y`,\ndisregarding the input features, would get a :math:`R^2` score of\n0.0.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples. For some estimators this may be a precomputed\n    kernel matrix or a list of generic objects instead with shape\n    ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n    is the number of samples used in the fitting for the estimator.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True values for `X`.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n\nNotes\n-----\nThe :math:`R^2` score used when calling ``score`` on a regressor uses\n``multioutput='uniform_average'`` from version 0.23 to keep consistent\nwith default value of :func:`~sklearn.metrics.r2_score`.\nThis influences the ``score`` method of all the multioutput\nregressors (except for\n:class:`~sklearn.multioutput.MultiOutputRegressor`)."
            }
          ],
          "fullDocstring": "Mixin class for all regression estimators in scikit-learn."
        },
        {
          "name": "TransformerMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values (None for unsupervised transformations)."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input samples.\n\ny :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n    Target values (None for unsupervised transformations).\n\n**fit_params : dict\n    Additional fit parameters.\n\nReturns\n-------\nX_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array."
            }
          ],
          "fullDocstring": "Mixin class for all transformers in scikit-learn."
        }
      ],
      "functions": [
        {
          "name": "clone",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The estimator or group of estimators to be cloned."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Constructs a new unfitted estimator with the same parameters.\n\nClone does a deep copy of the model in an estimator\nwithout actually copying attached data. It yields a new estimator\nwith the same parameters that has not been fitted on any data.\n\nIf the estimator's `random_state` parameter is an integer (or if the\nestimator doesn't have a `random_state` parameter), an *exact clone* is\nreturned: the clone and the original estimator will give the exact same\nresults. Otherwise, *statistical clone* is returned: the clone might\nyield different results from the original estimator. More details can be\nfound in :ref:`randomness`.\n\nParameters\n----------\nestimator : {list, tuple, set} of estimator instance or a single             estimator instance\n    The estimator or group of estimators to be cloned.\n\nsafe : bool, default=True\n    If safe is False, clone will fall back to a deep copy on objects\n    that are not estimators."
        },
        {
          "name": "is_classifier",
          "decorators": [],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimator object to test."
            }
          ],
          "results": [
            {
              "name": "out",
              "type": "bool",
              "description": "True if estimator is a classifier and False otherwise."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Return True if the given estimator is (probably) a classifier.\n\nParameters\n----------\nestimator : object\n    Estimator object to test.\n\nReturns\n-------\nout : bool\n    True if estimator is a classifier and False otherwise."
        },
        {
          "name": "is_outlier_detector",
          "decorators": [],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimator object to test."
            }
          ],
          "results": [
            {
              "name": "out",
              "type": "bool",
              "description": "True if estimator is an outlier detector and False otherwise."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Return True if the given estimator is (probably) an outlier detector.\n\nParameters\n----------\nestimator : estimator instance\n    Estimator object to test.\n\nReturns\n-------\nout : bool\n    True if estimator is an outlier detector and False otherwise."
        },
        {
          "name": "is_regressor",
          "decorators": [],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimator object to test."
            }
          ],
          "results": [
            {
              "name": "out",
              "type": "bool",
              "description": "True if estimator is a regressor and False otherwise."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Return True if the given estimator is (probably) a regressor.\n\nParameters\n----------\nestimator : estimator instance\n    Estimator object to test.\n\nReturns\n-------\nout : bool\n    True if estimator is a regressor and False otherwise."
        }
      ]
    },
    {
      "name": "sklearn.calibration",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "contextlib",
          "declaration": "suppress",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "inspect",
          "declaration": "signature",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "log",
          "alias": null
        },
        {
          "module": "scipy.optimize",
          "declaration": "fmin_bfgs",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "xlogy",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.isotonic",
          "declaration": "IsotonicRegression",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "cross_val_predict",
          "alias": null
        },
        {
          "module": "sklearn.pipeline",
          "declaration": "Pipeline",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "label_binarize",
          "alias": null
        },
        {
          "module": "sklearn.svm",
          "declaration": "LinearSVC",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "indexable",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "CalibratedClassifierCV",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "ClassifierMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the calibrated model.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n\nReturns\n-------\nself : object\n    Returns an instance of self."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The samples."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "The predicted probas."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Calibrated probabilities of classification.\n\nThis function returns calibrated probabilities of classification\naccording to each class on an array of test vectors X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The samples.\n\nReturns\n-------\nC : ndarray of shape (n_samples, n_classes)\n    The predicted probas."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The samples."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "The predicted class."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the target of new samples. The predicted class is the\nclass that has the highest probability, and can thus be different\nfrom the prediction of the uncalibrated classifier.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The samples.\n\nReturns\n-------\nC : ndarray of shape (n_samples,)\n    The predicted class."
            }
          ],
          "fullDocstring": "Probability calibration with isotonic regression or logistic regression.\n\nThis class uses cross-validation to both estimate the parameters of a\nclassifier and subsequently calibrate a classifier. With default\n`ensemble=True`, for each cv split it\nfits a copy of the base estimator to the training subset, and calibrates it\nusing the testing subset. For prediction, predicted probabilities are\naveraged across these individual calibrated classifiers. When\n`ensemble=False`, cross-validation is used to obtain unbiased predictions,\nvia :func:`~sklearn.model_selection.cross_val_predict`, which are then\nused for calibration. For prediction, the base estimator, trained using all\nthe data, is used. This is the method implemented when `probabilities=True`\nfor :mod:`sklearn.svm` estimators.\n\nAlready fitted classifiers can be calibrated via the parameter\n`cv=\"prefit\"`. In this case, no cross-validation is used and all provided\ndata is used for calibration. The user has to take care manually that data\nfor model fitting and calibration are disjoint.\n\nThe calibration is based on the :term:`decision_function` method of the\n`base_estimator` if it exists, else on :term:`predict_proba`.\n\nRead more in the :ref:`User Guide <calibration>`.\n\nParameters\n----------\nbase_estimator : estimator instance, default=None\n    The classifier whose output need to be calibrated to provide more\n    accurate `predict_proba` outputs. The default classifier is\n    a :class:`~sklearn.svm.LinearSVC`.\n\nmethod : {'sigmoid', 'isotonic'}, default='sigmoid'\n    The method to use for calibration. Can be 'sigmoid' which\n    corresponds to Platt's method (i.e. a logistic regression model) or\n    'isotonic' which is a non-parametric approach. It is not advised to\n    use isotonic calibration with too few calibration samples\n    ``(<<1000)`` since it tends to overfit.\n\ncv : int, cross-validation generator, iterable or \"prefit\",             default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n    neither binary nor multiclass, :class:`~sklearn.model_selection.KFold`\n    is used.\n\n    Refer to the :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    If \"prefit\" is passed, it is assumed that `base_estimator` has been\n    fitted already and all data is used for calibration.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors.\n\n    Base estimator clones are fitted in parallel across cross-validation\n    iterations. Therefore parallelism happens only when `cv != \"prefit\"`.\n\n    See :term:`Glossary <n_jobs>` for more details.\n\n    .. versionadded:: 0.24\n\nensemble : bool, default=True\n    Determines how the calibrator is fitted when `cv` is not `'prefit'`.\n    Ignored if `cv='prefit'`.\n\n    If `True`, the `base_estimator` is fitted using training data and\n    calibrated using testing data, for each `cv` fold. The final estimator\n    is an ensemble of `n_cv` fitted classifer and calibrator pairs, where\n    `n_cv` is the number of cross-validation folds. The output is the\n    average predicted probabilities of all pairs.\n\n    If `False`, `cv` is used to compute unbiased predictions, via\n    :func:`~sklearn.model_selection.cross_val_predict`, which are then\n    used for calibration. At prediction time, the classifier used is the\n    `base_estimator` trained on all the data.\n    Note that this method is also internally implemented  in\n    :mod:`sklearn.svm` estimators with the `probabilities=True` parameter.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    The class labels.\n\ncalibrated_classifiers_ : list (len() equal to cv or 1 if `cv=\"prefit\"`             or `ensemble=False`)\n    The list of classifier and calibrator pairs.\n\n    - When `cv=\"prefit\"`, the fitted `base_estimator` and fitted\n      calibrator.\n    - When `cv` is not \"prefit\" and `ensemble=True`, `n_cv` fitted\n      `base_estimator` and calibrator pairs. `n_cv` is the number of\n      cross-validation folds.\n    - When `cv` is not \"prefit\" and `ensemble=False`, the `base_estimator`,\n      fitted on all the data, and fitted calibrator.\n\n    .. versionchanged:: 0.24\n        Single calibrated classifier case when `ensemble=False`.\n\nExamples\n--------\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.calibration import CalibratedClassifierCV\n>>> X, y = make_classification(n_samples=100, n_features=2,\n...                            n_redundant=0, random_state=42)\n>>> base_clf = GaussianNB()\n>>> calibrated_clf = CalibratedClassifierCV(base_estimator=base_clf, cv=3)\n>>> calibrated_clf.fit(X, y)\nCalibratedClassifierCV(base_estimator=GaussianNB(), cv=3)\n>>> len(calibrated_clf.calibrated_classifiers_)\n3\n>>> calibrated_clf.predict_proba(X)[:5, :]\narray([[0.110..., 0.889...],\n       [0.072..., 0.927...],\n       [0.928..., 0.071...],\n       [0.928..., 0.071...],\n       [0.071..., 0.928...]])\n\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = make_classification(n_samples=100, n_features=2,\n...                            n_redundant=0, random_state=42)\n>>> X_train, X_calib, y_train, y_calib = train_test_split(\n...        X, y, random_state=42\n... )\n>>> base_clf = GaussianNB()\n>>> base_clf.fit(X_train, y_train)\nGaussianNB()\n>>> calibrated_clf = CalibratedClassifierCV(\n...     base_estimator=base_clf,\n...     cv=\"prefit\"\n... )\n>>> calibrated_clf.fit(X_calib, y_calib)\nCalibratedClassifierCV(base_estimator=GaussianNB(), cv='prefit')\n>>> len(calibrated_clf.calibrated_classifiers_)\n1\n>>> calibrated_clf.predict_proba([[-0.5, 0.5]])\narray([[0.936..., 0.063...]])\n\nReferences\n----------\n.. [1] Obtaining calibrated probability estimates from decision trees\n       and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\n.. [2] Transforming Classifier Scores into Accurate Multiclass\n       Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n\n.. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n       Regularized Likelihood Methods, J. Platt, (1999)\n\n.. [4] Predicting Good Probabilities with Supervised Learning,\n       A. Niculescu-Mizil & R. Caruana, ICML 2005"
        }
      ],
      "functions": [
        {
          "name": "calibration_curve",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True targets."
            },
            {
              "name": "y_prob",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Probabilities of the positive class."
            }
          ],
          "results": [
            {
              "name": "prob_true",
              "type": null,
              "description": "The proportion of samples whose class is the positive class, in each\nbin (fraction of positives)."
            },
            {
              "name": "prob_pred",
              "type": null,
              "description": "The mean predicted probability in each bin."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute true and predicted probabilities for a calibration curve.\n\nThe method assumes the inputs come from a binary classifier, and\ndiscretize the [0, 1] interval into bins.\n\nCalibration curves may also be referred to as reliability diagrams.\n\nRead more in the :ref:`User Guide <calibration>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    True targets.\n\ny_prob : array-like of shape (n_samples,)\n    Probabilities of the positive class.\n\nnormalize : bool, default=False\n    Whether y_prob needs to be normalized into the [0, 1] interval, i.e.\n    is not a proper probability. If True, the smallest value in y_prob\n    is linearly mapped onto 0 and the largest one onto 1.\n\nn_bins : int, default=5\n    Number of bins to discretize the [0, 1] interval. A bigger number\n    requires more data. Bins with no samples (i.e. without\n    corresponding values in `y_prob`) will not be returned, thus the\n    returned arrays may have less than `n_bins` values.\n\nstrategy : {'uniform', 'quantile'}, default='uniform'\n    Strategy used to define the widths of the bins.\n\n    uniform\n        The bins have identical widths.\n    quantile\n        The bins have the same number of samples and depend on `y_prob`.\n\nReturns\n-------\nprob_true : ndarray of shape (n_bins,) or smaller\n    The proportion of samples whose class is the positive class, in each\n    bin (fraction of positives).\n\nprob_pred : ndarray of shape (n_bins,) or smaller\n    The mean predicted probability in each bin.\n\nReferences\n----------\nAlexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\nProbabilities With Supervised Learning, in Proceedings of the 22nd\nInternational Conference on Machine Learning (ICML).\nSee section 4 (Qualitative Analysis of Predictions).\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.calibration import calibration_curve\n>>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])\n>>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])\n>>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n>>> prob_true\narray([0. , 0.5, 1. ])\n>>> prob_pred\narray([0.2  , 0.525, 0.85 ])"
        }
      ]
    },
    {
      "name": "sklearn.cluster",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._affinity_propagation",
          "declaration": "AffinityPropagation",
          "alias": null
        },
        {
          "module": "sklearn._affinity_propagation",
          "declaration": "affinity_propagation",
          "alias": null
        },
        {
          "module": "sklearn._agglomerative",
          "declaration": "AgglomerativeClustering",
          "alias": null
        },
        {
          "module": "sklearn._agglomerative",
          "declaration": "FeatureAgglomeration",
          "alias": null
        },
        {
          "module": "sklearn._agglomerative",
          "declaration": "linkage_tree",
          "alias": null
        },
        {
          "module": "sklearn._agglomerative",
          "declaration": "ward_tree",
          "alias": null
        },
        {
          "module": "sklearn._bicluster",
          "declaration": "SpectralBiclustering",
          "alias": null
        },
        {
          "module": "sklearn._bicluster",
          "declaration": "SpectralCoclustering",
          "alias": null
        },
        {
          "module": "sklearn._birch",
          "declaration": "Birch",
          "alias": null
        },
        {
          "module": "sklearn._dbscan",
          "declaration": "DBSCAN",
          "alias": null
        },
        {
          "module": "sklearn._dbscan",
          "declaration": "dbscan",
          "alias": null
        },
        {
          "module": "sklearn._kmeans",
          "declaration": "KMeans",
          "alias": null
        },
        {
          "module": "sklearn._kmeans",
          "declaration": "MiniBatchKMeans",
          "alias": null
        },
        {
          "module": "sklearn._kmeans",
          "declaration": "k_means",
          "alias": null
        },
        {
          "module": "sklearn._kmeans",
          "declaration": "kmeans_plusplus",
          "alias": null
        },
        {
          "module": "sklearn._mean_shift",
          "declaration": "MeanShift",
          "alias": null
        },
        {
          "module": "sklearn._mean_shift",
          "declaration": "estimate_bandwidth",
          "alias": null
        },
        {
          "module": "sklearn._mean_shift",
          "declaration": "get_bin_seeds",
          "alias": null
        },
        {
          "module": "sklearn._mean_shift",
          "declaration": "mean_shift",
          "alias": null
        },
        {
          "module": "sklearn._optics",
          "declaration": "OPTICS",
          "alias": null
        },
        {
          "module": "sklearn._optics",
          "declaration": "cluster_optics_dbscan",
          "alias": null
        },
        {
          "module": "sklearn._optics",
          "declaration": "cluster_optics_xi",
          "alias": null
        },
        {
          "module": "sklearn._optics",
          "declaration": "compute_optics_graph",
          "alias": null
        },
        {
          "module": "sklearn._spectral",
          "declaration": "SpectralClustering",
          "alias": null
        },
        {
          "module": "sklearn._spectral",
          "declaration": "spectral_clustering",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.cluster._affinity_propagation",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn._config",
          "declaration": "config_context",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "euclidean_distances",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "pairwise_distances_argmin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "as_float_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "AffinityPropagation",
          "decorators": [],
          "superclasses": [
            "ClusterMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster, or similarities / affinities between\ninstances if ``affinity='precomputed'``. If a sparse feature matrix\nis provided, it will be converted into a sparse ``csr_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the clustering from features, or affinity matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n    Training instances to cluster, or similarities / affinities between\n    instances if ``affinity='precomputed'``. If a sparse feature matrix\n    is provided, it will be converted into a sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data to predict. If a sparse matrix is provided, it will be\nconverted into a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Cluster labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the closest cluster each sample in X belongs to.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict. If a sparse matrix is provided, it will be\n    converted into a sparse ``csr_matrix``.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels."
            },
            {
              "name": "fit_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster, or similarities / affinities between\ninstances if ``affinity='precomputed'``. If a sparse feature matrix\nis provided, it will be converted into a sparse ``csr_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Cluster labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the clustering from features or affinity matrix, and return\ncluster labels.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n    Training instances to cluster, or similarities / affinities between\n    instances if ``affinity='precomputed'``. If a sparse feature matrix\n    is provided, it will be converted into a sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels."
            }
          ],
          "fullDocstring": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n\nParameters\n----------\ndamping : float, default=0.5\n    Damping factor (between 0.5 and 1) is the extent to\n    which the current value is maintained relative to\n    incoming values (weighted 1 - damping). This in order\n    to avoid numerical oscillations when updating these\n    values (messages).\n\nmax_iter : int, default=200\n    Maximum number of iterations.\n\nconvergence_iter : int, default=15\n    Number of iterations with no change in the number\n    of estimated clusters that stops the convergence.\n\ncopy : bool, default=True\n    Make a copy of input data.\n\npreference : array-like of shape (n_samples,) or float, default=None\n    Preferences for each point - points with larger values of\n    preferences are more likely to be chosen as exemplars. The number\n    of exemplars, ie of clusters, is influenced by the input\n    preferences value. If the preferences are not passed as arguments,\n    they will be set to the median of the input similarities.\n\naffinity : {'euclidean', 'precomputed'}, default='euclidean'\n    Which affinity to use. At the moment 'precomputed' and\n    ``euclidean`` are supported. 'euclidean' uses the\n    negative squared euclidean distance between points.\n\nverbose : bool, default=False\n    Whether to be verbose.\n\nrandom_state : int, RandomState instance or None, default=0\n    Pseudo-random number generator to control the starting state.\n    Use an int for reproducible results across function calls.\n    See the :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.23\n        this parameter was previously hardcoded as 0.\n\nAttributes\n----------\ncluster_centers_indices_ : ndarray of shape (n_clusters,)\n    Indices of cluster centers.\n\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Cluster centers (if affinity != ``precomputed``).\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point.\n\naffinity_matrix_ : ndarray of shape (n_samples, n_samples)\n    Stores the affinity matrix used in ``fit``.\n\nn_iter_ : int\n    Number of iterations taken to converge.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n<sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\nThe algorithmic complexity of affinity propagation is quadratic\nin the number of points.\n\nWhen ``fit`` does not converge, ``cluster_centers_`` becomes an empty\narray and all training samples will be labelled as ``-1``. In addition,\n``predict`` will then label every sample as ``-1``.\n\nWhen all training samples have equal similarities and equal preferences,\nthe assignment of cluster centers and labels depends on the preference.\nIf the preference is smaller than the similarities, ``fit`` will result in\na single cluster center and label ``0`` for every sample. Otherwise, every\ntraining sample becomes its own cluster center and is assigned a unique\nlabel.\n\nReferences\n----------\n\nBrendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\nBetween Data Points\", Science Feb. 2007\n\nExamples\n--------\n>>> from sklearn.cluster import AffinityPropagation\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AffinityPropagation(random_state=5).fit(X)\n>>> clustering\nAffinityPropagation(random_state=5)\n>>> clustering.labels_\narray([0, 0, 0, 1, 1, 1])\n>>> clustering.predict([[0, 0], [4, 4]])\narray([0, 1])\n>>> clustering.cluster_centers_\narray([[1, 2],\n       [4, 2]])"
        }
      ],
      "functions": [
        {
          "name": "affinity_propagation",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "S",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix of similarities between points."
            }
          ],
          "results": [
            {
              "name": "cluster_centers_indices",
              "type": null,
              "description": "Index of clusters centers."
            },
            {
              "name": "labels",
              "type": null,
              "description": "Cluster labels for each point."
            },
            {
              "name": "n_iter",
              "type": "int",
              "description": "Number of iterations run. Returned only if `return_n_iter` is\nset to True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n\nParameters\n----------\n\nS : array-like of shape (n_samples, n_samples)\n    Matrix of similarities between points.\n\npreference : array-like of shape (n_samples,) or float, default=None\n    Preferences for each point - points with larger values of\n    preferences are more likely to be chosen as exemplars. The number of\n    exemplars, i.e. of clusters, is influenced by the input preferences\n    value. If the preferences are not passed as arguments, they will be\n    set to the median of the input similarities (resulting in a moderate\n    number of clusters). For a smaller amount of clusters, this can be set\n    to the minimum value of the similarities.\n\nconvergence_iter : int, default=15\n    Number of iterations with no change in the number\n    of estimated clusters that stops the convergence.\n\nmax_iter : int, default=200\n    Maximum number of iterations\n\ndamping : float, default=0.5\n    Damping factor between 0.5 and 1.\n\ncopy : bool, default=True\n    If copy is False, the affinity matrix is modified inplace by the\n    algorithm, for memory efficiency.\n\nverbose : bool, default=False\n    The verbosity level.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nrandom_state : int, RandomState instance or None, default=0\n    Pseudo-random number generator to control the starting state.\n    Use an int for reproducible results across function calls.\n    See the :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.23\n        this parameter was previously hardcoded as 0.\n\nReturns\n-------\n\ncluster_centers_indices : ndarray of shape (n_clusters,)\n    Index of clusters centers.\n\nlabels : ndarray of shape (n_samples,)\n    Cluster labels for each point.\n\nn_iter : int\n    Number of iterations run. Returned only if `return_n_iter` is\n    set to True.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n<sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\nWhen the algorithm does not converge, it returns an empty array as\n``cluster_center_indices`` and ``-1`` as label for each training sample.\n\nWhen all training samples have equal similarities and equal preferences,\nthe assignment of cluster centers and labels depends on the preference.\nIf the preference is smaller than the similarities, a single cluster center\nand label ``0`` for every sample will be returned. Otherwise, every\ntraining sample becomes its own cluster center and is assigned a unique\nlabel.\n\nReferences\n----------\nBrendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\nBetween Data Points\", Science Feb. 2007"
        }
      ]
    },
    {
      "name": "sklearn.cluster._agglomerative",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "heapq",
          "declaration": "heapify",
          "alias": null
        },
        {
          "module": "heapq",
          "declaration": "heappop",
          "alias": null
        },
        {
          "module": "heapq",
          "declaration": "heappush",
          "alias": null
        },
        {
          "module": "heapq",
          "declaration": "heappushpop",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy.sparse.csgraph",
          "declaration": "connected_components",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "_hierarchical_fast",
          "alias": "_hierarchical"
        },
        {
          "module": "sklearn.cluster._feature_agglomeration",
          "declaration": "AgglomerationTransform",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "paired_distances",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_distances",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "DistanceMetric",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._dist_metrics",
          "declaration": "METRIC_MAPPING",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils._fast_dict",
          "declaration": "IntFloatDict",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_astype_copy_false",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_memory",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "AgglomerativeClustering",
          "decorators": [],
          "superclasses": [
            "ClusterMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster, or distances between instances if\n``affinity='precomputed'``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the hierarchical clustering from features, or distance matrix.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``affinity='precomputed'``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself"
            },
            {
              "name": "fit_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster, or distances between instances if\n``affinity='precomputed'``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Cluster labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the hierarchical clustering from features or distance matrix,\nand return cluster labels.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``affinity='precomputed'``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels."
            }
          ],
          "fullDocstring": "Agglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases\na given linkage distance.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nn_clusters : int or None, default=2\n    The number of clusters to find. It must be ``None`` if\n    ``distance_threshold`` is not ``None``.\n\naffinity : str or callable, default='euclidean'\n    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n    \"manhattan\", \"cosine\", or \"precomputed\".\n    If linkage is \"ward\", only \"euclidean\" is accepted.\n    If \"precomputed\", a distance matrix (instead of a similarity matrix)\n    is needed as input for the fit method.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n    By default, no caching is done. If a string is given, it is the\n    path to the caching directory.\n\nconnectivity : array-like or callable, default=None\n    Connectivity matrix. Defines for each sample the neighboring\n    samples following a given structure of the data.\n    This can be a connectivity matrix itself or a callable that transforms\n    the data into a connectivity matrix, such as derived from\n    kneighbors_graph. Default is ``None``, i.e, the\n    hierarchical clustering algorithm is unstructured.\n\ncompute_full_tree : 'auto' or bool, default='auto'\n    Stop early the construction of the tree at ``n_clusters``. This is\n    useful to decrease computation time if the number of clusters is not\n    small compared to the number of samples. This option is useful only\n    when specifying a connectivity matrix. Note also that when varying the\n    number of clusters and using caching, it may be advantageous to compute\n    the full tree. It must be ``True`` if ``distance_threshold`` is not\n    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n    to `True` when `distance_threshold` is not `None` or that `n_clusters`\n    is inferior to the maximum between 100 or `0.02 * n_samples`.\n    Otherwise, \"auto\" is equivalent to `False`.\n\nlinkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n    Which linkage criterion to use. The linkage criterion determines which\n    distance to use between sets of observation. The algorithm will merge\n    the pairs of cluster that minimize this criterion.\n\n    - 'ward' minimizes the variance of the clusters being merged.\n    - 'average' uses the average of the distances of each observation of\n      the two sets.\n    - 'complete' or 'maximum' linkage uses the maximum distances between\n      all observations of the two sets.\n    - 'single' uses the minimum of the distances between all observations\n      of the two sets.\n\n    .. versionadded:: 0.20\n        Added the 'single' option\n\ndistance_threshold : float, default=None\n    The linkage distance threshold above which, clusters will not be\n    merged. If not ``None``, ``n_clusters`` must be ``None`` and\n    ``compute_full_tree`` must be ``True``.\n\n    .. versionadded:: 0.21\n\ncompute_distances : bool, default=False\n    Computes distances between clusters even if `distance_threshold` is not\n    used. This can be used to make dendrogram visualization, but introduces\n    a computational and memory overhead.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nn_clusters_ : int\n    The number of clusters found by the algorithm. If\n    ``distance_threshold=None``, it will be equal to the given\n    ``n_clusters``.\n\nlabels_ : ndarray of shape (n_samples)\n    cluster labels for each point\n\nn_leaves_ : int\n    Number of leaves in the hierarchical tree.\n\nn_connected_components_ : int\n    The estimated number of connected components in the graph.\n\n    .. versionadded:: 0.21\n        ``n_connected_components_`` was added to replace ``n_components_``.\n\nchildren_ : array-like of shape (n_samples-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`\n\ndistances_ : array-like of shape (n_nodes-1,)\n    Distances between nodes in the corresponding place in `children_`.\n    Only computed if `distance_threshold` is used or `compute_distances`\n    is set to `True`.\n\nExamples\n--------\n>>> from sklearn.cluster import AgglomerativeClustering\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AgglomerativeClustering().fit(X)\n>>> clustering\nAgglomerativeClustering()\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])"
        },
        {
          "name": "FeatureAgglomeration",
          "decorators": [],
          "superclasses": [
            "AgglomerativeClustering",
            "AgglomerationTransform"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the hierarchical clustering on the data\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data\n\ny : Ignored\n\nReturns\n-------\nself"
            },
            {
              "name": "fit_predict",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Agglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features\ninstead of samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nn_clusters : int, default=2\n    The number of clusters to find. It must be ``None`` if\n    ``distance_threshold`` is not ``None``.\n\naffinity : str or callable, default='euclidean'\n    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n    \"manhattan\", \"cosine\", or 'precomputed'.\n    If linkage is \"ward\", only \"euclidean\" is accepted.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n    By default, no caching is done. If a string is given, it is the\n    path to the caching directory.\n\nconnectivity : array-like or callable, default=None\n    Connectivity matrix. Defines for each feature the neighboring\n    features following a given structure of the data.\n    This can be a connectivity matrix itself or a callable that transforms\n    the data into a connectivity matrix, such as derived from\n    kneighbors_graph. Default is None, i.e, the\n    hierarchical clustering algorithm is unstructured.\n\ncompute_full_tree : 'auto' or bool, default='auto'\n    Stop early the construction of the tree at n_clusters. This is useful\n    to decrease computation time if the number of clusters is not small\n    compared to the number of features. This option is useful only when\n    specifying a connectivity matrix. Note also that when varying the\n    number of clusters and using caching, it may be advantageous to compute\n    the full tree. It must be ``True`` if ``distance_threshold`` is not\n    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n    to `True` when `distance_threshold` is not `None` or that `n_clusters`\n    is inferior to the maximum between 100 or `0.02 * n_samples`.\n    Otherwise, \"auto\" is equivalent to `False`.\n\nlinkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n    Which linkage criterion to use. The linkage criterion determines which\n    distance to use between sets of features. The algorithm will merge\n    the pairs of cluster that minimize this criterion.\n\n    - ward minimizes the variance of the clusters being merged.\n    - average uses the average of the distances of each feature of\n      the two sets.\n    - complete or maximum linkage uses the maximum distances between\n      all features of the two sets.\n    - single uses the minimum of the distances between all observations\n      of the two sets.\n\npooling_func : callable, default=np.mean\n    This combines the values of agglomerated features into a single\n    value, and should accept an array of shape [M, N] and the keyword\n    argument `axis=1`, and reduce it to an array of size [M].\n\ndistance_threshold : float, default=None\n    The linkage distance threshold above which, clusters will not be\n    merged. If not ``None``, ``n_clusters`` must be ``None`` and\n    ``compute_full_tree`` must be ``True``.\n\n    .. versionadded:: 0.21\n\ncompute_distances : bool, default=False\n    Computes distances between clusters even if `distance_threshold` is not\n    used. This can be used to make dendrogram visualization, but introduces\n    a computational and memory overhead.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nn_clusters_ : int\n    The number of clusters found by the algorithm. If\n    ``distance_threshold=None``, it will be equal to the given\n    ``n_clusters``.\n\nlabels_ : array-like of (n_features,)\n    cluster labels for each feature.\n\nn_leaves_ : int\n    Number of leaves in the hierarchical tree.\n\nn_connected_components_ : int\n    The estimated number of connected components in the graph.\n\n    .. versionadded:: 0.21\n        ``n_connected_components_`` was added to replace ``n_components_``.\n\nchildren_ : array-like of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_features`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_features` is a non-leaf\n    node and has children `children_[i - n_features]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_features + i`\n\ndistances_ : array-like of shape (n_nodes-1,)\n    Distances between nodes in the corresponding place in `children_`.\n    Only computed if `distance_threshold` is used or `compute_distances`\n    is set to `True`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import datasets, cluster\n>>> digits = datasets.load_digits()\n>>> images = digits.images\n>>> X = np.reshape(images, (len(images), -1))\n>>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n>>> agglo.fit(X)\nFeatureAgglomeration(n_clusters=32)\n>>> X_reduced = agglo.transform(X)\n>>> X_reduced.shape\n(1797, 32)"
        }
      ],
      "functions": [
        {
          "name": "linkage_tree",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "feature matrix representing n_samples samples to be clustered"
            },
            {
              "name": "connectivity",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "connectivity matrix. Defines for each sample the neighboring samples\nfollowing a given structure of the data. The matrix is assumed to\nbe symmetric and only the upper triangular half is used.\nDefault is None, i.e, the Ward algorithm is unstructured."
            },
            {
              "name": "n_clusters",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Stop early the construction of the tree at n_clusters. This is\nuseful to decrease computation time if the number of clusters is\nnot small compared to the number of samples. In this case, the\ncomplete tree is not computed, thus the 'children' output is of\nlimited use, and the 'parents' output should rather be used.\nThis option is valid only when specifying a connectivity matrix."
            },
            {
              "name": "linkage",
              "type": "Any",
              "hasDefault": true,
              "default": "complete",
              "limitation": null,
              "ignored": false,
              "description": "Which linkage criteria to use. The linkage criterion determines which\ndistance to use between sets of observation.\n    - average uses the average of the distances of each observation of\n      the two sets\n    - complete or maximum linkage uses the maximum distances between\n      all observations of the two sets.\n    - single uses the minimum of the distances between all observations\n      of the two sets."
            },
            {
              "name": "affinity",
              "type": "Any",
              "hasDefault": true,
              "default": "euclidean",
              "limitation": null,
              "ignored": false,
              "description": "which metric to use. Can be \"euclidean\", \"manhattan\", or any\ndistance know to paired distance (see metric.pairwise)"
            },
            {
              "name": "return_distance",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "whether or not to return the distances between the clusters."
            }
          ],
          "results": [
            {
              "name": "children",
              "type": null,
              "description": "The children of each non-leaf node. Values less than `n_samples`\ncorrespond to leaves of the tree which are the original samples.\nA node `i` greater than or equal to `n_samples` is a non-leaf\nnode and has children `children_[i - n_samples]`. Alternatively\nat the i-th iteration, children[i][0] and children[i][1]\nare merged to form node `n_samples + i`"
            },
            {
              "name": "n_connected_components",
              "type": "int",
              "description": "The number of connected components in the graph."
            },
            {
              "name": "n_leaves",
              "type": "int",
              "description": "The number of leaves in the tree."
            },
            {
              "name": "parents",
              "type": null,
              "description": "The parent of each node. Only returned when a connectivity matrix\nis specified, elsewhere 'None' is returned."
            },
            {
              "name": "distances",
              "type": null,
              "description": "Returned when return_distance is set to True.\n\ndistances[i] refers to the distance between children[i][0] and\nchildren[i][1] when they are merged."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Linkage agglomerative clustering based on a Feature matrix.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    feature matrix representing n_samples samples to be clustered\n\nconnectivity : sparse matrix, default=None\n    connectivity matrix. Defines for each sample the neighboring samples\n    following a given structure of the data. The matrix is assumed to\n    be symmetric and only the upper triangular half is used.\n    Default is None, i.e, the Ward algorithm is unstructured.\n\nn_clusters : int, default=None\n    Stop early the construction of the tree at n_clusters. This is\n    useful to decrease computation time if the number of clusters is\n    not small compared to the number of samples. In this case, the\n    complete tree is not computed, thus the 'children' output is of\n    limited use, and the 'parents' output should rather be used.\n    This option is valid only when specifying a connectivity matrix.\n\nlinkage : {\"average\", \"complete\", \"single\"}, default=\"complete\"\n    Which linkage criteria to use. The linkage criterion determines which\n    distance to use between sets of observation.\n        - average uses the average of the distances of each observation of\n          the two sets\n        - complete or maximum linkage uses the maximum distances between\n          all observations of the two sets.\n        - single uses the minimum of the distances between all observations\n          of the two sets.\n\naffinity : str or callable, default=\"euclidean\".\n    which metric to use. Can be \"euclidean\", \"manhattan\", or any\n    distance know to paired distance (see metric.pairwise)\n\nreturn_distance : bool, default=False\n    whether or not to return the distances between the clusters.\n\nReturns\n-------\nchildren : ndarray of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`\n\nn_connected_components : int\n    The number of connected components in the graph.\n\nn_leaves : int\n    The number of leaves in the tree.\n\nparents : ndarray of shape (n_nodes, ) or None\n    The parent of each node. Only returned when a connectivity matrix\n    is specified, elsewhere 'None' is returned.\n\ndistances : ndarray of shape (n_nodes-1,)\n    Returned when return_distance is set to True.\n\n    distances[i] refers to the distance between children[i][0] and\n    children[i][1] when they are merged.\n\nSee Also\n--------\nward_tree : Hierarchical clustering with ward linkage."
        },
        {
          "name": "ward_tree",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "feature matrix representing n_samples samples to be clustered"
            }
          ],
          "results": [
            {
              "name": "children",
              "type": null,
              "description": "The children of each non-leaf node. Values less than `n_samples`\ncorrespond to leaves of the tree which are the original samples.\nA node `i` greater than or equal to `n_samples` is a non-leaf\nnode and has children `children_[i - n_samples]`. Alternatively\nat the i-th iteration, children[i][0] and children[i][1]\nare merged to form node `n_samples + i`"
            },
            {
              "name": "n_connected_components",
              "type": "int",
              "description": "The number of connected components in the graph."
            },
            {
              "name": "n_leaves",
              "type": "int",
              "description": "The number of leaves in the tree"
            },
            {
              "name": "parents",
              "type": null,
              "description": "The parent of each node. Only returned when a connectivity matrix\nis specified, elsewhere 'None' is returned."
            },
            {
              "name": "distances",
              "type": null,
              "description": "Only returned if return_distance is set to True (for compatibility).\nThe distances between the centers of the nodes. `distances[i]`\ncorresponds to a weighted euclidean distance between\nthe nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\nleaves of the tree, then `distances[i]` is their unweighted euclidean\ndistance. Distances are updated in the following way\n(from scipy.hierarchy.linkage):\n\nThe new entry :math:`d(u,v)` is computed as follows,\n\n.. math::\n\n   d(u,v) = \\sqrt{\\frac{|v|+|s|}\n                       {T}d(v,s)^2\n                + \\frac{|v|+|t|}\n                       {T}d(v,t)^2\n                - \\frac{|v|}\n                       {T}d(s,t)^2}\n\nwhere :math:`u` is the newly joined cluster consisting of\nclusters :math:`s` and :math:`t`, :math:`v` is an unused\ncluster in the forest, :math:`T=|v|+|s|+|t|`, and\n:math:`|*|` is the cardinality of its argument. This is also\nknown as the incremental algorithm."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Ward clustering based on a Feature matrix.\n\nRecursively merges the pair of clusters that minimally increases\nwithin-cluster variance.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    feature matrix representing n_samples samples to be clustered\n\nconnectivity : sparse matrix, default=None\n    connectivity matrix. Defines for each sample the neighboring samples\n    following a given structure of the data. The matrix is assumed to\n    be symmetric and only the upper triangular half is used.\n    Default is None, i.e, the Ward algorithm is unstructured.\n\nn_clusters : int, default=None\n    Stop early the construction of the tree at n_clusters. This is\n    useful to decrease computation time if the number of clusters is\n    not small compared to the number of samples. In this case, the\n    complete tree is not computed, thus the 'children' output is of\n    limited use, and the 'parents' output should rather be used.\n    This option is valid only when specifying a connectivity matrix.\n\nreturn_distance : bool, default=None\n    If True, return the distance between the clusters.\n\nReturns\n-------\nchildren : ndarray of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`\n\nn_connected_components : int\n    The number of connected components in the graph.\n\nn_leaves : int\n    The number of leaves in the tree\n\nparents : ndarray of shape (n_nodes,) or None\n    The parent of each node. Only returned when a connectivity matrix\n    is specified, elsewhere 'None' is returned.\n\ndistances : ndarray of shape (n_nodes-1,)\n    Only returned if return_distance is set to True (for compatibility).\n    The distances between the centers of the nodes. `distances[i]`\n    corresponds to a weighted euclidean distance between\n    the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n    leaves of the tree, then `distances[i]` is their unweighted euclidean\n    distance. Distances are updated in the following way\n    (from scipy.hierarchy.linkage):\n\n    The new entry :math:`d(u,v)` is computed as follows,\n\n    .. math::\n\n       d(u,v) = \\sqrt{\\frac{|v|+|s|}\n                           {T}d(v,s)^2\n                    + \\frac{|v|+|t|}\n                           {T}d(v,t)^2\n                    - \\frac{|v|}\n                           {T}d(s,t)^2}\n\n    where :math:`u` is the newly joined cluster consisting of\n    clusters :math:`s` and :math:`t`, :math:`v` is an unused\n    cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n    :math:`|*|` is the cardinality of its argument. This is also\n    known as the incremental algorithm."
        }
      ]
    },
    {
      "name": "sklearn.cluster._bicluster",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "norm",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "dia_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "scipy.sparse.linalg",
          "declaration": "eigsh",
          "alias": null
        },
        {
          "module": "scipy.sparse.linalg",
          "declaration": "svds",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BiclusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "KMeans",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "MiniBatchKMeans",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "make_nonnegative",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "randomized_svd",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "assert_all_finite",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseSpectral",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "BiclusterMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Creates a biclustering for X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\ny : Ignored"
            }
          ],
          "fullDocstring": "Base class for spectral biclustering."
        },
        {
          "name": "SpectralBiclustering",
          "decorators": [],
          "superclasses": [
            "BaseSpectral"
          ],
          "methods": [],
          "fullDocstring": "Spectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has\nan underlying checkerboard structure. For instance, if there are\ntwo row partitions and three column partitions, each row will\nbelong to three biclusters, and each column will belong to two\nbiclusters. The outer product of the corresponding row and column\nlabel vectors gives this checkerboard structure.\n\nRead more in the :ref:`User Guide <spectral_biclustering>`.\n\nParameters\n----------\nn_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3\n    The number of row and column clusters in the checkerboard\n    structure.\n\nmethod : {'bistochastic', 'scale', 'log'}, default='bistochastic'\n    Method of normalizing and converting singular vectors into\n    biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n    The authors recommend using 'log'. If the data is sparse,\n    however, log normalization will not work, which is why the\n    default is 'bistochastic'.\n\n    .. warning::\n       if `method='log'`, the data must be sparse.\n\nn_components : int, default=6\n    Number of singular vectors to check.\n\nn_best : int, default=3\n    Number of best singular vectors to which to project the data\n    for clustering.\n\nsvd_method : {'randomized', 'arpack'}, default='randomized'\n    Selects the algorithm for finding singular vectors. May be\n    'randomized' or 'arpack'. If 'randomized', uses\n    :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\n    for large matrices. If 'arpack', uses\n    `scipy.sparse.linalg.svds`, which is more accurate, but\n    possibly slower in some cases.\n\nn_svd_vecs : int, default=None\n    Number of vectors to use in calculating the SVD. Corresponds\n    to `ncv` when `svd_method=arpack` and `n_oversamples` when\n    `svd_method` is 'randomized`.\n\nmini_batch : bool, default=False\n    Whether to use mini-batch k-means, which is faster but may get\n    different results.\n\ninit : {'k-means++', 'random'} or ndarray of (n_clusters, n_features),             default='k-means++'\n    Method for initialization of k-means algorithm; defaults to\n    'k-means++'.\n\nn_init : int, default=10\n    Number of random initializations that are tried with the\n    k-means algorithm.\n\n    If mini-batch k-means is used, the best initialization is\n    chosen and the algorithm runs once. Otherwise, the algorithm\n    is run for each initialization and the best solution chosen.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nrandom_state : int, RandomState instance, default=None\n    Used for randomizing the singular value decomposition and the k-means\n    initialization. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrows_ : array-like of shape (n_row_clusters, n_rows)\n    Results of the clustering. `rows[i, r]` is True if\n    cluster `i` contains row `r`. Available only after calling ``fit``.\n\ncolumns_ : array-like of shape (n_column_clusters, n_columns)\n    Results of the clustering, like `rows`.\n\nrow_labels_ : array-like of shape (n_rows,)\n    Row partition labels.\n\ncolumn_labels_ : array-like of shape (n_cols,)\n    Column partition labels.\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralBiclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> clustering.column_labels_\narray([0, 1], dtype=int32)\n>>> clustering\nSpectralBiclustering(n_clusters=2, random_state=0)\n\nReferences\n----------\n\n* Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray\n  data: coclustering genes and conditions\n  <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__."
        },
        {
          "name": "SpectralCoclustering",
          "decorators": [],
          "superclasses": [
            "BaseSpectral"
          ],
          "methods": [],
          "fullDocstring": "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed\nnormalized cut of the bipartite graph created from `X` as follows:\nthe edge between row vertex `i` and column vertex `j` has weight\n`X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each\nrow and each column belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the :ref:`User Guide <spectral_coclustering>`.\n\nParameters\n----------\nn_clusters : int, default=3\n    The number of biclusters to find.\n\nsvd_method : {'randomized', 'arpack'}, default='randomized'\n    Selects the algorithm for finding singular vectors. May be\n    'randomized' or 'arpack'. If 'randomized', use\n    :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n    for large matrices. If 'arpack', use\n    :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n    possibly slower in some cases.\n\nn_svd_vecs : int, default=None\n    Number of vectors to use in calculating the SVD. Corresponds\n    to `ncv` when `svd_method=arpack` and `n_oversamples` when\n    `svd_method` is 'randomized`.\n\nmini_batch : bool, default=False\n    Whether to use mini-batch k-means, which is faster but may get\n    different results.\n\ninit : {'k-means++', 'random', or ndarray of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization of k-means algorithm; defaults to\n    'k-means++'.\n\nn_init : int, default=10\n    Number of random initializations that are tried with the\n    k-means algorithm.\n\n    If mini-batch k-means is used, the best initialization is\n    chosen and the algorithm runs once. Otherwise, the algorithm\n    is run for each initialization and the best solution chosen.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nrandom_state : int, RandomState instance, default=None\n    Used for randomizing the singular value decomposition and the k-means\n    initialization. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrows_ : array-like of shape (n_row_clusters, n_rows)\n    Results of the clustering. `rows[i, r]` is True if\n    cluster `i` contains row `r`. Available only after calling ``fit``.\n\ncolumns_ : array-like of shape (n_column_clusters, n_columns)\n    Results of the clustering, like `rows`.\n\nrow_labels_ : array-like of shape (n_rows,)\n    The bicluster label of each row.\n\ncolumn_labels_ : array-like of shape (n_cols,)\n    The bicluster label of each column.\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralCoclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_ #doctest: +SKIP\narray([0, 1, 1, 0, 0, 0], dtype=int32)\n>>> clustering.column_labels_ #doctest: +SKIP\narray([0, 0], dtype=int32)\n>>> clustering\nSpectralCoclustering(n_clusters=2, random_state=0)\n\nReferences\n----------\n\n* Dhillon, Inderjit S, 2001. `Co-clustering documents and words using\n  bipartite spectral graph partitioning\n  <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.cluster._birch",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "math",
          "declaration": "sqrt",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn._config",
          "declaration": "config_context",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "AgglomerativeClustering",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "pairwise_distances_argmin",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "euclidean_distances",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "Birch",
          "decorators": [],
          "superclasses": [
            "ClusterMixin",
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Build a CF Tree for the input data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself\n    Fitted estimator."
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data. If X is not provided, only the global clustering\nstep is done."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Online learning. Prevents rebuilding of CFTree from scratch.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features),             default=None\n    Input data. If X is not provided, only the global clustering\n    step is done.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself\n    Fitted estimator."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Labelled data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict data using the ``centroids_`` of subclusters.\n\nAvoid computation of the row norms of X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data.\n\nReturns\n-------\nlabels : ndarray of shape(n_samples,)\n    Labelled data."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data."
                }
              ],
              "results": [
                {
                  "name": "X_trans",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X into subcluster centroids dimension.\n\nEach dimension represents the distance from the sample point to each\ncluster centroid.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data.\n\nReturns\n-------\nX_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n    Transformed data."
            }
          ],
          "fullDocstring": "Implements the Birch clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an\nalternative to :class:`MiniBatchKMeans`. It constructs a tree\ndata structure with the cluster centroids being read off the leaf.\nThese can be either the final cluster centroids or can be provided as input\nto another clustering algorithm such as :class:`AgglomerativeClustering`.\n\nRead more in the :ref:`User Guide <birch>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\nthreshold : float, default=0.5\n    The radius of the subcluster obtained by merging a new sample and the\n    closest subcluster should be lesser than the threshold. Otherwise a new\n    subcluster is started. Setting this value to be very low promotes\n    splitting and vice-versa.\n\nbranching_factor : int, default=50\n    Maximum number of CF subclusters in each node. If a new samples enters\n    such that the number of subclusters exceed the branching_factor then\n    that node is split into two nodes with the subclusters redistributed\n    in each. The parent subcluster of that node is removed and two new\n    subclusters are added as parents of the 2 split nodes.\n\nn_clusters : int, instance of sklearn.cluster model, default=3\n    Number of clusters after the final clustering step, which treats the\n    subclusters from the leaves as new samples.\n\n    - `None` : the final clustering step is not performed and the\n      subclusters are returned as they are.\n\n    - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n      is fit treating the subclusters as new samples and the initial data\n      is mapped to the label of the closest subcluster.\n\n    - `int` : the model fit is :class:`AgglomerativeClustering` with\n      `n_clusters` set to be equal to the int.\n\ncompute_labels : bool, default=True\n    Whether or not to compute labels for each fit.\n\ncopy : bool, default=True\n    Whether or not to make a copy of the given data. If set to False,\n    the initial data will be overwritten.\n\nAttributes\n----------\nroot_ : _CFNode\n    Root of the CFTree.\n\ndummy_leaf_ : _CFNode\n    Start pointer to all the leaves.\n\nsubcluster_centers_ : ndarray\n    Centroids of all subclusters read directly from the leaves.\n\nsubcluster_labels_ : ndarray\n    Labels assigned to the centroids of the subclusters after\n    they are clustered globally.\n\nlabels_ : ndarray of shape (n_samples,)\n    Array of labels assigned to the input data.\n    if partial_fit is used instead of fit, they are assigned to the\n    last batch of data.\n\nSee Also\n--------\nMiniBatchKMeans : Alternative implementation that does incremental updates\n    of the centers' positions using mini-batches.\n\nNotes\n-----\nThe tree data structure consists of nodes with each node consisting of\na number of subclusters. The maximum number of subclusters in a node\nis determined by the branching factor. Each subcluster maintains a\nlinear sum, squared sum and the number of samples in that subcluster.\nIn addition, each subcluster can also have a node as its child, if the\nsubcluster is not a member of a leaf node.\n\nFor a new point entering the root, it is merged with the subcluster closest\nto it and the linear sum, squared sum and the number of samples of that\nsubcluster are updated. This is done recursively till the properties of\nthe leaf node are updated.\n\nReferences\n----------\n* Tian Zhang, Raghu Ramakrishnan, Maron Livny\n  BIRCH: An efficient data clustering method for large databases.\n  https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n* Roberto Perdisci\n  JBirch - Java implementation of BIRCH clustering algorithm\n  https://code.google.com/archive/p/jbirch\n\nExamples\n--------\n>>> from sklearn.cluster import Birch\n>>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n>>> brc = Birch(n_clusters=None)\n>>> brc.fit(X)\nBirch(n_clusters=None)\n>>> brc.predict(X)\narray([0, 0, 0, 1, 1, 1])"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.cluster._dbscan",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.cluster._dbscan_inner",
          "declaration": "dbscan_inner",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "DBSCAN",
          "decorators": [],
          "superclasses": [
            "ClusterMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster, or distances between instances if\n``metric='precomputed'``. If a sparse matrix is provided, it will\nbe converted into a sparse ``csr_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weight of each sample, such that a sample with a weight of at least\n``min_samples`` is by itself a core sample; a sample with a\nnegative weight may inhibit its eps-neighbor from being core.\nNote that weights are absolute, and default to 1."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform DBSCAN clustering from features, or distance matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``metric='precomputed'``. If a sparse matrix is provided, it will\n    be converted into a sparse ``csr_matrix``.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight of each sample, such that a sample with a weight of at least\n    ``min_samples`` is by itself a core sample; a sample with a\n    negative weight may inhibit its eps-neighbor from being core.\n    Note that weights are absolute, and default to 1.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself"
            },
            {
              "name": "fit_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster, or distances between instances if\n``metric='precomputed'``. If a sparse matrix is provided, it will\nbe converted into a sparse ``csr_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weight of each sample, such that a sample with a weight of at least\n``min_samples`` is by itself a core sample; a sample with a\nnegative weight may inhibit its eps-neighbor from being core.\nNote that weights are absolute, and default to 1."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Cluster labels. Noisy samples are given the label -1."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform DBSCAN clustering from features or distance matrix,\nand return cluster labels.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``metric='precomputed'``. If a sparse matrix is provided, it will\n    be converted into a sparse ``csr_matrix``.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight of each sample, such that a sample with a weight of at least\n    ``min_samples`` is by itself a core sample; a sample with a\n    negative weight may inhibit its eps-neighbor from being core.\n    Note that weights are absolute, and default to 1.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels. Noisy samples are given the label -1."
            }
          ],
          "fullDocstring": "Perform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise.\nFinds core samples of high density and expands clusters from them.\nGood for data which contains clusters of similar density.\n\nRead more in the :ref:`User Guide <dbscan>`.\n\nParameters\n----------\neps : float, default=0.5\n    The maximum distance between two samples for one to be considered\n    as in the neighborhood of the other. This is not a maximum bound\n    on the distances of points within a cluster. This is the most\n    important DBSCAN parameter to choose appropriately for your data set\n    and distance function.\n\nmin_samples : int, default=5\n    The number of samples (or total weight) in a neighborhood for a point\n    to be considered as a core point. This includes the point itself.\n\nmetric : string, or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string or callable, it must be one of\n    the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n    its metric parameter.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square. X may be a :term:`Glossary <sparse graph>`, in which\n    case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n    .. versionadded:: 0.17\n       metric *precomputed* to accept precomputed sparse matrix.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\n    .. versionadded:: 0.19\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    The algorithm to be used by the NearestNeighbors module\n    to compute pointwise distances and find nearest neighbors.\n    See NearestNeighbors module documentation for details.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or cKDTree. This can affect the speed\n    of the construction and query, as well as the memory required\n    to store the tree. The optimal value depends\n    on the nature of the problem.\n\np : float, default=None\n    The power of the Minkowski metric to be used to calculate distance\n    between points. If None, then ``p=2`` (equivalent to the Euclidean\n    distance).\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\ncore_sample_indices_ : ndarray of shape (n_core_samples,)\n    Indices of core samples.\n\ncomponents_ : ndarray of shape (n_core_samples, n_features)\n    Copy of each core sample found by training.\n\nlabels_ : ndarray of shape (n_samples)\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples are given the label -1.\n\nExamples\n--------\n>>> from sklearn.cluster import DBSCAN\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 2], [2, 3],\n...               [8, 7], [8, 8], [25, 80]])\n>>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n>>> clustering.labels_\narray([ 0,  0,  0,  1,  1, -1])\n>>> clustering\nDBSCAN(eps=3, min_samples=2)\n\nSee Also\n--------\nOPTICS : A similar clustering at multiple values of eps. Our implementation\n    is optimized for memory usage.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_dbscan.py\n<sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending\non the ``algorithm``.\n\nOne way to avoid the query complexity is to pre-compute sparse\nneighborhoods in chunks using\n:func:`NearestNeighbors.radius_neighbors_graph\n<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n``mode='distance'``, then using ``metric='precomputed'`` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use ``sample_weight`` instead.\n\n:class:`cluster.OPTICS` provides a similar clustering with lower memory\nusage.\n\nReferences\n----------\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\nAlgorithm for Discovering Clusters in Large Spatial Databases with Noise\".\nIn: Proceedings of the 2nd International Conference on Knowledge Discovery\nand Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\nDBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\nACM Transactions on Database Systems (TODS), 42(3), 19."
        }
      ],
      "functions": [
        {
          "name": "dbscan",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A feature array, or array of distances between samples if\n``metric='precomputed'``."
            },
            {
              "name": "eps",
              "type": "Any",
              "hasDefault": true,
              "default": "0.5",
              "limitation": null,
              "ignored": false,
              "description": "The maximum distance between two samples for one to be considered\nas in the neighborhood of the other. This is not a maximum bound\non the distances of points within a cluster. This is the most\nimportant DBSCAN parameter to choose appropriately for your data set\nand distance function."
            }
          ],
          "results": [
            {
              "name": "core_samples",
              "type": null,
              "description": "Indices of core samples."
            },
            {
              "name": "labels",
              "type": null,
              "description": "Cluster labels for each point.  Noisy samples are given the label -1."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Perform DBSCAN clustering from vector array or distance matrix.\n\nRead more in the :ref:`User Guide <dbscan>`.\n\nParameters\n----------\nX : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples)\n    A feature array, or array of distances between samples if\n    ``metric='precomputed'``.\n\neps : float, default=0.5\n    The maximum distance between two samples for one to be considered\n    as in the neighborhood of the other. This is not a maximum bound\n    on the distances of points within a cluster. This is the most\n    important DBSCAN parameter to choose appropriately for your data set\n    and distance function.\n\nmin_samples : int, default=5\n    The number of samples (or total weight) in a neighborhood for a point\n    to be considered as a core point. This includes the point itself.\n\nmetric : str or callable, default='minkowski'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string or callable, it must be one of\n    the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n    its metric parameter.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit.\n    X may be a :term:`sparse graph <sparse graph>`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\n    .. versionadded:: 0.19\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    The algorithm to be used by the NearestNeighbors module\n    to compute pointwise distances and find nearest neighbors.\n    See NearestNeighbors module documentation for details.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or cKDTree. This can affect the speed\n    of the construction and query, as well as the memory required\n    to store the tree. The optimal value depends\n    on the nature of the problem.\n\np : float, default=2\n    The power of the Minkowski metric to be used to calculate distance\n    between points.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight of each sample, such that a sample with a weight of at least\n    ``min_samples`` is by itself a core sample; a sample with negative\n    weight may inhibit its eps-neighbor from being core.\n    Note that weights are absolute, and default to 1.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search. ``None`` means\n    1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n    using all processors. See :term:`Glossary <n_jobs>` for more details.\n    If precomputed distance are used, parallel execution is not available\n    and thus n_jobs will have no effect.\n\nReturns\n-------\ncore_samples : ndarray of shape (n_core_samples,)\n    Indices of core samples.\n\nlabels : ndarray of shape (n_samples,)\n    Cluster labels for each point.  Noisy samples are given the label -1.\n\nSee Also\n--------\nDBSCAN : An estimator interface for this clustering algorithm.\nOPTICS : A similar estimator interface clustering at multiple values of\n    eps. Our implementation is optimized for memory usage.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_dbscan.py\n<sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending\non the ``algorithm``.\n\nOne way to avoid the query complexity is to pre-compute sparse\nneighborhoods in chunks using\n:func:`NearestNeighbors.radius_neighbors_graph\n<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n``mode='distance'``, then using ``metric='precomputed'`` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use ``sample_weight`` instead.\n\n:func:`cluster.optics <sklearn.cluster.optics>` provides a similar\nclustering with lower memory usage.\n\nReferences\n----------\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\nAlgorithm for Discovering Clusters in Large Spatial Databases with Noise\".\nIn: Proceedings of the 2nd International Conference on Knowledge Discovery\nand Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\nDBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\nACM Transactions on Database Systems (TODS), 42(3), 19."
        }
      ]
    },
    {
      "name": "sklearn.cluster._feature_agglomeration",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "AgglomerationTransform",
          "decorators": [],
          "superclasses": [
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "A M by N array of M observations in N dimensions or a length\nM array of M one-dimensional observations."
                }
              ],
              "results": [
                {
                  "name": "Y",
                  "type": null,
                  "description": "The pooled values for each feature cluster."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform a new matrix using the built clustering\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or (n_samples,)\n    A M by N array of M observations in N dimensions or a length\n    M array of M one-dimensional observations.\n\nReturns\n-------\nY : ndarray of shape (n_samples, n_clusters) or (n_clusters,)\n    The pooled values for each feature cluster."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "Xred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The values to be assigned to each cluster of samples"
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "A vector of size n_samples with the values of Xred assigned to\neach of the cluster of samples."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Inverse the transformation.\nReturn a vector of size nb_features with the values of Xred assigned\nto each group of features\n\nParameters\n----------\nXred : array-like of shape (n_samples, n_clusters) or (n_clusters,)\n    The values to be assigned to each cluster of samples\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features) or (n_features,)\n    A vector of size n_samples with the values of Xred assigned to\n    each of the cluster of samples."
            }
          ],
          "fullDocstring": "A class for feature agglomeration via the transform interface"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.cluster._kmeans",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_elkan",
          "declaration": "elkan_iter_chunked_dense",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_elkan",
          "declaration": "elkan_iter_chunked_sparse",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_elkan",
          "declaration": "init_bounds_dense",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_elkan",
          "declaration": "init_bounds_sparse",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_fast",
          "declaration": "CHUNK_SIZE",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_fast",
          "declaration": "_inertia_dense",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_fast",
          "declaration": "_inertia_sparse",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_fast",
          "declaration": "_mini_batch_update_csr",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_lloyd",
          "declaration": "lloyd_iter_chunked_dense",
          "alias": null
        },
        {
          "module": "sklearn.cluster._k_means_lloyd",
          "declaration": "lloyd_iter_chunked_sparse",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "euclidean_distances",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_batches",
          "alias": null
        },
        {
          "module": "sklearn.utils._openmp_helpers",
          "declaration": "_openmp_effective_n_threads",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "stable_cumsum",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "mean_variance_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs_fast",
          "declaration": "assign_rows_csr",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "threadpoolctl",
          "declaration": "threadpool_info",
          "alias": null
        },
        {
          "module": "threadpoolctl",
          "declaration": "threadpool_limits",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KMeans",
          "decorators": [],
          "superclasses": [
            "ClusterMixin",
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster. It must be noted that the data\nwill be converted to C ordering, which will cause a memory\ncopy if the given data is not C-contiguous.\nIf a sparse matrix is passed, a copy will be made if it's not in\nCSR format."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight.\n\n.. versionadded:: 0.20"
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute k-means clustering.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training instances to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory\n    copy if the given data is not C-contiguous.\n    If a sparse matrix is passed, a copy will be made if it's not in\n    CSR format.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nself\n    Fitted estimator."
            },
            {
              "name": "fit_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data to transform."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Index of the cluster each sample belongs to."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by\npredict(X).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data to transform."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "X transformed in the new space."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_clusters)\n    X transformed in the new space."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data to transform."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "X transformed in the new space."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster\ncenters. Note that even if X is sparse, the array returned by\n`transform` will typically be dense.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_clusters)\n    X transformed in the new space."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data to predict."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Index of the cluster each sample belongs to."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "Opposite of the value of X on the K-means objective."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Opposite of the value of X on the K-means objective.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nscore : float\n    Opposite of the value of X on the K-means objective."
            }
          ],
          "fullDocstring": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nprecompute_distances : {'auto', True, False}, default='auto'\n    Precompute distances (faster but takes more memory).\n\n    'auto' : do not precompute distances if n_samples * n_clusters > 12\n    million. This corresponds to about 100MB overhead per job using\n    double precision.\n\n    True : always precompute distances.\n\n    False : never precompute distances.\n\n    .. deprecated:: 0.23\n        'precompute_distances' was deprecated in version 0.22 and will be\n        removed in 1.0 (renaming of 0.25). It has no effect.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nn_jobs : int, default=None\n    The number of OpenMP threads to use for the computation. Parallelism is\n    sample-wise on the main cython loop which assigns each sample to its\n    closest center.\n\n    ``None`` or ``-1`` means using all processors.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nalgorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n    K-means algorithm to use. The classical EM-style algorithm is \"full\".\n    The \"elkan\" variation is more efficient on data with well-defined\n    clusters, by using the triangle inequality. However it's more memory\n    intensive due to the allocation of an extra array of shape\n    (n_samples, n_clusters).\n\n    For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n    might change in the future for a better heuristic.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center.\n\nn_iter_ : int\n    Number of iterations run.\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), were n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n'How slow is the k-means method?' SoCG2006)\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])"
        },
        {
          "name": "MiniBatchKMeans",
          "decorators": [],
          "superclasses": [
            "KMeans"
          ],
          "methods": [
            {
              "name": "counts_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "init_size_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "random_state_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster. It must be noted that the data\nwill be converted to C ordering, which will cause a memory copy\nif the given data is not C-contiguous."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight (default: None).\n\n.. versionadded:: 0.20"
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the centroids on X by chunking it into mini-batches.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training instances to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory copy\n    if the given data is not C-contiguous.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight (default: None).\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nself"
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Coordinates of the data points to cluster. It must be noted that\nX will be copied if it is not C-contiguous."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight (default: None)."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Update k means estimate on a single mini-batch X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Coordinates of the data points to cluster. It must be noted that\n    X will be copied if it is not C-contiguous.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight (default: None).\n\nReturns\n-------\nself"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data to predict."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight (default: None)."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Index of the cluster each sample belongs to."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight (default: None).\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to."
            }
          ],
          "fullDocstring": "Mini-Batch K-Means clustering.\n\nRead more in the :ref:`User Guide <mini_batch_kmeans>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nmax_iter : int, default=100\n    Maximum number of iterations over the complete dataset before\n    stopping independently of any early stopping criterion heuristics.\n\nbatch_size : int, default=100\n    Size of the mini batches.\n\nverbose : int, default=0\n    Verbosity mode.\n\ncompute_labels : bool, default=True\n    Compute label assignment and inertia for the complete dataset\n    once the minibatch optimization has converged in fit.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization and\n    random reassignment. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=0.0\n    Control early stopping based on the relative center changes as\n    measured by a smoothed, variance-normalized of the mean center\n    squared position changes. This early stopping heuristics is\n    closer to the one used for the batch variant of the algorithms\n    but induces a slight computational and memory overhead over the\n    inertia heuristic.\n\n    To disable convergence detection based on normalized center\n    change, set tol to 0.0 (default).\n\nmax_no_improvement : int, default=10\n    Control early stopping based on the consecutive number of mini\n    batches that does not yield an improvement on the smoothed inertia.\n\n    To disable convergence detection based on inertia, set\n    max_no_improvement to None.\n\ninit_size : int, default=None\n    Number of samples to randomly sample for speeding up the\n    initialization (sometimes at the expense of accuracy): the\n    only algorithm is initialized by running a batch KMeans on a\n    random subset of the data. This needs to be larger than n_clusters.\n\n    If `None`, `init_size= 3 * batch_size`.\n\nn_init : int, default=3\n    Number of random initializations that are tried.\n    In contrast to KMeans, the algorithm is only run once, using the\n    best of the ``n_init`` initializations as measured by inertia.\n\nreassignment_ratio : float, default=0.01\n    Control the fraction of the maximum number of counts for a\n    center to be reassigned. A higher value means that low count\n    centers are more easily reassigned, which means that the\n    model will take longer to converge, but should converge in a\n    better clustering.\n\nAttributes\n----------\n\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels_ : int\n    Labels of each point (if compute_labels is set to True).\n\ninertia_ : float\n    The value of the inertia criterion associated with the chosen\n    partition (if compute_labels is set to True). The inertia is\n    defined as the sum of square distances of samples to their nearest\n    neighbor.\n\nn_iter_ : int\n    Number of batches processed.\n\ncounts_ : ndarray of shape (n_clusters,)\n    Weigth sum of each cluster.\n\n    .. deprecated:: 0.24\n       This attribute is deprecated in 0.24 and will be removed in\n       1.1 (renaming of 0.26).\n\ninit_size_ : int\n    The effective number of samples used for the initialization.\n\n    .. deprecated:: 0.24\n       This attribute is deprecated in 0.24 and will be removed in\n       1.1 (renaming of 0.26).\n\nSee Also\n--------\nKMeans : The classic implementation of the clustering method based on the\n    Lloyd's algorithm. It consumes the whole set of input data at each\n    iteration.\n\nNotes\n-----\nSee https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\nExamples\n--------\n>>> from sklearn.cluster import MiniBatchKMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 0], [4, 4],\n...               [4, 5], [0, 1], [2, 2],\n...               [3, 2], [5, 5], [1, -1]])\n>>> # manually fit on batches\n>>> kmeans = MiniBatchKMeans(n_clusters=2,\n...                          random_state=0,\n...                          batch_size=6)\n>>> kmeans = kmeans.partial_fit(X[0:6,:])\n>>> kmeans = kmeans.partial_fit(X[6:12,:])\n>>> kmeans.cluster_centers_\narray([[2. , 1. ],\n       [3.5, 4.5]])\n>>> kmeans.predict([[0, 0], [4, 4]])\narray([0, 1], dtype=int32)\n>>> # fit on the whole data\n>>> kmeans = MiniBatchKMeans(n_clusters=2,\n...                          random_state=0,\n...                          batch_size=6,\n...                          max_iter=10).fit(X)\n>>> kmeans.cluster_centers_\narray([[3.95918367, 2.40816327],\n       [1.12195122, 1.3902439 ]])\n>>> kmeans.predict([[0, 0], [4, 4]])\narray([1, 0], dtype=int32)"
        }
      ],
      "functions": [
        {
          "name": "k_means",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The observations to cluster. It must be noted that the data\nwill be converted to C ordering, which will cause a memory copy\nif the given data is not C-contiguous."
            },
            {
              "name": "n_clusters",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The number of clusters to form as well as the number of\ncentroids to generate."
            }
          ],
          "results": [
            {
              "name": "centroid",
              "type": null,
              "description": "Centroids found at the last iteration of k-means."
            },
            {
              "name": "label",
              "type": null,
              "description": "label[i] is the code or index of the centroid the\ni'th observation is closest to."
            },
            {
              "name": "inertia",
              "type": "float",
              "description": "The final value of the inertia criterion (sum of squared distances to\nthe closest centroid for all observations in the training set)."
            },
            {
              "name": "best_n_iter",
              "type": "int",
              "description": "Number of iterations corresponding to the best results.\nReturned only if `return_n_iter` is set to True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "K-means clustering algorithm.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The observations to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory copy\n    if the given data is not C-contiguous.\n\nn_clusters : int\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nprecompute_distances : {'auto', True, False}\n    Precompute distances (faster but takes more memory).\n\n    'auto' : do not precompute distances if n_samples * n_clusters > 12\n    million. This corresponds to about 100MB overhead per job using\n    double precision.\n\n    True : always precompute distances\n\n    False : never precompute distances\n\n    .. deprecated:: 0.23\n        'precompute_distances' was deprecated in version 0.23 and will be\n        removed in 1.0 (renaming of 0.25). It has no effect.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm to run.\n\nverbose : bool, default=False\n    Verbosity mode.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nn_jobs : int, default=None\n    The number of OpenMP threads to use for the computation. Parallelism is\n    sample-wise on the main cython loop which assigns each sample to its\n    closest center.\n\n    ``None`` or ``-1`` means using all processors.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nalgorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n    K-means algorithm to use. The classical EM-style algorithm is \"full\".\n    The \"elkan\" variation is more efficient on data with well-defined\n    clusters, by using the triangle inequality. However it's more memory\n    intensive due to the allocation of an extra array of shape\n    (n_samples, n_clusters).\n\n    For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n    might change in the future for a better heuristic.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\ncentroid : ndarray of shape (n_clusters, n_features)\n    Centroids found at the last iteration of k-means.\n\nlabel : ndarray of shape (n_samples,)\n    label[i] is the code or index of the centroid the\n    i'th observation is closest to.\n\ninertia : float\n    The final value of the inertia criterion (sum of squared distances to\n    the closest centroid for all observations in the training set).\n\nbest_n_iter : int\n    Number of iterations corresponding to the best results.\n    Returned only if `return_n_iter` is set to True."
        },
        {
          "name": "kmeans_plusplus",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to pick seeds from."
            },
            {
              "name": "n_clusters",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The number of centroids to initialize"
            }
          ],
          "results": [
            {
              "name": "centers",
              "type": null,
              "description": "The inital centers for k-means."
            },
            {
              "name": "indices",
              "type": null,
              "description": "The index location of the chosen centers in the data array X. For a\ngiven index and center, X[index] = center."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Init n_clusters seeds according to k-means++\n\n.. versionadded:: 0.24\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to pick seeds from.\n\nn_clusters : int\n    The number of centroids to initialize\n\nx_squared_norms : array-like of shape (n_samples,), default=None\n    Squared Euclidean norm of each data point.\n\nrandom_state : int or RandomState instance, default=None\n    Determines random number generation for centroid initialization. Pass\n    an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_local_trials : int, default=None\n    The number of seeding trials for each center (except the first),\n    of which the one reducing inertia the most is greedily chosen.\n    Set to None to make the number of trials depend logarithmically\n    on the number of seeds (2+log(k)).\n\nReturns\n-------\ncenters : ndarray of shape (n_clusters, n_features)\n    The inital centers for k-means.\n\nindices : ndarray of shape (n_clusters,)\n    The index location of the chosen centers in the data array X. For a\n    given index and center, X[index] = center.\n\nNotes\n-----\nSelects initial cluster centers for k-mean clustering in a smart way\nto speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n\"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\non Discrete algorithms. 2007\n\nExamples\n--------\n\n>>> from sklearn.cluster import kmeans_plusplus\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n>>> centers\narray([[10,  4],\n       [ 1,  0]])\n>>> indices\narray([4, 2])"
        }
      ]
    },
    {
      "name": "sklearn.cluster._mean_shift",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "defaultdict",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "sklearn._config",
          "declaration": "config_context",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_distances_argmin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_batches",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "MeanShift",
          "decorators": [],
          "superclasses": [
            "ClusterMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples to cluster."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform clustering.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Samples to cluster.\n\ny : Ignored"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data to predict."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Index of the cluster each sample belongs to."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the closest cluster each sample in X belongs to.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to."
            }
          ],
          "fullDocstring": "Mean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \"blobs\" in a smooth density of\nsamples. It is a centroid-based algorithm, which works by updating\ncandidates for centroids to be the mean of the points within a given\nregion. These candidates are then filtered in a post-processing stage to\neliminate near-duplicates to form the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the :ref:`User Guide <mean_shift>`.\n\nParameters\n----------\nbandwidth : float, default=None\n    Bandwidth used in the RBF kernel.\n\n    If not given, the bandwidth is estimated using\n    sklearn.cluster.estimate_bandwidth; see the documentation for that\n    function for hints on scalability (see also the Notes, below).\n\nseeds : array-like of shape (n_samples, n_features), default=None\n    Seeds used to initialize kernels. If not set,\n    the seeds are calculated by clustering.get_bin_seeds\n    with bandwidth as the grid size and default values for\n    other parameters.\n\nbin_seeding : bool, default=False\n    If true, initial kernel locations are not locations of all\n    points, but rather the location of the discretized version of\n    points, where points are binned onto a grid whose coarseness\n    corresponds to the bandwidth. Setting this option to True will speed\n    up the algorithm because fewer seeds will be initialized.\n    The default value is False.\n    Ignored if seeds argument is not None.\n\nmin_bin_freq : int, default=1\n   To speed up the algorithm, accept only those bins with at least\n   min_bin_freq points as seeds.\n\ncluster_all : bool, default=True\n    If true, then all points are clustered, even those orphans that are\n    not within any kernel. Orphans are assigned to the nearest kernel.\n    If false, then orphans are given cluster label -1.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by computing\n    each of the n_init runs in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmax_iter : int, default=300\n    Maximum number of iterations, per seed point before the clustering\n    operation terminates (for that seed point), if has not converged yet.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point.\n\nn_iter_ : int\n    Maximum number of iterations performed on each seed.\n\n    .. versionadded:: 0.22\n\nExamples\n--------\n>>> from sklearn.cluster import MeanShift\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = MeanShift(bandwidth=2).fit(X)\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])\n>>> clustering.predict([[0, 0], [5, 5]])\narray([1, 0])\n>>> clustering\nMeanShift(bandwidth=2)\n\nNotes\n-----\n\nScalability:\n\nBecause this implementation uses a flat kernel and\na Ball Tree to look up members of each kernel, the complexity will tend\ntowards O(T*n*log(n)) in lower dimensions, with n the number of samples\nand T the number of points. In higher dimensions the complexity will\ntend towards O(T*n^2).\n\nScalability can be boosted by using fewer seeds, for example by using\na higher value of min_bin_freq in the get_bin_seeds function.\n\nNote that the estimate_bandwidth function is much less scalable than the\nmean shift algorithm and will be the bottleneck if it is used.\n\nReferences\n----------\n\nDorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\nfeature space analysis\". IEEE Transactions on Pattern Analysis and\nMachine Intelligence. 2002. pp. 603-619."
        }
      ],
      "functions": [
        {
          "name": "estimate_bandwidth",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input points."
            }
          ],
          "results": [
            {
              "name": "bandwidth",
              "type": "float",
              "description": "The bandwidth parameter."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Estimate the bandwidth to use with the mean-shift algorithm.\n\nThat this function takes time at least quadratic in n_samples. For large\ndatasets, it's wise to set that parameter to a small value.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input points.\n\nquantile : float, default=0.3\n    should be between [0, 1]\n    0.5 means that the median of all pairwise distances is used.\n\nn_samples : int, default=None\n    The number of samples to use. If not given, all samples are used.\n\nrandom_state : int, RandomState instance, default=None\n    The generator used to randomly select the samples from input points\n    for bandwidth estimation. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nbandwidth : float\n    The bandwidth parameter."
        },
        {
          "name": "get_bin_seeds",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input points, the same points that will be used in mean_shift."
            },
            {
              "name": "bin_size",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Controls the coarseness of the binning. Smaller values lead\nto more seeding (which is computationally more expensive). If you're\nnot sure how to set this, set it to the value of the bandwidth used\nin clustering.mean_shift."
            },
            {
              "name": "min_bin_freq",
              "type": "Any",
              "hasDefault": true,
              "default": "1",
              "limitation": null,
              "ignored": false,
              "description": "Only bins with at least min_bin_freq will be selected as seeds.\nRaising this value decreases the number of seeds found, which\nmakes mean_shift computationally cheaper."
            }
          ],
          "results": [
            {
              "name": "bin_seeds",
              "type": null,
              "description": "Points used as initial kernel positions in clustering.mean_shift."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Finds seeds for mean_shift.\n\nFinds seeds by first binning data onto a grid whose lines are\nspaced bin_size apart, and then choosing those bins with at least\nmin_bin_freq points.\n\nParameters\n----------\n\nX : array-like of shape (n_samples, n_features)\n    Input points, the same points that will be used in mean_shift.\n\nbin_size : float\n    Controls the coarseness of the binning. Smaller values lead\n    to more seeding (which is computationally more expensive). If you're\n    not sure how to set this, set it to the value of the bandwidth used\n    in clustering.mean_shift.\n\nmin_bin_freq : int, default=1\n    Only bins with at least min_bin_freq will be selected as seeds.\n    Raising this value decreases the number of seeds found, which\n    makes mean_shift computationally cheaper.\n\nReturns\n-------\nbin_seeds : array-like of shape (n_samples, n_features)\n    Points used as initial kernel positions in clustering.mean_shift."
        },
        {
          "name": "mean_shift",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data."
            }
          ],
          "results": [
            {
              "name": "cluster_centers",
              "type": null,
              "description": "Coordinates of cluster centers."
            },
            {
              "name": "labels",
              "type": null,
              "description": "Cluster labels for each point."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Perform mean shift clustering of data using a flat kernel.\n\nRead more in the :ref:`User Guide <mean_shift>`.\n\nParameters\n----------\n\nX : array-like of shape (n_samples, n_features)\n    Input data.\n\nbandwidth : float, default=None\n    Kernel bandwidth.\n\n    If bandwidth is not given, it is determined using a heuristic based on\n    the median of all pairwise distances. This will take quadratic time in\n    the number of samples. The sklearn.cluster.estimate_bandwidth function\n    can be used to do this more efficiently.\n\nseeds : array-like of shape (n_seeds, n_features) or None\n    Point used as initial kernel locations. If None and bin_seeding=False,\n    each data point is used as a seed. If None and bin_seeding=True,\n    see bin_seeding.\n\nbin_seeding : bool, default=False\n    If true, initial kernel locations are not locations of all\n    points, but rather the location of the discretized version of\n    points, where points are binned onto a grid whose coarseness\n    corresponds to the bandwidth. Setting this option to True will speed\n    up the algorithm because fewer seeds will be initialized.\n    Ignored if seeds argument is not None.\n\nmin_bin_freq : int, default=1\n   To speed up the algorithm, accept only those bins with at least\n   min_bin_freq points as seeds.\n\ncluster_all : bool, default=True\n    If true, then all points are clustered, even those orphans that are\n    not within any kernel. Orphans are assigned to the nearest kernel.\n    If false, then orphans are given cluster label -1.\n\nmax_iter : int, default=300\n    Maximum number of iterations, per seed point before the clustering\n    operation terminates (for that seed point), if has not converged yet.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by computing\n    each of the n_init runs in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.17\n       Parallel Execution using *n_jobs*.\n\nReturns\n-------\n\ncluster_centers : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels : ndarray of shape (n_samples,)\n    Cluster labels for each point.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_mean_shift.py\n<sphx_glr_auto_examples_cluster_plot_mean_shift.py>`."
        }
      ]
    },
    {
      "name": "sklearn.cluster._optics",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "pairwise_distances",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_batches",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "get_chunk_n_rows",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "OPTICS",
          "decorators": [],
          "superclasses": [
            "ClusterMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "A feature array, or array of distances between samples if\nmetric='precomputed'."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The instance."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform OPTICS clustering.\n\nExtracts an ordered list of points and reachability distances, and\nperforms initial clustering using ``max_eps`` distance specified at\nOPTICS object instantiation.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features), or                 (n_samples, n_samples) if metric=\u2019precomputed\u2019\n    A feature array, or array of distances between samples if\n    metric='precomputed'.\n\ny : ignored\n    Ignored.\n\nReturns\n-------\nself : instance of OPTICS\n    The instance."
            }
          ],
          "fullDocstring": "Estimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely\nrelated to DBSCAN, finds core sample of high density and expands clusters\nfrom them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\nneighborhood radius. Better suited for usage on large datasets than the\ncurrent sklearn implementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method\n(cluster_method = 'dbscan') or an automatic\ntechnique proposed in [1]_ (cluster_method = 'xi').\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the :ref:`User Guide <optics>`.\n\nParameters\n----------\nmin_samples : int > 1 or float between 0 and 1, default=5\n    The number of samples in a neighborhood for a point to be considered as\n    a core point. Also, up and down steep regions can't have more than\n    ``min_samples`` consecutive non-steep points. Expressed as an absolute\n    number or a fraction of the number of samples (rounded to be at least\n    2).\n\nmax_eps : float, default=np.inf\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. Default value of ``np.inf`` will\n    identify clusters across all scales; reducing ``max_eps`` will result\n    in shorter run times.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string. If metric is\n    \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\ncluster_method : str, default='xi'\n    The extraction method used to extract clusters using the calculated\n    reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n\neps : float, default=None\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. By default it assumes the same value\n    as ``max_eps``.\n    Used only when ``cluster_method='dbscan'``.\n\nxi : float between 0 and 1, default=0.05\n    Determines the minimum steepness on the reachability plot that\n    constitutes a cluster boundary. For example, an upwards point in the\n    reachability plot is defined by the ratio from one point to its\n    successor being at most 1-xi.\n    Used only when ``cluster_method='xi'``.\n\npredecessor_correction : bool, default=True\n    Correct clusters according to the predecessors calculated by OPTICS\n    [2]_. This parameter has minimal effect on most datasets.\n    Used only when ``cluster_method='xi'``.\n\nmin_cluster_size : int > 1 or float between 0 and 1, default=None\n    Minimum number of samples in an OPTICS cluster, expressed as an\n    absolute number or a fraction of the number of samples (rounded to be\n    at least 2). If ``None``, the value of ``min_samples`` is used instead.\n    Used only when ``cluster_method='xi'``.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method. (default)\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nlabels_ : ndarray of shape (n_samples,)\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples and points which are not included in a leaf cluster\n    of ``cluster_hierarchy_`` are labeled as -1.\n\nreachability_ : ndarray of shape (n_samples,)\n    Reachability distances per sample, indexed by object order. Use\n    ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\nordering_ : ndarray of shape (n_samples,)\n    The cluster ordered list of sample indices.\n\ncore_distances_ : ndarray of shape (n_samples,)\n    Distance at which each sample becomes a core point, indexed by object\n    order. Points which will never be core have a distance of inf. Use\n    ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\npredecessor_ : ndarray of shape (n_samples,)\n    Point that a sample was reached from, indexed by object order.\n    Seed points have a predecessor of -1.\n\ncluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n    The list of clusters in the form of ``[start, end]`` in each row, with\n    all indices inclusive. The clusters are ordered according to\n    ``(end, -start)`` (ascending) so that larger clusters encompassing\n    smaller clusters come after those smaller ones. Since ``labels_`` does\n    not reflect the hierarchy, usually\n    ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n    note that these indices are of the ``ordering_``, i.e.\n    ``X[ordering_][start:end + 1]`` form a cluster.\n    Only available when ``cluster_method='xi'``.\n\nSee Also\n--------\nDBSCAN : A similar clustering for a specified neighborhood radius (eps).\n    Our implementation is optimized for runtime.\n\nReferences\n----------\n.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n   and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n   structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n.. [2] Schubert, Erich, Michael Gertz.\n   \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n   the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\nExamples\n--------\n>>> from sklearn.cluster import OPTICS\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 5], [3, 6],\n...               [8, 7], [8, 8], [7, 3]])\n>>> clustering = OPTICS(min_samples=2).fit(X)\n>>> clustering.labels_\narray([0, 0, 0, 1, 1, 1])"
        }
      ],
      "functions": [
        {
          "name": "cluster_optics_dbscan",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "labels_",
              "type": null,
              "description": "The estimated labels."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Performs DBSCAN extraction for an arbitrary epsilon.\n\nExtracting the clusters runs in linear time. Note that this results in\n``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\nsimilar settings and ``eps``, only if ``eps`` is close to ``max_eps``.\n\nParameters\n----------\nreachability : array of shape (n_samples,)\n    Reachability distances calculated by OPTICS (``reachability_``)\n\ncore_distances : array of shape (n_samples,)\n    Distances at which points become core (``core_distances_``)\n\nordering : array of shape (n_samples,)\n    OPTICS ordered point indices (``ordering_``)\n\neps : float\n    DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\n    will be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\n    to one another.\n\nReturns\n-------\nlabels_ : array of shape (n_samples,)\n    The estimated labels."
        },
        {
          "name": "cluster_optics_xi",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "labels",
              "type": null,
              "description": "The labels assigned to samples. Points which are not included\nin any cluster are labeled as -1."
            },
            {
              "name": "clusters",
              "type": null,
              "description": "The list of clusters in the form of ``[start, end]`` in each row, with\nall indices inclusive. The clusters are ordered according to ``(end,\n-start)`` (ascending) so that larger clusters encompassing smaller\nclusters come after such nested smaller clusters. Since ``labels`` does\nnot reflect the hierarchy, usually ``len(clusters) >\nnp.unique(labels)``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Automatically extract clusters according to the Xi-steep method.\n\nParameters\n----------\nreachability : ndarray of shape (n_samples,)\n    Reachability distances calculated by OPTICS (`reachability_`)\n\npredecessor : ndarray of shape (n_samples,)\n    Predecessors calculated by OPTICS.\n\nordering : ndarray of shape (n_samples,)\n    OPTICS ordered point indices (`ordering_`)\n\nmin_samples : int > 1 or float between 0 and 1\n    The same as the min_samples given to OPTICS. Up and down steep regions\n    can't have more then ``min_samples`` consecutive non-steep points.\n    Expressed as an absolute number or a fraction of the number of samples\n    (rounded to be at least 2).\n\nmin_cluster_size : int > 1 or float between 0 and 1, default=None\n    Minimum number of samples in an OPTICS cluster, expressed as an\n    absolute number or a fraction of the number of samples (rounded to be\n    at least 2). If ``None``, the value of ``min_samples`` is used instead.\n\nxi : float between 0 and 1, default=0.05\n    Determines the minimum steepness on the reachability plot that\n    constitutes a cluster boundary. For example, an upwards point in the\n    reachability plot is defined by the ratio from one point to its\n    successor being at most 1-xi.\n\npredecessor_correction : bool, default=True\n    Correct clusters based on the calculated predecessors.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    The labels assigned to samples. Points which are not included\n    in any cluster are labeled as -1.\n\nclusters : ndarray of shape (n_clusters, 2)\n    The list of clusters in the form of ``[start, end]`` in each row, with\n    all indices inclusive. The clusters are ordered according to ``(end,\n    -start)`` (ascending) so that larger clusters encompassing smaller\n    clusters come after such nested smaller clusters. Since ``labels`` does\n    not reflect the hierarchy, usually ``len(clusters) >\n    np.unique(labels)``."
        },
        {
          "name": "compute_optics_graph",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A feature array, or array of distances between samples if\nmetric='precomputed'"
            }
          ],
          "results": [
            {
              "name": "ordering_",
              "type": null,
              "description": "The cluster ordered list of sample indices."
            },
            {
              "name": "core_distances_",
              "type": null,
              "description": "Distance at which each sample becomes a core point, indexed by object\norder. Points which will never be core have a distance of inf. Use\n``clust.core_distances_[clust.ordering_]`` to access in cluster order."
            },
            {
              "name": "reachability_",
              "type": null,
              "description": "Reachability distances per sample, indexed by object order. Use\n``clust.reachability_[clust.ordering_]`` to access in cluster order."
            },
            {
              "name": "predecessor_",
              "type": null,
              "description": "Point that a sample was reached from, indexed by object order.\nSeed points have a predecessor of -1."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the OPTICS reachability graph.\n\nRead more in the :ref:`User Guide <optics>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features), or             (n_samples, n_samples) if metric=\u2019precomputed\u2019.\n    A feature array, or array of distances between samples if\n    metric='precomputed'\n\nmin_samples : int > 1 or float between 0 and 1\n    The number of samples in a neighborhood for a point to be considered\n    as a core point. Expressed as an absolute number or a fraction of the\n    number of samples (rounded to be at least 2).\n\nmax_eps : float, default=np.inf\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. Default value of ``np.inf`` will\n    identify clusters across all scales; reducing ``max_eps`` will result\n    in shorter run times.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string. If metric is\n    \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method. (default)\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nordering_ : array of shape (n_samples,)\n    The cluster ordered list of sample indices.\n\ncore_distances_ : array of shape (n_samples,)\n    Distance at which each sample becomes a core point, indexed by object\n    order. Points which will never be core have a distance of inf. Use\n    ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\nreachability_ : array of shape (n_samples,)\n    Reachability distances per sample, indexed by object order. Use\n    ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\npredecessor_ : array of shape (n_samples,)\n    Point that a sample was reached from, indexed by object order.\n    Seed points have a predecessor of -1.\n\nReferences\n----------\n.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n   and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n   structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60."
        }
      ]
    },
    {
      "name": "sklearn.cluster._spectral",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.cluster._kmeans",
          "declaration": "k_means",
          "alias": null
        },
        {
          "module": "sklearn.manifold",
          "declaration": "spectral_embedding",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_kernels",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "kneighbors_graph",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "as_float_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "SpectralClustering",
          "decorators": [],
          "superclasses": [
            "ClusterMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster, or similarities / affinities between\ninstances if ``affinity='precomputed'``. If a sparse matrix is\nprovided in a format other than ``csr_matrix``, ``csc_matrix``,\nor ``coo_matrix``, it will be converted into a sparse\n``csr_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform spectral clustering from features, or affinity matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n    Training instances to cluster, or similarities / affinities between\n    instances if ``affinity='precomputed'``. If a sparse matrix is\n    provided in a format other than ``csr_matrix``, ``csc_matrix``,\n    or ``coo_matrix``, it will be converted into a sparse\n    ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself"
            },
            {
              "name": "fit_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training instances to cluster, or similarities / affinities between\ninstances if ``affinity='precomputed'``. If a sparse matrix is\nprovided in a format other than ``csr_matrix``, ``csc_matrix``,\nor ``coo_matrix``, it will be converted into a sparse\n``csr_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present here for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Cluster labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform spectral clustering from features, or affinity matrix,\nand return cluster labels.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n    Training instances to cluster, or similarities / affinities between\n    instances if ``affinity='precomputed'``. If a sparse matrix is\n    provided in a format other than ``csr_matrix``, ``csc_matrix``,\n    or ``coo_matrix``, it will be converted into a sparse\n    ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels."
            }
          ],
          "fullDocstring": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster. For instance when clusters are\nnested circles on the 2D plane.\n\nIf affinity is the adjacency matrix of a graph, this method can be\nused to find normalized graph cuts.\n\nWhen calling ``fit``, an affinity matrix is constructed using either\nkernel function such the Gaussian (aka RBF) kernel of the euclidean\ndistanced ``d(X, X)``::\n\n        np.exp(-gamma * d(X,X) ** 2)\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, using ``precomputed``, a user-provided affinity\nmatrix can be used.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n\nParameters\n----------\nn_clusters : int, default=8\n    The dimension of the projection subspace.\n\neigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n    The eigenvalue decomposition strategy to use. AMG requires pyamg\n    to be installed. It can be faster on very large, sparse problems,\n    but may also lead to instabilities. If None, then ``'arpack'`` is\n    used.\n\nn_components : int, default=n_clusters\n    Number of eigen vectors to use for the spectral embedding\n\nrandom_state : int, RandomState instance, default=None\n    A pseudo random number generator used for the initialization of the\n    lobpcg eigen vectors decomposition when ``eigen_solver='amg'`` and by\n    the K-Means initialization. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\n\ngamma : float, default=1.0\n    Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n    Ignored for ``affinity='nearest_neighbors'``.\n\naffinity : str or callable, default='rbf'\n    How to construct the affinity matrix.\n     - 'nearest_neighbors' : construct the affinity matrix by computing a\n       graph of nearest neighbors.\n     - 'rbf' : construct the affinity matrix using a radial basis function\n       (RBF) kernel.\n     - 'precomputed' : interpret ``X`` as a precomputed affinity matrix.\n     - 'precomputed_nearest_neighbors' : interpret ``X`` as a sparse graph\n       of precomputed nearest neighbors, and constructs the affinity matrix\n       by selecting the ``n_neighbors`` nearest neighbors.\n     - one of the kernels supported by\n       :func:`~sklearn.metrics.pairwise_kernels`.\n\n    Only kernels that produce similarity scores (non-negative values that\n    increase with similarity) should be used. This property is not checked\n    by the clustering algorithm.\n\nn_neighbors : int, default=10\n    Number of neighbors to use when constructing the affinity matrix using\n    the nearest neighbors method. Ignored for ``affinity='rbf'``.\n\neigen_tol : float, default=0.0\n    Stopping criterion for eigendecomposition of the Laplacian matrix\n    when ``eigen_solver='arpack'``.\n\nassign_labels : {'kmeans', 'discretize'}, default='kmeans'\n    The strategy to use to assign labels in the embedding\n    space. There are two ways to assign labels after the laplacian\n    embedding. k-means can be applied and is a popular choice. But it can\n    also be sensitive to initialization. Discretization is another approach\n    which is less sensitive to random initialization.\n\ndegree : float, default=3\n    Degree of the polynomial kernel. Ignored by other kernels.\n\ncoef0 : float, default=1\n    Zero coefficient for polynomial and sigmoid kernels.\n    Ignored by other kernels.\n\nkernel_params : dict of str to any, default=None\n    Parameters (keyword arguments) and values for kernel passed as\n    callable object. Ignored by other kernels.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run when `affinity='nearest_neighbors'`\n    or `affinity='precomputed_nearest_neighbors'`. The neighbors search\n    will be done in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    Verbosity mode.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\naffinity_matrix_ : array-like of shape (n_samples, n_samples)\n    Affinity matrix used for clustering. Available only if after calling\n    ``fit``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralClustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralClustering(n_clusters=2,\n...         assign_labels=\"discretize\",\n...         random_state=0).fit(X)\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])\n>>> clustering\nSpectralClustering(assign_labels='discretize', n_clusters=2,\n    random_state=0)\n\nNotes\n-----\nIf you have an affinity matrix, such as a distance matrix,\nfor which 0 means identical elements, and high values means\nvery dissimilar elements, it can be transformed in a\nsimilarity matrix that is well suited for the algorithm by\napplying the Gaussian (RBF, heat) kernel::\n\n    np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\nWhere ``delta`` is a free parameter representing the width of the Gaussian\nkernel.\n\nAnother alternative is to take a symmetric version of the k\nnearest neighbors connectivity matrix of the points.\n\nIf the pyamg package is installed, it is used: this greatly\nspeeds up computation.\n\nReferences\n----------\n\n- Normalized cuts and image segmentation, 2000\n  Jianbo Shi, Jitendra Malik\n  http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n- A Tutorial on Spectral Clustering, 2007\n  Ulrike von Luxburg\n  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n- Multiclass spectral clustering, 2003\n  Stella X. Yu, Jianbo Shi\n  https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf"
        }
      ],
      "functions": [
        {
          "name": "discretize",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "vectors",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The embedding space of the samples."
            }
          ],
          "results": [
            {
              "name": "labels",
              "type": null,
              "description": "The labels of the clusters."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Search for a partition matrix (clustering) which is closest to the\neigenvector embedding.\n\nParameters\n----------\nvectors : array-like of shape (n_samples, n_clusters)\n    The embedding space of the samples.\n\ncopy : bool, default=True\n    Whether to copy vectors, or perform in-place normalization.\n\nmax_svd_restarts : int, default=30\n    Maximum number of attempts to restart SVD if convergence fails\n\nn_iter_max : int, default=30\n    Maximum number of iterations to attempt in rotation and partition\n    matrix search if machine precision convergence is not reached\n\nrandom_state : int, RandomState instance, default=None\n    Determines random number generation for rotation matrix initialization.\n    Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nlabels : array of integers, shape: n_samples\n    The labels of the clusters.\n\nReferences\n----------\n\n- Multiclass spectral clustering, 2003\n  Stella X. Yu, Jianbo Shi\n  https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n\nNotes\n-----\n\nThe eigenvector embedding is used to iteratively search for the\nclosest discrete partition.  First, the eigenvector embedding is\nnormalized to the space of partition matrices. An optimal discrete\npartition matrix closest to this normalized embedding multiplied by\nan initial rotation is calculated.  Fixing this discrete partition\nmatrix, an optimal rotation matrix is calculated.  These two\ncalculations are performed until convergence.  The discrete partition\nmatrix is returned as the clustering solution.  Used in spectral\nclustering, this method tends to be faster and more robust to random\ninitialization than k-means."
        },
        {
          "name": "spectral_clustering",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "affinity",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The affinity matrix describing the relationship of the samples to\nembed. **Must be symmetric**.\n\nPossible examples:\n  - adjacency matrix of a graph,\n  - heat kernel of the pairwise distance matrix of the samples,\n  - symmetric k-nearest neighbours connectivity matrix of the samples."
            }
          ],
          "results": [
            {
              "name": "labels",
              "type": null,
              "description": "The labels of the clusters."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster. For instance, when clusters are\nnested circles on the 2D plane.\n\nIf affinity is the adjacency matrix of a graph, this method can be\nused to find normalized graph cuts.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n\nParameters\n----------\naffinity : {array-like, sparse matrix} of shape (n_samples, n_samples)\n    The affinity matrix describing the relationship of the samples to\n    embed. **Must be symmetric**.\n\n    Possible examples:\n      - adjacency matrix of a graph,\n      - heat kernel of the pairwise distance matrix of the samples,\n      - symmetric k-nearest neighbours connectivity matrix of the samples.\n\nn_clusters : int, default=None\n    Number of clusters to extract.\n\nn_components : int, default=n_clusters\n    Number of eigen vectors to use for the spectral embedding\n\neigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n    The eigenvalue decomposition strategy to use. AMG requires pyamg\n    to be installed. It can be faster on very large, sparse problems,\n    but may also lead to instabilities. If None, then ``'arpack'`` is\n    used.\n\nrandom_state : int, RandomState instance, default=None\n    A pseudo random number generator used for the initialization of the\n    lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by\n    the K-Means initialization. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\n\neigen_tol : float, default=0.0\n    Stopping criterion for eigendecomposition of the Laplacian matrix\n    when using arpack eigen_solver.\n\nassign_labels : {'kmeans', 'discretize'}, default='kmeans'\n    The strategy to use to assign labels in the embedding\n    space.  There are two ways to assign labels after the laplacian\n    embedding.  k-means can be applied and is a popular choice. But it can\n    also be sensitive to initialization. Discretization is another\n    approach which is less sensitive to random initialization. See\n    the 'Multiclass spectral clustering' paper referenced below for\n    more details on the discretization approach.\n\nverbose : bool, default=False\n    Verbosity mode.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nlabels : array of integers, shape: n_samples\n    The labels of the clusters.\n\nReferences\n----------\n\n- Normalized cuts and image segmentation, 2000\n  Jianbo Shi, Jitendra Malik\n  http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n- A Tutorial on Spectral Clustering, 2007\n  Ulrike von Luxburg\n  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n- Multiclass spectral clustering, 2003\n  Stella X. Yu, Jianbo Shi\n  https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n\nNotes\n-----\nThe graph should contain only one connect component, elsewhere\nthe results make little sense.\n\nThis algorithm solves the normalized cut for k=2: it is a\nnormalized spectral clustering."
        }
      ]
    },
    {
      "name": "sklearn.cluster.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.compose",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._column_transformer",
          "declaration": "ColumnTransformer",
          "alias": null
        },
        {
          "module": "sklearn._column_transformer",
          "declaration": "make_column_selector",
          "alias": null
        },
        {
          "module": "sklearn._column_transformer",
          "declaration": "make_column_transformer",
          "alias": null
        },
        {
          "module": "sklearn._target",
          "declaration": "TransformedTargetRegressor",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.compose._column_transformer",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "itertools",
          "declaration": "chain",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.pipeline",
          "declaration": "_fit_transform_one",
          "alias": null
        },
        {
          "module": "sklearn.pipeline",
          "declaration": "_name_estimators",
          "alias": null
        },
        {
          "module": "sklearn.pipeline",
          "declaration": "_transform_one",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "FunctionTransformer",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_determine_key_type",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_get_column_indices",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils._estimator_html_repr",
          "declaration": "_VisualBlock",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "_BaseComposition",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "ColumnTransformer",
          "decorators": [],
          "superclasses": [
            "_BaseComposition",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
                }
              ],
              "results": [
                {
                  "name": "params",
                  "type": "Dict",
                  "description": "Parameter names mapped to their values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the\nestimators contained within the `transformers` of the\n`ColumnTransformer`.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : dict\n    Parameter names mapped to their values."
            },
            {
              "name": "set_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``. Note that you\ncan directly set the parameters of the estimators contained in\n`transformers` of `ColumnTransformer`.\n\nReturns\n-------\nself"
            },
            {
              "name": "named_transformers_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Access the fitted transformer by name.\n\nRead-only attribute to access any transformer by given name.\nKeys are transformer names and values are the fitted transformer\nobjects."
            },
            {
              "name": "get_feature_names",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "feature_names",
                  "type": null,
                  "description": "Names of the features produced by transform."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get feature names from all transformers.\n\nReturns\n-------\nfeature_names : list of strings\n    Names of the features produced by transform."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, of which specified subsets are used to fit the\ntransformers."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Targets for supervised learning."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "This estimator"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit all transformers using X.\n\nParameters\n----------\nX : {array-like, dataframe} of shape (n_samples, n_features)\n    Input data, of which specified subsets are used to fit the\n    transformers.\n\ny : array-like of shape (n_samples,...), default=None\n    Targets for supervised learning.\n\nReturns\n-------\nself : ColumnTransformer\n    This estimator"
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, of which specified subsets are used to fit the\ntransformers."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Targets for supervised learning."
                }
              ],
              "results": [
                {
                  "name": "X_t",
                  "type": null,
                  "description": "hstack of results of transformers. sum_n_components is the\nsum of n_components (output dimension) over transformers. If\nany result is a sparse matrix, everything will be converted to\nsparse matrices."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit all transformers, transform the data and concatenate results.\n\nParameters\n----------\nX : {array-like, dataframe} of shape (n_samples, n_features)\n    Input data, of which specified subsets are used to fit the\n    transformers.\n\ny : array-like of shape (n_samples,), default=None\n    Targets for supervised learning.\n\nReturns\n-------\nX_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n    hstack of results of transformers. sum_n_components is the\n    sum of n_components (output dimension) over transformers. If\n    any result is a sparse matrix, everything will be converted to\n    sparse matrices."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to be transformed by subset."
                }
              ],
              "results": [
                {
                  "name": "X_t",
                  "type": null,
                  "description": "hstack of results of transformers. sum_n_components is the\nsum of n_components (output dimension) over transformers. If\nany result is a sparse matrix, everything will be converted to\nsparse matrices."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X separately by each transformer, concatenate results.\n\nParameters\n----------\nX : {array-like, dataframe} of shape (n_samples, n_features)\n    The data to be transformed by subset.\n\nReturns\n-------\nX_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n    hstack of results of transformers. sum_n_components is the\n    sum of n_components (output dimension) over transformers. If\n    any result is a sparse matrix, everything will be converted to\n    sparse matrices."
            }
          ],
          "fullDocstring": "Applies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input\nto be transformed separately and the features generated by each transformer\nwill be concatenated to form a single feature space.\nThis is useful for heterogeneous or columnar data, to combine several\nfeature extraction mechanisms or transformations into a single transformer.\n\nRead more in the :ref:`User Guide <column_transformer>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\ntransformers : list of tuples\n    List of (name, transformer, columns) tuples specifying the\n    transformer objects to be applied to subsets of the data.\n\n    name : str\n        Like in Pipeline and FeatureUnion, this allows the transformer and\n        its parameters to be set using ``set_params`` and searched in grid\n        search.\n    transformer : {'drop', 'passthrough'} or estimator\n        Estimator must support :term:`fit` and :term:`transform`.\n        Special-cased strings 'drop' and 'passthrough' are accepted as\n        well, to indicate to drop the columns or to pass them through\n        untransformed, respectively.\n    columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable\n        Indexes the data on its second axis. Integers are interpreted as\n        positional columns, while strings can reference DataFrame columns\n        by name.  A scalar string or int should be used where\n        ``transformer`` expects X to be a 1d array-like (vector),\n        otherwise a 2d array will be passed to the transformer.\n        A callable is passed the input data `X` and can return any of the\n        above. To select multiple columns by name or dtype, you can use\n        :obj:`make_column_selector`.\n\nremainder : {'drop', 'passthrough'} or estimator, default='drop'\n    By default, only the specified columns in `transformers` are\n    transformed and combined in the output, and the non-specified\n    columns are dropped. (default of ``'drop'``).\n    By specifying ``remainder='passthrough'``, all remaining columns that\n    were not specified in `transformers` will be automatically passed\n    through. This subset of columns is concatenated with the output of\n    the transformers.\n    By setting ``remainder`` to be an estimator, the remaining\n    non-specified columns will use the ``remainder`` estimator. The\n    estimator must support :term:`fit` and :term:`transform`.\n    Note that using this feature requires that the DataFrame columns\n    input at :term:`fit` and :term:`transform` have identical order.\n\nsparse_threshold : float, default=0.3\n    If the output of the different transformers contains sparse matrices,\n    these will be stacked as a sparse matrix if the overall density is\n    lower than this value. Use ``sparse_threshold=0`` to always return\n    dense.  When the transformed output consists of all dense data, the\n    stacked result will be dense, and this keyword will be ignored.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ntransformer_weights : dict, default=None\n    Multiplicative weights for features per transformer. The output of the\n    transformer is multiplied by these weights. Keys are transformer names,\n    values the weights.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each transformer will be\n    printed as it is completed.\n\nAttributes\n----------\ntransformers_ : list\n    The collection of fitted transformers as tuples of\n    (name, fitted_transformer, column). `fitted_transformer` can be an\n    estimator, 'drop', or 'passthrough'. In case there were no columns\n    selected, this will be the unfitted transformer.\n    If there are remaining columns, the final element is a tuple of the\n    form:\n    ('remainder', transformer, remaining_columns) corresponding to the\n    ``remainder`` parameter. If there are remaining columns, then\n    ``len(transformers_)==len(transformers)+1``, otherwise\n    ``len(transformers_)==len(transformers)``.\n\nnamed_transformers_ : :class:`~sklearn.utils.Bunch`\n    Read-only attribute to access any transformer by given name.\n    Keys are transformer names and values are the fitted transformer\n    objects.\n\nsparse_output_ : bool\n    Boolean flag indicating whether the output of ``transform`` is a\n    sparse matrix or a dense numpy array, which depends on the output\n    of the individual transformers and the `sparse_threshold` keyword.\n\nNotes\n-----\nThe order of the columns in the transformed feature matrix follows the\norder of how the columns are specified in the `transformers` list.\nColumns of the original feature matrix that are not specified are\ndropped from the resulting transformed feature matrix, unless specified\nin the `passthrough` keyword. Those columns specified with `passthrough`\nare added at the right to the output of the transformers.\n\nSee Also\n--------\nmake_column_transformer : Convenience function for\n    combining the outputs of multiple transformer objects applied to\n    column subsets of the original feature space.\nmake_column_selector : Convenience function for selecting\n    columns based on datatype or the columns name with a regex pattern.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.compose import ColumnTransformer\n>>> from sklearn.preprocessing import Normalizer\n>>> ct = ColumnTransformer(\n...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n>>> X = np.array([[0., 1., 2., 2.],\n...               [1., 1., 0., 1.]])\n>>> # Normalizer scales each row of X to unit norm. A separate scaling\n>>> # is applied for the two first and two last elements of each\n>>> # row independently.\n>>> ct.fit_transform(X)\narray([[0. , 1. , 0.5, 0.5],\n       [0.5, 0.5, 0. , 1. ]])"
        },
        {
          "name": "make_column_selector",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "Create a callable to select columns to be used with\n:class:`ColumnTransformer`.\n\n:func:`make_column_selector` can select columns based on datatype or the\ncolumns name with a regex. When using multiple selection criteria, **all**\ncriteria must match for a column to be selected.\n\nParameters\n----------\npattern : str, default=None\n    Name of columns containing this regex pattern will be included. If\n    None, column selection will not be selected based on pattern.\n\ndtype_include : column dtype or list of column dtypes, default=None\n    A selection of dtypes to include. For more details, see\n    :meth:`pandas.DataFrame.select_dtypes`.\n\ndtype_exclude : column dtype or list of column dtypes, default=None\n    A selection of dtypes to exclude. For more details, see\n    :meth:`pandas.DataFrame.select_dtypes`.\n\nReturns\n-------\nselector : callable\n    Callable for column selection to be used by a\n    :class:`ColumnTransformer`.\n\nSee Also\n--------\nColumnTransformer : Class that allows combining the\n    outputs of multiple transformer objects used on column subsets\n    of the data into a single feature space.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n>>> from sklearn.compose import make_column_transformer\n>>> from sklearn.compose import make_column_selector\n>>> import pandas as pd  # doctest: +SKIP\n>>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP\n>>> ct = make_column_transformer(\n...       (StandardScaler(),\n...        make_column_selector(dtype_include=np.number)),  # rating\n...       (OneHotEncoder(),\n...        make_column_selector(dtype_include=object)))  # city\n>>> ct.fit_transform(X)  # doctest: +SKIP\narray([[ 0.90453403,  1.        ,  0.        ,  0.        ],\n       [-1.50755672,  1.        ,  0.        ,  0.        ],\n       [-0.30151134,  0.        ,  1.        ,  0.        ],\n       [ 0.90453403,  0.        ,  0.        ,  1.        ]])"
        }
      ],
      "functions": [
        {
          "name": "make_column_transformer",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "ct",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Construct a ColumnTransformer from the given transformers.\n\nThis is a shorthand for the ColumnTransformer constructor; it does not\nrequire, and does not permit, naming the transformers. Instead, they will\nbe given names automatically based on their types. It also does not allow\nweighting with ``transformer_weights``.\n\nRead more in the :ref:`User Guide <make_column_transformer>`.\n\nParameters\n----------\n*transformers : tuples\n    Tuples of the form (transformer, columns) specifying the\n    transformer objects to be applied to subsets of the data.\n\n    transformer : {'drop', 'passthrough'} or estimator\n        Estimator must support :term:`fit` and :term:`transform`.\n        Special-cased strings 'drop' and 'passthrough' are accepted as\n        well, to indicate to drop the columns or to pass them through\n        untransformed, respectively.\n    columns : str,  array-like of str, int, array-like of int, slice,                 array-like of bool or callable\n        Indexes the data on its second axis. Integers are interpreted as\n        positional columns, while strings can reference DataFrame columns\n        by name. A scalar string or int should be used where\n        ``transformer`` expects X to be a 1d array-like (vector),\n        otherwise a 2d array will be passed to the transformer.\n        A callable is passed the input data `X` and can return any of the\n        above. To select multiple columns by name or dtype, you can use\n        :obj:`make_column_selector`.\n\nremainder : {'drop', 'passthrough'} or estimator, default='drop'\n    By default, only the specified columns in `transformers` are\n    transformed and combined in the output, and the non-specified\n    columns are dropped. (default of ``'drop'``).\n    By specifying ``remainder='passthrough'``, all remaining columns that\n    were not specified in `transformers` will be automatically passed\n    through. This subset of columns is concatenated with the output of\n    the transformers.\n    By setting ``remainder`` to be an estimator, the remaining\n    non-specified columns will use the ``remainder`` estimator. The\n    estimator must support :term:`fit` and :term:`transform`.\n\nsparse_threshold : float, default=0.3\n    If the transformed output consists of a mix of sparse and dense data,\n    it will be stacked as a sparse matrix if the density is lower than this\n    value. Use ``sparse_threshold=0`` to always return dense.\n    When the transformed output consists of all sparse or all dense data,\n    the stacked result will be sparse or dense, respectively, and this\n    keyword will be ignored.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each transformer will be\n    printed as it is completed.\n\nReturns\n-------\nct : ColumnTransformer\n\nSee Also\n--------\nColumnTransformer : Class that allows combining the\n    outputs of multiple transformer objects used on column subsets\n    of the data into a single feature space.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n>>> from sklearn.compose import make_column_transformer\n>>> make_column_transformer(\n...     (StandardScaler(), ['numerical_column']),\n...     (OneHotEncoder(), ['categorical_column']))\nColumnTransformer(transformers=[('standardscaler', StandardScaler(...),\n                                 ['numerical_column']),\n                                ('onehotencoder', OneHotEncoder(...),\n                                 ['categorical_column'])])"
        }
      ]
    },
    {
      "name": "sklearn.compose._target",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "FunctionTransformer",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "TransformedTargetRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model according to the given training data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\n**fit_params : dict\n    Parameters passed to the ``fit`` method of the underlying\n    regressor.\n\n\nReturns\n-------\nself : object"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples."
                }
              ],
              "results": [
                {
                  "name": "y_hat",
                  "type": null,
                  "description": "Predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the base regressor, applying inverse.\n\nThe regressor is used to predict and the ``inverse_func`` or\n``inverse_transform`` is applied before returning the prediction.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Samples.\n\nReturns\n-------\ny_hat : ndarray of shape (n_samples,)\n    Predicted values."
            },
            {
              "name": "n_features_in_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Meta-estimator to regress on a transformed target.\n\nUseful for applying a non-linear transformation to the target ``y`` in\nregression problems. This transformation can be given as a Transformer\nsuch as the QuantileTransformer or as a function and its inverse such as\n``log`` and ``exp``.\n\nThe computation during ``fit`` is::\n\n    regressor.fit(X, func(y))\n\nor::\n\n    regressor.fit(X, transformer.transform(y))\n\nThe computation during ``predict`` is::\n\n    inverse_func(regressor.predict(X))\n\nor::\n\n    transformer.inverse_transform(regressor.predict(X))\n\nRead more in the :ref:`User Guide <transformed_target_regressor>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nregressor : object, default=None\n    Regressor object such as derived from ``RegressorMixin``. This\n    regressor will automatically be cloned each time prior to fitting.\n    If regressor is ``None``, ``LinearRegression()`` is created and used.\n\ntransformer : object, default=None\n    Estimator object such as derived from ``TransformerMixin``. Cannot be\n    set at the same time as ``func`` and ``inverse_func``. If\n    ``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,\n    the transformer will be an identity transformer. Note that the\n    transformer will be cloned during fitting. Also, the transformer is\n    restricting ``y`` to be a numpy array.\n\nfunc : function, default=None\n    Function to apply to ``y`` before passing to ``fit``. Cannot be set at\n    the same time as ``transformer``. The function needs to return a\n    2-dimensional array. If ``func`` is ``None``, the function used will be\n    the identity function.\n\ninverse_func : function, default=None\n    Function to apply to the prediction of the regressor. Cannot be set at\n    the same time as ``transformer`` as well. The function needs to return\n    a 2-dimensional array. The inverse function is used to return\n    predictions to the same space of the original training labels.\n\ncheck_inverse : bool, default=True\n    Whether to check that ``transform`` followed by ``inverse_transform``\n    or ``func`` followed by ``inverse_func`` leads to the original targets.\n\nAttributes\n----------\nregressor_ : object\n    Fitted regressor.\n\ntransformer_ : object\n    Transformer used in ``fit`` and ``predict``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.compose import TransformedTargetRegressor\n>>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n...                                 func=np.log, inverse_func=np.exp)\n>>> X = np.arange(4).reshape(-1, 1)\n>>> y = np.exp(2 * X).ravel()\n>>> tt.fit(X, y)\nTransformedTargetRegressor(...)\n>>> tt.score(X, y)\n1.0\n>>> tt.regressor_.coef_\narray([2.])\n\nNotes\n-----\nInternally, the target ``y`` is always converted into a 2-dimensional array\nto be used by scikit-learn transformers. At the time of prediction, the\noutput will be reshaped to a have the same number of dimensions as ``y``.\n\nSee :ref:`examples/compose/plot_transformed_target.py\n<sphx_glr_auto_examples_compose_plot_transformed_target.py>`."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.conftest",
      "imports": [
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "pytest",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "functools",
          "declaration": "wraps",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "environ",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "fetch_20newsgroups",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "fetch_20newsgroups_vectorized",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "fetch_california_housing",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "fetch_covtype",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "fetch_kddcup99",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "fetch_olivetti_faces",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "fetch_rcv1",
          "alias": null
        },
        {
          "module": "sklearn.utils._openmp_helpers",
          "declaration": "_openmp_effective_n_threads",
          "alias": null
        },
        {
          "module": "threadpoolctl",
          "declaration": "threadpool_limits",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "pyplot",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "pyplot",
              "type": null,
              "description": "The ``matplotlib.pyplot`` module."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Setup and teardown fixture for matplotlib.\n\nThis fixture checks if we can import matplotlib. If not, the tests will be\nskipped. Otherwise, we setup matplotlib backend and close the figures\nafter running the functions.\n\nReturns\n-------\npyplot : module\n    The ``matplotlib.pyplot`` module."
        },
        {
          "name": "pytest_collection_modifyitems",
          "decorators": [],
          "parameters": [
            {
              "name": "config",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "items",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Called after collect is completed.\n\nParameters\n----------\nconfig : pytest config\nitems : list of collected items"
        },
        {
          "name": "pytest_runtest_setup",
          "decorators": [],
          "parameters": [
            {
              "name": "item",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "item to be processed"
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Set the number of openmp threads based on the number of workers\nxdist is using to prevent oversubscription.\n\nParameters\n----------\nitem : pytest item\n    item to be processed"
        }
      ]
    },
    {
      "name": "sklearn.covariance",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._elliptic_envelope",
          "declaration": "EllipticEnvelope",
          "alias": null
        },
        {
          "module": "sklearn._empirical_covariance",
          "declaration": "EmpiricalCovariance",
          "alias": null
        },
        {
          "module": "sklearn._empirical_covariance",
          "declaration": "empirical_covariance",
          "alias": null
        },
        {
          "module": "sklearn._empirical_covariance",
          "declaration": "log_likelihood",
          "alias": null
        },
        {
          "module": "sklearn._graph_lasso",
          "declaration": "GraphicalLasso",
          "alias": null
        },
        {
          "module": "sklearn._graph_lasso",
          "declaration": "GraphicalLassoCV",
          "alias": null
        },
        {
          "module": "sklearn._graph_lasso",
          "declaration": "graphical_lasso",
          "alias": null
        },
        {
          "module": "sklearn._robust_covariance",
          "declaration": "MinCovDet",
          "alias": null
        },
        {
          "module": "sklearn._robust_covariance",
          "declaration": "fast_mcd",
          "alias": null
        },
        {
          "module": "sklearn._shrunk_covariance",
          "declaration": "LedoitWolf",
          "alias": null
        },
        {
          "module": "sklearn._shrunk_covariance",
          "declaration": "OAS",
          "alias": null
        },
        {
          "module": "sklearn._shrunk_covariance",
          "declaration": "ShrunkCovariance",
          "alias": null
        },
        {
          "module": "sklearn._shrunk_covariance",
          "declaration": "ledoit_wolf",
          "alias": null
        },
        {
          "module": "sklearn._shrunk_covariance",
          "declaration": "ledoit_wolf_shrinkage",
          "alias": null
        },
        {
          "module": "sklearn._shrunk_covariance",
          "declaration": "oas",
          "alias": null
        },
        {
          "module": "sklearn._shrunk_covariance",
          "declaration": "shrunk_covariance",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.covariance._elliptic_envelope",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "OutlierMixin",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "MinCovDet",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "EllipticEnvelope",
          "decorators": [],
          "superclasses": [
            "OutlierMixin",
            "MinCovDet"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the EllipticEnvelope model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : Ignored\n    Not used, present for API consistency by convention."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data matrix."
                }
              ],
              "results": [
                {
                  "name": "decision",
                  "type": null,
                  "description": "Decision function of the samples.\nIt is equal to the shifted Mahalanobis distances.\nThe threshold for being an outlier is 0, which ensures a\ncompatibility with other outlier detection algorithms."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the decision function of the given observations.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\ndecision : ndarray of shape (n_samples,)\n    Decision function of the samples.\n    It is equal to the shifted Mahalanobis distances.\n    The threshold for being an outlier is 0, which ensures a\n    compatibility with other outlier detection algorithms."
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data matrix."
                }
              ],
              "results": [
                {
                  "name": "negative_mahal_distances",
                  "type": null,
                  "description": "Opposite of the Mahalanobis distances."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the negative Mahalanobis distances.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\nnegative_mahal_distances : array-like of shape (n_samples,)\n    Opposite of the Mahalanobis distances."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data matrix."
                }
              ],
              "results": [
                {
                  "name": "is_inlier",
                  "type": null,
                  "description": "Returns -1 for anomalies/outliers and +1 for inliers."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the labels (1 inlier, -1 outlier) of X according to the\nfitted model.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\nis_inlier : ndarray of shape (n_samples,)\n    Returns -1 for anomalies/outliers and +1 for inliers."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True labels for X."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "Mean accuracy of self.predict(X) w.r.t. y."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True labels for X.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    Mean accuracy of self.predict(X) w.r.t. y."
            }
          ],
          "fullDocstring": "An object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the :ref:`User Guide <outlier_detection>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, the support of robust location and covariance estimates\n    is computed, and a covariance estimate is recomputed from it,\n    without centering the data.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, the robust location and covariance are directly computed\n    with the FastMCD algorithm without additional treatment.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. If None, the minimum value of support_fraction will\n    be used within the algorithm: `[n_sample + n_features + 1] / 2`.\n    Range is (0, 1).\n\ncontamination : float, default=0.1\n    The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. Range is (0, 0.5).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling\n    the data. Pass an int for reproducible results across multiple function\n    calls. See :term: `Glossary <random_state>`.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated robust location.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated robust covariance matrix.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nsupport_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute the\n    robust estimates of location and shape.\n\noffset_ : float\n    Offset used to define the decision function from the raw scores.\n    We have the relation: ``decision_function = score_samples - offset_``.\n    The offset depends on the contamination parameter and is defined in\n    such a way we obtain the expected number of outliers (samples with\n    decision function < 0) in training.\n\n    .. versionadded:: 0.20\n\nraw_location_ : ndarray of shape (n_features,)\n    The raw robust estimated location before correction and re-weighting.\n\nraw_covariance_ : ndarray of shape (n_features, n_features)\n    The raw robust estimated covariance before correction and re-weighting.\n\nraw_support_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute\n    the raw robust estimates of location and shape, before correction\n    and re-weighting.\n\ndist_ : ndarray of shape (n_samples,)\n    Mahalanobis distances of the training set (on which :meth:`fit` is\n    called) observations.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import EllipticEnvelope\n>>> true_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n...                                                  cov=true_cov,\n...                                                  size=500)\n>>> cov = EllipticEnvelope(random_state=0).fit(X)\n>>> # predict returns 1 for an inlier and -1 for an outlier\n>>> cov.predict([[0, 0],\n...              [3, 3]])\narray([ 1, -1])\n>>> cov.covariance_\narray([[0.7411..., 0.2535...],\n       [0.2535..., 0.3053...]])\n>>> cov.location_\narray([0.0813... , 0.0427...])\n\nSee Also\n--------\nEmpiricalCovariance, MinCovDet\n\nNotes\n-----\nOutlier detection from covariance estimation may break or not\nperform well in high-dimensional settings. In particular, one will\nalways take care to work with ``n_samples > n_features ** 2``.\n\nReferences\n----------\n.. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n   minimum covariance determinant estimator\" Technometrics 41(3), 212\n   (1999)"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.covariance._empirical_covariance",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_distances",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "fast_logdet",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "EmpiricalCovariance",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "get_precision",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "precision_",
                  "type": null,
                  "description": "The precision matrix associated to the current covariance object."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Getter for the precision matrix.\n\nReturns\n-------\nprecision_ : array-like of shape (n_features, n_features)\n    The precision matrix associated to the current covariance object."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fits the Maximum Likelihood Estimator covariance model\naccording to the given training data and parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n  Training data, where n_samples is the number of samples and\n  n_features is the number of features.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X_test",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test data of which we compute the likelihood, where n_samples is\nthe number of samples and n_features is the number of features.\nX_test is assumed to be drawn from the same distribution than\nthe data used in fit (including centering)."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "res",
                  "type": "float",
                  "description": "The likelihood of the data set with `self.covariance_` as an\nestimator of its covariance matrix."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n\nParameters\n----------\nX_test : array-like of shape (n_samples, n_features)\n    Test data of which we compute the likelihood, where n_samples is\n    the number of samples and n_features is the number of features.\n    X_test is assumed to be drawn from the same distribution than\n    the data used in fit (including centering).\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nres : float\n    The likelihood of the data set with `self.covariance_` as an\n    estimator of its covariance matrix."
            },
            {
              "name": "error_norm",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "comp_cov",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The covariance to compare with."
                },
                {
                  "name": "norm",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "frobenius",
                  "limitation": null,
                  "ignored": false,
                  "description": "The type of norm used to compute the error. Available error types:\n- 'frobenius' (default): sqrt(tr(A^t.A))\n- 'spectral': sqrt(max(eigenvalues(A^t.A))\nwhere A is the error ``(comp_cov - self.covariance_)``."
                },
                {
                  "name": "scaling",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True (default), the squared error norm is divided by n_features.\nIf False, the squared error norm is not rescaled."
                },
                {
                  "name": "squared",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Whether to compute the squared error norm or the error norm.\nIf True (default), the squared error norm is returned.\nIf False, the error norm is returned."
                }
              ],
              "results": [
                {
                  "name": "result",
                  "type": "float",
                  "description": "The Mean Squared Error (in the sense of the Frobenius norm) between\n`self` and `comp_cov` covariance estimators."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n\nParameters\n----------\ncomp_cov : array-like of shape (n_features, n_features)\n    The covariance to compare with.\n\nnorm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n    The type of norm used to compute the error. Available error types:\n    - 'frobenius' (default): sqrt(tr(A^t.A))\n    - 'spectral': sqrt(max(eigenvalues(A^t.A))\n    where A is the error ``(comp_cov - self.covariance_)``.\n\nscaling : bool, default=True\n    If True (default), the squared error norm is divided by n_features.\n    If False, the squared error norm is not rescaled.\n\nsquared : bool, default=True\n    Whether to compute the squared error norm or the error norm.\n    If True (default), the squared error norm is returned.\n    If False, the error norm is returned.\n\nReturns\n-------\nresult : float\n    The Mean Squared Error (in the sense of the Frobenius norm) between\n    `self` and `comp_cov` covariance estimators."
            },
            {
              "name": "mahalanobis",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The observations, the Mahalanobis distances of the which we\ncompute. Observations are assumed to be drawn from the same\ndistribution than the data used in fit."
                }
              ],
              "results": [
                {
                  "name": "dist",
                  "type": null,
                  "description": "Squared Mahalanobis distances of the observations."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Computes the squared Mahalanobis distances of given observations.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The observations, the Mahalanobis distances of the which we\n    compute. Observations are assumed to be drawn from the same\n    distribution than the data used in fit.\n\nReturns\n-------\ndist : ndarray of shape (n_samples,)\n    Squared Mahalanobis distances of the observations."
            }
          ],
          "fullDocstring": "Maximum likelihood covariance estimator\n\nRead more in the :ref:`User Guide <covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specifies if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, data are not centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False (default), data are centered before computation.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo-inverse matrix.\n    (stored only if store_precision is True)\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import EmpiricalCovariance\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                             cov=real_cov,\n...                             size=500)\n>>> cov = EmpiricalCovariance().fit(X)\n>>> cov.covariance_\narray([[0.7569..., 0.2818...],\n       [0.2818..., 0.3928...]])\n>>> cov.location_\narray([0.0622..., 0.0193...])"
        }
      ],
      "functions": [
        {
          "name": "empirical_covariance",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data from which to compute the covariance estimate"
            }
          ],
          "results": [
            {
              "name": "covariance",
              "type": null,
              "description": "Empirical covariance (Maximum Likelihood Estimator)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the Maximum likelihood covariance estimator\n\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data will be centered before computation.\n\nReturns\n-------\ncovariance : ndarray of shape (n_features, n_features)\n    Empirical covariance (Maximum Likelihood Estimator).\n\nExamples\n--------\n>>> from sklearn.covariance import empirical_covariance\n>>> X = [[1,1,1],[1,1,1],[1,1,1],\n...      [0,0,0],[0,0,0],[0,0,0]]\n>>> empirical_covariance(X)\narray([[0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25]])"
        },
        {
          "name": "log_likelihood",
          "decorators": [],
          "parameters": [
            {
              "name": "emp_cov",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Maximum Likelihood Estimator of covariance."
            },
            {
              "name": "precision",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The precision matrix of the covariance model to be tested."
            }
          ],
          "results": [
            {
              "name": "log_likelihood_",
              "type": "float",
              "description": "Sample mean of the log-likelihood."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the sample mean of the log_likelihood under a covariance model\n\ncomputes the empirical expected log-likelihood (accounting for the\nnormalization terms and scaling), allowing for universal comparison (beyond\nthis software package)\n\nParameters\n----------\nemp_cov : ndarray of shape (n_features, n_features)\n    Maximum Likelihood Estimator of covariance.\n\nprecision : ndarray of shape (n_features, n_features)\n    The precision matrix of the covariance model to be tested.\n\nReturns\n-------\nlog_likelihood_ : float\n    Sample mean of the log-likelihood."
        }
      ]
    },
    {
      "name": "sklearn.covariance._graph_lasso",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "operator",
          "alias": null
        },
        {
          "module": "sys",
          "alias": null
        },
        {
          "module": "time",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections.abc",
          "declaration": "Sequence",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "EmpiricalCovariance",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "empirical_covariance",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "log_likelihood",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "_cd_fast",
          "alias": "cd_fast"
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "lars_path_gram",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "cross_val_score",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_random_state",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "GraphicalLasso",
          "decorators": [],
          "superclasses": [
            "EmpiricalCovariance"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data from which to compute the covariance estimate"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fits the GraphicalLasso model to X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "Sparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLasso has been renamed to GraphicalLasso\n\nParameters\n----------\nalpha : float, default=0.01\n    The regularization parameter: the higher alpha, the more\n    regularization, the sparser the inverse covariance.\n    Range is (0, inf].\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where p > n. Elsewhere prefer cd\n    which is more numerically stable.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. Range is (0, inf].\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. Range is (0, inf].\n\nmax_iter : int, default=100\n    The maximum number of iterations.\n\nverbose : bool, default=False\n    If verbose is True, the objective function and dual gap are\n    plotted at each iteration.\n\nassume_centered : bool, default=False\n    If True, data are not centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data are centered before computation.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n\nn_iter_ : int\n    Number of iterations run.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import GraphicalLasso\n>>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n...                      [0.0, 0.4, 0.0, 0.0],\n...                      [0.2, 0.0, 0.3, 0.1],\n...                      [0.0, 0.0, 0.1, 0.7]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n...                                   cov=true_cov,\n...                                   size=200)\n>>> cov = GraphicalLasso().fit(X)\n>>> np.around(cov.covariance_, decimals=3)\narray([[0.816, 0.049, 0.218, 0.019],\n       [0.049, 0.364, 0.017, 0.034],\n       [0.218, 0.017, 0.322, 0.093],\n       [0.019, 0.034, 0.093, 0.69 ]])\n>>> np.around(cov.location_, decimals=3)\narray([0.073, 0.04 , 0.038, 0.143])\n\nSee Also\n--------\ngraphical_lasso, GraphicalLassoCV"
        },
        {
          "name": "GraphicalLassoCV",
          "decorators": [],
          "superclasses": [
            "GraphicalLasso"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data from which to compute the covariance estimate"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fits the GraphicalLasso covariance model to X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "grid_scores_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "cv_alphas_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLassoCV has been renamed to GraphicalLassoCV\n\nParameters\n----------\nalphas : int or array-like of shape (n_alphas,), dtype=float, default=4\n    If an integer is given, it fixes the number of points on the\n    grids of alpha to be used. If a list is given, it gives the\n    grid to be used. See the notes in the class docstring for\n    more details. Range is (0, inf] when floats given.\n\nn_refinements : int, default=4\n    The number of times the grid is refined. Not used if explicit\n    values of alphas are passed. Range is [1, inf).\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.20\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. Range is (0, inf].\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. Range is (0, inf].\n\nmax_iter : int, default=100\n    Maximum number of iterations.\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where number of features is greater\n    than number of samples. Elsewhere prefer cd which is more numerically\n    stable.\n\nn_jobs : int, default=None\n    number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\nverbose : bool, default=False\n    If verbose is True, the objective function and duality gap are\n    printed at each iteration.\n\nassume_centered : bool, default=False\n    If True, data are not centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data are centered before computation.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated precision matrix (inverse covariance).\n\nalpha_ : float\n    Penalization parameter selected.\n\ncv_alphas_ : list of shape (n_alphas,), dtype=float\n    All penalization parameters explored.\n\n    .. deprecated:: 0.24\n        The `cv_alphas_` attribute is deprecated in version 0.24 in favor\n        of `cv_results_['alphas']` and will be removed in version\n        1.1 (renaming of 0.26).\n\ngrid_scores_ : ndarray of shape (n_alphas, n_folds)\n    Log-likelihood score on left-out data across folds.\n\n    .. deprecated:: 0.24\n        The `grid_scores_` attribute is deprecated in version 0.24 in favor\n        of `cv_results_` and will be removed in version\n        1.1 (renaming of 0.26).\n\ncv_results_ : dict of ndarrays\n    A dict with keys:\n\n    alphas : ndarray of shape (n_alphas,)\n        All penalization parameters explored.\n\n    split(k)_score : ndarray of shape (n_alphas,)\n        Log-likelihood score on left-out data across (k)th fold.\n\n    mean_score : ndarray of shape (n_alphas,)\n        Mean of scores over the folds.\n\n    std_score : ndarray of shape (n_alphas,)\n        Standard deviation of scores over the folds.\n\n    .. versionadded:: 0.24\n\nn_iter_ : int\n    Number of iterations run for the optimal alpha.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import GraphicalLassoCV\n>>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n...                      [0.0, 0.4, 0.0, 0.0],\n...                      [0.2, 0.0, 0.3, 0.1],\n...                      [0.0, 0.0, 0.1, 0.7]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n...                                   cov=true_cov,\n...                                   size=200)\n>>> cov = GraphicalLassoCV().fit(X)\n>>> np.around(cov.covariance_, decimals=3)\narray([[0.816, 0.051, 0.22 , 0.017],\n       [0.051, 0.364, 0.018, 0.036],\n       [0.22 , 0.018, 0.322, 0.094],\n       [0.017, 0.036, 0.094, 0.69 ]])\n>>> np.around(cov.location_, decimals=3)\narray([0.073, 0.04 , 0.038, 0.143])\n\nSee Also\n--------\ngraphical_lasso, GraphicalLasso\n\nNotes\n-----\nThe search for the optimal penalization parameter (alpha) is done on an\niteratively refined grid: first the cross-validated scores on a grid are\ncomputed, then a new refined grid is centered around the maximum, and so\non.\n\nOne of the challenges which is faced here is that the solvers can\nfail to converge to a well-conditioned estimate. The corresponding\nvalues of alpha then come out as missing values, but the optimum may\nbe close to these missing values."
        }
      ],
      "functions": [
        {
          "name": "alpha_max",
          "decorators": [],
          "parameters": [
            {
              "name": "emp_cov",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The sample covariance matrix."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Find the maximum alpha for which there are some non-zeros off-diagonal.\n\nParameters\n----------\nemp_cov : ndarray of shape (n_features, n_features)\n    The sample covariance matrix.\n\nNotes\n-----\nThis results from the bound for the all the Lasso that are solved\nin GraphicalLasso: each time, the row of cov corresponds to Xy. As the\nbound for alpha is given by `max(abs(Xy))`, the result follows."
        },
        {
          "name": "graphical_lasso",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "emp_cov",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Empirical covariance from which to compute the covariance estimate."
            },
            {
              "name": "alpha",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The regularization parameter: the higher alpha, the more\nregularization, the sparser the inverse covariance.\nRange is (0, inf]."
            }
          ],
          "results": [
            {
              "name": "covariance",
              "type": null,
              "description": "The estimated covariance matrix."
            },
            {
              "name": "precision",
              "type": null,
              "description": "The estimated (sparse) precision matrix."
            },
            {
              "name": "costs",
              "type": null,
              "description": "The list of values of the objective function and the dual gap at\neach iteration. Returned only if return_costs is True."
            },
            {
              "name": "n_iter",
              "type": "int",
              "description": "Number of iterations. Returned only if `return_n_iter` is set to True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "l1-penalized covariance estimator\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    graph_lasso has been renamed to graphical_lasso\n\nParameters\n----------\nemp_cov : ndarray of shape (n_features, n_features)\n    Empirical covariance from which to compute the covariance estimate.\n\nalpha : float\n    The regularization parameter: the higher alpha, the more\n    regularization, the sparser the inverse covariance.\n    Range is (0, inf].\n\ncov_init : array of shape (n_features, n_features), default=None\n    The initial guess for the covariance. If None, then the empirical\n    covariance is used.\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where p > n. Elsewhere prefer cd\n    which is more numerically stable.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. Range is (0, inf].\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. Range is (0, inf].\n\nmax_iter : int, default=100\n    The maximum number of iterations.\n\nverbose : bool, default=False\n    If verbose is True, the objective function and dual gap are\n    printed at each iteration.\n\nreturn_costs : bool, default=Flase\n    If return_costs is True, the objective function and dual gap\n    at each iteration are returned.\n\neps : float, default=eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Default is `np.finfo(np.float64).eps`.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\ncovariance : ndarray of shape (n_features, n_features)\n    The estimated covariance matrix.\n\nprecision : ndarray of shape (n_features, n_features)\n    The estimated (sparse) precision matrix.\n\ncosts : list of (objective, dual_gap) pairs\n    The list of values of the objective function and the dual gap at\n    each iteration. Returned only if return_costs is True.\n\nn_iter : int\n    Number of iterations. Returned only if `return_n_iter` is set to True.\n\nSee Also\n--------\nGraphicalLasso, GraphicalLassoCV\n\nNotes\n-----\nThe algorithm employed to solve this problem is the GLasso algorithm,\nfrom the Friedman 2008 Biostatistics paper. It is the same algorithm\nas in the R `glasso` package.\n\nOne possible difference with the `glasso` R package is that the\ndiagonal coefficients are not penalized."
        },
        {
          "name": "graphical_lasso_path",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data from which to compute the covariance estimate."
            },
            {
              "name": "alphas",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The list of regularization parameters, decreasing order."
            },
            {
              "name": "cov_init",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The initial guess for the covariance."
            },
            {
              "name": "X_test",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Optional test matrix to measure generalisation error."
            },
            {
              "name": "mode",
              "type": "Any",
              "hasDefault": true,
              "default": "cd",
              "limitation": null,
              "ignored": false,
              "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for\nvery sparse underlying graphs, where p > n. Elsewhere prefer cd\nwhich is more numerically stable."
            },
            {
              "name": "tol",
              "type": "Any",
              "hasDefault": true,
              "default": "0.0001",
              "limitation": null,
              "ignored": false,
              "description": "The tolerance to declare convergence: if the dual gap goes below\nthis value, iterations are stopped. The tolerance must be a positive\nnumber."
            },
            {
              "name": "enet_tol",
              "type": "Any",
              "hasDefault": true,
              "default": "0.0001",
              "limitation": null,
              "ignored": false,
              "description": "The tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction\nfor a given column update, not of the overall parameter estimate. Only\nused for mode='cd'. The tolerance must be a positive number."
            },
            {
              "name": "max_iter",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The maximum number of iterations. This parameter should be a strictly\npositive integer."
            },
            {
              "name": "verbose",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "The higher the verbosity flag, the more information is printed\nduring the fitting."
            }
          ],
          "results": [
            {
              "name": "covariances_",
              "type": null,
              "description": "The estimated covariance matrices."
            },
            {
              "name": "precisions_",
              "type": null,
              "description": "The estimated (sparse) precision matrices."
            },
            {
              "name": "scores_",
              "type": null,
              "description": "The generalisation error (log-likelihood) on the test data.\nReturned only if test data is passed."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "l1-penalized covariance estimator along a path of decreasing alphas\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate.\n\nalphas : array-like of shape (n_alphas,)\n    The list of regularization parameters, decreasing order.\n\ncov_init : array of shape (n_features, n_features), default=None\n    The initial guess for the covariance.\n\nX_test : array of shape (n_test_samples, n_features), default=None\n    Optional test matrix to measure generalisation error.\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where p > n. Elsewhere prefer cd\n    which is more numerically stable.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. The tolerance must be a positive\n    number.\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. The tolerance must be a positive number.\n\nmax_iter : int, default=100\n    The maximum number of iterations. This parameter should be a strictly\n    positive integer.\n\nverbose : int or bool, default=False\n    The higher the verbosity flag, the more information is printed\n    during the fitting.\n\nReturns\n-------\ncovariances_ : list of shape (n_alphas,) of ndarray of shape             (n_features, n_features)\n    The estimated covariance matrices.\n\nprecisions_ : list of shape (n_alphas,) of ndarray of shape             (n_features, n_features)\n    The estimated (sparse) precision matrices.\n\nscores_ : list of shape (n_alphas,), dtype=float\n    The generalisation error (log-likelihood) on the test data.\n    Returned only if test data is passed."
        }
      ]
    },
    {
      "name": "sklearn.covariance._robust_covariance",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy.stats",
          "declaration": "chi2",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "EmpiricalCovariance",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "empirical_covariance",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "fast_logdet",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "MinCovDet",
          "decorators": [],
          "superclasses": [
            "EmpiricalCovariance"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where `n_samples` is the number of samples\nand `n_features` is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fits a Minimum Covariance Determinant with the FastMCD algorithm.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\n\ny: Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "correct_covariance",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "data",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data matrix, with p features and n samples.\nThe data set must be the one which was used to compute\nthe raw estimates."
                }
              ],
              "results": [
                {
                  "name": "covariance_corrected",
                  "type": null,
                  "description": "Corrected robust covariance estimate."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested\nby Rousseeuw and Van Driessen in [RVD]_.\n\nParameters\n----------\ndata : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n    The data set must be the one which was used to compute\n    the raw estimates.\n\nReturns\n-------\ncovariance_corrected : ndarray of shape (n_features, n_features)\n    Corrected robust covariance estimate.\n\nReferences\n----------\n\n.. [RVD] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS"
            },
            {
              "name": "reweight_covariance",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "data",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data matrix, with p features and n samples.\nThe data set must be the one which was used to compute\nthe raw estimates."
                }
              ],
              "results": [
                {
                  "name": "location_reweighted",
                  "type": null,
                  "description": "Re-weighted robust location estimate."
                },
                {
                  "name": "covariance_reweighted",
                  "type": null,
                  "description": "Re-weighted robust covariance estimate."
                },
                {
                  "name": "support_reweighted",
                  "type": null,
                  "description": "A mask of the observations that have been used to compute\nthe re-weighted robust location and covariance estimates."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Re-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw's method (equivalent to\ndeleting outlying observations from the data set before\ncomputing location and covariance estimates) described\nin [RVDriessen]_.\n\nParameters\n----------\ndata : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n    The data set must be the one which was used to compute\n    the raw estimates.\n\nReturns\n-------\nlocation_reweighted : ndarray of shape (n_features,)\n    Re-weighted robust location estimate.\n\ncovariance_reweighted : ndarray of shape (n_features, n_features)\n    Re-weighted robust covariance estimate.\n\nsupport_reweighted : ndarray of shape (n_samples,), dtype=bool\n    A mask of the observations that have been used to compute\n    the re-weighted robust location and covariance estimates.\n\nReferences\n----------\n\n.. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS"
            }
          ],
          "fullDocstring": "Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied\non Gaussian-distributed data, but could still be relevant on data\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\nlikely to fail in such a case).\nOne should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, the support of the robust location and the covariance\n    estimates is computed, and a covariance estimate is recomputed from\n    it, without centering the data.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, the robust location and covariance are directly computed\n    with the FastMCD algorithm without additional treatment.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. Default is None, which implies that the minimum\n    value of support_fraction will be used within the algorithm:\n    `(n_sample + n_features + 1) / 2`. The parameter must be in the range\n    (0, 1).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nAttributes\n----------\nraw_location_ : ndarray of shape (n_features,)\n    The raw robust estimated location before correction and re-weighting.\n\nraw_covariance_ : ndarray of shape (n_features, n_features)\n    The raw robust estimated covariance before correction and re-weighting.\n\nraw_support_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute\n    the raw robust estimates of location and shape, before correction\n    and re-weighting.\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated robust location.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated robust covariance matrix.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nsupport_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute\n    the robust estimates of location and shape.\n\ndist_ : ndarray of shape (n_samples,)\n    Mahalanobis distances of the training set (on which :meth:`fit` is\n    called) observations.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import MinCovDet\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=500)\n>>> cov = MinCovDet(random_state=0).fit(X)\n>>> cov.covariance_\narray([[0.7411..., 0.2535...],\n       [0.2535..., 0.3053...]])\n>>> cov.location_\narray([0.0813... , 0.0427...])\n\nReferences\n----------\n\n.. [Rouseeuw1984] P. J. Rousseeuw. Least median of squares regression.\n    J. Am Stat Ass, 79:871, 1984.\n.. [Rousseeuw] A Fast Algorithm for the Minimum Covariance Determinant\n    Estimator, 1999, American Statistical Association and the American\n    Society for Quality, TECHNOMETRICS\n.. [ButlerDavies] R. W. Butler, P. L. Davies and M. Jhun,\n    Asymptotics For The Minimum Covariance Determinant Estimator,\n    The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400"
        }
      ],
      "functions": [
        {
          "name": "c_step",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data set in which we look for the n_support observations whose\nscatter matrix has minimum determinant."
            },
            {
              "name": "n_support",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of observations to compute the robust estimates of location\nand covariance from. This parameter must be greater than\n`n_samples / 2`."
            },
            {
              "name": "remaining_iterations",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of iterations to perform.\nAccording to [Rouseeuw1999]_, two iterations are sufficient to get\nclose to the minimum, and we never need more than 30 to reach\nconvergence."
            },
            {
              "name": "initial_estimates",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Initial estimates of location and shape from which to run the c_step\nprocedure:\n- initial_estimates[0]: an initial location estimate\n- initial_estimates[1]: an initial covariance estimate"
            },
            {
              "name": "verbose",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Verbose mode."
            },
            {
              "name": "cov_computation_method",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The function which will be used to compute the covariance.\nMust return array of shape (n_features, n_features)."
            },
            {
              "name": "random_state",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Determines the pseudo random number generator for shuffling the data.\nPass an int for reproducible results across multiple function calls.\nSee :term: `Glossary <random_state>`."
            }
          ],
          "results": [
            {
              "name": "location",
              "type": null,
              "description": "Robust location estimates."
            },
            {
              "name": "covariance",
              "type": null,
              "description": "Robust covariance estimates."
            },
            {
              "name": "support",
              "type": null,
              "description": "A mask for the `n_support` observations whose scatter matrix has\nminimum determinant."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "C_step procedure described in [Rouseeuw1984]_ aiming at computing MCD.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data set in which we look for the n_support observations whose\n    scatter matrix has minimum determinant.\n\nn_support : int\n    Number of observations to compute the robust estimates of location\n    and covariance from. This parameter must be greater than\n    `n_samples / 2`.\n\nremaining_iterations : int, default=30\n    Number of iterations to perform.\n    According to [Rouseeuw1999]_, two iterations are sufficient to get\n    close to the minimum, and we never need more than 30 to reach\n    convergence.\n\ninitial_estimates : tuple of shape (2,), default=None\n    Initial estimates of location and shape from which to run the c_step\n    procedure:\n    - initial_estimates[0]: an initial location estimate\n    - initial_estimates[1]: an initial covariance estimate\n\nverbose : bool, default=False\n    Verbose mode.\n\ncov_computation_method : callable,             default=:func:`sklearn.covariance.empirical_covariance`\n    The function which will be used to compute the covariance.\n    Must return array of shape (n_features, n_features).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nReturns\n-------\nlocation : ndarray of shape (n_features,)\n    Robust location estimates.\n\ncovariance : ndarray of shape (n_features, n_features)\n    Robust covariance estimates.\n\nsupport : ndarray of shape (n_samples,)\n    A mask for the `n_support` observations whose scatter matrix has\n    minimum determinant.\n\nReferences\n----------\n.. [Rouseeuw1999] A Fast Algorithm for the Minimum Covariance Determinant\n    Estimator, 1999, American Statistical Association and the American\n    Society for Quality, TECHNOMETRICS"
        },
        {
          "name": "fast_mcd",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data matrix, with p features and n samples."
            },
            {
              "name": "support_fraction",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The proportion of points to be included in the support of the raw\nMCD estimate. Default is `None`, which implies that the minimum\nvalue of `support_fraction` will be used within the algorithm:\n`(n_sample + n_features + 1) / 2`. This parameter must be in the\nrange (0, 1)."
            },
            {
              "name": "cov_computation_method",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The function which will be used to compute the covariance.\nMust return an array of shape (n_features, n_features)."
            },
            {
              "name": "random_state",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Determines the pseudo random number generator for shuffling the data.\nPass an int for reproducible results across multiple function calls.\nSee :term: `Glossary <random_state>`."
            }
          ],
          "results": [
            {
              "name": "location",
              "type": null,
              "description": "Robust location of the data."
            },
            {
              "name": "covariance",
              "type": null,
              "description": "Robust covariance of the features."
            },
            {
              "name": "support",
              "type": null,
              "description": "A mask of the observations that have been used to compute\nthe robust location and covariance estimates of the data set."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Estimates the Minimum Covariance Determinant matrix.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. Default is `None`, which implies that the minimum\n    value of `support_fraction` will be used within the algorithm:\n    `(n_sample + n_features + 1) / 2`. This parameter must be in the\n    range (0, 1).\n\ncov_computation_method : callable,             default=:func:`sklearn.covariance.empirical_covariance`\n    The function which will be used to compute the covariance.\n    Must return an array of shape (n_features, n_features).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nReturns\n-------\nlocation : ndarray of shape (n_features,)\n    Robust location of the data.\n\ncovariance : ndarray of shape (n_features, n_features)\n    Robust covariance of the features.\n\nsupport : ndarray of shape (n_samples,), dtype=bool\n    A mask of the observations that have been used to compute\n    the robust location and covariance estimates of the data set.\n\nNotes\n-----\nThe FastMCD algorithm has been introduced by Rousseuw and Van Driessen\nin \"A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n1999, American Statistical Association and the American Society\nfor Quality, TECHNOMETRICS\".\nThe principle is to compute robust estimates and random subsets before\npooling them into a larger subsets, and finally into the full data set.\nDepending on the size of the initial sample, we have one, two or three\nsuch computation levels.\n\nNote that only raw estimates are returned. If one is interested in\nthe correction and reweighting steps described in [RouseeuwVan]_,\nsee the MinCovDet object.\n\nReferences\n----------\n\n.. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS\n\n.. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,\n    Asymptotics For The Minimum Covariance Determinant Estimator,\n    The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400"
        },
        {
          "name": "select_candidates",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data (sub)set in which we look for the n_support purest observations."
            },
            {
              "name": "n_support",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples the pure data set must contain.\nThis parameter must be in the range `[(n + p + 1)/2] < n_support < n`."
            },
            {
              "name": "n_trials",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of different initial sets of observations from which to\nrun the algorithm. This parameter should be a strictly positive\ninteger.\nInstead of giving a number of trials to perform, one can provide a\nlist of initial estimates that will be used to iteratively run\nc_step procedures. In this case:\n- n_trials[0]: array-like, shape (n_trials, n_features)\n  is the list of `n_trials` initial location estimates\n- n_trials[1]: array-like, shape (n_trials, n_features, n_features)\n  is the list of `n_trials` initial covariances estimates"
            },
            {
              "name": "select",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of best candidates results to return. This parameter must be\na strictly positive integer."
            },
            {
              "name": "n_iter",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Maximum number of iterations for the c_step procedure.\n(2 is enough to be close to the final solution. \"Never\" exceeds 20).\nThis parameter must be a strictly positive integer."
            },
            {
              "name": "verbose",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Control the output verbosity."
            },
            {
              "name": "cov_computation_method",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The function which will be used to compute the covariance.\nMust return an array of shape (n_features, n_features)."
            },
            {
              "name": "random_state",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Determines the pseudo random number generator for shuffling the data.\nPass an int for reproducible results across multiple function calls.\nSee :term: `Glossary <random_state>`."
            }
          ],
          "results": [
            {
              "name": "best_locations",
              "type": null,
              "description": "The `select` location estimates computed from the `select` best\nsupports found in the data set (`X`)."
            },
            {
              "name": "best_covariances",
              "type": null,
              "description": "The `select` covariance estimates computed from the `select`\nbest supports found in the data set (`X`)."
            },
            {
              "name": "best_supports",
              "type": null,
              "description": "The `select` best supports found in the data set (`X`)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Finds the best pure subset of observations to compute MCD from it.\n\nThe purpose of this function is to find the best sets of n_support\nobservations with respect to a minimization of their covariance\nmatrix determinant. Equivalently, it removes n_samples-n_support\nobservations to construct what we call a pure data set (i.e. not\ncontaining outliers). The list of the observations of the pure\ndata set is referred to as the `support`.\n\nStarting from a random support, the pure data set is found by the\nc_step procedure introduced by Rousseeuw and Van Driessen in\n[RV]_.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data (sub)set in which we look for the n_support purest observations.\n\nn_support : int\n    The number of samples the pure data set must contain.\n    This parameter must be in the range `[(n + p + 1)/2] < n_support < n`.\n\nn_trials : int or tuple of shape (2,)\n    Number of different initial sets of observations from which to\n    run the algorithm. This parameter should be a strictly positive\n    integer.\n    Instead of giving a number of trials to perform, one can provide a\n    list of initial estimates that will be used to iteratively run\n    c_step procedures. In this case:\n    - n_trials[0]: array-like, shape (n_trials, n_features)\n      is the list of `n_trials` initial location estimates\n    - n_trials[1]: array-like, shape (n_trials, n_features, n_features)\n      is the list of `n_trials` initial covariances estimates\n\nselect : int, default=1\n    Number of best candidates results to return. This parameter must be\n    a strictly positive integer.\n\nn_iter : int, default=30\n    Maximum number of iterations for the c_step procedure.\n    (2 is enough to be close to the final solution. \"Never\" exceeds 20).\n    This parameter must be a strictly positive integer.\n\nverbose : bool, default=False\n    Control the output verbosity.\n\ncov_computation_method : callable,             default=:func:`sklearn.covariance.empirical_covariance`\n    The function which will be used to compute the covariance.\n    Must return an array of shape (n_features, n_features).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nSee Also\n---------\nc_step\n\nReturns\n-------\nbest_locations : ndarray of shape (select, n_features)\n    The `select` location estimates computed from the `select` best\n    supports found in the data set (`X`).\n\nbest_covariances : ndarray of shape (select, n_features, n_features)\n    The `select` covariance estimates computed from the `select`\n    best supports found in the data set (`X`).\n\nbest_supports : ndarray of shape (select, n_samples)\n    The `select` best supports found in the data set (`X`).\n\nReferences\n----------\n.. [RV] A Fast Algorithm for the Minimum Covariance Determinant\n    Estimator, 1999, American Statistical Association and the American\n    Society for Quality, TECHNOMETRICS"
        }
      ]
    },
    {
      "name": "sklearn.covariance._shrunk_covariance",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.covariance",
          "declaration": "EmpiricalCovariance",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "empirical_covariance",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "LedoitWolf",
          "decorators": [],
          "superclasses": [
            "EmpiricalCovariance"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where `n_samples` is the number of samples\nand `n_features` is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the Ledoit-Wolf shrunk covariance model according to the given\ntraining data and parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "LedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage\ncoefficient is computed using O. Ledoit and M. Wolf's formula as\ndescribed in \"A Well-Conditioned Estimator for Large-Dimensional\nCovariance Matrices\", Ledoit and Wolf, Journal of Multivariate\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False (default), data will be centered before computation.\n\nblock_size : int, default=1000\n    Size of blocks into which the covariance matrix will be split\n    during its Ledoit-Wolf estimation. This is purely a memory\n    optimization and does not affect results.\n\nAttributes\n----------\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix.\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nshrinkage_ : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate. Range is [0, 1].\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import LedoitWolf\n>>> real_cov = np.array([[.4, .2],\n...                      [.2, .8]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=50)\n>>> cov = LedoitWolf().fit(X)\n>>> cov.covariance_\narray([[0.4406..., 0.1616...],\n       [0.1616..., 0.8022...]])\n>>> cov.location_\narray([ 0.0595... , -0.0075...])\n\nNotes\n-----\nThe regularised covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\nand shrinkage is given by the Ledoit and Wolf formula (see References)\n\nReferences\n----------\n\"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\",\nLedoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\nFebruary 2004, pages 365-411."
        },
        {
          "name": "OAS",
          "decorators": [],
          "superclasses": [
            "EmpiricalCovariance"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where `n_samples` is the number of samples\nand `n_features` is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the Oracle Approximating Shrinkage covariance model\naccording to the given training data and parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "Oracle Approximating Shrinkage Estimator\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nOAS is a particular form of shrinkage described in\n\"Shrinkage Algorithms for MMSE Covariance Estimation\"\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\nThe formula used here does not correspond to the one given in the\narticle. In the original article, formula (23) states that 2/p is\nmultiplied by Trace(cov*cov) in both the numerator and denominator, but\nthis operation is omitted because for a large p, the value of 2/p is\nso small that it doesn't affect the value of the estimator.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False (default), data will be centered before computation.\n\nAttributes\n----------\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix.\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nshrinkage_ : float\n  coefficient in the convex combination used for the computation\n  of the shrunk estimate. Range is [0, 1].\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import OAS\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                             cov=real_cov,\n...                             size=500)\n>>> oas = OAS().fit(X)\n>>> oas.covariance_\narray([[0.7533..., 0.2763...],\n       [0.2763..., 0.3964...]])\n>>> oas.precision_\narray([[ 1.7833..., -1.2431... ],\n       [-1.2431...,  3.3889...]])\n>>> oas.shrinkage_\n0.0195...\n\nNotes\n-----\nThe regularised covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\nand shrinkage is given by the OAS formula (see References)\n\nReferences\n----------\n\"Shrinkage Algorithms for MMSE Covariance Estimation\"\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010."
        },
        {
          "name": "ShrunkCovariance",
          "decorators": [],
          "superclasses": [
            "EmpiricalCovariance"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the shrunk covariance model according to the given training data\nand parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny: Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "Covariance estimator with shrinkage\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data will be centered before computation.\n\nshrinkage : float, default=0.1\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate. Range is [0, 1].\n\nAttributes\n----------\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import ShrunkCovariance\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=500)\n>>> cov = ShrunkCovariance().fit(X)\n>>> cov.covariance_\narray([[0.7387..., 0.2536...],\n       [0.2536..., 0.4110...]])\n>>> cov.location_\narray([0.0622..., 0.0193...])\n\nNotes\n-----\nThe regularized covariance is given by:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features"
        }
      ],
      "functions": [
        {
          "name": "ledoit_wolf",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data from which to compute the covariance estimate"
            }
          ],
          "results": [
            {
              "name": "shrunk_cov",
              "type": null,
              "description": "Shrunk covariance."
            },
            {
              "name": "shrinkage",
              "type": "float",
              "description": "Coefficient in the convex combination used for the computation\nof the shrunk estimate."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, data will be centered before computation.\n\nblock_size : int, default=1000\n    Size of blocks into which the covariance matrix will be split.\n    This is purely a memory optimization and does not affect results.\n\nReturns\n-------\nshrunk_cov : ndarray of shape (n_features, n_features)\n    Shrunk covariance.\n\nshrinkage : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\nNotes\n-----\nThe regularized (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features"
        },
        {
          "name": "ledoit_wolf_shrinkage",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage."
            },
            {
              "name": "assume_centered",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "If True, data will not be centered before computation.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, data will be centered before computation."
            },
            {
              "name": "block_size",
              "type": "Any",
              "hasDefault": true,
              "default": "1000",
              "limitation": null,
              "ignored": false,
              "description": "Size of blocks into which the covariance matrix will be split."
            }
          ],
          "results": [
            {
              "name": "shrinkage",
              "type": "float",
              "description": "Coefficient in the convex combination used for the computation\nof the shrunk estimate."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage.\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, data will be centered before computation.\n\nblock_size : int, default=1000\n    Size of blocks into which the covariance matrix will be split.\n\nReturns\n-------\nshrinkage : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\nNotes\n-----\nThe regularized (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features"
        },
        {
          "name": "oas",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data from which to compute the covariance estimate."
            }
          ],
          "results": [
            {
              "name": "shrunk_cov",
              "type": null,
              "description": "Shrunk covariance."
            },
            {
              "name": "shrinkage",
              "type": "float",
              "description": "Coefficient in the convex combination used for the computation\nof the shrunk estimate."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Estimate covariance with the Oracle Approximating Shrinkage algorithm.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate.\n\nassume_centered : bool, default=False\n  If True, data will not be centered before computation.\n  Useful to work with data whose mean is significantly equal to\n  zero but is not exactly zero.\n  If False, data will be centered before computation.\n\nReturns\n-------\nshrunk_cov : array-like of shape (n_features, n_features)\n    Shrunk covariance.\n\nshrinkage : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\nNotes\n-----\nThe regularised (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\n\nThe formula we used to implement the OAS is slightly modified compared\nto the one given in the article. See :class:`OAS` for more details."
        },
        {
          "name": "shrunk_covariance",
          "decorators": [],
          "parameters": [
            {
              "name": "emp_cov",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Covariance matrix to be shrunk"
            },
            {
              "name": "shrinkage",
              "type": "Any",
              "hasDefault": true,
              "default": "0.1",
              "limitation": null,
              "ignored": false,
              "description": "Coefficient in the convex combination used for the computation\nof the shrunk estimate. Range is [0, 1]."
            }
          ],
          "results": [
            {
              "name": "shrunk_cov",
              "type": null,
              "description": "Shrunk covariance."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Calculates a covariance matrix shrunk on the diagonal\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nemp_cov : array-like of shape (n_features, n_features)\n    Covariance matrix to be shrunk\n\nshrinkage : float, default=0.1\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate. Range is [0, 1].\n\nReturns\n-------\nshrunk_cov : ndarray of shape (n_features, n_features)\n    Shrunk covariance.\n\nNotes\n-----\nThe regularized (shrunk) covariance is given by:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features"
        }
      ]
    },
    {
      "name": "sklearn.cross_decomposition",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._pls",
          "declaration": "CCA",
          "alias": null
        },
        {
          "module": "sklearn._pls",
          "declaration": "PLSCanonical",
          "alias": null
        },
        {
          "module": "sklearn._pls",
          "declaration": "PLSRegression",
          "alias": null
        },
        {
          "module": "sklearn._pls",
          "declaration": "PLSSVD",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.cross_decomposition._pls",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "pinv2",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "svd",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "svd_flip",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "FLOAT_DTYPES",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "CCA",
          "decorators": [],
          "superclasses": [
            "_PLS"
          ],
          "methods": [],
          "fullDocstring": "Canonical Correlation Analysis, also known as \"Mode B\" PLS.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\nParameters\n----------\nn_components : int, default=2\n    Number of components to keep. Should be in `[1, min(n_samples,\n    n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\nmax_iter : int, default=500\n    the maximum number of iterations of the power method.\n\ntol : float, default=1e-06\n    The tolerance used as convergence criteria in the power method: the\n    algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n    than `tol`, where `u` corresponds to the left singular vector.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the cross-covariance matrices of each\n    iteration.\n\ny_weights_ : ndarray of shape (n_targets, n_components)\n    The right singular vectors of the cross-covariance matrices of each\n    iteration.\n\nx_loadings_ : ndarray of shape (n_features, n_components)\n    The loadings of `X`.\n\ny_loadings_ : ndarray of shape (n_targets, n_components)\n    The loadings of `Y`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\n    .. deprecated:: 0.24\n       `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\n    .. deprecated:: 0.24\n       `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\nx_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `X`.\n\ny_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `Y`.\n\ncoef_ : ndarray of shape (n_features, n_targets)\n    The coefficients of the linear model such that `Y` is approximated as\n    `Y = X @ coef_`.\n\nn_iter_ : list of shape (n_components,)\n    Number of iterations of the power method, for each\n    component.\n\nExamples\n--------\n>>> from sklearn.cross_decomposition import CCA\n>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\n>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n>>> cca = CCA(n_components=1)\n>>> cca.fit(X, Y)\nCCA(n_components=1)\n>>> X_c, Y_c = cca.transform(X, Y)\n\nSee Also\n--------\nPLSCanonical\nPLSSVD"
        },
        {
          "name": "PLSCanonical",
          "decorators": [],
          "superclasses": [
            "_PLS"
          ],
          "methods": [],
          "fullDocstring": "Partial Least Squares transformer and regressor.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8\n\nParameters\n----------\nn_components : int, default=2\n    Number of components to keep. Should be in `[1, min(n_samples,\n    n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\nalgorithm : {'nipals', 'svd'}, default='nipals'\n    The algorithm used to estimate the first singular vectors of the\n    cross-covariance matrix. 'nipals' uses the power method while 'svd'\n    will compute the whole SVD.\n\nmax_iter : int, default=500\n    the maximum number of iterations of the power method when\n    `algorithm='nipals'`. Ignored otherwise.\n\ntol : float, default=1e-06\n    The tolerance used as convergence criteria in the power method: the\n    algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n    than `tol`, where `u` corresponds to the left singular vector.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the cross-covariance matrices of each\n    iteration.\n\ny_weights_ : ndarray of shape (n_targets, n_components)\n    The right singular vectors of the cross-covariance matrices of each\n    iteration.\n\nx_loadings_ : ndarray of shape (n_features, n_components)\n    The loadings of `X`.\n\ny_loadings_ : ndarray of shape (n_targets, n_components)\n    The loadings of `Y`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\n    .. deprecated:: 0.24\n       `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\n    .. deprecated:: 0.24\n       `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\nx_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `X`.\n\ny_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `Y`.\n\ncoef_ : ndarray of shape (n_features, n_targets)\n    The coefficients of the linear model such that `Y` is approximated as\n    `Y = X @ coef_`.\n\nn_iter_ : list of shape (n_components,)\n    Number of iterations of the power method, for each\n    component. Empty if `algorithm='svd'`.\n\nExamples\n--------\n>>> from sklearn.cross_decomposition import PLSCanonical\n>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n>>> plsca = PLSCanonical(n_components=2)\n>>> plsca.fit(X, Y)\nPLSCanonical()\n>>> X_c, Y_c = plsca.transform(X, Y)\n\nSee Also\n--------\nCCA\nPLSSVD"
        },
        {
          "name": "PLSRegression",
          "decorators": [],
          "superclasses": [
            "_PLS"
          ],
          "methods": [],
          "fullDocstring": "PLS regression\n\nPLSRegression is also known as PLS2 or PLS1, depending on the number of\ntargets.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8\n\nParameters\n----------\nn_components : int, default=2\n    Number of components to keep. Should be in `[1, min(n_samples,\n    n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\nalgorithm : {'nipals', 'svd'}, default='nipals'\n    The algorithm used to estimate the first singular vectors of the\n    cross-covariance matrix. 'nipals' uses the power method while 'svd'\n    will compute the whole SVD.\n\nmax_iter : int, default=500\n    The maximum number of iterations of the power method when\n    `algorithm='nipals'`. Ignored otherwise.\n\ntol : float, default=1e-06\n    The tolerance used as convergence criteria in the power method: the\n    algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n    than `tol`, where `u` corresponds to the left singular vector.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the cross-covariance matrices of each\n    iteration.\n\ny_weights_ : ndarray of shape (n_targets, n_components)\n    The right singular vectors of the cross-covariance matrices of each\n    iteration.\n\nx_loadings_ : ndarray of shape (n_features, n_components)\n    The loadings of `X`.\n\ny_loadings_ : ndarray of shape (n_targets, n_components)\n    The loadings of `Y`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\nx_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `X`.\n\ny_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `Y`.\n\ncoef_ : ndarray of shape (n_features, n_targets)\n    The coefficients of the linear model such that `Y` is approximated as\n    `Y = X @ coef_`.\n\nn_iter_ : list of shape (n_components,)\n    Number of iterations of the power method, for each\n    component. Empty if `algorithm='svd'`.\n\nExamples\n--------\n>>> from sklearn.cross_decomposition import PLSRegression\n>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n>>> pls2 = PLSRegression(n_components=2)\n>>> pls2.fit(X, Y)\nPLSRegression()\n>>> Y_pred = pls2.predict(X)"
        },
        {
          "name": "PLSSVD",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training samples."
                },
                {
                  "name": "Y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Targets."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit model to data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training samples.\n\nY : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Targets."
            },
            {
              "name": "x_scores_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "y_scores_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "x_mean_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "y_mean_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "x_std_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "y_std_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples to be transformed."
                },
                {
                  "name": "Y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Targets."
                }
              ],
              "results": [
                {
                  "name": "out",
                  "type": null,
                  "description": "The transformed data `X_tranformed` if `Y` is not None,\n`(X_transformed, Y_transformed)` otherwise."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply the dimensionality reduction.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Samples to be transformed.\n\nY : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n    Targets.\n\nReturns\n-------\nout : array-like or tuple of array-like\n    The transformed data `X_tranformed` if `Y` is not None,\n    `(X_transformed, Y_transformed)` otherwise."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Targets."
                }
              ],
              "results": [
                {
                  "name": "out",
                  "type": null,
                  "description": "The transformed data `X_tranformed` if `Y` is not None,\n`(X_transformed, Y_transformed)` otherwise."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn and apply the dimensionality reduction.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training samples.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n    Targets.\n\nReturns\n-------\nout : array-like or tuple of array-like\n    The transformed data `X_tranformed` if `Y` is not None,\n    `(X_transformed, Y_transformed)` otherwise."
            }
          ],
          "fullDocstring": "Partial Least Square SVD.\n\nThis transformer simply performs a SVD on the crosscovariance matrix X'Y.\nIt is able to project both the training data `X` and the targets `Y`. The\ntraining data X is projected on the left singular vectors, while the\ntargets are projected on the right singular vectors.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8\n\nParameters\n----------\nn_components : int, default=2\n    The number of components to keep. Should be in `[1,\n    min(n_samples, n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the SVD of the cross-covariance matrix.\n    Used to project `X` in `transform`.\n\ny_weights_ : ndarray of (n_targets, n_components)\n    The right singular vectors of the SVD of the cross-covariance matrix.\n    Used to project `X` in `transform`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\n    .. deprecated:: 0.24\n       `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\n    .. deprecated:: 0.24\n       `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.cross_decomposition import PLSSVD\n>>> X = np.array([[0., 0., 1.],\n...               [1., 0., 0.],\n...               [2., 2., 2.],\n...               [2., 5., 4.]])\n>>> Y = np.array([[0.1, -0.2],\n...               [0.9, 1.1],\n...               [6.2, 5.9],\n...               [11.9, 12.3]])\n>>> pls = PLSSVD(n_components=2).fit(X, Y)\n>>> X_c, Y_c = pls.transform(X, Y)\n>>> X_c.shape, Y_c.shape\n((4, 2), (4, 2))\n\nSee Also\n--------\nPLSCanonical\nCCA"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.datasets",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._base",
          "declaration": "clear_data_home",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_boston",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_breast_cancer",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_diabetes",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_digits",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_files",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_iris",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_linnerud",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_sample_image",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_sample_images",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "load_wine",
          "alias": null
        },
        {
          "module": "sklearn._california_housing",
          "declaration": "fetch_california_housing",
          "alias": null
        },
        {
          "module": "sklearn._covtype",
          "declaration": "fetch_covtype",
          "alias": null
        },
        {
          "module": "sklearn._kddcup99",
          "declaration": "fetch_kddcup99",
          "alias": null
        },
        {
          "module": "sklearn._lfw",
          "declaration": "fetch_lfw_pairs",
          "alias": null
        },
        {
          "module": "sklearn._lfw",
          "declaration": "fetch_lfw_people",
          "alias": null
        },
        {
          "module": "sklearn._olivetti_faces",
          "declaration": "fetch_olivetti_faces",
          "alias": null
        },
        {
          "module": "sklearn._openml",
          "declaration": "fetch_openml",
          "alias": null
        },
        {
          "module": "sklearn._rcv1",
          "declaration": "fetch_rcv1",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_biclusters",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_blobs",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_checkerboard",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_circles",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_classification",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_friedman1",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_friedman2",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_friedman3",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_gaussian_quantiles",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_hastie_10_2",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_low_rank_matrix",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_moons",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_multilabel_classification",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_regression",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_s_curve",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_sparse_coded_signal",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_sparse_spd_matrix",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_sparse_uncorrelated",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_spd_matrix",
          "alias": null
        },
        {
          "module": "sklearn._samples_generator",
          "declaration": "make_swiss_roll",
          "alias": null
        },
        {
          "module": "sklearn._species_distributions",
          "declaration": "fetch_species_distributions",
          "alias": null
        },
        {
          "module": "sklearn._svmlight_format_io",
          "declaration": "dump_svmlight_file",
          "alias": null
        },
        {
          "module": "sklearn._svmlight_format_io",
          "declaration": "load_svmlight_file",
          "alias": null
        },
        {
          "module": "sklearn._svmlight_format_io",
          "declaration": "load_svmlight_files",
          "alias": null
        },
        {
          "module": "sklearn._twenty_newsgroups",
          "declaration": "fetch_20newsgroups",
          "alias": null
        },
        {
          "module": "sklearn._twenty_newsgroups",
          "declaration": "fetch_20newsgroups_vectorized",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.datasets._base",
      "imports": [
        {
          "module": "csv",
          "alias": null
        },
        {
          "module": "hashlib",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "shutil",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "namedtuple",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "environ",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "listdir",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "makedirs",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "dirname",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "exists",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "expanduser",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "isdir",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "splitext",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_pandas_support",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "urllib.request",
          "declaration": "urlretrieve",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "clear_data_home",
          "decorators": [],
          "parameters": [
            {
              "name": "data_home",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The path to scikit-learn data directory. If `None`, the default path\nis `~/sklearn_learn_data`."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Delete all the content of the data home cache.\n\nParameters\n----------\ndata_home : str, default=None\n    The path to scikit-learn data directory. If `None`, the default path\n    is `~/sklearn_learn_data`."
        },
        {
          "name": "get_data_home",
          "decorators": [],
          "parameters": [
            {
              "name": "data_home",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The path to scikit-learn data directory. If `None`, the default path\nis `~/sklearn_learn_data`."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Return the path of the scikit-learn data dir.\n\nThis folder is used by some large dataset loaders to avoid downloading the\ndata several times.\n\nBy default the data dir is set to a folder named 'scikit_learn_data' in the\nuser home folder.\n\nAlternatively, it can be set by the 'SCIKIT_LEARN_DATA' environment\nvariable or programmatically by giving an explicit folder path. The '~'\nsymbol is expanded to the user home folder.\n\nIf the folder does not already exist, it is automatically created.\n\nParameters\n----------\ndata_home : str, default=None\n    The path to scikit-learn data directory. If `None`, the default path\n    is `~/sklearn_learn_data`."
        },
        {
          "name": "load_boston",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : ndarray of shape (506, 13)\n    The data matrix.\ntarget : ndarray of shape (506, )\n    The regression target.\nfilename : str\n    The physical location of boston csv dataset.\n\n    .. versionadded:: 0.20\n\nDESCR : str\n    The full description of the dataset.\nfeature_names : ndarray\n    The names of features"
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.18"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load and return the boston house-prices dataset (regression).\n\n==============   ==============\nSamples total               506\nDimensionality               13\nFeatures         real, positive\nTargets           real 5. - 50.\n==============   ==============\n\nRead more in the :ref:`User Guide <boston_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : ndarray of shape (506, 13)\n        The data matrix.\n    target : ndarray of shape (506, )\n        The regression target.\n    filename : str\n        The physical location of boston csv dataset.\n\n        .. versionadded:: 0.20\n\n    DESCR : str\n        The full description of the dataset.\n    feature_names : ndarray\n        The names of features\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18\n\nNotes\n-----\n    .. versionchanged:: 0.20\n        Fixed a wrong data point at [445, 0].\n\nExamples\n--------\n>>> from sklearn.datasets import load_boston\n>>> X, y = load_boston(return_X_y=True)\n>>> print(X.shape)\n(506, 13)"
        },
        {
          "name": "load_breast_cancer",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : {ndarray, dataframe} of shape (569, 30)\n    The data matrix. If `as_frame=True`, `data` will be a pandas\n    DataFrame.\ntarget: {ndarray, Series} of shape (569,)\n    The classification target. If `as_frame=True`, `target` will be\n    a pandas Series.\nfeature_names: list\n    The names of the dataset columns.\ntarget_names: list\n    The names of target classes.\nframe: DataFrame of shape (569, 31)\n    Only present when `as_frame=True`. DataFrame with `data` and\n    `target`.\n\n    .. versionadded:: 0.23\nDESCR: str\n    The full description of the dataset.\nfilename: str\n    The path to the location of the data.\n\n    .. versionadded:: 0.20"
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.18"
            },
            {
              "name": "",
              "type": "Any",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load and return the breast cancer wisconsin dataset (classification).\n\nThe breast cancer dataset is a classic and very easy binary classification\ndataset.\n\n=================   ==============\nClasses                          2\nSamples per class    212(M),357(B)\nSamples total                  569\nDimensionality                  30\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <breast_cancer_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (569, 30)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (569,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (569, 31)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    filename: str\n        The path to the location of the data.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18\n\nThe copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\ndownloaded from:\nhttps://goo.gl/U2Uwz2\n\nExamples\n--------\nLet's say you are interested in the samples 10, 50, and 85, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_breast_cancer\n>>> data = load_breast_cancer()\n>>> data.target[[10, 50, 85]]\narray([0, 1, 0])\n>>> list(data.target_names)\n['malignant', 'benign']"
        },
        {
          "name": "load_data",
          "decorators": [],
          "parameters": [
            {
              "name": "module_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The module path."
            },
            {
              "name": "data_file_name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Name of csv file to be loaded from\nmodule_path/data/data_file_name. For example 'wine_data.csv'."
            }
          ],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "A 2D array with each row representing one sample and each column\nrepresenting the features of a given sample."
            },
            {
              "name": "target",
              "type": null,
              "description": "A 1D array holding target variables for all the samples in `data.\nFor example target[0] is the target varible for data[0]."
            },
            {
              "name": "target_names",
              "type": null,
              "description": "A 1D array containing the names of the classifications. For example\ntarget_names[0] is the name of the target[0] class."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Loads data from module_path/data/data_file_name.\n\nParameters\n----------\nmodule_path : string\n    The module path.\n\ndata_file_name : string\n    Name of csv file to be loaded from\n    module_path/data/data_file_name. For example 'wine_data.csv'.\n\nReturns\n-------\ndata : Numpy array\n    A 2D array with each row representing one sample and each column\n    representing the features of a given sample.\n\ntarget : Numpy array\n    A 1D array holding target variables for all the samples in `data.\n    For example target[0] is the target varible for data[0].\n\ntarget_names : Numpy array\n    A 1D array containing the names of the classifications. For example\n    target_names[0] is the name of the target[0] class."
        },
        {
          "name": "load_diabetes",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : {ndarray, dataframe} of shape (442, 10)\n    The data matrix. If `as_frame=True`, `data` will be a pandas\n    DataFrame.\ntarget: {ndarray, Series} of shape (442,)\n    The regression target. If `as_frame=True`, `target` will be\n    a pandas Series.\nfeature_names: list\n    The names of the dataset columns.\nframe: DataFrame of shape (442, 11)\n    Only present when `as_frame=True`. DataFrame with `data` and\n    `target`.\n\n    .. versionadded:: 0.23\nDESCR: str\n    The full description of the dataset.\ndata_filename: str\n    The path to the location of the data.\ntarget_filename: str\n    The path to the location of the target."
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.18"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load and return the diabetes dataset (regression).\n\n==============   ==================\nSamples total    442\nDimensionality   10\nFeatures         real, -.2 < x < .2\nTargets          integer 25 - 346\n==============   ==================\n\nRead more in the :ref:`User Guide <diabetes_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False.\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (442, 10)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (442,)\n        The regression target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    frame: DataFrame of shape (442, 11)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    data_filename: str\n        The path to the location of the data.\n    target_filename: str\n        The path to the location of the target.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18"
        },
        {
          "name": "load_digits",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : {ndarray, dataframe} of shape (1797, 64)\n    The flattened data matrix. If `as_frame=True`, `data` will be\n    a pandas DataFrame.\ntarget: {ndarray, Series} of shape (1797,)\n    The classification target. If `as_frame=True`, `target` will be\n    a pandas Series.\nfeature_names: list\n    The names of the dataset columns.\ntarget_names: list\n    The names of target classes.\n\n    .. versionadded:: 0.20\n\nframe: DataFrame of shape (1797, 65)\n    Only present when `as_frame=True`. DataFrame with `data` and\n    `target`.\n\n    .. versionadded:: 0.23\nimages: {ndarray} of shape (1797, 8, 8)\n    The raw image data.\nDESCR: str\n    The full description of the dataset."
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.18"
            },
            {
              "name": "",
              "type": "Any",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load and return the digits dataset (classification).\n\nEach datapoint is a 8x8 image of a digit.\n\n=================   ==============\nClasses                         10\nSamples per class             ~180\nSamples total                 1797\nDimensionality                  64\nFeatures             integers 0-16\n=================   ==============\n\nRead more in the :ref:`User Guide <digits_dataset>`.\n\nParameters\n----------\nn_class : int, default=10\n    The number of classes to return. Between 0 and 10.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (1797, 64)\n        The flattened data matrix. If `as_frame=True`, `data` will be\n        a pandas DataFrame.\n    target: {ndarray, Series} of shape (1797,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n\n        .. versionadded:: 0.20\n\n    frame: DataFrame of shape (1797, 65)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    images: {ndarray} of shape (1797, 8, 8)\n        The raw image data.\n    DESCR: str\n        The full description of the dataset.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nExamples\n--------\nTo load the data and visualize the images::\n\n    >>> from sklearn.datasets import load_digits\n    >>> digits = load_digits()\n    >>> print(digits.data.shape)\n    (1797, 64)\n    >>> import matplotlib.pyplot as plt #doctest: +SKIP\n    >>> plt.gray() #doctest: +SKIP\n    >>> plt.matshow(digits.images[0]) #doctest: +SKIP\n    >>> plt.show() #doctest: +SKIP"
        },
        {
          "name": "load_files",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "container_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Path to the main folder holding one subfolder per category"
            }
          ],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : list of str\n    Only present when `load_content=True`.\n    The raw text data to learn.\ntarget : ndarray\n    The target labels (integer index).\ntarget_names : list\n    The names of target classes.\nDESCR : str\n    The full description of the dataset.\nfilenames: ndarray\n    The filenames holding the dataset."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load text files with categories as subfolder names.\n\nIndividual samples are assumed to be files stored a two levels folder\nstructure such as the following:\n\n    container_folder/\n        category_1_folder/\n            file_1.txt\n            file_2.txt\n            ...\n            file_42.txt\n        category_2_folder/\n            file_43.txt\n            file_44.txt\n            ...\n\nThe folder names are used as supervised signal label names. The individual\nfile names are not important.\n\nThis function does not try to extract features into a numpy array or scipy\nsparse matrix. In addition, if load_content is false it does not try to\nload the files in memory.\n\nTo use text files in a scikit-learn classification or clustering algorithm,\nyou will need to use the :mod`~sklearn.feature_extraction.text` module to\nbuild a feature extraction transformer that suits your problem.\n\nIf you set load_content=True, you should also specify the encoding of the\ntext using the 'encoding' parameter. For many modern text files, 'utf-8'\nwill be the correct encoding. If you leave encoding equal to None, then the\ncontent will be made of bytes instead of Unicode, and you will not be able\nto use most functions in :mod:`~sklearn.feature_extraction.text`.\n\nSimilar feature extractors should be built for other kind of unstructured\ndata input such as images, audio, video, ...\n\nRead more in the :ref:`User Guide <datasets>`.\n\nParameters\n----------\ncontainer_path : str or unicode\n    Path to the main folder holding one subfolder per category\n\ndescription : str or unicode, default=None\n    A paragraph describing the characteristic of the dataset: its source,\n    reference, etc.\n\ncategories : list of str, default=None\n    If None (default), load all the categories. If not None, list of\n    category names to load (other categories ignored).\n\nload_content : bool, default=True\n    Whether to load or not the content of the different files. If true a\n    'data' attribute containing the text information is present in the data\n    structure returned. If not, a filenames attribute gives the path to the\n    files.\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data: might be important for models that\n    make the assumption that the samples are independent and identically\n    distributed (i.i.d.), such as stochastic gradient descent.\n\nencoding : str, default=None\n    If None, do not try to decode the content of the files (e.g. for images\n    or other non-text content). If not None, encoding to use to decode text\n    files to Unicode if load_content is True.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. Passed as keyword\n    argument 'errors' to bytes.decode.\n\nrandom_state : int, RandomState instance or None, default=0\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : list of str\n        Only present when `load_content=True`.\n        The raw text data to learn.\n    target : ndarray\n        The target labels (integer index).\n    target_names : list\n        The names of target classes.\n    DESCR : str\n        The full description of the dataset.\n    filenames: ndarray\n        The filenames holding the dataset."
        },
        {
          "name": "load_iris",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : {ndarray, dataframe} of shape (150, 4)\n    The data matrix. If `as_frame=True`, `data` will be a pandas\n    DataFrame.\ntarget: {ndarray, Series} of shape (150,)\n    The classification target. If `as_frame=True`, `target` will be\n    a pandas Series.\nfeature_names: list\n    The names of the dataset columns.\ntarget_names: list\n    The names of target classes.\nframe: DataFrame of shape (150, 5)\n    Only present when `as_frame=True`. DataFrame with `data` and\n    `target`.\n\n    .. versionadded:: 0.23\nDESCR: str\n    The full description of the dataset.\nfilename: str\n    The path to the location of the data.\n\n    .. versionadded:: 0.20"
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.18"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load and return the iris dataset (classification).\n\nThe iris dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class               50\nSamples total                  150\nDimensionality                   4\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <iris_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (150, 4)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (150,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (150, 5)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    filename: str\n        The path to the location of the data.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18\n\nNotes\n-----\n    .. versionchanged:: 0.20\n        Fixed two wrong data points according to Fisher's paper.\n        The new version is the same as in R, but not as in the UCI\n        Machine Learning Repository.\n\nExamples\n--------\nLet's say you are interested in the samples 10, 25, and 50, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_iris\n>>> data = load_iris()\n>>> data.target[[10, 25, 50]]\narray([0, 0, 1])\n>>> list(data.target_names)\n['setosa', 'versicolor', 'virginica']"
        },
        {
          "name": "load_linnerud",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : {ndarray, dataframe} of shape (20, 3)\n    The data matrix. If `as_frame=True`, `data` will be a pandas\n    DataFrame.\ntarget: {ndarray, dataframe} of shape (20, 3)\n    The regression targets. If `as_frame=True`, `target` will be\n    a pandas DataFrame.\nfeature_names: list\n    The names of the dataset columns.\ntarget_names: list\n    The names of the target columns.\nframe: DataFrame of shape (20, 6)\n    Only present when `as_frame=True`. DataFrame with `data` and\n    `target`.\n\n    .. versionadded:: 0.23\nDESCR: str\n    The full description of the dataset.\ndata_filename: str\n    The path to the location of the data.\ntarget_filename: str\n    The path to the location of the target.\n\n    .. versionadded:: 0.20"
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.18"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load and return the physical excercise linnerud dataset.\n\nThis dataset is suitable for multi-ouput regression tasks.\n\n==============   ============================\nSamples total    20\nDimensionality   3 (for both data and target)\nFeatures         integer\nTargets          integer\n==============   ============================\n\nRead more in the :ref:`User Guide <linnerrud_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric, string or categorical). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (20, 3)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, dataframe} of shape (20, 3)\n        The regression targets. If `as_frame=True`, `target` will be\n        a pandas DataFrame.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of the target columns.\n    frame: DataFrame of shape (20, 6)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    data_filename: str\n        The path to the location of the data.\n    target_filename: str\n        The path to the location of the target.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18"
        },
        {
          "name": "load_sample_image",
          "decorators": [],
          "parameters": [
            {
              "name": "image_name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The name of the sample image loaded"
            }
          ],
          "results": [
            {
              "name": "img",
              "type": null,
              "description": "The image as a numpy array: height x width x color"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load the numpy array of a single sample image\n\nRead more in the :ref:`User Guide <sample_images>`.\n\nParameters\n----------\nimage_name : {`china.jpg`, `flower.jpg`}\n    The name of the sample image loaded\n\nReturns\n-------\nimg : 3D array\n    The image as a numpy array: height x width x color\n\nExamples\n--------\n\n>>> from sklearn.datasets import load_sample_image\n>>> china = load_sample_image('china.jpg')   # doctest: +SKIP\n>>> china.dtype                              # doctest: +SKIP\ndtype('uint8')\n>>> china.shape                              # doctest: +SKIP\n(427, 640, 3)\n>>> flower = load_sample_image('flower.jpg') # doctest: +SKIP\n>>> flower.dtype                             # doctest: +SKIP\ndtype('uint8')\n>>> flower.shape                             # doctest: +SKIP\n(427, 640, 3)"
        },
        {
          "name": "load_sample_images",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\nimages : list of ndarray of shape (427, 640, 3)\n    The two sample image.\nfilenames : list\n    The filenames for the images.\nDESCR : str\n    The full description of the dataset."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load sample images for image manipulation.\n\nLoads both, ``china`` and ``flower``.\n\nRead more in the :ref:`User Guide <sample_images>`.\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    images : list of ndarray of shape (427, 640, 3)\n        The two sample image.\n    filenames : list\n        The filenames for the images.\n    DESCR : str\n        The full description of the dataset.\n\nExamples\n--------\nTo load the data and visualize the images:\n\n>>> from sklearn.datasets import load_sample_images\n>>> dataset = load_sample_images()     #doctest: +SKIP\n>>> len(dataset.images)                #doctest: +SKIP\n2\n>>> first_img_data = dataset.images[0] #doctest: +SKIP\n>>> first_img_data.shape               #doctest: +SKIP\n(427, 640, 3)\n>>> first_img_data.dtype               #doctest: +SKIP\ndtype('uint8')"
        },
        {
          "name": "load_wine",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : {ndarray, dataframe} of shape (178, 13)\n    The data matrix. If `as_frame=True`, `data` will be a pandas\n    DataFrame.\ntarget: {ndarray, Series} of shape (178,)\n    The classification target. If `as_frame=True`, `target` will be\n    a pandas Series.\nfeature_names: list\n    The names of the dataset columns.\ntarget_names: list\n    The names of target classes.\nframe: DataFrame of shape (178, 14)\n    Only present when `as_frame=True`. DataFrame with `data` and\n    `target`.\n\n    .. versionadded:: 0.23\nDESCR: str\n    The full description of the dataset."
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ""
            },
            {
              "name": "",
              "type": "Any",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load and return the wine dataset (classification).\n\n.. versionadded:: 0.18\n\nThe wine dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class        [59,71,48]\nSamples total                  178\nDimensionality                  13\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <wine_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (178, 13)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (178,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (178, 14)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n\n(data, target) : tuple if ``return_X_y`` is True\n\nThe copy of UCI ML Wine Data Set dataset is downloaded and modified to fit\nstandard format from:\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n\nExamples\n--------\nLet's say you are interested in the samples 10, 80, and 140, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_wine\n>>> data = load_wine()\n>>> data.target[[10, 80, 140]]\narray([0, 1, 2])\n>>> list(data.target_names)\n['class_0', 'class_1', 'class_2']"
        }
      ]
    },
    {
      "name": "sklearn.datasets._california_housing",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "logging",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "tarfile",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "os",
          "declaration": "makedirs",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "remove",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "dirname",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "exists",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "RemoteFileMetadata",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_convert_data_dataframe",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_fetch_remote",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_pkl_filepath",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "fetch_california_housing",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "dataset",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : ndarray, shape (20640, 8)\n    Each row corresponding to the 8 feature values in order.\n    If ``as_frame`` is True, ``data`` is a pandas object.\ntarget : numpy array of shape (20640,)\n    Each value corresponds to the average\n    house value in units of 100,000.\n    If ``as_frame`` is True, ``target`` is a pandas object.\nfeature_names : list of length 8\n    Array of ordered feature names used in the dataset.\nDESCR : string\n    Description of the California housing dataset.\nframe : pandas DataFrame\n    Only present when `as_frame=True`. DataFrame with ``data`` and\n    ``target``.\n\n    .. versionadded:: 0.23"
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.20"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load the California housing dataset (regression).\n\n==============   ==============\nSamples total             20640\nDimensionality                8\nFeatures                   real\nTarget           real 0.15 - 5.\n==============   ==============\n\nRead more in the :ref:`User Guide <california_housing_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\n\nreturn_X_y : bool, default=False.\n    If True, returns ``(data.data, data.target)`` instead of a Bunch\n    object.\n\n    .. versionadded:: 0.20\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric, string or categorical). The target is\n    a pandas DataFrame or Series depending on the number of target_columns.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndataset : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : ndarray, shape (20640, 8)\n        Each row corresponding to the 8 feature values in order.\n        If ``as_frame`` is True, ``data`` is a pandas object.\n    target : numpy array of shape (20640,)\n        Each value corresponds to the average\n        house value in units of 100,000.\n        If ``as_frame`` is True, ``target`` is a pandas object.\n    feature_names : list of length 8\n        Array of ordered feature names used in the dataset.\n    DESCR : string\n        Description of the California housing dataset.\n    frame : pandas DataFrame\n        Only present when `as_frame=True`. DataFrame with ``data`` and\n        ``target``.\n\n        .. versionadded:: 0.23\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20\n\nNotes\n-----\n\nThis dataset consists of 20,640 samples and 9 features."
        }
      ]
    },
    {
      "name": "sklearn.datasets._covtype",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "logging",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "gzip",
          "declaration": "GzipFile",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "makedirs",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "remove",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "dirname",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "exists",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "RemoteFileMetadata",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_convert_data_dataframe",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_fetch_remote",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_pkl_filepath",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "fetch_covtype",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "dataset",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : ndarray of shape (581012, 54)\n    Each row corresponds to the 54 features in the dataset.\ntarget : ndarray of shape (581012,)\n    Each value corresponds to one of\n    the 7 forest covertypes with values\n    ranging between 1 to 7.\nframe : dataframe of shape (581012, 53)\n    Only present when `as_frame=True`. Contains `data` and `target`.\nDESCR : str\n    Description of the forest covertype dataset.\nfeature_names : list\n    The names of the dataset columns.\ntarget_names: list\n    The names of the target columns."
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.20"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load the covertype dataset (classification).\n\nDownload it if necessary.\n\n=================   ============\nClasses                        7\nSamples total             581012\nDimensionality                54\nFeatures                     int\n=================   ============\n\nRead more in the :ref:`User Guide <covtype_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=False\n    Whether to shuffle dataset.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data.data, data.target)`` instead of a Bunch\n    object.\n\n    .. versionadded:: 0.20\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is a pandas DataFrame or\n    Series depending on the number of target columns. If `return_X_y` is\n    True, then (`data`, `target`) will be pandas DataFrames or Series as\n    described below.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ndataset : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : ndarray of shape (581012, 54)\n        Each row corresponds to the 54 features in the dataset.\n    target : ndarray of shape (581012,)\n        Each value corresponds to one of\n        the 7 forest covertypes with values\n        ranging between 1 to 7.\n    frame : dataframe of shape (581012, 53)\n        Only present when `as_frame=True`. Contains `data` and `target`.\n    DESCR : str\n        Description of the forest covertype dataset.\n    feature_names : list\n        The names of the dataset columns.\n    target_names: list\n        The names of the target columns.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20"
        }
      ]
    },
    {
      "name": "sklearn.datasets._kddcup99",
      "imports": [
        {
          "module": "errno",
          "alias": null
        },
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "logging",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "gzip",
          "declaration": "GzipFile",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "dirname",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "exists",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "RemoteFileMetadata",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_convert_data_dataframe",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_fetch_remote",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "shuffle",
          "alias": "shuffle_method"
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "fetch_kddcup99",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : {ndarray, dataframe} of shape (494021, 41)\n    The data matrix to learn. If `as_frame=True`, `data` will be a\n    pandas DataFrame.\ntarget : {ndarray, series} of shape (494021,)\n    The regression target for each sample. If `as_frame=True`, `target`\n    will be a pandas Series.\nframe : dataframe of shape (494021, 42)\n    Only present when `as_frame=True`. Contains `data` and `target`.\nDESCR : str\n    The full description of the dataset.\nfeature_names : list\n    The names of the dataset columns\ntarget_names: list\n    The names of the target columns"
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.20"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load the kddcup99 dataset (classification).\n\nDownload it if necessary.\n\n=================   ====================================\nClasses                                               23\nSamples total                                    4898431\nDimensionality                                        41\nFeatures            discrete (int) or continuous (float)\n=================   ====================================\n\nRead more in the :ref:`User Guide <kddcup99_dataset>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nsubset : {'SA', 'SF', 'http', 'smtp'}, default=None\n    To return the corresponding classical subsets of kddcup 99.\n    If None, return the entire kddcup 99 dataset.\n\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n    .. versionadded:: 0.19\n\nshuffle : bool, default=False\n    Whether to shuffle dataset.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling and for\n    selection of abnormal samples if `subset='SA'`. Pass an int for\n    reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\npercent10 : bool, default=True\n    Whether to load only 10 percent of the data.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.20\n\nas_frame : bool, default=False\n    If `True`, returns a pandas Dataframe for the ``data`` and ``target``\n    objects in the `Bunch` returned object; `Bunch` return object will also\n    have a ``frame`` member.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (494021, 41)\n        The data matrix to learn. If `as_frame=True`, `data` will be a\n        pandas DataFrame.\n    target : {ndarray, series} of shape (494021,)\n        The regression target for each sample. If `as_frame=True`, `target`\n        will be a pandas Series.\n    frame : dataframe of shape (494021, 42)\n        Only present when `as_frame=True`. Contains `data` and `target`.\n    DESCR : str\n        The full description of the dataset.\n    feature_names : list\n        The names of the dataset columns\n    target_names: list\n        The names of the target columns\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20"
        }
      ]
    },
    {
      "name": "sklearn.datasets._lfw",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "logging",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Memory",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "listdir",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "makedirs",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "remove",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "dirname",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "exists",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "isdir",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "RemoteFileMetadata",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_fetch_remote",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "parse_version",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "fetch_lfw_pairs",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : ndarray of shape (2200, 5828). Shape depends on ``subset``.\n    Each row corresponds to 2 ravel'd face images\n    of original size 62 x 47 pixels.\n    Changing the ``slice_``, ``resize`` or ``subset`` parameters\n    will change the shape of the output.\npairs : ndarray of shape (2200, 2, 62, 47). Shape depends on ``subset``\n    Each row has 2 face images corresponding\n    to same or different person from the dataset\n    containing 5749 people. Changing the ``slice_``,\n    ``resize`` or ``subset`` parameters will change the shape of the\n    output.\ntarget : numpy array of shape (2200,). Shape depends on ``subset``.\n    Labels associated to each pair of images.\n    The two label values being different persons or the same person.\nDESCR : string\n    Description of the Labeled Faces in the Wild (LFW) dataset."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).\n\nDownload it if necessary.\n\n=================   =======================\nClasses                                   2\nSamples total                         13233\nDimensionality                         5828\nFeatures            real, between 0 and 255\n=================   =======================\n\nIn the official `README.txt`_ this task is described as the\n\"Restricted\" task.  As I am not sure as to implement the\n\"Unrestricted\" variant correctly, I left it as unsupported for now.\n\n  .. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n\nThe original images are 250 x 250 pixels, but the default slice and resize\narguments reduce them to 62 x 47.\n\nRead more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n\nParameters\n----------\nsubset : {'train', 'test', '10_folds'}, default='train'\n    Select the dataset to load: 'train' for the development training\n    set, 'test' for the development test set, and '10_folds' for the\n    official evaluation set that is meant to be used with a 10-folds\n    cross validation.\n\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By\n    default all scikit-learn data is stored in '~/scikit_learn_data'\n    subfolders.\n\nfunneled : bool, default=True\n    Download and use the funneled variant of the dataset.\n\nresize : float, default=0.5\n    Ratio used to resize the each face picture.\n\ncolor : bool, default=False\n    Keep the 3 RGB channels instead of averaging them to a single\n    gray level channel. If color is True the shape of the data has\n    one more dimension than the shape with color = False.\n\nslice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n    Provide a custom 2D slice (height, width) to extract the\n    'interesting' part of the jpeg files and avoid use statistical\n    correlation from the background\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : ndarray of shape (2200, 5828). Shape depends on ``subset``.\n        Each row corresponds to 2 ravel'd face images\n        of original size 62 x 47 pixels.\n        Changing the ``slice_``, ``resize`` or ``subset`` parameters\n        will change the shape of the output.\n    pairs : ndarray of shape (2200, 2, 62, 47). Shape depends on ``subset``\n        Each row has 2 face images corresponding\n        to same or different person from the dataset\n        containing 5749 people. Changing the ``slice_``,\n        ``resize`` or ``subset`` parameters will change the shape of the\n        output.\n    target : numpy array of shape (2200,). Shape depends on ``subset``.\n        Labels associated to each pair of images.\n        The two label values being different persons or the same person.\n    DESCR : string\n        Description of the Labeled Faces in the Wild (LFW) dataset."
        },
        {
          "name": "fetch_lfw_people",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "dataset",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : numpy array of shape (13233, 2914)\n    Each row corresponds to a ravelled face image\n    of original size 62 x 47 pixels.\n    Changing the ``slice_`` or resize parameters will change the\n    shape of the output.\nimages : numpy array of shape (13233, 62, 47)\n    Each row is a face image corresponding to one of the 5749 people in\n    the dataset. Changing the ``slice_``\n    or resize parameters will change the shape of the output.\ntarget : numpy array of shape (13233,)\n    Labels associated to each face image.\n    Those labels range from 0-5748 and correspond to the person IDs.\nDESCR : string\n    Description of the Labeled Faces in the Wild (LFW) dataset."
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.20"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load the Labeled Faces in the Wild (LFW) people dataset (classification).\n\nDownload it if necessary.\n\n=================   =======================\nClasses                                5749\nSamples total                         13233\nDimensionality                         5828\nFeatures            real, between 0 and 255\n=================   =======================\n\nRead more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nfunneled : bool, default=True\n    Download and use the funneled variant of the dataset.\n\nresize : float, default=0.5\n    Ratio used to resize the each face picture.\n\nmin_faces_per_person : int, default=None\n    The extracted dataset will only retain pictures of people that have at\n    least `min_faces_per_person` different pictures.\n\ncolor : bool, default=False\n    Keep the 3 RGB channels instead of averaging them to a single\n    gray level channel. If color is True the shape of the data has\n    one more dimension than the shape with color = False.\n\nslice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n    Provide a custom 2D slice (height, width) to extract the\n    'interesting' part of the jpeg files and avoid use statistical\n    correlation from the background\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\n    object. See below for more information about the `dataset.data` and\n    `dataset.target` object.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ndataset : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : numpy array of shape (13233, 2914)\n        Each row corresponds to a ravelled face image\n        of original size 62 x 47 pixels.\n        Changing the ``slice_`` or resize parameters will change the\n        shape of the output.\n    images : numpy array of shape (13233, 62, 47)\n        Each row is a face image corresponding to one of the 5749 people in\n        the dataset. Changing the ``slice_``\n        or resize parameters will change the shape of the output.\n    target : numpy array of shape (13233,)\n        Labels associated to each face image.\n        Those labels range from 0-5748 and correspond to the person IDs.\n    DESCR : string\n        Description of the Labeled Faces in the Wild (LFW) dataset.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20"
        }
      ]
    },
    {
      "name": "sklearn.datasets._olivetti_faces",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "os",
          "declaration": "makedirs",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "remove",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "dirname",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "exists",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "scipy.io.matlab",
          "declaration": "loadmat",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "RemoteFileMetadata",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_fetch_remote",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_pkl_filepath",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "fetch_olivetti_faces",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata: ndarray, shape (400, 4096)\n    Each row corresponds to a ravelled\n    face image of original size 64 x 64 pixels.\nimages : ndarray, shape (400, 64, 64)\n    Each row is a face image\n    corresponding to one of the 40 subjects of the dataset.\ntarget : ndarray, shape (400,)\n    Labels associated to each face image.\n    Those labels are ranging from 0-39 and correspond to the\n    Subject IDs.\nDESCR : str\n    Description of the modified Olivetti Faces Dataset."
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.22"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load the Olivetti faces data-set from AT&T (classification).\n\nDownload it if necessary.\n\n=================   =====================\nClasses                                40\nSamples total                         400\nDimensionality                       4096\nFeatures            real, between 0 and 1\n=================   =====================\n\nRead more in the :ref:`User Guide <olivetti_faces_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nshuffle : bool, default=False\n    If True the order of the dataset is shuffled to avoid having\n    images of the same person grouped.\n\nrandom_state : int, RandomState instance or None, default=0\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns `(data, target)` instead of a `Bunch` object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data: ndarray, shape (400, 4096)\n        Each row corresponds to a ravelled\n        face image of original size 64 x 64 pixels.\n    images : ndarray, shape (400, 64, 64)\n        Each row is a face image\n        corresponding to one of the 40 subjects of the dataset.\n    target : ndarray, shape (400,)\n        Labels associated to each face image.\n        Those labels are ranging from 0-39 and correspond to the\n        Subject IDs.\n    DESCR : str\n        Description of the modified Olivetti Faces Dataset.\n\n(data, target) : tuple if `return_X_y=True`\n    .. versionadded:: 0.22"
        }
      ]
    },
    {
      "name": "sklearn.datasets._openml",
      "imports": [
        {
          "module": "gzip",
          "alias": null
        },
        {
          "module": "hashlib",
          "alias": null
        },
        {
          "module": "itertools",
          "alias": null
        },
        {
          "module": "json",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "alias": null
        },
        {
          "module": "shutil",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "OrderedDict",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Generator",
          "alias": null
        },
        {
          "module": "contextlib",
          "declaration": "closing",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "wraps",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn.externals",
          "declaration": "_arff",
          "alias": null
        },
        {
          "module": "sklearn.externals._arff",
          "declaration": "ArffContainerType",
          "alias": null
        },
        {
          "module": "sklearn.externals._arff",
          "declaration": "ArffSparseDataType",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_chunk_generator",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_pandas_support",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "get_chunk_n_rows",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Any",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Callable",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Dict",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "List",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Optional",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Tuple",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Union",
          "alias": null
        },
        {
          "module": "urllib.error",
          "declaration": "HTTPError",
          "alias": null
        },
        {
          "module": "urllib.request",
          "declaration": "Request",
          "alias": null
        },
        {
          "module": "urllib.request",
          "declaration": "urlopen",
          "alias": null
        },
        {
          "module": "warnings",
          "declaration": "warn",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "OpenMLError",
          "decorators": [],
          "superclasses": [
            "ValueError"
          ],
          "methods": [],
          "fullDocstring": "HTTP 412 is a specific OpenML error code, indicating a generic error"
        }
      ],
      "functions": [
        {
          "name": "fetch_openml",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "String identifier of the dataset. Note that OpenML can have multiple\ndatasets with the same name."
            }
          ],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : np.array, scipy.sparse.csr_matrix of floats, or pandas DataFrame\n    The feature matrix. Categorical features are encoded as ordinals.\ntarget : np.array, pandas Series or DataFrame\n    The regression target or classification labels, if applicable.\n    Dtype is float if numeric, and object if categorical. If\n    ``as_frame`` is True, ``target`` is a pandas object.\nDESCR : str\n    The full description of the dataset\nfeature_names : list\n    The names of the dataset columns\ntarget_names: list\n    The names of the target columns\n\n.. versionadded:: 0.22\n\ncategories : dict or None\n    Maps each categorical feature name to a list of values, such\n    that the value encoded as i is ith in the list. If ``as_frame``\n    is True, this is None.\ndetails : dict\n    More metadata from OpenML\nframe : pandas DataFrame\n    Only present when `as_frame=True`. DataFrame with ``data`` and\n    ``target``."
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. note:: EXPERIMENTAL\n\n    This interface is **experimental** and subsequent releases may\n    change attributes without notice (although there should only be\n    minor changes to ``data`` and ``target``).\n\nMissing values in the 'data' are represented as NaN's. Missing values\nin 'target' are represented as NaN's (numerical target) or None\n(categorical target)"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Fetch dataset from openml by name or dataset id.\n\nDatasets are uniquely identified by either an integer ID or by a\ncombination of name and version (i.e. there might be multiple\nversions of the 'iris' dataset). Please give either name or data_id\n(not both). In case a name is given, a version can also be\nprovided.\n\nRead more in the :ref:`User Guide <openml>`.\n\n.. versionadded:: 0.20\n\n.. note:: EXPERIMENTAL\n\n    The API is experimental (particularly the return value structure),\n    and might have small backward-incompatible changes without notice\n    or warning in future releases.\n\nParameters\n----------\nname : str, default=None\n    String identifier of the dataset. Note that OpenML can have multiple\n    datasets with the same name.\n\nversion : int or 'active', default='active'\n    Version of the dataset. Can only be provided if also ``name`` is given.\n    If 'active' the oldest version that's still active is used. Since\n    there may be more than one active version of a dataset, and those\n    versions may fundamentally be different from one another, setting an\n    exact version is highly recommended.\n\ndata_id : int, default=None\n    OpenML ID of the dataset. The most specific way of retrieving a\n    dataset. If data_id is not given, name (and potential version) are\n    used to obtain a dataset.\n\ndata_home : str, default=None\n    Specify another download and cache folder for the data sets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ntarget_column : str, list or None, default='default-target'\n    Specify the column name in the data to use as target. If\n    'default-target', the standard target column a stored on the server\n    is used. If ``None``, all columns are returned as data and the\n    target is ``None``. If list (of strings), all columns with these names\n    are returned as multi-target (Note: not all scikit-learn classifiers\n    can handle all types of multi-output combinations)\n\ncache : bool, default=True\n    Whether to cache downloaded datasets using joblib.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` objects.\n\nas_frame : bool or 'auto', default='auto'\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric, string or categorical). The target is\n    a pandas DataFrame or Series depending on the number of target_columns.\n    The Bunch will contain a ``frame`` attribute with the target and the\n    data. If ``return_X_y`` is True, then ``(data, target)`` will be pandas\n    DataFrames or Series as describe above.\n\n    If as_frame is 'auto', the data and target will be converted to\n    DataFrame or Series as if as_frame is set to True, unless the dataset\n    is stored in sparse format.\n\n    .. versionchanged:: 0.24\n       The default value of `as_frame` changed from `False` to `'auto'`\n       in 0.24.\n\nReturns\n-------\n\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : np.array, scipy.sparse.csr_matrix of floats, or pandas DataFrame\n        The feature matrix. Categorical features are encoded as ordinals.\n    target : np.array, pandas Series or DataFrame\n        The regression target or classification labels, if applicable.\n        Dtype is float if numeric, and object if categorical. If\n        ``as_frame`` is True, ``target`` is a pandas object.\n    DESCR : str\n        The full description of the dataset\n    feature_names : list\n        The names of the dataset columns\n    target_names: list\n        The names of the target columns\n\n    .. versionadded:: 0.22\n\n    categories : dict or None\n        Maps each categorical feature name to a list of values, such\n        that the value encoded as i is ith in the list. If ``as_frame``\n        is True, this is None.\n    details : dict\n        More metadata from OpenML\n    frame : pandas DataFrame\n        Only present when `as_frame=True`. DataFrame with ``data`` and\n        ``target``.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. note:: EXPERIMENTAL\n\n        This interface is **experimental** and subsequent releases may\n        change attributes without notice (although there should only be\n        minor changes to ``data`` and ``target``).\n\n    Missing values in the 'data' are represented as NaN's. Missing values\n    in 'target' are represented as NaN's (numerical target) or None\n    (categorical target)"
        }
      ]
    },
    {
      "name": "sklearn.datasets._rcv1",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "logging",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "gzip",
          "declaration": "GzipFile",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "makedirs",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "remove",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "dirname",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "exists",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "RemoteFileMetadata",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_fetch_remote",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_pkl_filepath",
          "alias": null
        },
        {
          "module": "sklearn.datasets._svmlight_format_io",
          "declaration": "load_svmlight_files",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "shuffle",
          "alias": "shuffle_"
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "fetch_rcv1",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "dataset",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : sparse matrix of shape (804414, 47236), dtype=np.float64\n    The array has 0.16% of non zero values. Will be of CSR format.\ntarget : sparse matrix of shape (804414, 103), dtype=np.uint8\n    Each sample has a value of 1 in its categories, and 0 in others.\n    The array has 3.15% of non zero values. Will be of CSR format.\nsample_id : ndarray of shape (804414,), dtype=np.uint32,\n    Identification number of each sample, as ordered in dataset.data.\ntarget_names : ndarray of shape (103,), dtype=object\n    Names of each target (RCV1 topics), as ordered in dataset.target.\nDESCR : str\n    Description of the RCV1 dataset."
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.20"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load the RCV1 multilabel dataset (classification).\n\nDownload it if necessary.\n\nVersion: RCV1-v2, vectors, full sets, topics multilabels.\n\n=================   =====================\nClasses                               103\nSamples total                      804414\nDimensionality                      47236\nFeatures            real, between 0 and 1\n=================   =====================\n\nRead more in the :ref:`User Guide <rcv1_dataset>`.\n\n.. versionadded:: 0.17\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nsubset : {'train', 'test', 'all'}, default='all'\n    Select the dataset to load: 'train' for the training set\n    (23149 samples), 'test' for the test set (781265 samples),\n    'all' for both, with the training samples first if shuffle is False.\n    This follows the official LYRL2004 chronological split.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=False\n    Whether to shuffle dataset.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\n    object. See below for more information about the `dataset.data` and\n    `dataset.target` object.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ndataset : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : sparse matrix of shape (804414, 47236), dtype=np.float64\n        The array has 0.16% of non zero values. Will be of CSR format.\n    target : sparse matrix of shape (804414, 103), dtype=np.uint8\n        Each sample has a value of 1 in its categories, and 0 in others.\n        The array has 3.15% of non zero values. Will be of CSR format.\n    sample_id : ndarray of shape (804414,), dtype=np.uint32,\n        Identification number of each sample, as ordered in dataset.data.\n    target_names : ndarray of shape (103,), dtype=object\n        Names of each target (RCV1 topics), as ordered in dataset.target.\n    DESCR : str\n        Description of the RCV1 dataset.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20"
        }
      ]
    },
    {
      "name": "sklearn.datasets._samples_generator",
      "imports": [
        {
          "module": "array",
          "alias": null
        },
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "collections.abc",
          "declaration": "Iterable",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "MultiLabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "shuffle",
          "alias": "util_shuffle"
        },
        {
          "module": "sklearn.utils.random",
          "declaration": "sample_without_replacement",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "make_biclusters",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "shape",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The shape of the result."
            },
            {
              "name": "n_clusters",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The number of biclusters."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The generated array."
            },
            {
              "name": "rows",
              "type": null,
              "description": "The indicators for cluster membership of each row."
            },
            {
              "name": "cols",
              "type": null,
              "description": "The indicators for cluster membership of each column."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate an array with constant block diagonal structure for\nbiclustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nshape : iterable of shape (n_rows, n_cols)\n    The shape of the result.\n\nn_clusters : int\n    The number of biclusters.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise.\n\nminval : int, default=10\n    Minimum value of a bicluster.\n\nmaxval : int, default=100\n    Maximum value of a bicluster.\n\nshuffle : bool, default=True\n    Shuffle the samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape `shape`\n    The generated array.\n\nrows : ndarray of shape (n_clusters, X.shape[0])\n    The indicators for cluster membership of each row.\n\ncols : ndarray of shape (n_clusters, X.shape[1])\n    The indicators for cluster membership of each column.\n\nReferences\n----------\n\n.. [1] Dhillon, I. S. (2001, August). Co-clustering documents and\n    words using bipartite spectral graph partitioning. In Proceedings\n    of the seventh ACM SIGKDD international conference on Knowledge\n    discovery and data mining (pp. 269-274). ACM.\n\nSee Also\n--------\nmake_checkerboard"
        },
        {
          "name": "make_blobs",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "If int, it is the total number of points equally divided among\nclusters.\nIf array-like, each element of the sequence indicates\nthe number of samples per cluster.\n\n.. versionchanged:: v0.20\n    one can now pass an array-like to the ``n_samples`` parameter"
            },
            {
              "name": "n_features",
              "type": "Any",
              "hasDefault": true,
              "default": "2",
              "limitation": null,
              "ignored": false,
              "description": "The number of features for each sample."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The generated samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The integer labels for cluster membership of each sample."
            },
            {
              "name": "centers",
              "type": null,
              "description": "The centers of each cluster. Only returned if\n``return_centers=True``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate isotropic Gaussian blobs for clustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int or array-like, default=100\n    If int, it is the total number of points equally divided among\n    clusters.\n    If array-like, each element of the sequence indicates\n    the number of samples per cluster.\n\n    .. versionchanged:: v0.20\n        one can now pass an array-like to the ``n_samples`` parameter\n\nn_features : int, default=2\n    The number of features for each sample.\n\ncenters : int or ndarray of shape (n_centers, n_features), default=None\n    The number of centers to generate, or the fixed center locations.\n    If n_samples is an int and centers is None, 3 centers are generated.\n    If n_samples is array-like, centers must be\n    either None or an array of length equal to the length of n_samples.\n\ncluster_std : float or array-like of float, default=1.0\n    The standard deviation of the clusters.\n\ncenter_box : tuple of float (min, max), default=(-10.0, 10.0)\n    The bounding box for each cluster center when centers are\n    generated at random.\n\nshuffle : bool, default=True\n    Shuffle the samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nreturn_centers : bool, default=False\n    If True, then return the centers of each cluster\n\n    .. versionadded:: 0.23\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels for cluster membership of each sample.\n\ncenters : ndarray of shape (n_centers, n_features)\n    The centers of each cluster. Only returned if\n    ``return_centers=True``.\n\nExamples\n--------\n>>> from sklearn.datasets import make_blobs\n>>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n...                   random_state=0)\n>>> print(X.shape)\n(10, 2)\n>>> y\narray([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])\n>>> X, y = make_blobs(n_samples=[3, 3, 4], centers=None, n_features=2,\n...                   random_state=0)\n>>> print(X.shape)\n(10, 2)\n>>> y\narray([0, 1, 2, 0, 2, 2, 2, 1, 1, 0])\n\nSee Also\n--------\nmake_classification : A more intricate variant."
        },
        {
          "name": "make_checkerboard",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "shape",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The shape of the result."
            },
            {
              "name": "n_clusters",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The number of row and column clusters."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The generated array."
            },
            {
              "name": "rows",
              "type": null,
              "description": "The indicators for cluster membership of each row."
            },
            {
              "name": "cols",
              "type": null,
              "description": "The indicators for cluster membership of each column."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate an array with block checkerboard structure for\nbiclustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nshape : tuple of shape (n_rows, n_cols)\n    The shape of the result.\n\nn_clusters : int or array-like or shape (n_row_clusters, n_column_clusters)\n    The number of row and column clusters.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise.\n\nminval : int, default=10\n    Minimum value of a bicluster.\n\nmaxval : int, default=100\n    Maximum value of a bicluster.\n\nshuffle : bool, default=True\n    Shuffle the samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape `shape`\n    The generated array.\n\nrows : ndarray of shape (n_clusters, X.shape[0])\n    The indicators for cluster membership of each row.\n\ncols : ndarray of shape (n_clusters, X.shape[1])\n    The indicators for cluster membership of each column.\n\n\nReferences\n----------\n\n.. [1] Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003).\n    Spectral biclustering of microarray data: coclustering genes\n    and conditions. Genome research, 13(4), 703-716.\n\nSee Also\n--------\nmake_biclusters"
        },
        {
          "name": "make_circles",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "If int, it is the total number of points generated.\nFor odd numbers, the inner circle will have one point more than the\nouter circle.\nIf two-element tuple, number of points in outer circle and inner\ncircle.\n\n.. versionchanged:: 0.23\n   Added two-element tuple."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The generated samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The integer labels (0 or 1) for class membership of each sample."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Make a large circle containing a smaller circle in 2d.\n\nA simple toy dataset to visualize clustering and classification\nalgorithms.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int or tuple of shape (2,), dtype=int, default=100\n    If int, it is the total number of points generated.\n    For odd numbers, the inner circle will have one point more than the\n    outer circle.\n    If two-element tuple, number of points in outer circle and inner\n    circle.\n\n    .. versionchanged:: 0.23\n       Added two-element tuple.\n\nshuffle : bool, default=True\n    Whether to shuffle the samples.\n\nnoise : float, default=None\n    Standard deviation of Gaussian noise added to the data.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling and noise.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nfactor : float, default=.8\n    Scale factor between inner and outer circle in the range `(0, 1)`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 2)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels (0 or 1) for class membership of each sample."
        },
        {
          "name": "make_classification",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples."
            },
            {
              "name": "n_features",
              "type": "Any",
              "hasDefault": true,
              "default": "20",
              "limitation": null,
              "ignored": false,
              "description": "The total number of features. These comprise ``n_informative``\ninformative features, ``n_redundant`` redundant features,\n``n_repeated`` duplicated features and\n``n_features-n_informative-n_redundant-n_repeated`` useless features\ndrawn at random."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The generated samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The integer labels for class membership of each sample."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a random n-class classification problem.\n\nThis initially creates clusters of points normally distributed (std=1)\nabout vertices of an ``n_informative``-dimensional hypercube with sides of\nlength ``2*class_sep`` and assigns an equal number of clusters to each\nclass. It introduces interdependence between these features and adds\nvarious types of further noise to the data.\n\nWithout shuffling, ``X`` horizontally stacks features in the following\norder: the primary ``n_informative`` features, followed by ``n_redundant``\nlinear combinations of the informative features, followed by ``n_repeated``\nduplicates, drawn randomly with replacement from the informative and\nredundant features. The remaining features are filled with random noise.\nThus, without shuffling, all useful features are contained in the columns\n``X[:, :n_informative + n_redundant + n_repeated]``.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=20\n    The total number of features. These comprise ``n_informative``\n    informative features, ``n_redundant`` redundant features,\n    ``n_repeated`` duplicated features and\n    ``n_features-n_informative-n_redundant-n_repeated`` useless features\n    drawn at random.\n\nn_informative : int, default=2\n    The number of informative features. Each class is composed of a number\n    of gaussian clusters each located around the vertices of a hypercube\n    in a subspace of dimension ``n_informative``. For each cluster,\n    informative features are drawn independently from  N(0, 1) and then\n    randomly linearly combined within each cluster in order to add\n    covariance. The clusters are then placed on the vertices of the\n    hypercube.\n\nn_redundant : int, default=2\n    The number of redundant features. These features are generated as\n    random linear combinations of the informative features.\n\nn_repeated : int, default=0\n    The number of duplicated features, drawn randomly from the informative\n    and the redundant features.\n\nn_classes : int, default=2\n    The number of classes (or labels) of the classification problem.\n\nn_clusters_per_class : int, default=2\n    The number of clusters per class.\n\nweights : array-like of shape (n_classes,) or (n_classes - 1,),              default=None\n    The proportions of samples assigned to each class. If None, then\n    classes are balanced. Note that if ``len(weights) == n_classes - 1``,\n    then the last class weight is automatically inferred.\n    More than ``n_samples`` samples may be returned if the sum of\n    ``weights`` exceeds 1. Note that the actual class proportions will\n    not exactly match ``weights`` when ``flip_y`` isn't 0.\n\nflip_y : float, default=0.01\n    The fraction of samples whose class is assigned randomly. Larger\n    values introduce noise in the labels and make the classification\n    task harder. Note that the default setting flip_y > 0 might lead\n    to less than ``n_classes`` in y in some cases.\n\nclass_sep : float, default=1.0\n    The factor multiplying the hypercube size.  Larger values spread\n    out the clusters/classes and make the classification task easier.\n\nhypercube : bool, default=True\n    If True, the clusters are put on the vertices of a hypercube. If\n    False, the clusters are put on the vertices of a random polytope.\n\nshift : float, ndarray of shape (n_features,) or None, default=0.0\n    Shift features by the specified value. If None, then features\n    are shifted by a random value drawn in [-class_sep, class_sep].\n\nscale : float, ndarray of shape (n_features,) or None, default=1.0\n    Multiply features by the specified value. If None, then features\n    are scaled by a random value drawn in [1, 100]. Note that scaling\n    happens after shifting.\n\nshuffle : bool, default=True\n    Shuffle the samples and the features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels for class membership of each sample.\n\nNotes\n-----\nThe algorithm is adapted from Guyon [1] and was designed to generate\nthe \"Madelon\" dataset.\n\nReferences\n----------\n.. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n       selection benchmark\", 2003.\n\nSee Also\n--------\nmake_blobs : Simplified variant.\nmake_multilabel_classification : Unrelated generator for multilabel tasks."
        },
        {
          "name": "make_friedman1",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples."
            },
            {
              "name": "n_features",
              "type": "Any",
              "hasDefault": true,
              "default": "10",
              "limitation": null,
              "ignored": false,
              "description": "The number of features. Should be at least 5."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The input samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The output values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate the \"Friedman #1\" regression problem.\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are independent features uniformly distributed on the interval\n[0, 1]. The output `y` is created according to the formula::\n\n    y(X) = 10 * sin(pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4] + noise * N(0, 1).\n\nOut of the `n_features` features, only 5 are actually used to compute\n`y`. The remaining features are independent of `y`.\n\nThe number of features has to be >= 5.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=10\n    The number of features. Should be at least 5.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise applied to the output.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset noise. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n       of Statistics 19 (1), pages 1-67, 1991.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n       pages 123-140, 1996."
        },
        {
          "name": "make_friedman2",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The input samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The output values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate the \"Friedman #2\" regression problem.\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are 4 independent features uniformly distributed on the\nintervals::\n\n    0 <= X[:, 0] <= 100,\n    40 * pi <= X[:, 1] <= 560 * pi,\n    0 <= X[:, 2] <= 1,\n    1 <= X[:, 3] <= 11.\n\nThe output `y` is created according to the formula::\n\n    y(X) = (X[:, 0] ** 2 + (X[:, 1] * X[:, 2]  - 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.5 + noise * N(0, 1).\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise applied to the output.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset noise. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 4)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n       of Statistics 19 (1), pages 1-67, 1991.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n       pages 123-140, 1996."
        },
        {
          "name": "make_friedman3",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The input samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The output values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate the \"Friedman #3\" regression problem.\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are 4 independent features uniformly distributed on the\nintervals::\n\n    0 <= X[:, 0] <= 100,\n    40 * pi <= X[:, 1] <= 560 * pi,\n    0 <= X[:, 2] <= 1,\n    1 <= X[:, 3] <= 11.\n\nThe output `y` is created according to the formula::\n\n    y(X) = arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) / X[:, 0]) + noise * N(0, 1).\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise applied to the output.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset noise. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 4)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n       of Statistics 19 (1), pages 1-67, 1991.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n       pages 123-140, 1996."
        },
        {
          "name": "make_gaussian_quantiles",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The generated samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The integer labels for quantile membership of each sample."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate isotropic Gaussian and label samples by quantile.\n\nThis classification dataset is constructed by taking a multi-dimensional\nstandard normal distribution and defining classes separated by nested\nconcentric multi-dimensional spheres such that roughly equal numbers of\nsamples are in each class (quantiles of the :math:`\\chi^2` distribution).\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nmean : ndarray of shape (n_features,), default=None\n    The mean of the multi-dimensional normal distribution.\n    If None then use the origin (0, 0, ...).\n\ncov : float, default=1.0\n    The covariance matrix will be this value times the unit matrix. This\n    dataset only produces symmetric normal distributions.\n\nn_samples : int, default=100\n    The total number of points equally divided among classes.\n\nn_features : int, default=2\n    The number of features for each sample.\n\nn_classes : int, default=3\n    The number of classes\n\nshuffle : bool, default=True\n    Shuffle the samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels for quantile membership of each sample.\n\nNotes\n-----\nThe dataset is from Zhu et al [1].\n\nReferences\n----------\n.. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009."
        },
        {
          "name": "make_hastie_10_2",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "12000",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The input samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The output values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generates data for binary classification used in\nHastie et al. 2009, Example 10.2.\n\nThe ten features are standard independent Gaussian and\nthe target ``y`` is defined by::\n\n  y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=12000\n    The number of samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 10)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n       Learning Ed. 2\", Springer, 2009.\n\nSee Also\n--------\nmake_gaussian_quantiles : A generalization of this dataset approach."
        },
        {
          "name": "make_low_rank_matrix",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples."
            },
            {
              "name": "n_features",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of features."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The matrix."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a mostly low rank matrix with bell-shaped singular values.\n\nMost of the variance can be explained by a bell-shaped curve of width\neffective_rank: the low rank part of the singular values profile is::\n\n    (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n\nThe remaining singular values' tail is fat, decreasing as::\n\n    tail_strength * exp(-0.1 * i / effective_rank).\n\nThe low rank part of the profile can be considered the structured\nsignal part of the data while the tail can be considered the noisy\npart of the data that cannot be summarized by a low number of linear\ncomponents (singular vectors).\n\nThis kind of singular profiles is often seen in practice, for instance:\n - gray level pictures of faces\n - TF-IDF vectors of text documents crawled from the web\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=100\n    The number of features.\n\neffective_rank : int, default=10\n    The approximate number of singular vectors required to explain most of\n    the data by linear combinations.\n\ntail_strength : float, default=0.5\n    The relative importance of the fat noisy tail of the singular values\n    profile. The value should be between 0 and 1.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The matrix."
        },
        {
          "name": "make_moons",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "If int, the total number of points generated.\nIf two-element tuple, number of points in each of two moons.\n\n.. versionchanged:: 0.23\n   Added two-element tuple."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The generated samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The integer labels (0 or 1) for class membership of each sample."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Make two interleaving half circles.\n\nA simple toy dataset to visualize clustering and classification\nalgorithms. Read more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int or tuple of shape (2,), dtype=int, default=100\n    If int, the total number of points generated.\n    If two-element tuple, number of points in each of two moons.\n\n    .. versionchanged:: 0.23\n       Added two-element tuple.\n\nshuffle : bool, default=True\n    Whether to shuffle the samples.\n\nnoise : float, default=None\n    Standard deviation of Gaussian noise added to the data.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling and noise.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 2)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels (0 or 1) for class membership of each sample."
        },
        {
          "name": "make_multilabel_classification",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples."
            },
            {
              "name": "n_features",
              "type": "Any",
              "hasDefault": true,
              "default": "20",
              "limitation": null,
              "ignored": false,
              "description": "The total number of features."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The generated samples."
            },
            {
              "name": "Y",
              "type": null,
              "description": "The label sets. Sparse matrix should be of CSR format."
            },
            {
              "name": "p_c",
              "type": null,
              "description": "The probability of each class being drawn. Only returned if\n``return_distributions=True``."
            },
            {
              "name": "p_w_c",
              "type": null,
              "description": "The probability of each feature being drawn given each class.\nOnly returned if ``return_distributions=True``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a random multilabel classification problem.\n\nFor each sample, the generative process is:\n    - pick the number of labels: n ~ Poisson(n_labels)\n    - n times, choose a class c: c ~ Multinomial(theta)\n    - pick the document length: k ~ Poisson(length)\n    - k times, choose a word: w ~ Multinomial(theta_c)\n\nIn the above process, rejection sampling is used to make sure that\nn is never zero or more than `n_classes`, and that the document length\nis never zero. Likewise, we reject classes which have already been chosen.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=20\n    The total number of features.\n\nn_classes : int, default=5\n    The number of classes of the classification problem.\n\nn_labels : int, default=2\n    The average number of labels per instance. More precisely, the number\n    of labels per sample is drawn from a Poisson distribution with\n    ``n_labels`` as its expected value, but samples are bounded (using\n    rejection sampling) by ``n_classes``, and must be nonzero if\n    ``allow_unlabeled`` is False.\n\nlength : int, default=50\n    The sum of the features (number of words if documents) is drawn from\n    a Poisson distribution with this expected value.\n\nallow_unlabeled : bool, default=True\n    If ``True``, some instances might not belong to any class.\n\nsparse : bool, default=False\n    If ``True``, return a sparse feature matrix\n\n    .. versionadded:: 0.17\n       parameter to allow *sparse* output.\n\nreturn_indicator : {'dense', 'sparse'} or False, default='dense'\n    If ``'dense'`` return ``Y`` in the dense binary indicator format. If\n    ``'sparse'`` return ``Y`` in the sparse binary indicator format.\n    ``False`` returns a list of lists of labels.\n\nreturn_distributions : bool, default=False\n    If ``True``, return the prior class probability and conditional\n    probabilities of features given classes, from which the data was\n    drawn.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The generated samples.\n\nY : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    The label sets. Sparse matrix should be of CSR format.\n\np_c : ndarray of shape (n_classes,)\n    The probability of each class being drawn. Only returned if\n    ``return_distributions=True``.\n\np_w_c : ndarray of shape (n_features, n_classes)\n    The probability of each feature being drawn given each class.\n    Only returned if ``return_distributions=True``."
        },
        {
          "name": "make_regression",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples."
            },
            {
              "name": "n_features",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of features."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The input samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The output values."
            },
            {
              "name": "coef",
              "type": null,
              "description": "The coefficient of the underlying linear model. It is returned only if\ncoef is True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a random regression problem.\n\nThe input set can either be well conditioned (by default) or have a low\nrank-fat tail singular profile. See :func:`make_low_rank_matrix` for\nmore details.\n\nThe output is generated by applying a (potentially biased) random linear\nregression model with `n_informative` nonzero regressors to the previously\ngenerated input and some gaussian centered noise with some adjustable\nscale.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=100\n    The number of features.\n\nn_informative : int, default=10\n    The number of informative features, i.e., the number of features used\n    to build the linear model used to generate the output.\n\nn_targets : int, default=1\n    The number of regression targets, i.e., the dimension of the y output\n    vector associated with a sample. By default, the output is a scalar.\n\nbias : float, default=0.0\n    The bias term in the underlying linear model.\n\neffective_rank : int, default=None\n    if not None:\n        The approximate number of singular vectors required to explain most\n        of the input data by linear combinations. Using this kind of\n        singular spectrum in the input allows the generator to reproduce\n        the correlations often observed in practice.\n    if None:\n        The input set is well conditioned, centered and gaussian with\n        unit variance.\n\ntail_strength : float, default=0.5\n    The relative importance of the fat noisy tail of the singular values\n    profile if `effective_rank` is not None. When a float, it should be\n    between 0 and 1.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise applied to the output.\n\nshuffle : bool, default=True\n    Shuffle the samples and the features.\n\ncoef : bool, default=False\n    If True, the coefficients of the underlying linear model are returned.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The input samples.\n\ny : ndarray of shape (n_samples,) or (n_samples, n_targets)\n    The output values.\n\ncoef : ndarray of shape (n_features,) or (n_features, n_targets)\n    The coefficient of the underlying linear model. It is returned only if\n    coef is True."
        },
        {
          "name": "make_s_curve",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of sample points on the S curve."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The points."
            },
            {
              "name": "t",
              "type": null,
              "description": "The univariate position of the sample according to the main dimension\nof the points in the manifold."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate an S curve dataset.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of sample points on the S curve.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 3)\n    The points.\n\nt : ndarray of shape (n_samples,)\n    The univariate position of the sample according to the main dimension\n    of the points in the manifold."
        },
        {
          "name": "make_sparse_coded_signal",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of samples to generate"
            }
          ],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "The encoded signal (Y)."
            },
            {
              "name": "dictionary",
              "type": null,
              "description": "The dictionary with normalized components (D)."
            },
            {
              "name": "code",
              "type": null,
              "description": "The sparse code such that each column of this matrix has exactly\nn_nonzero_coefs non-zero items (X)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a signal as a sparse combination of dictionary elements.\n\nReturns a matrix Y = DX, such as D is (n_features, n_components),\nX is (n_components, n_samples) and each column of X has exactly\nn_nonzero_coefs non-zero elements.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int\n    Number of samples to generate\n\nn_components : int\n    Number of components in the dictionary\n\nn_features : int\n    Number of features of the dataset to generate\n\nn_nonzero_coefs : int\n    Number of active (non-zero) coefficients in each sample\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\ndata : ndarray of shape (n_features, n_samples)\n    The encoded signal (Y).\n\ndictionary : ndarray of shape (n_features, n_components)\n    The dictionary with normalized components (D).\n\ncode : ndarray of shape (n_components, n_samples)\n    The sparse code such that each column of this matrix has exactly\n    n_nonzero_coefs non-zero items (X)."
        },
        {
          "name": "make_sparse_spd_matrix",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "dim",
              "type": "Any",
              "hasDefault": true,
              "default": "1",
              "limitation": null,
              "ignored": false,
              "description": "The size of the random matrix to generate."
            }
          ],
          "results": [
            {
              "name": "prec",
              "type": null,
              "description": "The generated matrix."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a sparse symmetric definite positive matrix.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\ndim : int, default=1\n    The size of the random matrix to generate.\n\nalpha : float, default=0.95\n    The probability that a coefficient is zero (see notes). Larger values\n    enforce more sparsity. The value should be in the range 0 and 1.\n\nnorm_diag : bool, default=False\n    Whether to normalize the output matrix to make the leading diagonal\n    elements all 1\n\nsmallest_coef : float, default=0.1\n    The value of the smallest coefficient between 0 and 1.\n\nlargest_coef : float, default=0.9\n    The value of the largest coefficient between 0 and 1.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nprec : sparse matrix of shape (dim, dim)\n    The generated matrix.\n\nNotes\n-----\nThe sparsity is actually imposed on the cholesky factor of the matrix.\nThus alpha does not translate directly into the filling fraction of\nthe matrix itself.\n\nSee Also\n--------\nmake_spd_matrix"
        },
        {
          "name": "make_sparse_uncorrelated",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of samples."
            },
            {
              "name": "n_features",
              "type": "Any",
              "hasDefault": true,
              "default": "10",
              "limitation": null,
              "ignored": false,
              "description": "The number of features."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The input samples."
            },
            {
              "name": "y",
              "type": null,
              "description": "The output values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a random regression problem with sparse uncorrelated design.\n\nThis dataset is described in Celeux et al [1]. as::\n\n    X ~ N(0, 1)\n    y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]\n\nOnly the first 4 features are informative. The remaining features are\nuseless.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=10\n    The number of features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert,\n       \"Regularization in regression: comparing Bayesian and frequentist\n       methods in a poorly informative situation\", 2009."
        },
        {
          "name": "make_spd_matrix",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_dim",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The matrix dimension."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The random symmetric, positive-definite matrix."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a random symmetric, positive-definite matrix.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_dim : int\n    The matrix dimension.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_dim, n_dim)\n    The random symmetric, positive-definite matrix.\n\nSee Also\n--------\nmake_sparse_spd_matrix"
        },
        {
          "name": "make_swiss_roll",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "The number of sample points on the S curve."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "The points."
            },
            {
              "name": "t",
              "type": null,
              "description": "The univariate position of the sample according to the main dimension\nof the points in the manifold."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a swiss roll dataset.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of sample points on the S curve.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 3)\n    The points.\n\nt : ndarray of shape (n_samples,)\n    The univariate position of the sample according to the main dimension\n    of the points in the manifold.\n\nNotes\n-----\nThe algorithm is from Marsland [1].\n\nReferences\n----------\n.. [1] S. Marsland, \"Machine Learning: An Algorithmic Perspective\",\n       Chapter 10, 2009.\n       http://seat.massey.ac.nz/personal/s.r.marsland/Code/10/lle.py"
        }
      ]
    },
    {
      "name": "sklearn.datasets._species_distributions",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "logging",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "io",
          "declaration": "BytesIO",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "makedirs",
          "alias": null
        },
        {
          "module": "os",
          "declaration": "remove",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "exists",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "RemoteFileMetadata",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_fetch_remote",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_pkl_filepath",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "construct_grids",
          "decorators": [],
          "parameters": [
            {
              "name": "batch",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The object returned by :func:`fetch_species_distributions`"
            }
          ],
          "results": [
            {
              "name": "(xgrid, ygrid)",
              "type": null,
              "description": "The grid corresponding to the values in batch.coverages"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Construct the map grid from the batch object\n\nParameters\n----------\nbatch : Batch object\n    The object returned by :func:`fetch_species_distributions`\n\nReturns\n-------\n(xgrid, ygrid) : 1-D arrays\n    The grid corresponding to the values in batch.coverages"
        },
        {
          "name": "fetch_species_distributions",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "data",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ncoverages : array, shape = [14, 1592, 1212]\n    These represent the 14 features measured\n    at each point of the map grid.\n    The latitude/longitude values for the grid are discussed below.\n    Missing data is represented by the value -9999.\ntrain : record array, shape = (1624,)\n    The training points for the data.  Each point has three fields:\n\n    - train['species'] is the species name\n    - train['dd long'] is the longitude, in degrees\n    - train['dd lat'] is the latitude, in degrees\ntest : record array, shape = (620,)\n    The test points for the data.  Same format as the training data.\nNx, Ny : integers\n    The number of longitudes (x) and latitudes (y) in the grid\nx_left_lower_corner, y_left_lower_corner : floats\n    The (x,y) position of the lower-left corner, in degrees\ngrid_size : float\n    The spacing between points of the grid, in degrees"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Loader for species distribution dataset from Phillips et. al. (2006)\n\nRead more in the :ref:`User Guide <datasets>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    coverages : array, shape = [14, 1592, 1212]\n        These represent the 14 features measured\n        at each point of the map grid.\n        The latitude/longitude values for the grid are discussed below.\n        Missing data is represented by the value -9999.\n    train : record array, shape = (1624,)\n        The training points for the data.  Each point has three fields:\n\n        - train['species'] is the species name\n        - train['dd long'] is the longitude, in degrees\n        - train['dd lat'] is the latitude, in degrees\n    test : record array, shape = (620,)\n        The test points for the data.  Same format as the training data.\n    Nx, Ny : integers\n        The number of longitudes (x) and latitudes (y) in the grid\n    x_left_lower_corner, y_left_lower_corner : floats\n        The (x,y) position of the lower-left corner, in degrees\n    grid_size : float\n        The spacing between points of the grid, in degrees\n\nReferences\n----------\n\n* `\"Maximum entropy modeling of species geographic distributions\"\n  <http://rob.schapire.net/papers/ecolmod.pdf>`_\n  S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n  190:231-259, 2006.\n\nNotes\n-----\n\nThis dataset represents the geographic distribution of species.\nThe dataset is provided by Phillips et. al. (2006).\n\nThe two species are:\n\n- `\"Bradypus variegatus\"\n  <http://www.iucnredlist.org/details/3038/0>`_ ,\n  the Brown-throated Sloth.\n\n- `\"Microryzomys minutus\"\n  <http://www.iucnredlist.org/details/13408/0>`_ ,\n  also known as the Forest Small Rice Rat, a rodent that lives in Peru,\n  Colombia, Ecuador, Peru, and Venezuela.\n\n- For an example of using this dataset with scikit-learn, see\n  :ref:`examples/applications/plot_species_distribution_modeling.py\n  <sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py>`."
        }
      ]
    },
    {
      "name": "sklearn.datasets._svmlight_format_io",
      "imports": [
        {
          "module": "io",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "os.path",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "contextlib",
          "declaration": "closing",
          "alias": null
        },
        {
          "module": "sklearn",
          "declaration": "__version__",
          "alias": null
        },
        {
          "module": "sklearn.datasets._svmlight_format_fast",
          "declaration": "_load_svmlight_file",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "IS_PYPY",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "dump_svmlight_file",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values. Class labels must be an\ninteger or float, or array-like objects of integer or float for\nmultilabel classifications."
            },
            {
              "name": "f",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If string, specifies the path that will contain the data.\nIf file-like, data will be written to f. f should be opened in binary\nmode."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Dump the dataset in svmlight / libsvm file format.\n\nThis format is a text-based format, with one sample per line. It does\nnot store zero valued features hence is suitable for sparse dataset.\n\nThe first element of each line can be used to store a target variable\nto predict.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : {array-like, sparse matrix}, shape = [n_samples (, n_labels)]\n    Target values. Class labels must be an\n    integer or float, or array-like objects of integer or float for\n    multilabel classifications.\n\nf : string or file-like in binary mode\n    If string, specifies the path that will contain the data.\n    If file-like, data will be written to f. f should be opened in binary\n    mode.\n\nzero_based : boolean, default=True\n    Whether column indices should be written zero-based (True) or one-based\n    (False).\n\ncomment : string, default=None\n    Comment to insert at the top of the file. This should be either a\n    Unicode string, which will be encoded as UTF-8, or an ASCII byte\n    string.\n    If a comment is given, then it will be preceded by one that identifies\n    the file as having been dumped by scikit-learn. Note that not all\n    tools grok comments in SVMlight files.\n\nquery_id : array-like of shape (n_samples,), default=None\n    Array containing pairwise preference constraints (qid in svmlight\n    format).\n\nmultilabel : boolean, default=False\n    Samples may have several labels each (see\n    https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\n    .. versionadded:: 0.17\n       parameter *multilabel* to support multilabel datasets."
        },
        {
          "name": "load_svmlight_file",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "f",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "(Path to) a file to load. If a path ends in \".gz\" or \".bz2\", it will\nbe uncompressed on the fly. If an integer is passed, it is assumed to\nbe a file descriptor. A file-like or file descriptor will not be closed\nby this function. A file-like object must be opened in binary mode."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": ""
            },
            {
              "name": "y",
              "type": null,
              "description": "tuples of length n_samples."
            },
            {
              "name": "query_id",
              "type": null,
              "description": "query_id for each sample. Only returned when query_id is set to\nTrue."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load datasets in the svmlight / libsvm format into sparse CSR matrix\n\nThis format is a text-based format, with one sample per line. It does\nnot store zero valued features hence is suitable for sparse dataset.\n\nThe first element of each line can be used to store a target variable\nto predict.\n\nThis format is used as the default format for both svmlight and the\nlibsvm command line programs.\n\nParsing a text based source can be expensive. When working on\nrepeatedly on the same dataset, it is recommended to wrap this\nloader with joblib.Memory.cache to store a memmapped backup of the\nCSR results of the first call and benefit from the near instantaneous\nloading of memmapped structures for the subsequent calls.\n\nIn case the file contains a pairwise preference constraint (known\nas \"qid\" in the svmlight format) these are ignored unless the\nquery_id parameter is set to True. These pairwise preference\nconstraints can be used to constraint the combination of samples\nwhen using pairwise loss functions (as is the case in some\nlearning to rank problems) so that only pairs with the same\nquery_id value are considered.\n\nThis implementation is written in Cython and is reasonably fast.\nHowever, a faster API-compatible loader is also available at:\n\n  https://github.com/mblondel/svmlight-loader\n\nParameters\n----------\nf : str, file-like or int\n    (Path to) a file to load. If a path ends in \".gz\" or \".bz2\", it will\n    be uncompressed on the fly. If an integer is passed, it is assumed to\n    be a file descriptor. A file-like or file descriptor will not be closed\n    by this function. A file-like object must be opened in binary mode.\n\nn_features : int, default=None\n    The number of features to use. If None, it will be inferred. This\n    argument is useful to load several files that are subsets of a\n    bigger sliced dataset: each subset might not have examples of\n    every feature, hence the inferred shape might vary from one\n    slice to another.\n    n_features is only required if ``offset`` or ``length`` are passed a\n    non-default value.\n\ndtype : numpy data type, default=np.float64\n    Data type of dataset to be loaded. This will be the data type of the\n    output numpy arrays ``X`` and ``y``.\n\nmultilabel : bool, default=False\n    Samples may have several labels each (see\n    https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\nzero_based : bool or \"auto\", default=\"auto\"\n    Whether column indices in f are zero-based (True) or one-based\n    (False). If column indices are one-based, they are transformed to\n    zero-based to match Python/NumPy conventions.\n    If set to \"auto\", a heuristic check is applied to determine this from\n    the file contents. Both kinds of files occur \"in the wild\", but they\n    are unfortunately not self-identifying. Using \"auto\" or True should\n    always be safe when no ``offset`` or ``length`` is passed.\n    If ``offset`` or ``length`` are passed, the \"auto\" mode falls back\n    to ``zero_based=True`` to avoid having the heuristic check yield\n    inconsistent results on different segments of the file.\n\nquery_id : bool, default=False\n    If True, will return the query_id array for each file.\n\noffset : int, default=0\n    Ignore the offset first bytes by seeking forward, then\n    discarding the following bytes up until the next new line\n    character.\n\nlength : int, default=-1\n    If strictly positive, stop reading any new line of data once the\n    position in the file has reached the (offset + length) bytes threshold.\n\nReturns\n-------\nX : scipy.sparse matrix of shape (n_samples, n_features)\n\ny : ndarray of shape (n_samples,), or, in the multilabel a list of\n    tuples of length n_samples.\n\nquery_id : array of shape (n_samples,)\n   query_id for each sample. Only returned when query_id is set to\n   True.\n\nSee Also\n--------\nload_svmlight_files : Similar function for loading multiple files in this\n    format, enforcing the same number of features/columns on all of them.\n\nExamples\n--------\nTo use joblib.Memory to cache the svmlight file::\n\n    from joblib import Memory\n    from .datasets import load_svmlight_file\n    mem = Memory(\"./mycache\")\n\n    @mem.cache\n    def get_data():\n        data = load_svmlight_file(\"mysvmlightfile\")\n        return data[0], data[1]\n\n    X, y = get_data()"
        },
        {
          "name": "load_svmlight_files",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "files",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "(Paths of) files to load. If a path ends in \".gz\" or \".bz2\", it will\nbe uncompressed on the fly. If an integer is passed, it is assumed to\nbe a file descriptor. File-likes and file descriptors will not be\nclosed by this function. File-like objects must be opened in binary\nmode."
            }
          ],
          "results": [
            {
              "name": "",
              "type": "Any",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load dataset from multiple files in SVMlight format\n\nThis function is equivalent to mapping load_svmlight_file over a list of\nfiles, except that the results are concatenated into a single, flat list\nand the samples vectors are constrained to all have the same number of\nfeatures.\n\nIn case the file contains a pairwise preference constraint (known\nas \"qid\" in the svmlight format) these are ignored unless the\nquery_id parameter is set to True. These pairwise preference\nconstraints can be used to constraint the combination of samples\nwhen using pairwise loss functions (as is the case in some\nlearning to rank problems) so that only pairs with the same\nquery_id value are considered.\n\nParameters\n----------\nfiles : array-like, dtype=str, file-like or int\n    (Paths of) files to load. If a path ends in \".gz\" or \".bz2\", it will\n    be uncompressed on the fly. If an integer is passed, it is assumed to\n    be a file descriptor. File-likes and file descriptors will not be\n    closed by this function. File-like objects must be opened in binary\n    mode.\n\nn_features : int, default=None\n    The number of features to use. If None, it will be inferred from the\n    maximum column index occurring in any of the files.\n\n    This can be set to a higher value than the actual number of features\n    in any of the input files, but setting it to a lower value will cause\n    an exception to be raised.\n\ndtype : numpy data type, default=np.float64\n    Data type of dataset to be loaded. This will be the data type of the\n    output numpy arrays ``X`` and ``y``.\n\nmultilabel : bool, default=False\n    Samples may have several labels each (see\n    https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\nzero_based : bool or \"auto\", default=\"auto\"\n    Whether column indices in f are zero-based (True) or one-based\n    (False). If column indices are one-based, they are transformed to\n    zero-based to match Python/NumPy conventions.\n    If set to \"auto\", a heuristic check is applied to determine this from\n    the file contents. Both kinds of files occur \"in the wild\", but they\n    are unfortunately not self-identifying. Using \"auto\" or True should\n    always be safe when no offset or length is passed.\n    If offset or length are passed, the \"auto\" mode falls back\n    to zero_based=True to avoid having the heuristic check yield\n    inconsistent results on different segments of the file.\n\nquery_id : bool, default=False\n    If True, will return the query_id array for each file.\n\noffset : int, default=0\n    Ignore the offset first bytes by seeking forward, then\n    discarding the following bytes up until the next new line\n    character.\n\nlength : int, default=-1\n    If strictly positive, stop reading any new line of data once the\n    position in the file has reached the (offset + length) bytes threshold.\n\nReturns\n-------\n[X1, y1, ..., Xn, yn]\nwhere each (Xi, yi) pair is the result from load_svmlight_file(files[i]).\n\nIf query_id is set to True, this will return instead [X1, y1, q1,\n..., Xn, yn, qn] where (Xi, yi, qi) is the result from\nload_svmlight_file(files[i])\n\nNotes\n-----\nWhen fitting a model to a matrix X_train and evaluating it against a\nmatrix X_test, it is essential that X_train and X_test have the same\nnumber of features (X_train.shape[1] == X_test.shape[1]). This may not\nbe the case if you load the files individually with load_svmlight_file.\n\nSee Also\n--------\nload_svmlight_file"
        }
      ]
    },
    {
      "name": "sklearn.datasets._twenty_newsgroups",
      "imports": [
        {
          "module": "codecs",
          "alias": null
        },
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "logging",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "pickle",
          "alias": null
        },
        {
          "module": "re",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "shutil",
          "alias": null
        },
        {
          "module": "tarfile",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "os.path",
          "declaration": "dirname",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "sklearn",
          "declaration": "preprocessing",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "get_data_home",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "load_files",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "RemoteFileMetadata",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_convert_data_dataframe",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_fetch_remote",
          "alias": null
        },
        {
          "module": "sklearn.datasets._base",
          "declaration": "_pkl_filepath",
          "alias": null
        },
        {
          "module": "sklearn.feature_extraction.text",
          "declaration": "CountVectorizer",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "fetch_20newsgroups",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "bunch",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata : list of shape (n_samples,)\n    The data list to learn.\ntarget: ndarray of shape (n_samples,)\n    The target labels.\nfilenames: list of shape (n_samples,)\n    The path to the location of the data.\nDESCR: str\n    The full description of the dataset.\ntarget_names: list of shape (n_classes,)\n    The names of target classes."
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": ".. versionadded:: 0.22"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load the filenames and data from the 20 newsgroups dataset (classification).\n\nDownload it if necessary.\n\n=================   ==========\nClasses                     20\nSamples total            18846\nDimensionality               1\nFeatures                  text\n=================   ==========\n\nRead more in the :ref:`User Guide <20newsgroups_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify a download and cache folder for the datasets. If None,\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nsubset : {'train', 'test', 'all'}, default='train'\n    Select the dataset to load: 'train' for the training set, 'test'\n    for the test set, 'all' for both, with shuffled ordering.\n\ncategories : array-like, dtype=str or unicode, default=None\n    If None (default), load all the categories.\n    If not None, list of category names to load (other categories\n    ignored).\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data: might be important for models that\n    make the assumption that the samples are independent and identically\n    distributed (i.i.d.), such as stochastic gradient descent.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nremove : tuple, default=()\n    May contain any subset of ('headers', 'footers', 'quotes'). Each of\n    these are kinds of text that will be detected and removed from the\n    newsgroup posts, preventing classifiers from overfitting on\n    metadata.\n\n    'headers' removes newsgroup headers, 'footers' removes blocks at the\n    ends of posts that look like signatures, and 'quotes' removes lines\n    that appear to be quoting another post.\n\n    'headers' follows an exact standard; the other filters are not always\n    correct.\n\ndownload_if_missing : bool, default=True\n    If False, raise an IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns `(data.data, data.target)` instead of a Bunch\n    object.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\nbunch : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : list of shape (n_samples,)\n        The data list to learn.\n    target: ndarray of shape (n_samples,)\n        The target labels.\n    filenames: list of shape (n_samples,)\n        The path to the location of the data.\n    DESCR: str\n        The full description of the dataset.\n    target_names: list of shape (n_classes,)\n        The names of target classes.\n\n(data, target) : tuple if `return_X_y=True`\n    .. versionadded:: 0.22"
        },
        {
          "name": "fetch_20newsgroups_vectorized",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [],
          "results": [
            {
              "name": "bunch",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\ndata: {sparse matrix, dataframe} of shape (n_samples, n_features)\n    The input data matrix. If ``as_frame`` is `True`, ``data`` is\n    a pandas DataFrame with sparse columns.\ntarget: {ndarray, series} of shape (n_samples,)\n    The target labels. If ``as_frame`` is `True`, ``target`` is a\n    pandas Series.\ntarget_names: list of shape (n_classes,)\n    The names of target classes.\nDESCR: str\n    The full description of the dataset.\nframe: dataframe of shape (n_samples, n_features + 1)\n    Only present when `as_frame=True`. Pandas DataFrame with ``data``\n    and ``target``.\n\n    .. versionadded:: 0.24"
            },
            {
              "name": "(data, target)",
              "type": null,
              "description": "`data` and `target` would be of the format defined in the `Bunch`\ndescription above.\n\n.. versionadded:: 0.20"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load and vectorize the 20 newsgroups dataset (classification).\n\nDownload it if necessary.\n\nThis is a convenience function; the transformation is done using the\ndefault settings for\n:class:`~sklearn.feature_extraction.text.CountVectorizer`. For more\nadvanced usage (stopword filtering, n-gram extraction, etc.), combine\nfetch_20newsgroups with a custom\n:class:`~sklearn.feature_extraction.text.CountVectorizer`,\n:class:`~sklearn.feature_extraction.text.HashingVectorizer`,\n:class:`~sklearn.feature_extraction.text.TfidfTransformer` or\n:class:`~sklearn.feature_extraction.text.TfidfVectorizer`.\n\nThe resulting counts are normalized using\n:func:`sklearn.preprocessing.normalize` unless normalize is set to False.\n\n=================   ==========\nClasses                     20\nSamples total            18846\nDimensionality          130107\nFeatures                  real\n=================   ==========\n\nRead more in the :ref:`User Guide <20newsgroups_dataset>`.\n\nParameters\n----------\nsubset : {'train', 'test', 'all'}, default='train'\n    Select the dataset to load: 'train' for the training set, 'test'\n    for the test set, 'all' for both, with shuffled ordering.\n\nremove : tuple, default=()\n    May contain any subset of ('headers', 'footers', 'quotes'). Each of\n    these are kinds of text that will be detected and removed from the\n    newsgroup posts, preventing classifiers from overfitting on\n    metadata.\n\n    'headers' removes newsgroup headers, 'footers' removes blocks at the\n    ends of posts that look like signatures, and 'quotes' removes lines\n    that appear to be quoting another post.\n\ndata_home : str, default=None\n    Specify an download and cache folder for the datasets. If None,\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True\n    If False, raise an IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data.data, data.target)`` instead of a Bunch\n    object.\n\n    .. versionadded:: 0.20\n\nnormalize : bool, default=True\n    If True, normalizes each document's feature vector to unit norm using\n    :func:`sklearn.preprocessing.normalize`.\n\n    .. versionadded:: 0.22\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric, string, or categorical). The target is\n    a pandas DataFrame or Series depending on the number of\n    `target_columns`.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nbunch : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data: {sparse matrix, dataframe} of shape (n_samples, n_features)\n        The input data matrix. If ``as_frame`` is `True`, ``data`` is\n        a pandas DataFrame with sparse columns.\n    target: {ndarray, series} of shape (n_samples,)\n        The target labels. If ``as_frame`` is `True`, ``target`` is a\n        pandas Series.\n    target_names: list of shape (n_classes,)\n        The names of target classes.\n    DESCR: str\n        The full description of the dataset.\n    frame: dataframe of shape (n_samples, n_features + 1)\n        Only present when `as_frame=True`. Pandas DataFrame with ``data``\n        and ``target``.\n\n        .. versionadded:: 0.24\n\n(data, target) : tuple if ``return_X_y`` is True\n    `data` and `target` would be of the format defined in the `Bunch`\n    description above.\n\n    .. versionadded:: 0.20"
        },
        {
          "name": "strip_newsgroup_footer",
          "decorators": [],
          "parameters": [
            {
              "name": "text",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The text from which to remove the signature block."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Given text in \"news\" format, attempt to remove a signature block.\n\nAs a rough heuristic, we assume that signatures are set apart by either\na blank line or a line made of hyphens, and that it is the last such line\nin the file (disregarding blank lines at the end).\n\nParameters\n----------\ntext : str\n    The text from which to remove the signature block."
        },
        {
          "name": "strip_newsgroup_header",
          "decorators": [],
          "parameters": [
            {
              "name": "text",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The text from which to remove the signature block."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Given text in \"news\" format, strip the headers, by removing everything\nbefore the first blank line.\n\nParameters\n----------\ntext : str\n    The text from which to remove the signature block."
        },
        {
          "name": "strip_newsgroup_quoting",
          "decorators": [],
          "parameters": [
            {
              "name": "text",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The text from which to remove the signature block."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Given text in \"news\" format, strip lines beginning with the quote\ncharacters > or |, plus lines that often introduce a quoted section\n(for example, because they contain the string 'writes:'.)\n\nParameters\n----------\ntext : str\n    The text from which to remove the signature block."
        }
      ]
    },
    {
      "name": "sklearn.datasets.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "platform",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.decomposition",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._dict_learning",
          "declaration": "DictionaryLearning",
          "alias": null
        },
        {
          "module": "sklearn._dict_learning",
          "declaration": "MiniBatchDictionaryLearning",
          "alias": null
        },
        {
          "module": "sklearn._dict_learning",
          "declaration": "SparseCoder",
          "alias": null
        },
        {
          "module": "sklearn._dict_learning",
          "declaration": "dict_learning",
          "alias": null
        },
        {
          "module": "sklearn._dict_learning",
          "declaration": "dict_learning_online",
          "alias": null
        },
        {
          "module": "sklearn._dict_learning",
          "declaration": "sparse_encode",
          "alias": null
        },
        {
          "module": "sklearn._factor_analysis",
          "declaration": "FactorAnalysis",
          "alias": null
        },
        {
          "module": "sklearn._fastica",
          "declaration": "FastICA",
          "alias": null
        },
        {
          "module": "sklearn._fastica",
          "declaration": "fastica",
          "alias": null
        },
        {
          "module": "sklearn._incremental_pca",
          "declaration": "IncrementalPCA",
          "alias": null
        },
        {
          "module": "sklearn._kernel_pca",
          "declaration": "KernelPCA",
          "alias": null
        },
        {
          "module": "sklearn._lda",
          "declaration": "LatentDirichletAllocation",
          "alias": null
        },
        {
          "module": "sklearn._nmf",
          "declaration": "NMF",
          "alias": null
        },
        {
          "module": "sklearn._nmf",
          "declaration": "non_negative_factorization",
          "alias": null
        },
        {
          "module": "sklearn._pca",
          "declaration": "PCA",
          "alias": null
        },
        {
          "module": "sklearn._sparse_pca",
          "declaration": "MiniBatchSparsePCA",
          "alias": null
        },
        {
          "module": "sklearn._sparse_pca",
          "declaration": "SparsePCA",
          "alias": null
        },
        {
          "module": "sklearn._truncated_svd",
          "declaration": "TruncatedSVD",
          "alias": null
        },
        {
          "module": "utils.extmath",
          "declaration": "randomized_svd",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.decomposition._base",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.decomposition._dict_learning",
      "imports": [
        {
          "module": "itertools",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "sys",
          "alias": null
        },
        {
          "module": "time",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "ceil",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "Lars",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "Lasso",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "LassoLars",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "orthogonal_mp_gram",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_batches",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_even_slices",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "randomized_svd",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "DictionaryLearning",
          "decorators": [],
          "superclasses": [
            "_BaseSparseCoding",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where `n_samples` in the number of samples\nand `n_features` is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the object itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model from data in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where `n_samples` in the number of samples\n    and `n_features` is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the object itself."
            }
          ],
          "fullDocstring": "Dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n    (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nn_components : int, default=n_features\n    Number of dictionary elements to extract.\n\nalpha : float, default=1.0\n    Sparsity controlling parameter.\n\nmax_iter : int, default=1000\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-8\n    Tolerance for numerical error.\n\nfit_algorithm : {'lars', 'cd'}, default='lars'\n    * `'lars'`: uses the least angle regression method to solve the lasso\n      problem (:func:`~sklearn.linear_model.lars_path`);\n    * `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\n      faster if the estimated components are sparse.\n\n    .. versionadded:: 0.17\n       *cd* coordinate descent method to improve speed.\n\ntransform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n    Algorithm used to transform the data:\n\n    - `'lars'`: uses the least angle regression method\n      (:func:`~sklearn.linear_model.lars_path`);\n    - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n    - `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\n      will be faster if the estimated components are sparse.\n    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution.\n    - `'threshold'`: squashes to zero all coefficients less than alpha from\n      the projection ``dictionary * X'``.\n\n    .. versionadded:: 0.17\n       *lasso_cd* coordinate descent method to improve speed.\n\ntransform_n_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case. If `None`, then\n    `transform_n_nonzero_coefs=int(n_features / 10)`.\n\ntransform_alpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n    If `None`, default to 1.0\n\nn_jobs : int or None, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ncode_init : ndarray of shape (n_samples, n_components), default=None\n    Initial value for the code, for warm restart.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    Initial values for the dictionary, for warm restart.\n\nverbose : bool, default=False\n    To control the verbosity of the procedure.\n\nsplit_sign : bool, default=False\n    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary\n\n    .. versionadded:: 0.20\n\ntransform_max_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `'lasso_lars'`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    dictionary atoms extracted from the data\n\nerror_ : array\n    vector of errors at each iteration\n\nn_iter_ : int\n    Number of iterations run.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import DictionaryLearning\n>>> X, dictionary, code = make_sparse_coded_signal(\n...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42,\n... )\n>>> dict_learner = DictionaryLearning(\n...     n_components=15, transform_algorithm='lasso_lars', random_state=42,\n... )\n>>> X_transformed = dict_learner.fit_transform(X)\n\nWe can check the level of sparsity of `X_transformed`:\n\n>>> np.mean(X_transformed == 0)\n0.88...\n\nWe can compare the average squared euclidean norm of the reconstruction\nerror of the sparse coded signal relative to the squared euclidean norm of\nthe original signal:\n\n>>> X_hat = X_transformed @ dict_learner.components_\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n0.07...\n\nNotes\n-----\n**References:**\n\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\nfor sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\nSee Also\n--------\nSparseCoder\nMiniBatchDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA"
        },
        {
          "name": "MiniBatchDictionaryLearning",
          "decorators": [],
          "superclasses": [
            "_BaseSparseCoding",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the instance itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model from data in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself."
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "iter_offset",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The number of iteration on data batches that has been\nperformed before this call to partial_fit. This is optional:\nif no number is passed, the memory of the object is\nused."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the instance itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Updates the model using the data in X as a mini-batch.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\niter_offset : int, default=None\n    The number of iteration on data batches that has been\n    performed before this call to partial_fit. This is optional:\n    if no number is passed, the memory of the object is\n    used.\n\nReturns\n-------\nself : object\n    Returns the instance itself."
            }
          ],
          "fullDocstring": "Mini-batch dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n   (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of dictionary elements to extract.\n\nalpha : float, default=1\n    Sparsity controlling parameter.\n\nn_iter : int, default=1000\n    Total number of iterations to perform.\n\nfit_algorithm : {'lars', 'cd'}, default='lars'\n    The algorithm used:\n\n    - `'lars'`: uses the least angle regression method to solve the lasso\n      problem (`linear_model.lars_path`)\n    - `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). Lars will be faster if\n      the estimated components are sparse.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nbatch_size : int, default=3\n    Number of samples in each mini-batch.\n\nshuffle : bool, default=True\n    Whether to shuffle the samples before forming batches.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    initial value of the dictionary for warm restart scenarios\n\ntransform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n    Algorithm used to transform the data:\n\n    - `'lars'`: uses the least angle regression method\n      (`linear_model.lars_path`);\n    - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n    - `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\n      if the estimated components are sparse.\n    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution.\n    - `'threshold'`: squashes to zero all coefficients less than alpha from\n      the projection ``dictionary * X'``.\n\ntransform_n_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case. If `None`, then\n    `transform_n_nonzero_coefs=int(n_features / 10)`.\n\ntransform_alpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n    If `None`, default to 1.\n\nverbose : bool, default=False\n    To control the verbosity of the procedure.\n\nsplit_sign : bool, default=False\n    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary.\n\n    .. versionadded:: 0.20\n\ntransform_max_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `'lasso_lars'`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Components extracted from the data.\n\ninner_stats_ : tuple of (A, B) ndarrays\n    Internal sufficient statistics that are kept by the algorithm.\n    Keeping them is useful in online settings, to avoid losing the\n    history of the evolution, but they shouldn't have any use for the\n    end user.\n    `A` `(n_components, n_components)` is the dictionary covariance matrix.\n    `B` `(n_features, n_components)` is the data approximation matrix.\n\nn_iter_ : int\n    Number of iterations run.\n\niter_offset_ : int\n    The number of iteration on data batches that has been\n    performed before.\n\nrandom_state_ : RandomState instance\n    RandomState instance that is generated either from a seed, the random\n    number generattor or by `np.random`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import MiniBatchDictionaryLearning\n>>> X, dictionary, code = make_sparse_coded_signal(\n...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42)\n>>> dict_learner = MiniBatchDictionaryLearning(\n...     n_components=15, transform_algorithm='lasso_lars', random_state=42,\n... )\n>>> X_transformed = dict_learner.fit_transform(X)\n\nWe can check the level of sparsity of `X_transformed`:\n\n>>> np.mean(X_transformed == 0)\n0.87...\n\nWe can compare the average squared euclidean norm of the reconstruction\nerror of the sparse coded signal relative to the squared euclidean norm of\nthe original signal:\n\n>>> X_hat = X_transformed @ dict_learner.components_\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n0.10...\n\nNotes\n-----\n**References:**\n\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\nfor sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\nSee Also\n--------\nSparseCoder\nDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA"
        },
        {
          "name": "SparseCoder",
          "decorators": [],
          "superclasses": [
            "_BaseSparseCoding",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Do nothing and return the estimator unchanged.\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n\nParameters\n----------\nX : Ignored\n\ny : Ignored\n\nReturns\n-------\nself : object"
            },
            {
              "name": "components_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test data to be transformed, must have the same number of\nfeatures as the data used to train the model."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Encode the data as a sparse combination of the dictionary atoms.\n\nCoding method is determined by the object parameter\n`transform_algorithm`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Test data to be transformed, must have the same number of\n    features as the data used to train the model.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Transformed data."
            },
            {
              "name": "n_components_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "n_features_in_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Sparse coding\n\nFinds a sparse representation of data against a fixed, precomputed\ndictionary.\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\n    X ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.\n\nParameters\n----------\ndictionary : ndarray of shape (n_components, n_features)\n    The dictionary atoms used for sparse coding. Lines are assumed to be\n    normalized to unit norm.\n\ntransform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n    Algorithm used to transform the data:\n\n    - `'lars'`: uses the least angle regression method\n      (`linear_model.lars_path`);\n    - `'lasso_lars'`: uses Lars to compute the Lasso solution;\n    - `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\n      the estimated components are sparse;\n    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution;\n    - `'threshold'`: squashes to zero all coefficients less than alpha from\n      the projection ``dictionary * X'``.\n\ntransform_n_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case. If `None`, then\n    `transform_n_nonzero_coefs=int(n_features / 10)`.\n\ntransform_alpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n    If `None`, default to 1.\n\nsplit_sign : bool, default=False\n    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\ntransform_max_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `lasso_lars`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    The unchanged dictionary atoms.\n\n    .. deprecated:: 0.24\n       This attribute is deprecated in 0.24 and will be removed in\n       1.1 (renaming of 0.26). Use `dictionary` instead.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import SparseCoder\n>>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n>>> dictionary = np.array(\n...     [[0, 1, 0],\n...      [-1, -1, 2],\n...      [1, 1, 1],\n...      [0, 1, 1],\n...      [0, 2, 1]],\n...    dtype=np.float64\n... )\n>>> coder = SparseCoder(\n...     dictionary=dictionary, transform_algorithm='lasso_lars',\n...     transform_alpha=1e-10,\n... )\n>>> coder.transform(X)\narray([[ 0.,  0., -1.,  0.,  0.],\n       [ 0.,  1.,  1.,  0.,  0.]])\n\nSee Also\n--------\nDictionaryLearning\nMiniBatchDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA\nsparse_encode"
        }
      ],
      "functions": [
        {
          "name": "dict_learning",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data matrix."
            },
            {
              "name": "n_components",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of dictionary atoms to extract."
            }
          ],
          "results": [
            {
              "name": "code",
              "type": null,
              "description": "The sparse code factor in the matrix factorization."
            },
            {
              "name": "dictionary",
              "type": null,
              "description": "The dictionary factor in the matrix factorization."
            },
            {
              "name": "errors",
              "type": "Array",
              "description": "Vector of errors at each iteration."
            },
            {
              "name": "n_iter",
              "type": "int",
              "description": "Number of iterations run. Returned only if `return_n_iter` is\nset to True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Solves a dictionary learning matrix factorization problem.\n\nFinds the best dictionary and the corresponding sparse code for\napproximating the data matrix X by solving::\n\n    (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                 (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nwhere V is the dictionary and U is the sparse code.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data matrix.\n\nn_components : int\n    Number of dictionary atoms to extract.\n\nalpha : int\n    Sparsity controlling parameter.\n\nmax_iter : int, default=100\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-8\n    Tolerance for the stopping condition.\n\nmethod : {'lars', 'cd'}, default='lars'\n    The method used:\n\n    * `'lars'`: uses the least angle regression method to solve the lasso\n       problem (`linear_model.lars_path`);\n    * `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). Lars will be faster if\n      the estimated components are sparse.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    Initial value for the dictionary for warm restart scenarios.\n\ncode_init : ndarray of shape (n_samples, n_components), default=None\n    Initial value for the sparse code for warm restart scenarios.\n\ncallback : callable, default=None\n    Callable that gets invoked every five iterations\n\nverbose : bool, default=False\n    To control the verbosity of the procedure.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for randomly initializing the dictionary. Pass an int for\n    reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary.\n\n    .. versionadded:: 0.20\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\nmethod_max_iter : int, default=1000\n    Maximum number of iterations to perform.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\ncode : ndarray of shape (n_samples, n_components)\n    The sparse code factor in the matrix factorization.\n\ndictionary : ndarray of shape (n_components, n_features),\n    The dictionary factor in the matrix factorization.\n\nerrors : array\n    Vector of errors at each iteration.\n\nn_iter : int\n    Number of iterations run. Returned only if `return_n_iter` is\n    set to True.\n\nSee Also\n--------\ndict_learning_online\nDictionaryLearning\nMiniBatchDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA"
        },
        {
          "name": "dict_learning_online",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data matrix."
            },
            {
              "name": "n_components",
              "type": "Any",
              "hasDefault": true,
              "default": "2",
              "limitation": null,
              "ignored": false,
              "description": "Number of dictionary atoms to extract."
            }
          ],
          "results": [
            {
              "name": "code",
              "type": null,
              "description": "The sparse code (only returned if `return_code=True`)."
            },
            {
              "name": "dictionary",
              "type": null,
              "description": "The solutions to the dictionary learning problem."
            },
            {
              "name": "n_iter",
              "type": "int",
              "description": "Number of iterations run. Returned only if `return_n_iter` is\nset to `True`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Solves a dictionary learning matrix factorization problem online.\n\nFinds the best dictionary and the corresponding sparse code for\napproximating the data matrix X by solving::\n\n    (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                 (U,V)\n                 with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nwhere V is the dictionary and U is the sparse code. This is\naccomplished by repeatedly iterating over mini-batches by slicing\nthe input data.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data matrix.\n\nn_components : int, default=2\n    Number of dictionary atoms to extract.\n\nalpha : float, default=1\n    Sparsity controlling parameter.\n\nn_iter : int, default=100\n    Number of mini-batch iterations to perform.\n\nreturn_code : bool, default=True\n    Whether to also return the code U or just the dictionary `V`.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    Initial value for the dictionary for warm restart scenarios.\n\ncallback : callable, default=None\n    callable that gets invoked every five iterations.\n\nbatch_size : int, default=3\n    The number of samples to take in each batch.\n\nverbose : bool, default=False\n    To control the verbosity of the procedure.\n\nshuffle : bool, default=True\n    Whether to shuffle the data before splitting it in batches.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmethod : {'lars', 'cd'}, default='lars'\n    * `'lars'`: uses the least angle regression method to solve the lasso\n      problem (`linear_model.lars_path`);\n    * `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). Lars will be faster if\n      the estimated components are sparse.\n\niter_offset : int, default=0\n    Number of previous iterations completed on the dictionary used for\n    initialization.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nreturn_inner_stats : bool, default=False\n    Return the inner statistics A (dictionary covariance) and B\n    (data approximation). Useful to restart the algorithm in an\n    online setting. If `return_inner_stats` is `True`, `return_code` is\n    ignored.\n\ninner_stats : tuple of (A, B) ndarrays, default=None\n    Inner sufficient statistics that are kept by the algorithm.\n    Passing them at initialization is useful in online settings, to\n    avoid losing the history of the evolution.\n    `A` `(n_components, n_components)` is the dictionary covariance matrix.\n    `B` `(n_features, n_components)` is the data approximation matrix.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary.\n\n    .. versionadded:: 0.20\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\nmethod_max_iter : int, default=1000\n    Maximum number of iterations to perform when solving the lasso problem.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\ncode : ndarray of shape (n_samples, n_components),\n    The sparse code (only returned if `return_code=True`).\n\ndictionary : ndarray of shape (n_components, n_features),\n    The solutions to the dictionary learning problem.\n\nn_iter : int\n    Number of iterations run. Returned only if `return_n_iter` is\n    set to `True`.\n\nSee Also\n--------\ndict_learning\nDictionaryLearning\nMiniBatchDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA"
        },
        {
          "name": "sparse_encode",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data matrix."
            },
            {
              "name": "dictionary",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The dictionary matrix against which to solve the sparse coding of\nthe data. Some of the algorithms assume normalized rows for meaningful\noutput."
            }
          ],
          "results": [
            {
              "name": "code",
              "type": null,
              "description": "The sparse codes"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Sparse coding\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\n    X ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data matrix.\n\ndictionary : ndarray of shape (n_components, n_features)\n    The dictionary matrix against which to solve the sparse coding of\n    the data. Some of the algorithms assume normalized rows for meaningful\n    output.\n\ngram : ndarray of shape (n_components, n_components), default=None\n    Precomputed Gram matrix, `dictionary * dictionary'`.\n\ncov : ndarray of shape (n_components, n_samples), default=None\n    Precomputed covariance, `dictionary' * X`.\n\nalgorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'},             default='lasso_lars'\n    The algorithm used:\n\n    * `'lars'`: uses the least angle regression method\n      (`linear_model.lars_path`);\n    * `'lasso_lars'`: uses Lars to compute the Lasso solution;\n    * `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\n      the estimated components are sparse;\n    * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution;\n    * `'threshold'`: squashes to zero all coefficients less than\n      regularization from the projection `dictionary * data'`.\n\nn_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case. If `None`, then\n    `n_nonzero_coefs=int(n_features / 10)`.\n\nalpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n    If `None`, default to 1.\n\ncopy_cov : bool, default=True\n    Whether to copy the precomputed covariance matrix; if `False`, it may\n    be overwritten.\n\ninit : ndarray of shape (n_samples, n_components), default=None\n    Initialization value of the sparse codes. Only used if\n    `algorithm='lasso_cd'`.\n\nmax_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `'lasso_lars'`.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ncheck_input : bool, default=True\n    If `False`, the input arrays X and dictionary will not be checked.\n\nverbose : int, default=0\n    Controls the verbosity; the higher, the more messages.\n\npositive : bool, default=False\n    Whether to enforce positivity when finding the encoding.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ncode : ndarray of shape (n_samples, n_components)\n    The sparse codes\n\nSee Also\n--------\nsklearn.linear_model.lars_path\nsklearn.linear_model.orthogonal_mp\nsklearn.linear_model.Lasso\nSparseCoder"
        }
      ]
    },
    {
      "name": "sklearn.decomposition._factor_analysis",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "math",
          "declaration": "log",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "sqrt",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "fast_logdet",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "randomized_svd",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "squared_norm",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "FactorAnalysis",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the FactorAnalysis model to X using SVD based approach\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : Ignored\n\nReturns\n-------\nself"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "The latent variables of X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply dimensionality reduction to X using the model.\n\nCompute the expected mean of the latent variables.\nSee Barber, 21.2.33 (or Bishop, 12.66).\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    The latent variables of X."
            },
            {
              "name": "get_covariance",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "cov",
                  "type": null,
                  "description": "Estimated covariance of data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute data covariance with the FactorAnalysis model.\n\n``cov = components_.T * components_ + diag(noise_variance)``\n\nReturns\n-------\ncov : ndarray of shape (n_features, n_features)\n    Estimated covariance of data."
            },
            {
              "name": "get_precision",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "precision",
                  "type": null,
                  "description": "Estimated precision of data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute data precision matrix with the FactorAnalysis model.\n\nReturns\n-------\nprecision : ndarray of shape (n_features, n_features)\n    Estimated precision of data."
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data"
                }
              ],
              "results": [
                {
                  "name": "ll",
                  "type": null,
                  "description": "Log-likelihood of each sample under the current model"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the log-likelihood of each sample\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    The data\n\nReturns\n-------\nll : ndarray of shape (n_samples,)\n    Log-likelihood of each sample under the current model"
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "ll",
                  "type": "float",
                  "description": "Average log-likelihood of the samples under the current model"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the average log-likelihood of the samples\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    The data\n\ny : Ignored\n\nReturns\n-------\nll : float\n    Average log-likelihood of the samples under the current model"
            }
          ],
          "fullDocstring": "Factor Analysis (FA).\n\nA simple linear generative model with Gaussian latent variables.\n\nThe observations are assumed to be caused by a linear transformation of\nlower dimensional latent factors and added Gaussian noise.\nWithout loss of generality the factors are distributed according to a\nGaussian with zero mean and unit covariance. The noise is also zero mean\nand has an arbitrary diagonal covariance matrix.\n\nIf we would restrict the model further, by assuming that the Gaussian\nnoise is even isotropic (all diagonal entries are the same) we would obtain\n:class:`PPCA`.\n\nFactorAnalysis performs a maximum likelihood estimate of the so-called\n`loading` matrix, the transformation of the latent variables to the\nobserved ones, using SVD based approach.\n\nRead more in the :ref:`User Guide <FA>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nn_components : int, default=None\n    Dimensionality of latent space, the number of components\n    of ``X`` that are obtained after ``transform``.\n    If None, n_components is set to the number of features.\n\ntol : float, defaul=1e-2\n    Stopping tolerance for log-likelihood increase.\n\ncopy : bool, default=True\n    Whether to make a copy of X. If ``False``, the input X gets overwritten\n    during fitting.\n\nmax_iter : int, default=1000\n    Maximum number of iterations.\n\nnoise_variance_init : ndarray of shape (n_features,), default=None\n    The initial guess of the noise variance for each feature.\n    If None, it defaults to np.ones(n_features).\n\nsvd_method : {'lapack', 'randomized'}, default='randomized'\n    Which SVD method to use. If 'lapack' use standard SVD from\n    scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n    Defaults to 'randomized'. For most applications 'randomized' will\n    be sufficiently precise while providing significant speed gains.\n    Accuracy can also be improved by setting higher values for\n    `iterated_power`. If this is not sufficient, for maximum precision\n    you should choose 'lapack'.\n\niterated_power : int, default=3\n    Number of iterations for the power method. 3 by default. Only used\n    if ``svd_method`` equals 'randomized'.\n\nrotation : {'varimax', 'quartimax'}, default=None\n    If not None, apply the indicated rotation. Currently, varimax and\n    quartimax are implemented. See\n    `\"The varimax criterion for analytic rotation in factor analysis\"\n    <https://link.springer.com/article/10.1007%2FBF02289233>`_\n    H. F. Kaiser, 1958.\n\n    .. versionadded:: 0.24\n\nrandom_state : int or RandomState instance, default=0\n    Only used when ``svd_method`` equals 'randomized'. Pass an int for\n    reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Components with maximum variance.\n\nloglike_ : list of shape (n_iterations,)\n    The log likelihood at each iteration.\n\nnoise_variance_ : ndarray of shape (n_features,)\n    The estimated noise variance for each feature.\n\nn_iter_ : int\n    Number of iterations run.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import FactorAnalysis\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = FactorAnalysis(n_components=7, random_state=0)\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)\n\nReferences\n----------\n- David Barber, Bayesian Reasoning and Machine Learning,\n  Algorithm 21.1.\n\n- Christopher M. Bishop: Pattern Recognition and Machine Learning,\n  Chapter 12.2.4.\n\nSee Also\n--------\nPCA: Principal component analysis is also a latent linear variable model\n    which however assumes equal noise variance for each feature.\n    This extra assumption makes probabilistic PCA faster as it can be\n    computed in closed form.\nFastICA: Independent component analysis, a latent variable model with\n    non-Gaussian latent variables."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.decomposition._fastica",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "as_float_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "FLOAT_DTYPES",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "FastICA",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model and recover the sources from X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)"
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model to X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to transform, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "copy",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If False, data passed to fit can be overwritten. Defaults to True."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Recover the sources from X (apply the unmixing matrix).\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data to transform, where n_samples is the number of samples\n    and n_features is the number of features.\n\ncopy : bool, default=True\n    If False, data passed to fit can be overwritten. Defaults to True.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)"
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sources, where n_samples is the number of samples\nand n_components is the number of components."
                },
                {
                  "name": "copy",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If False, data passed to fit are overwritten. Defaults to True."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform the sources back to the mixed data (apply mixing matrix).\n\nParameters\n----------\nX : array-like of shape (n_samples, n_components)\n    Sources, where n_samples is the number of samples\n    and n_components is the number of components.\ncopy : bool, default=True\n    If False, data passed to fit are overwritten. Defaults to True.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_features)"
            }
          ],
          "fullDocstring": "FastICA: a fast algorithm for Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of components to use. If None is passed, all are used.\n\nalgorithm : {'parallel', 'deflation'}, default='parallel'\n    Apply parallel or deflational algorithm for FastICA.\n\nwhiten : bool, default=True\n    If whiten is false, the data is already considered to be\n    whitened, and no whitening is performed.\n\nfun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n    The functional form of the G function used in the\n    approximation to neg-entropy. Could be either 'logcosh', 'exp',\n    or 'cube'.\n    You can also provide your own function. It should return a tuple\n    containing the value of the function, and of its derivative, in the\n    point. Example::\n\n        def my_g(x):\n            return x ** 3, (3 * x ** 2).mean(axis=-1)\n\nfun_args : dict, default=None\n    Arguments to send to the functional form.\n    If empty and if fun='logcosh', fun_args will take value\n    {'alpha' : 1.0}.\n\nmax_iter : int, default=200\n    Maximum number of iterations during fit.\n\ntol : float, default=1e-4\n    Tolerance on update at each iteration.\n\nw_init : ndarray of shape (n_components, n_components), default=None\n    The mixing matrix to be used to initialize the algorithm.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used to initialize ``w_init`` when not specified, with a\n    normal distribution. Pass an int, for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    The linear operator to apply to the data to get the independent\n    sources. This is equal to the unmixing matrix when ``whiten`` is\n    False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when\n    ``whiten`` is True.\n\nmixing_ : ndarray of shape (n_features, n_components)\n    The pseudo-inverse of ``components_``. It is the linear operator\n    that maps independent sources to the data.\n\nmean_ : ndarray of shape(n_features,)\n    The mean over features. Only set if `self.whiten` is True.\n\nn_iter_ : int\n    If the algorithm is \"deflation\", n_iter is the\n    maximum number of iterations run across all components. Else\n    they are just the number of iterations taken to converge.\n\nwhitening_ : ndarray of shape (n_components, n_features)\n    Only set if whiten is 'True'. This is the pre-whitening matrix\n    that projects data onto the first `n_components` principal components.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import FastICA\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = FastICA(n_components=7,\n...         random_state=0)\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)\n\nNotes\n-----\nImplementation based on\n*A. Hyvarinen and E. Oja, Independent Component Analysis:\nAlgorithms and Applications, Neural Networks, 13(4-5), 2000,\npp. 411-430*"
        }
      ],
      "functions": [
        {
          "name": "fastica",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
            },
            {
              "name": "n_components",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of components to extract. If None no dimension reduction\nis performed."
            }
          ],
          "results": [
            {
              "name": "K",
              "type": null,
              "description": "If whiten is 'True', K is the pre-whitening matrix that projects data\nonto the first n_components principal components. If whiten is 'False',\nK is 'None'."
            },
            {
              "name": "W",
              "type": null,
              "description": "The square matrix that unmixes the data after whitening.\nThe mixing matrix is the pseudo-inverse of matrix ``W K``\nif K is not None, else it is the inverse of W."
            },
            {
              "name": "S",
              "type": null,
              "description": "Estimated source matrix"
            },
            {
              "name": "X_mean",
              "type": null,
              "description": "The mean over features. Returned only if return_X_mean is True."
            },
            {
              "name": "n_iter",
              "type": "int",
              "description": "If the algorithm is \"deflation\", n_iter is the\nmaximum number of iterations run across all components. Else\nthey are just the number of iterations taken to converge. This is\nreturned only when return_n_iter is set to `True`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Perform Fast Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\nn_components : int, default=None\n    Number of components to extract. If None no dimension reduction\n    is performed.\n\nalgorithm : {'parallel', 'deflation'}, default='parallel'\n    Apply a parallel or deflational FASTICA algorithm.\n\nwhiten : bool, default=True\n    If True perform an initial whitening of the data.\n    If False, the data is assumed to have already been\n    preprocessed: it should be centered, normed and white.\n    Otherwise you will get incorrect results.\n    In this case the parameter n_components will be ignored.\n\nfun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n    The functional form of the G function used in the\n    approximation to neg-entropy. Could be either 'logcosh', 'exp',\n    or 'cube'.\n    You can also provide your own function. It should return a tuple\n    containing the value of the function, and of its derivative, in the\n    point. The derivative should be averaged along its last dimension.\n    Example:\n\n    def my_g(x):\n        return x ** 3, np.mean(3 * x ** 2, axis=-1)\n\nfun_args : dict, default=None\n    Arguments to send to the functional form.\n    If empty or None and if fun='logcosh', fun_args will take value\n    {'alpha' : 1.0}\n\nmax_iter : int, default=200\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-04\n    A positive scalar giving the tolerance at which the\n    un-mixing matrix is considered to have converged.\n\nw_init : ndarray of shape (n_components, n_components), default=None\n    Initial un-mixing array of dimension (n.comp,n.comp).\n    If None (default) then an array of normal r.v.'s is used.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used to initialize ``w_init`` when not specified, with a\n    normal distribution. Pass an int, for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nreturn_X_mean : bool, default=False\n    If True, X_mean is returned too.\n\ncompute_sources : bool, default=True\n    If False, sources are not computed, but only the rotation matrix.\n    This can save memory when working with big data. Defaults to True.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\nK : ndarray of shape (n_components, n_features) or None\n    If whiten is 'True', K is the pre-whitening matrix that projects data\n    onto the first n_components principal components. If whiten is 'False',\n    K is 'None'.\n\nW : ndarray of shape (n_components, n_components)\n    The square matrix that unmixes the data after whitening.\n    The mixing matrix is the pseudo-inverse of matrix ``W K``\n    if K is not None, else it is the inverse of W.\n\nS : ndarray of shape (n_samples, n_components) or None\n    Estimated source matrix\n\nX_mean : ndarray of shape (n_features,)\n    The mean over features. Returned only if return_X_mean is True.\n\nn_iter : int\n    If the algorithm is \"deflation\", n_iter is the\n    maximum number of iterations run across all components. Else\n    they are just the number of iterations taken to converge. This is\n    returned only when return_n_iter is set to `True`.\n\nNotes\n-----\n\nThe data matrix X is considered to be a linear combination of\nnon-Gaussian (independent) components i.e. X = AS where columns of S\ncontain the independent components and A is a linear mixing\nmatrix. In short ICA attempts to `un-mix' the data by estimating an\nun-mixing matrix W where ``S = W K X.``\nWhile FastICA was proposed to estimate as many sources\nas features, it is possible to estimate less by setting\nn_components < n_features. It this case K is not a square matrix\nand the estimated A is the pseudo-inverse of ``W K``.\n\nThis implementation was originally made for data of shape\n[n_features, n_samples]. Now the input is transposed\nbefore the algorithm is applied. This makes it slightly\nfaster for Fortran-ordered input.\n\nImplemented using FastICA:\n*A. Hyvarinen and E. Oja, Independent Component Analysis:\nAlgorithms and Applications, Neural Networks, 13(4-5), 2000,\npp. 411-430*"
        }
      ]
    },
    {
      "name": "sklearn.decomposition._incremental_pca",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn.decomposition._base",
          "declaration": "_BasePCA",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_batches",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "_incremental_mean_and_var",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "svd_flip",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "IncrementalPCA",
          "decorators": [],
          "superclasses": [
            "_BasePCA"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the instance itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model with X, using minibatches of size batch_size.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself."
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Run check_array on X."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the instance itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Incremental fit with X. All of X is processed as a single batch.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples and\n    n_features is the number of features.\n\ncheck_input : bool, default=True\n    Run check_array on X.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data, where n_samples is the number of samples\nand n_features is the number of features."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply dimensionality reduction to X.\n\nX is projected on the first principal components previously extracted\nfrom a training set, using minibatches of size batch_size if X is\nsparse.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data, where n_samples is the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n\nExamples\n--------\n\n>>> import numpy as np\n>>> from sklearn.decomposition import IncrementalPCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2],\n...               [1, 1], [2, 1], [3, 2]])\n>>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n>>> ipca.fit(X)\nIncrementalPCA(batch_size=3, n_components=2)\n>>> ipca.transform(X) # doctest: +SKIP"
            }
          ],
          "fullDocstring": "Incremental principal components analysis (IPCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of\nthe data, keeping only the most significant singular vectors to\nproject the data to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nDepending on the size of the input data, this algorithm can be much more\nmemory efficient than a PCA, and allows sparse input.\n\nThis algorithm has constant memory complexity, on the order\nof ``batch_size * n_features``, enabling use of np.memmap files without\nloading the entire file into memory. For sparse matrices, the input\nis converted to dense in batches (in order to be able to subtract the\nmean) which avoids storing the entire dense matrix at any one time.\n\nThe computational overhead of each SVD is\n``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\nremain in memory at a time. There will be ``n_samples / batch_size`` SVD\ncomputations to get the principal components, versus 1 large SVD of\ncomplexity ``O(n_samples * n_features ** 2)`` for PCA.\n\nRead more in the :ref:`User Guide <IncrementalPCA>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\nn_components : int, default=None\n    Number of components to keep. If ``n_components`` is ``None``,\n    then ``n_components`` is set to ``min(n_samples, n_features)``.\n\nwhiten : bool, default=False\n    When True (False by default) the ``components_`` vectors are divided\n    by ``n_samples`` times ``components_`` to ensure uncorrelated outputs\n    with unit component-wise variances.\n\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometimes\n    improve the predictive accuracy of the downstream estimators by\n    making data respect some hard-wired assumptions.\n\ncopy : bool, default=True\n    If False, X will be overwritten. ``copy=False`` can be used to\n    save memory but is unsafe for general use.\n\nbatch_size : int, default=None\n    The number of samples to use for each batch. Only used when calling\n    ``fit``. If ``batch_size`` is ``None``, then ``batch_size``\n    is inferred from the data and set to ``5 * n_features``, to provide a\n    balance between approximation accuracy and memory consumption.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Components with maximum variance.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    Variance explained by each of the selected components.\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n    If all components are stored, the sum of explained variances is equal\n    to 1.0.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, aggregate over calls to ``partial_fit``.\n\nvar_ : ndarray of shape (n_features,)\n    Per-feature empirical variance, aggregate over calls to\n    ``partial_fit``.\n\nnoise_variance_ : float\n    The estimated noise covariance following the Probabilistic PCA model\n    from Tipping and Bishop 1999. See \"Pattern Recognition and\n    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n    http://www.miketipping.com/papers/met-mppca.pdf.\n\nn_components_ : int\n    The estimated number of components. Relevant when\n    ``n_components=None``.\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator. Will be reset on\n    new calls to fit, but increments across ``partial_fit`` calls.\n\nbatch_size_ : int\n    Inferred batch size from ``batch_size``.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import IncrementalPCA\n>>> from scipy import sparse\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n>>> # either partially fit on smaller batches of data\n>>> transformer.partial_fit(X[:100, :])\nIncrementalPCA(batch_size=200, n_components=7)\n>>> # or let the fit function itself divide the data into batches\n>>> X_sparse = sparse.csr_matrix(X)\n>>> X_transformed = transformer.fit_transform(X_sparse)\n>>> X_transformed.shape\n(1797, 7)\n\nNotes\n-----\nImplements the incremental PCA model from:\n*D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\nTracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\npp. 125-141, May 2008.*\nSee https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n\nThis model is an extension of the Sequential Karhunen-Loeve Transform from:\n*A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\nits Application to Images, IEEE Transactions on Image Processing, Volume 9,\nNumber 8, pp. 1371-1374, August 2000.*\nSee https://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf\n\nWe have specifically abstained from an optimization used by authors of both\npapers, a QR decomposition used in specific situations to reduce the\nalgorithmic complexity of the SVD. The source for this technique is\n*Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\nsection 5.4.4, pp 252-253.*. This technique has been omitted because it is\nadvantageous only when decomposing a matrix with ``n_samples`` (rows)\n>= 5/3 * ``n_features`` (columns), and hurts the readability of the\nimplemented algorithm. This would be a good opportunity for future\noptimization, if it is deemed necessary.\n\nReferences\n----------\nD. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\nTracking, International Journal of Computer Vision, Volume 77,\nIssue 1-3, pp. 125-141, May 2008.\n\nG. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\nSection 5.4.4, pp. 252-253.\n\nSee Also\n--------\nPCA\nKernelPCA\nSparsePCA\nTruncatedSVD"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.decomposition._kernel_pca",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy.sparse.linalg",
          "declaration": "eigsh",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_kernels",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "KernelCenterer",
          "alias": null
        },
        {
          "module": "sklearn.utils._arpack",
          "declaration": "_init_arpack_v0",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "svd_flip",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_psd_eigenvalues",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KernelPCA",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the instance itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model from data in X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nself : object\n    Returns the instance itself."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model from data in X and transform X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)"
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X back to original space.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_components)\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_features)\n\nReferences\n----------\n\"Learning to Find Pre-Images\", G BakIr et al, 2004."
            }
          ],
          "fullDocstring": "Kernel Principal component analysis (KPCA).\n\nNon-linear dimensionality reduction through the use of kernels (see\n:ref:`metrics`).\n\nRead more in the :ref:`User Guide <kernel_PCA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of components. If None, all non-zero components are kept.\n\nkernel : {'linear', 'poly',             'rbf', 'sigmoid', 'cosine', 'precomputed'}, default='linear'\n    Kernel used for PCA.\n\ngamma : float, default=None\n    Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n    kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.\n\ndegree : int, default=3\n    Degree for poly kernels. Ignored by other kernels.\n\ncoef0 : float, default=1\n    Independent term in poly and sigmoid kernels.\n    Ignored by other kernels.\n\nkernel_params : dict, default=None\n    Parameters (keyword arguments) and\n    values for kernel passed as callable object.\n    Ignored by other kernels.\n\nalpha : float, default=1.0\n    Hyperparameter of the ridge regression that learns the\n    inverse transform (when fit_inverse_transform=True).\n\nfit_inverse_transform : bool, default=False\n    Learn the inverse transform for non-precomputed kernels.\n    (i.e. learn to find the pre-image of a point)\n\neigen_solver : {'auto', 'dense', 'arpack'}, default='auto'\n    Select eigensolver to use. If n_components is much less than\n    the number of training samples, arpack may be more efficient\n    than the dense eigensolver.\n\ntol : float, default=0\n    Convergence tolerance for arpack.\n    If 0, optimal value will be chosen by arpack.\n\nmax_iter : int, default=None\n    Maximum number of iterations for arpack.\n    If None, optimal value will be chosen by arpack.\n\nremove_zero_eig : bool, default=False\n    If True, then all components with zero eigenvalues are removed, so\n    that the number of components in the output may be < n_components\n    (and sometimes even zero due to numerical instability).\n    When n_components is None, this parameter is ignored and components\n    with zero eigenvalues are removed regardless.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when ``eigen_solver`` == 'arpack'. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.18\n\ncopy_X : bool, default=True\n    If True, input X is copied and stored by the model in the `X_fit_`\n    attribute. If no further changes will be done to X, setting\n    `copy_X=False` saves memory by storing a reference.\n\n    .. versionadded:: 0.18\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18\n\nAttributes\n----------\nlambdas_ : ndarray of shape (n_components,)\n    Eigenvalues of the centered kernel matrix in decreasing order.\n    If `n_components` and `remove_zero_eig` are not set,\n    then all values are stored.\n\nalphas_ : ndarray of shape (n_samples, n_components)\n    Eigenvectors of the centered kernel matrix. If `n_components` and\n    `remove_zero_eig` are not set, then all components are stored.\n\ndual_coef_ : ndarray of shape (n_samples, n_features)\n    Inverse transform matrix. Only available when\n    ``fit_inverse_transform`` is True.\n\nX_transformed_fit_ : ndarray of shape (n_samples, n_components)\n    Projection of the fitted data on the kernel principal components.\n    Only available when ``fit_inverse_transform`` is True.\n\nX_fit_ : ndarray of shape (n_samples, n_features)\n    The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n    a reference. This attribute is used for the calls to transform.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import KernelPCA\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = KernelPCA(n_components=7, kernel='linear')\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)\n\nReferences\n----------\nKernel PCA was introduced in:\n    Bernhard Schoelkopf, Alexander J. Smola,\n    and Klaus-Robert Mueller. 1999. Kernel principal\n    component analysis. In Advances in kernel methods,\n    MIT Press, Cambridge, MA, USA 327-352."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.decomposition._lda",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "gammaln",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "logsumexp",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.decomposition._online_lda_fast",
          "declaration": "_dirichlet_expectation_1d",
          "alias": null
        },
        {
          "module": "sklearn.decomposition._online_lda_fast",
          "declaration": "_dirichlet_expectation_2d",
          "alias": null
        },
        {
          "module": "sklearn.decomposition._online_lda_fast",
          "declaration": "mean_change",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_batches",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_even_slices",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_non_negative",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "LatentDirichletAllocation",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Document word matrix."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Online VB with Mini-Batch update.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\ny : Ignored\n\nReturns\n-------\nself"
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Document word matrix."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn model for the data X with variational Bayes method.\n\nWhen `learning_method` is 'online', use mini-batch update.\nOtherwise, use batch update.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\ny : Ignored\n\nReturns\n-------\nself"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Document word matrix."
                }
              ],
              "results": [
                {
                  "name": "doc_topic_distr",
                  "type": null,
                  "description": "Document topic distribution for X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform data X according to the fitted model.\n\n   .. versionchanged:: 0.18\n      *doc_topic_distr* is now normalized\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\nReturns\n-------\ndoc_topic_distr : ndarray of shape (n_samples, n_components)\n    Document topic distribution for X."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Document word matrix."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "Use approximate bound as score."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Calculate approximate log-likelihood as score.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\ny : Ignored\n\nReturns\n-------\nscore : float\n    Use approximate bound as score."
            },
            {
              "name": "perplexity",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Document word matrix."
                },
                {
                  "name": "sub_sampling",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "Do sub-sampling or not."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "Perplexity score."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Calculate approximate perplexity for data X.\n\nPerplexity is defined as exp(-1. * log-likelihood per word)\n\n.. versionchanged:: 0.19\n   *doc_topic_distr* argument has been deprecated and is ignored\n   because user no longer has access to unnormalized distribution\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\nsub_sampling : bool\n    Do sub-sampling or not.\n\nReturns\n-------\nscore : float\n    Perplexity score."
            }
          ],
          "fullDocstring": "Latent Dirichlet Allocation with online variational Bayes algorithm\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <LatentDirichletAllocation>`.\n\nParameters\n----------\nn_components : int, default=10\n    Number of topics.\n\n    .. versionchanged:: 0.19\n        ``n_topics`` was renamed to ``n_components``\n\ndoc_topic_prior : float, default=None\n    Prior of document topic distribution `theta`. If the value is None,\n    defaults to `1 / n_components`.\n    In [1]_, this is called `alpha`.\n\ntopic_word_prior : float, default=None\n    Prior of topic word distribution `beta`. If the value is None, defaults\n    to `1 / n_components`.\n    In [1]_, this is called `eta`.\n\nlearning_method : {'batch', 'online'}, default='batch'\n    Method used to update `_component`. Only used in :meth:`fit` method.\n    In general, if the data size is large, the online update will be much\n    faster than the batch update.\n\n    Valid options::\n\n        'batch': Batch variational Bayes method. Use all training data in\n            each EM update.\n            Old `components_` will be overwritten in each iteration.\n        'online': Online variational Bayes method. In each EM update, use\n            mini-batch of training data to update the ``components_``\n            variable incrementally. The learning rate is controlled by the\n            ``learning_decay`` and the ``learning_offset`` parameters.\n\n    .. versionchanged:: 0.20\n        The default learning method is now ``\"batch\"``.\n\nlearning_decay : float, default=0.7\n    It is a parameter that control learning rate in the online learning\n    method. The value should be set between (0.5, 1.0] to guarantee\n    asymptotic convergence. When the value is 0.0 and batch_size is\n    ``n_samples``, the update method is same as batch learning. In the\n    literature, this is called kappa.\n\nlearning_offset : float, default=10.\n    A (positive) parameter that downweights early iterations in online\n    learning.  It should be greater than 1.0. In the literature, this is\n    called tau_0.\n\nmax_iter : int, default=10\n    The maximum number of iterations.\n\nbatch_size : int, default=128\n    Number of documents to use in each EM iteration. Only used in online\n    learning.\n\nevaluate_every : int, default=-1\n    How often to evaluate perplexity. Only used in `fit` method.\n    set it to 0 or negative number to not evaluate perplexity in\n    training at all. Evaluating perplexity can help you check convergence\n    in training process, but it will also increase total training time.\n    Evaluating perplexity in every iteration might increase training time\n    up to two-fold.\n\ntotal_samples : int, default=1e6\n    Total number of documents. Only used in the :meth:`partial_fit` method.\n\nperp_tol : float, default=1e-1\n    Perplexity tolerance in batch learning. Only used when\n    ``evaluate_every`` is greater than 0.\n\nmean_change_tol : float, default=1e-3\n    Stopping tolerance for updating document topic distribution in E-step.\n\nmax_doc_update_iter : int, default=100\n    Max number of iterations for updating document topic distribution in\n    the E-step.\n\nn_jobs : int, default=None\n    The number of jobs to use in the E-step.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int, default=0\n    Verbosity level.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Variational parameters for topic word distribution. Since the complete\n    conditional for topic word distribution is a Dirichlet,\n    ``components_[i, j]`` can be viewed as pseudocount that represents the\n    number of times word `j` was assigned to topic `i`.\n    It can also be viewed as distribution over the words for each topic\n    after normalization:\n    ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n\nexp_dirichlet_component_ : ndarray of shape (n_components, n_features)\n    Exponential value of expectation of log topic word distribution.\n    In the literature, this is `exp(E[log(beta)])`.\n\nn_batch_iter_ : int\n    Number of iterations of the EM step.\n\nn_iter_ : int\n    Number of passes over the dataset.\n\nbound_ : float\n    Final perplexity score on training set.\n\ndoc_topic_prior_ : float\n    Prior of document topic distribution `theta`. If the value is None,\n    it is `1 / n_components`.\n\nrandom_state_ : RandomState instance\n    RandomState instance that is generated either from a seed, the random\n    number generator or by `np.random`.\n\ntopic_word_prior_ : float\n    Prior of topic word distribution `beta`. If the value is None, it is\n    `1 / n_components`.\n\nExamples\n--------\n>>> from sklearn.decomposition import LatentDirichletAllocation\n>>> from sklearn.datasets import make_multilabel_classification\n>>> # This produces a feature matrix of token counts, similar to what\n>>> # CountVectorizer would produce on text.\n>>> X, _ = make_multilabel_classification(random_state=0)\n>>> lda = LatentDirichletAllocation(n_components=5,\n...     random_state=0)\n>>> lda.fit(X)\nLatentDirichletAllocation(...)\n>>> # get topics for some given samples:\n>>> lda.transform(X[-2:])\narray([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])\n\nReferences\n----------\n.. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n    Hoffman, David M. Blei, Francis Bach, 2010\n\n[2] \"Stochastic Variational Inference\", Matthew D. Hoffman, David M. Blei,\n    Chong Wang, John Paisley, 2013\n\n[3] Matthew D. Hoffman's onlineldavb code. Link:\n    https://github.com/blei-lab/onlineldavb"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.decomposition._nmf",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "time",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "math",
          "declaration": "sqrt",
          "alias": null
        },
        {
          "module": "sklearn._config",
          "declaration": "config_context",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.decomposition._cdnmf_fast",
          "declaration": "_update_cdnmf_fast",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "randomized_svd",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "squared_norm",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_non_negative",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "NMF",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data matrix to be decomposed"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "W",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "If init='custom', it is used as initial guess for the solution."
                },
                {
                  "name": "H",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "If init='custom', it is used as initial guess for the solution."
                }
              ],
              "results": [
                {
                  "name": "W",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn a NMF model for the data X and returns the transformed data.\n\nThis is more efficient than calling fit followed by transform.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Data matrix to be decomposed\n\ny : Ignored\n\nW : array-like of shape (n_samples, n_components)\n    If init='custom', it is used as initial guess for the solution.\n\nH : array-like of shape (n_components, n_features)\n    If init='custom', it is used as initial guess for the solution.\n\nReturns\n-------\nW : ndarray of shape (n_samples, n_components)\n    Transformed data."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data matrix to be decomposed"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn a NMF model for the data X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Data matrix to be decomposed\n\ny : Ignored\n\nReturns\n-------\nself"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data matrix to be transformed by the model."
                }
              ],
              "results": [
                {
                  "name": "W",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform the data X according to the fitted NMF model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Data matrix to be transformed by the model.\n\nReturns\n-------\nW : ndarray of shape (n_samples, n_components)\n    Transformed data."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "W",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Transformed data matrix."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Data matrix of original shape."
                },
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform data back to its original space.\n\nParameters\n----------\nW : {ndarray, sparse matrix} of shape (n_samples, n_components)\n    Transformed data matrix.\n\nReturns\n-------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Data matrix of original shape.\n\n.. versionadded:: 0.18"
            }
          ],
          "fullDocstring": "Non-Negative Matrix Factorization (NMF).\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        0.5 * ||X - WH||_{Fro}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n        + alpha * l1_{ratio} * ||vec(H)||_1\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nFor multiplicative-update ('mu') solver, the Frobenius norm\n(:math:`0.5 * ||X - WH||_{Fro}^2`) can be changed into another\nbeta-divergence loss, by changing the beta_loss parameter.\n\nThe objective function is minimized with an alternating minimization of W\nand H.\n\nRead more in the :ref:`User Guide <NMF>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of components, if n_components is not set all features\n    are kept.\n\ninit : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n    Method used to initialize the procedure.\n    Default: None.\n    Valid options:\n\n    - `None`: 'nndsvd' if n_components <= min(n_samples, n_features),\n      otherwise random.\n\n    - `'random'`: non-negative random matrices, scaled with:\n      sqrt(X.mean() / n_components)\n\n    - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n      initialization (better for sparseness)\n\n    - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n      (better when sparsity is not desired)\n\n    - `'nndsvdar'` NNDSVD with zeros filled with small random values\n      (generally faster, less accurate alternative to NNDSVDa\n      for when sparsity is not desired)\n\n    - `'custom'`: use custom matrices W and H\n\nsolver : {'cd', 'mu'}, default='cd'\n    Numerical solver to use:\n    'cd' is a Coordinate Descent solver.\n    'mu' is a Multiplicative Update solver.\n\n    .. versionadded:: 0.17\n       Coordinate Descent solver.\n\n    .. versionadded:: 0.19\n       Multiplicative Update solver.\n\nbeta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n    Beta divergence to be minimized, measuring the distance between X\n    and the dot product WH. Note that values different from 'frobenius'\n    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n    matrix X cannot contain zeros. Used only in 'mu' solver.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-4\n    Tolerance of the stopping condition.\n\nmax_iter : int, default=200\n    Maximum number of iterations before timing out.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initialisation (when ``init`` == 'nndsvdar' or\n    'random'), and in Coordinate Descent. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nalpha : float, default=0.\n    Constant that multiplies the regularization terms. Set it to zero to\n    have no regularization.\n\n    .. versionadded:: 0.17\n       *alpha* used in the Coordinate Descent solver.\n\nl1_ratio : float, default=0.\n    The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n    For l1_ratio = 0 the penalty is an elementwise L2 penalty\n    (aka Frobenius Norm).\n    For l1_ratio = 1 it is an elementwise L1 penalty.\n    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    .. versionadded:: 0.17\n       Regularization parameter *l1_ratio* used in the Coordinate Descent\n       solver.\n\nverbose : int, default=0\n    Whether to be verbose.\n\nshuffle : bool, default=False\n    If true, randomize the order of coordinates in the CD solver.\n\n    .. versionadded:: 0.17\n       *shuffle* parameter used in the Coordinate Descent solver.\n\nregularization : {'both', 'components', 'transformation', None},                      default='both'\n    Select whether the regularization affects the components (H), the\n    transformation (W), both or none of them.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Factorization matrix, sometimes called 'dictionary'.\n\nn_components_ : int\n    The number of components. It is same as the `n_components` parameter\n    if it was given. Otherwise, it will be same as the number of\n    features.\n\nreconstruction_err_ : float\n    Frobenius norm of the matrix difference, or beta-divergence, between\n    the training data ``X`` and the reconstructed data ``WH`` from\n    the fitted model.\n\nn_iter_ : int\n    Actual number of iterations.\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import NMF\n>>> model = NMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_\n\nReferences\n----------\nCichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\nlarge scale nonnegative matrix and tensor factorizations.\"\nIEICE transactions on fundamentals of electronics, communications and\ncomputer sciences 92.3: 708-721, 2009.\n\nFevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\nfactorization with the beta-divergence. Neural Computation, 23(9)."
        }
      ],
      "functions": [
        {
          "name": "non_negative_factorization",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Constant matrix."
            },
            {
              "name": "W",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If init='custom', it is used as initial guess for the solution."
            },
            {
              "name": "H",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If init='custom', it is used as initial guess for the solution.\nIf update_H=False, it is used as a constant, to solve for W only."
            },
            {
              "name": "n_components",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of components, if n_components is not set all features\nare kept."
            }
          ],
          "results": [
            {
              "name": "W",
              "type": null,
              "description": "Solution to the non-negative least squares problem."
            },
            {
              "name": "H",
              "type": null,
              "description": "Solution to the non-negative least squares problem."
            },
            {
              "name": "n_iter",
              "type": "int",
              "description": "Actual number of iterations."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Non-negative Matrix Factorization (NMF).\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        0.5 * ||X - WH||_{Fro}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n        + alpha * l1_{ratio} * ||vec(H)||_1\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nFor multiplicative-update ('mu') solver, the Frobenius norm\n:math:`(0.5 * ||X - WH||_{Fro}^2)` can be changed into another\nbeta-divergence loss, by changing the beta_loss parameter.\n\nThe objective function is minimized with an alternating minimization of W\nand H. If H is given and update_H=False, it solves for W only.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Constant matrix.\n\nW : array-like of shape (n_samples, n_components), default=None\n    If init='custom', it is used as initial guess for the solution.\n\nH : array-like of shape (n_components, n_features), default=None\n    If init='custom', it is used as initial guess for the solution.\n    If update_H=False, it is used as a constant, to solve for W only.\n\nn_components : int, default=None\n    Number of components, if n_components is not set all features\n    are kept.\n\ninit : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n    Method used to initialize the procedure.\n\n    Valid options:\n\n    - None: 'nndsvd' if n_components < n_features, otherwise 'random'.\n\n    - 'random': non-negative random matrices, scaled with:\n        sqrt(X.mean() / n_components)\n\n    - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n        initialization (better for sparseness)\n\n    - 'nndsvda': NNDSVD with zeros filled with the average of X\n        (better when sparsity is not desired)\n\n    - 'nndsvdar': NNDSVD with zeros filled with small random values\n        (generally faster, less accurate alternative to NNDSVDa\n        for when sparsity is not desired)\n\n    - 'custom': use custom matrices W and H if `update_H=True`. If\n      `update_H=False`, then only custom matrix H is used.\n\n    .. versionchanged:: 0.23\n        The default value of `init` changed from 'random' to None in 0.23.\n\nupdate_H : bool, default=True\n    Set to True, both W and H will be estimated from initial guesses.\n    Set to False, only W will be estimated.\n\nsolver : {'cd', 'mu'}, default='cd'\n    Numerical solver to use:\n\n    - 'cd' is a Coordinate Descent solver that uses Fast Hierarchical\n        Alternating Least Squares (Fast HALS).\n\n    - 'mu' is a Multiplicative Update solver.\n\n    .. versionadded:: 0.17\n       Coordinate Descent solver.\n\n    .. versionadded:: 0.19\n       Multiplicative Update solver.\n\nbeta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n    Beta divergence to be minimized, measuring the distance between X\n    and the dot product WH. Note that values different from 'frobenius'\n    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n    matrix X cannot contain zeros. Used only in 'mu' solver.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-4\n    Tolerance of the stopping condition.\n\nmax_iter : int, default=200\n    Maximum number of iterations before timing out.\n\nalpha : float, default=0.\n    Constant that multiplies the regularization terms.\n\nl1_ratio : float, default=0.\n    The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n    For l1_ratio = 0 the penalty is an elementwise L2 penalty\n    (aka Frobenius Norm).\n    For l1_ratio = 1 it is an elementwise L1 penalty.\n    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\nregularization : {'both', 'components', 'transformation'}, default=None\n    Select whether the regularization affects the components (H), the\n    transformation (W), both or none of them.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for NMF initialisation (when ``init`` == 'nndsvdar' or\n    'random'), and in Coordinate Descent. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    The verbosity level.\n\nshuffle : bool, default=False\n    If true, randomize the order of coordinates in the CD solver.\n\nReturns\n-------\nW : ndarray of shape (n_samples, n_components)\n    Solution to the non-negative least squares problem.\n\nH : ndarray of shape (n_components, n_features)\n    Solution to the non-negative least squares problem.\n\nn_iter : int\n    Actual number of iterations.\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import non_negative_factorization\n>>> W, H, n_iter = non_negative_factorization(X, n_components=2,\n... init='random', random_state=0)\n\nReferences\n----------\nCichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\nlarge scale nonnegative matrix and tensor factorizations.\"\nIEICE transactions on fundamentals of electronics, communications and\ncomputer sciences 92.3: 708-721, 2009.\n\nFevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\nfactorization with the beta-divergence. Neural Computation, 23(9)."
        },
        {
          "name": "norm",
          "decorators": [],
          "parameters": [
            {
              "name": "x",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Vector for which to compute the norm."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Dot product-based Euclidean norm implementation.\n\nSee: http://fseoane.net/blog/2011/computing-the-vector-norm/\n\nParameters\n----------\nx : array-like\n    Vector for which to compute the norm."
        },
        {
          "name": "trace_dot",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "First matrix."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Second matrix."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Trace of np.dot(X, Y.T).\n\nParameters\n----------\nX : array-like\n    First matrix.\nY : array-like\n    Second matrix."
        }
      ]
    },
    {
      "name": "sklearn.decomposition._pca",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "math",
          "declaration": "log",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "sqrt",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "scipy.sparse.linalg",
          "declaration": "svds",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "gammaln",
          "alias": null
        },
        {
          "module": "sklearn.decomposition._base",
          "declaration": "_BasePCA",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils._arpack",
          "declaration": "_init_arpack_v0",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "fast_logdet",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "randomized_svd",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "stable_cumsum",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "svd_flip",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "PCA",
          "decorators": [],
          "superclasses": [
            "_BasePCA"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the instance itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model with X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Transformed values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model with X and apply the dimensionality reduction on X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Transformed values.\n\nNotes\n-----\nThis method returns a Fortran-ordered array. To convert it to a\nC-ordered array, use 'np.ascontiguousarray'."
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data."
                }
              ],
              "results": [
                {
                  "name": "ll",
                  "type": null,
                  "description": "Log-likelihood of each sample under the current model."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the log-likelihood of each sample.\n\nSee. \"Pattern Recognition and Machine Learning\"\nby C. Bishop, 12.2.1 p. 574\nor http://www.miketipping.com/papers/met-mppca.pdf\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data.\n\nReturns\n-------\nll : ndarray of shape (n_samples,)\n    Log-likelihood of each sample under the current model."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "ll",
                  "type": "float",
                  "description": "Average log-likelihood of the samples under the current model."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the average log-likelihood of all samples.\n\nSee. \"Pattern Recognition and Machine Learning\"\nby C. Bishop, 12.2.1 p. 574\nor http://www.miketipping.com/papers/met-mppca.pdf\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data.\n\ny : Ignored\n\nReturns\n-------\nll : float\n    Average log-likelihood of the samples under the current model."
            }
          ],
          "fullDocstring": "Principal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the\ndata to project it to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated\nSVD by the method of Halko et al. 2009, depending on the shape of the input\ndata and the number of components to extract.\n\nIt can also use the scipy.sparse.linalg ARPACK implementation of the\ntruncated SVD.\n\nNotice that this class does not support sparse input. See\n:class:`TruncatedSVD` for an alternative with sparse data.\n\nRead more in the :ref:`User Guide <PCA>`.\n\nParameters\n----------\nn_components : int, float or 'mle', default=None\n    Number of components to keep.\n    if n_components is not set all components are kept::\n\n        n_components == min(n_samples, n_features)\n\n    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n    MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n    number of components such that the amount of variance that needs to be\n    explained is greater than the percentage specified by n_components.\n\n    If ``svd_solver == 'arpack'``, the number of components must be\n    strictly less than the minimum of n_features and n_samples.\n\n    Hence, the None case results in::\n\n        n_components == min(n_samples, n_features) - 1\n\ncopy : bool, default=True\n    If False, data passed to fit are overwritten and running\n    fit(X).transform(X) will not yield the expected results,\n    use fit_transform(X) instead.\n\nwhiten : bool, default=False\n    When True (False by default) the `components_` vectors are multiplied\n    by the square root of n_samples and then divided by the singular values\n    to ensure uncorrelated outputs with unit component-wise variances.\n\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometime\n    improve the predictive accuracy of the downstream estimators by\n    making their data respect some hard-wired assumptions.\n\nsvd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'\n    If auto :\n        The solver is selected by a default policy based on `X.shape` and\n        `n_components`: if the input data is larger than 500x500 and the\n        number of components to extract is lower than 80% of the smallest\n        dimension of the data, then the more efficient 'randomized'\n        method is enabled. Otherwise the exact full SVD is computed and\n        optionally truncated afterwards.\n    If full :\n        run exact full SVD calling the standard LAPACK solver via\n        `scipy.linalg.svd` and select the components by postprocessing\n    If arpack :\n        run SVD truncated to n_components calling ARPACK solver via\n        `scipy.sparse.linalg.svds`. It requires strictly\n        0 < n_components < min(X.shape)\n    If randomized :\n        run randomized SVD by the method of Halko et al.\n\n    .. versionadded:: 0.18.0\n\ntol : float, default=0.0\n    Tolerance for singular values computed by svd_solver == 'arpack'.\n    Must be of range [0.0, infinity).\n\n    .. versionadded:: 0.18.0\n\niterated_power : int or 'auto', default='auto'\n    Number of iterations for the power method computed by\n    svd_solver == 'randomized'.\n    Must be of range [0, infinity).\n\n    .. versionadded:: 0.18.0\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.18.0\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Principal axes in feature space, representing the directions of\n    maximum variance in the data. The components are sorted by\n    ``explained_variance_``.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    The amount of variance explained by each of the selected components.\n\n    Equal to n_components largest eigenvalues\n    of the covariance matrix of X.\n\n    .. versionadded:: 0.18\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n\n    If ``n_components`` is not set then all components are stored and the\n    sum of the ratios is equal to 1.0.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\n    .. versionadded:: 0.19\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n\n    Equal to `X.mean(axis=0)`.\n\nn_components_ : int\n    The estimated number of components. When n_components is set\n    to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n    number is estimated from input data. Otherwise it equals the parameter\n    n_components, or the lesser value of n_features and n_samples\n    if n_components is None.\n\nn_features_ : int\n    Number of features in the training data.\n\nn_samples_ : int\n    Number of samples in the training data.\n\nnoise_variance_ : float\n    The estimated noise covariance following the Probabilistic PCA model\n    from Tipping and Bishop 1999. See \"Pattern Recognition and\n    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n    http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n    compute the estimated data covariance and score samples.\n\n    Equal to the average of (min(n_features, n_samples) - n_components)\n    smallest eigenvalues of the covariance matrix of X.\n\nSee Also\n--------\nKernelPCA : Kernel Principal Component Analysis.\nSparsePCA : Sparse Principal Component Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\nIncrementalPCA : Incremental Principal Component Analysis.\n\nReferences\n----------\nFor n_components == 'mle', this class uses the method of *Minka, T. P.\n\"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604*\n\nImplements the probabilistic PCA model from:\nTipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\ncomponent analysis\". Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology), 61(3), 611-622.\nvia the score and score_samples methods.\nSee http://www.miketipping.com/papers/met-mppca.pdf\n\nFor svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\nFor svd_solver == 'randomized', see:\n*Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n\"Finding structure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions\".\nSIAM review, 53(2), 217-288.* and also\n*Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n\"A randomized algorithm for the decomposition of matrices\".\nApplied and Computational Harmonic Analysis, 30(1), 47-68.*\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import PCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> pca = PCA(n_components=2)\n>>> pca.fit(X)\nPCA(n_components=2)\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.0075...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=2, svd_solver='full')\n>>> pca.fit(X)\nPCA(n_components=2, svd_solver='full')\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.00755...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=1, svd_solver='arpack')\n>>> pca.fit(X)\nPCA(n_components=1, svd_solver='arpack')\n>>> print(pca.explained_variance_ratio_)\n[0.99244...]\n>>> print(pca.singular_values_)\n[6.30061...]"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.decomposition._sparse_pca",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.decomposition._dict_learning",
          "declaration": "dict_learning",
          "alias": null
        },
        {
          "module": "sklearn.decomposition._dict_learning",
          "declaration": "dict_learning_online",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "ridge_regression",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "MiniBatchSparsePCA",
          "decorators": [],
          "superclasses": [
            "SparsePCA"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the instance itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model from data in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself."
            }
          ],
          "fullDocstring": "Mini-batch Sparse Principal Components Analysis\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.\n\nParameters\n----------\nn_components : int, default=None\n    number of sparse atoms to extract\n\nalpha : int, default=1\n    Sparsity controlling parameter. Higher values lead to sparser\n    components.\n\nridge_alpha : float, default=0.01\n    Amount of ridge shrinkage to apply in order to improve\n    conditioning when calling the transform method.\n\nn_iter : int, default=100\n    number of iterations to perform for each mini batch\n\ncallback : callable, default=None\n    callable that gets invoked every five iterations\n\nbatch_size : int, default=3\n    the number of features to take in each mini batch\n\nverbose : int or bool, default=False\n    Controls the verbosity; the higher, the more messages. Defaults to 0.\n\nshuffle : bool, default=True\n    whether to shuffle the data before splitting it in batches\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmethod : {'lars', 'cd'}, default='lars'\n    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for random shuffling when ``shuffle`` is set to ``True``,\n    during online dictionary learning. Pass an int for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Sparse components extracted from the data.\n\nn_components_ : int\n    Estimated number of components.\n\n    .. versionadded:: 0.23\n\nn_iter_ : int\n    Number of iterations run.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n    Equal to ``X.mean(axis=0)``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.decomposition import MiniBatchSparsePCA\n>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n>>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n...                                  random_state=0)\n>>> transformer.fit(X)\nMiniBatchSparsePCA(...)\n>>> X_transformed = transformer.transform(X)\n>>> X_transformed.shape\n(200, 5)\n>>> # most values in the components_ are zero (sparsity)\n>>> np.mean(transformer.components_ == 0)\n0.94\n\nSee Also\n--------\nPCA\nSparsePCA\nDictionaryLearning"
        },
        {
          "name": "SparsePCA",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the instance itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model from data in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test data to be transformed, must have the same number of\nfeatures as the data used to train the model."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Least Squares projection of the data onto the sparse components.\n\nTo avoid instability issues in case the system is under-determined,\nregularization can be applied (Ridge regression) via the\n`ridge_alpha` parameter.\n\nNote that Sparse PCA components orthogonality is not enforced as in PCA\nhence one cannot use a simple linear projection.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Test data to be transformed, must have the same number of\n    features as the data used to train the model.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Transformed data."
            }
          ],
          "fullDocstring": "Sparse Principal Components Analysis (SparsePCA).\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of sparse atoms to extract.\n\nalpha : float, default=1\n    Sparsity controlling parameter. Higher values lead to sparser\n    components.\n\nridge_alpha : float, default=0.01\n    Amount of ridge shrinkage to apply in order to improve\n    conditioning when calling the transform method.\n\nmax_iter : int, default=1000\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-8\n    Tolerance for the stopping condition.\n\nmethod : {'lars', 'cd'}, default='lars'\n    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nU_init : ndarray of shape (n_samples, n_components), default=None\n    Initial values for the loadings for warm restart scenarios.\n\nV_init : ndarray of shape (n_components, n_features), default=None\n    Initial values for the components for warm restart scenarios.\n\nverbose : int or bool, default=False\n    Controls the verbosity; the higher, the more messages. Defaults to 0.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used during dictionary learning. Pass an int for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Sparse components extracted from the data.\n\nerror_ : ndarray\n    Vector of errors at each iteration.\n\nn_components_ : int\n    Estimated number of components.\n\n    .. versionadded:: 0.23\n\nn_iter_ : int\n    Number of iterations run.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n    Equal to ``X.mean(axis=0)``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.decomposition import SparsePCA\n>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n>>> transformer = SparsePCA(n_components=5, random_state=0)\n>>> transformer.fit(X)\nSparsePCA(...)\n>>> X_transformed = transformer.transform(X)\n>>> X_transformed.shape\n(200, 5)\n>>> # most values in the components_ are zero (sparsity)\n>>> np.mean(transformer.components_ == 0)\n0.9666...\n\nSee Also\n--------\nPCA\nMiniBatchSparsePCA\nDictionaryLearning"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.decomposition._truncated_svd",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "scipy.sparse.linalg",
          "declaration": "svds",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils._arpack",
          "declaration": "_init_arpack_v0",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "randomized_svd",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "svd_flip",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "mean_variance_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "TruncatedSVD",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the transformer object."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit model on training data X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the transformer object."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Reduced version of X. This will always be a dense array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit model to X and perform dimensionality reduction on X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : Ignored\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Reduced version of X. This will always be a dense array."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Reduced version of X. This will always be a dense array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform dimensionality reduction on X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Reduced version of X. This will always be a dense array."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data."
                }
              ],
              "results": [
                {
                  "name": "X_original",
                  "type": null,
                  "description": "Note that this is always a dense array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X back to its original space.\n\nReturns an array X_original whose transform would be X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_components)\n    New data.\n\nReturns\n-------\nX_original : ndarray of shape (n_samples, n_features)\n    Note that this is always a dense array."
            }
          ],
          "fullDocstring": "Dimensionality reduction using truncated SVD (aka LSA).\n\nThis transformer performs linear dimensionality reduction by means of\ntruncated singular value decomposition (SVD). Contrary to PCA, this\nestimator does not center the data before computing the singular value\ndecomposition. This means it can work with sparse matrices\nefficiently.\n\nIn particular, truncated SVD works on term count/tf-idf matrices as\nreturned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\nthat context, it is known as latent semantic analysis (LSA).\n\nThis estimator supports two algorithms: a fast randomized SVD solver, and\na \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n`X.T * X`, whichever is more efficient.\n\nRead more in the :ref:`User Guide <LSA>`.\n\nParameters\n----------\nn_components : int, default=2\n    Desired dimensionality of output data.\n    Must be strictly less than the number of features.\n    The default value is useful for visualisation. For LSA, a value of\n    100 is recommended.\n\nalgorithm : {'arpack', 'randomized'}, default='randomized'\n    SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n    (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n    algorithm due to Halko (2009).\n\nn_iter : int, default=5\n    Number of iterations for randomized SVD solver. Not used by ARPACK. The\n    default is larger than the default in\n    :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n    matrices that may have large slowly decaying spectrum.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used during randomized svd. Pass an int for reproducible results across\n    multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=0.\n    Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n    SVD solver.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n\nexplained_variance_ : ndarray of shape (n_components,)\n    The variance of the training samples transformed by a projection to\n    each component.\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n\nsingular_values_ : ndarray od shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\nExamples\n--------\n>>> from sklearn.decomposition import TruncatedSVD\n>>> from scipy.sparse import random as sparse_random\n>>> X = sparse_random(100, 100, density=0.01, format='csr',\n...                   random_state=42)\n>>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n>>> svd.fit(X)\nTruncatedSVD(n_components=5, n_iter=7, random_state=42)\n>>> print(svd.explained_variance_ratio_)\n[0.0646... 0.0633... 0.0639... 0.0535... 0.0406...]\n>>> print(svd.explained_variance_ratio_.sum())\n0.286...\n>>> print(svd.singular_values_)\n[1.553... 1.512...  1.510... 1.370... 1.199...]\n\nSee Also\n--------\nPCA\n\nReferences\n----------\nFinding structure with randomness: Stochastic algorithms for constructing\napproximate matrix decompositions\nHalko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n\nNotes\n-----\nSVD suffers from a problem called \"sign indeterminacy\", which means the\nsign of the ``components_`` and the output from transform depend on the\nalgorithm and random state. To work around this, fit instances of this\nclass to data once, then keep the instance around to do transformations."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.decomposition.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "numpy.distutils.misc_util",
          "declaration": "Configuration",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.discriminant_analysis",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "empirical_covariance",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "ledoit_wolf",
          "alias": null
        },
        {
          "module": "sklearn.covariance",
          "declaration": "shrunk_covariance",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "StandardScaler",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "softmax",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "unique_labels",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "LinearDiscriminantAnalysis",
          "decorators": [],
          "superclasses": [
            "LinearClassifierMixin",
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit LinearDiscriminantAnalysis model according to the given\n   training data and parameters.\n\n   .. versionchanged:: 0.19\n      *store_covariance* has been moved to main constructor.\n\n   .. versionchanged:: 0.19\n      *tol* has been moved to main constructor.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,)\n    Target values."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Project data to maximize class separation.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input data.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Transformed data."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Estimated probabilities."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Estimate probability.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input data.\n\nReturns\n-------\nC : ndarray of shape (n_samples, n_classes)\n    Estimated probabilities."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Estimated log probabilities."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Estimate log probability.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input data.\n\nReturns\n-------\nC : ndarray of shape (n_samples, n_classes)\n    Estimated log probabilities."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of samples (test vectors)."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Decision function values related to each class, per sample.\nIn the two-class case, the shape is (n_samples,), giving the\nlog likelihood ratio of the positive class."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply decision function to an array of samples.\n\nThe decision function is equal (up to a constant factor) to the\nlog-posterior of the model, i.e. `log p(y = k | x)`. In a binary\nclassification setting this instead corresponds to the difference\n`log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Array of samples (test vectors).\n\nReturns\n-------\nC : ndarray of shape (n_samples,) or (n_samples, n_classes)\n    Decision function values related to each class, per sample.\n    In the two-class case, the shape is (n_samples,), giving the\n    log likelihood ratio of the positive class."
            }
          ],
          "fullDocstring": "Linear Discriminant Analysis\n\nA classifier with a linear decision boundary, generated by fitting class\nconditional densities to the data and using Bayes' rule.\n\nThe model fits a Gaussian density to each class, assuming that all classes\nshare the same covariance matrix.\n\nThe fitted model can also be used to reduce the dimensionality of the input\nby projecting it to the most discriminative directions, using the\n`transform` method.\n\n.. versionadded:: 0.17\n   *LinearDiscriminantAnalysis*.\n\nRead more in the :ref:`User Guide <lda_qda>`.\n\nParameters\n----------\nsolver : {'svd', 'lsqr', 'eigen'}, default='svd'\n    Solver to use, possible values:\n      - 'svd': Singular value decomposition (default).\n        Does not compute the covariance matrix, therefore this solver is\n        recommended for data with a large number of features.\n      - 'lsqr': Least squares solution.\n        Can be combined with shrinkage or custom covariance estimator.\n      - 'eigen': Eigenvalue decomposition.\n        Can be combined with shrinkage or custom covariance estimator.\n\nshrinkage : 'auto' or float, default=None\n    Shrinkage parameter, possible values:\n      - None: no shrinkage (default).\n      - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n      - float between 0 and 1: fixed shrinkage parameter.\n\n    This should be left to None if `covariance_estimator` is used.\n    Note that shrinkage works only with 'lsqr' and 'eigen' solvers.\n\npriors : array-like of shape (n_classes,), default=None\n    The class prior probabilities. By default, the class proportions are\n    inferred from the training data.\n\nn_components : int, default=None\n    Number of components (<= min(n_classes - 1, n_features)) for\n    dimensionality reduction. If None, will be set to\n    min(n_classes - 1, n_features). This parameter only affects the\n    `transform` method.\n\nstore_covariance : bool, default=False\n    If True, explicitely compute the weighted within-class covariance\n    matrix when solver is 'svd'. The matrix is always computed\n    and stored for the other solvers.\n\n    .. versionadded:: 0.17\n\ntol : float, default=1.0e-4\n    Absolute threshold for a singular value of X to be considered\n    significant, used to estimate the rank of X. Dimensions whose\n    singular values are non-significant are discarded. Only used if\n    solver is 'svd'.\n\n    .. versionadded:: 0.17\n\ncovariance_estimator : covariance estimator, default=None\n    If not None, `covariance_estimator` is used to estimate\n    the covariance matrices instead of relying on the empirical\n    covariance estimator (with potential shrinkage).\n    The object should have a fit method and a ``covariance_`` attribute\n    like the estimators in :mod:`sklearn.covariance`.\n    if None the shrinkage parameter drives the estimate.\n\n    This should be left to None if `shrinkage` is used.\n    Note that `covariance_estimator` works only with 'lsqr' and 'eigen'\n    solvers.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,) or (n_classes, n_features)\n    Weight vector(s).\n\nintercept_ : ndarray of shape (n_classes,)\n    Intercept term.\n\ncovariance_ : array-like of shape (n_features, n_features)\n    Weighted within-class covariance matrix. It corresponds to\n    `sum_k prior_k * C_k` where `C_k` is the covariance matrix of the\n    samples in class `k`. The `C_k` are estimated using the (potentially\n    shrunk) biased estimator of covariance. If solver is 'svd', only\n    exists when `store_covariance` is True.\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n    If ``n_components`` is not set then all components are stored and the\n    sum of explained variances is equal to 1.0. Only available when eigen\n    or svd solver is used.\n\nmeans_ : array-like of shape (n_classes, n_features)\n    Class-wise means.\n\npriors_ : array-like of shape (n_classes,)\n    Class priors (sum to 1).\n\nscalings_ : array-like of shape (rank, n_classes - 1)\n    Scaling of the features in the space spanned by the class centroids.\n    Only available for 'svd' and 'eigen' solvers.\n\nxbar_ : array-like of shape (n_features,)\n    Overall mean. Only present if solver is 'svd'.\n\nclasses_ : array-like of shape (n_classes,)\n    Unique class labels.\n\nSee Also\n--------\nQuadraticDiscriminantAnalysis : Quadratic Discriminant Analysis.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = LinearDiscriminantAnalysis()\n>>> clf.fit(X, y)\nLinearDiscriminantAnalysis()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]"
        },
        {
          "name": "QuadraticDiscriminantAnalysis",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values (integers)"
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model according to the given training data and parameters.\n\n    .. versionchanged:: 0.19\n       ``store_covariances`` has been moved to main constructor as\n       ``store_covariance``\n\n    .. versionchanged:: 0.19\n       ``tol`` has been moved to main constructor.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values (integers)"
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of samples (test vectors)."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Decision function values related to each class, per sample.\nIn the two-class case, the shape is (n_samples,), giving the\nlog likelihood ratio of the positive class."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply decision function to an array of samples.\n\nThe decision function is equal (up to a constant factor) to the\nlog-posterior of the model, i.e. `log p(y = k | x)`. In a binary\nclassification setting this instead corresponds to the difference\n`log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Array of samples (test vectors).\n\nReturns\n-------\nC : ndarray of shape (n_samples,) or (n_samples, n_classes)\n    Decision function values related to each class, per sample.\n    In the two-class case, the shape is (n_samples,), giving the\n    log likelihood ratio of the positive class."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform classification on an array of test vectors X.\n\nThe predicted class C for each sample in X is returned.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nReturns\n-------\nC : ndarray of shape (n_samples,)"
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of samples/test vectors."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Posterior probabilities of classification per class."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return posterior probabilities of classification.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Array of samples/test vectors.\n\nReturns\n-------\nC : ndarray of shape (n_samples, n_classes)\n    Posterior probabilities of classification per class."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of samples/test vectors."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Posterior log-probabilities of classification per class."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return log of posterior probabilities of classification.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Array of samples/test vectors.\n\nReturns\n-------\nC : ndarray of shape (n_samples, n_classes)\n    Posterior log-probabilities of classification per class."
            }
          ],
          "fullDocstring": "Quadratic Discriminant Analysis\n\nA classifier with a quadratic decision boundary, generated\nby fitting class conditional densities to the data\nand using Bayes' rule.\n\nThe model fits a Gaussian density to each class.\n\n.. versionadded:: 0.17\n   *QuadraticDiscriminantAnalysis*\n\nRead more in the :ref:`User Guide <lda_qda>`.\n\nParameters\n----------\npriors : ndarray of shape (n_classes,), default=None\n    Class priors. By default, the class proportions are inferred from the\n    training data.\n\nreg_param : float, default=0.0\n    Regularizes the per-class covariance estimates by transforming S2 as\n    ``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``,\n    where S2 corresponds to the `scaling_` attribute of a given class.\n\nstore_covariance : bool, default=False\n    If True, the class covariance matrices are explicitely computed and\n    stored in the `self.covariance_` attribute.\n\n    .. versionadded:: 0.17\n\ntol : float, default=1.0e-4\n    Absolute threshold for a singular value to be considered significant,\n    used to estimate the rank of `Xk` where `Xk` is the centered matrix\n    of samples in class k. This parameter does not affect the\n    predictions. It only controls a warning that is raised when features\n    are considered to be colinear.\n\n    .. versionadded:: 0.17\n\nAttributes\n----------\ncovariance_ : list of len n_classes of ndarray             of shape (n_features, n_features)\n    For each class, gives the covariance matrix estimated using the\n    samples of that class. The estimations are unbiased. Only present if\n    `store_covariance` is True.\n\nmeans_ : array-like of shape (n_classes, n_features)\n    Class-wise means.\n\npriors_ : array-like of shape (n_classes,)\n    Class priors (sum to 1).\n\nrotations_ : list of len n_classes of ndarray of shape (n_features, n_k)\n    For each class k an array of shape (n_features, n_k), where\n    ``n_k = min(n_features, number of elements in class k)``\n    It is the rotation of the Gaussian distribution, i.e. its\n    principal axis. It corresponds to `V`, the matrix of eigenvectors\n    coming from the SVD of `Xk = U S Vt` where `Xk` is the centered\n    matrix of samples from class k.\n\nscalings_ : list of len n_classes of ndarray of shape (n_k,)\n    For each class, contains the scaling of\n    the Gaussian distributions along its principal axes, i.e. the\n    variance in the rotated coordinate system. It corresponds to `S^2 /\n    (n_samples - 1)`, where `S` is the diagonal matrix of singular values\n    from the SVD of `Xk`, where `Xk` is the centered matrix of samples\n    from class k.\n\nclasses_ : ndarray of shape (n_classes,)\n    Unique class labels.\n\nExamples\n--------\n>>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = QuadraticDiscriminantAnalysis()\n>>> clf.fit(X, y)\nQuadraticDiscriminantAnalysis()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]\n\nSee Also\n--------\nLinearDiscriminantAnalysis : Linear Discriminant Analysis."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.dummy",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "class_distribution",
          "alias": null
        },
        {
          "module": "sklearn.utils.random",
          "declaration": "_random_choice_csc",
          "alias": null
        },
        {
          "module": "sklearn.utils.stats",
          "declaration": "_weighted_percentile",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "DummyClassifier",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "BaseEstimator",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the random classifier.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Predicted target values for X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform classification on test vectors X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test data.\n\nReturns\n-------\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Predicted target values for X."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test data."
                }
              ],
              "results": [
                {
                  "name": "P",
                  "type": null,
                  "description": "Returns the probability of the sample for each class in\nthe model, where classes are ordered arithmetically, for each\noutput."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return probability estimates for the test vectors X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test data.\n\nReturns\n-------\nP : ndarray of shape (n_samples, n_classes) or list of such arrays\n    Returns the probability of the sample for each class in\n    the model, where classes are ordered arithmetically, for each\n    output."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, requires length = n_samples"
                }
              ],
              "results": [
                {
                  "name": "P",
                  "type": null,
                  "description": "Returns the log probability of the sample for each class in\nthe model, where classes are ordered arithmetically for each\noutput."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return log probability estimates for the test vectors X.\n\nParameters\n----------\nX : {array-like, object with finite length or shape}\n    Training data, requires length = n_samples\n\nReturns\n-------\nP : ndarray of shape (n_samples, n_classes) or list of such arrays\n    Returns the log probability of the sample for each class in\n    the model, where classes are ordered arithmetically for each\n    output."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples. Passing None as test samples gives the same result\nas passing real test samples, since DummyClassifier\noperates independently of the sampled observations."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True labels for X."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "Mean accuracy of self.predict(X) wrt. y."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\nParameters\n----------\nX : None or array-like of shape (n_samples, n_features)\n    Test samples. Passing None as test samples gives the same result\n    as passing real test samples, since DummyClassifier\n    operates independently of the sampled observations.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True labels for X.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    Mean accuracy of self.predict(X) wrt. y."
            }
          ],
          "fullDocstring": "DummyClassifier is a classifier that makes predictions using simple rules.\n\nThis classifier is useful as a simple baseline to compare with other\n(real) classifiers. Do not use it for real problems.\n\nRead more in the :ref:`User Guide <dummy_estimators>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nstrategy : {\"stratified\", \"most_frequent\", \"prior\", \"uniform\",             \"constant\"}, default=\"prior\"\n    Strategy to use to generate predictions.\n\n    * \"stratified\": generates predictions by respecting the training\n      set's class distribution.\n    * \"most_frequent\": always predicts the most frequent label in the\n      training set.\n    * \"prior\": always predicts the class that maximizes the class prior\n      (like \"most_frequent\") and ``predict_proba`` returns the class prior.\n    * \"uniform\": generates predictions uniformly at random.\n    * \"constant\": always predicts a constant label that is provided by\n      the user. This is useful for metrics that evaluate a non-majority\n      class\n\n      .. versionchanged:: 0.24\n         The default value of `strategy` has changed to \"prior\" in version\n         0.24.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness to generate the predictions when\n    ``strategy='stratified'`` or ``strategy='uniform'``.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nconstant : int or str or array-like of shape (n_outputs,)\n    The explicit constant as predicted by the \"constant\" strategy. This\n    parameter is useful only for the \"constant\" strategy.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,) or list of such arrays\n    Class labels for each output.\n\nn_classes_ : int or list of int\n    Number of label for each output.\n\nclass_prior_ : ndarray of shape (n_classes,) or list of such arrays\n    Probability of each class for each output.\n\nn_outputs_ : int\n    Number of outputs.\n\nsparse_output_ : bool\n    True if the array returned from predict is to be in sparse CSC format.\n    Is automatically set to True if the input y is passed in sparse format.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.dummy import DummyClassifier\n>>> X = np.array([-1, 1, 1, 1])\n>>> y = np.array([0, 1, 1, 1])\n>>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n>>> dummy_clf.fit(X, y)\nDummyClassifier(strategy='most_frequent')\n>>> dummy_clf.predict(X)\narray([1, 1, 1, 1])\n>>> dummy_clf.score(X, y)\n0.75"
        },
        {
          "name": "DummyRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseEstimator",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the random regressor.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test data."
                },
                {
                  "name": "return_std",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "Whether to return the standard deviation of posterior prediction.\nAll zeros in this case.\n\n.. versionadded:: 0.20"
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Predicted target values for X."
                },
                {
                  "name": "y_std",
                  "type": null,
                  "description": "Standard deviation of predictive distribution of query points."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform classification on test vectors X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test data.\n\nreturn_std : bool, default=False\n    Whether to return the standard deviation of posterior prediction.\n    All zeros in this case.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Predicted target values for X.\n\ny_std : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Standard deviation of predictive distribution of query points."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples. Passing None as test samples gives the same result\nas passing real test samples, since DummyRegressor\noperates independently of the sampled observations."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True values for X."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "R^2 of self.predict(X) wrt. y."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the residual\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the total\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nThe best possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n\nParameters\n----------\nX : None or array-like of shape (n_samples, n_features)\n    Test samples. Passing None as test samples gives the same result\n    as passing real test samples, since DummyRegressor\n    operates independently of the sampled observations.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True values for X.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    R^2 of self.predict(X) wrt. y."
            }
          ],
          "fullDocstring": "DummyRegressor is a regressor that makes predictions using\nsimple rules.\n\nThis regressor is useful as a simple baseline to compare with other\n(real) regressors. Do not use it for real problems.\n\nRead more in the :ref:`User Guide <dummy_estimators>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nstrategy : {\"mean\", \"median\", \"quantile\", \"constant\"}, default=\"mean\"\n    Strategy to use to generate predictions.\n\n    * \"mean\": always predicts the mean of the training set\n    * \"median\": always predicts the median of the training set\n    * \"quantile\": always predicts a specified quantile of the training set,\n      provided with the quantile parameter.\n    * \"constant\": always predicts a constant value that is provided by\n      the user.\n\nconstant : int or float or array-like of shape (n_outputs,), default=None\n    The explicit constant as predicted by the \"constant\" strategy. This\n    parameter is useful only for the \"constant\" strategy.\n\nquantile : float in [0.0, 1.0], default=None\n    The quantile to predict using the \"quantile\" strategy. A quantile of\n    0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the\n    maximum.\n\nAttributes\n----------\nconstant_ : ndarray of shape (1, n_outputs)\n    Mean or median or quantile of the training targets or constant value\n    given by the user.\n\nn_outputs_ : int\n    Number of outputs.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.dummy import DummyRegressor\n>>> X = np.array([1.0, 2.0, 3.0, 4.0])\n>>> y = np.array([2.0, 3.0, 5.0, 10.0])\n>>> dummy_regr = DummyRegressor(strategy=\"mean\")\n>>> dummy_regr.fit(X, y)\nDummyRegressor()\n>>> dummy_regr.predict(X)\narray([5., 5., 5., 5.])\n>>> dummy_regr.score(X, y)\n0.0"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble",
      "imports": [
        {
          "module": "typing",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn._bagging",
          "declaration": "BaggingClassifier",
          "alias": null
        },
        {
          "module": "sklearn._bagging",
          "declaration": "BaggingRegressor",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "BaseEnsemble",
          "alias": null
        },
        {
          "module": "sklearn._forest",
          "declaration": "ExtraTreesClassifier",
          "alias": null
        },
        {
          "module": "sklearn._forest",
          "declaration": "ExtraTreesRegressor",
          "alias": null
        },
        {
          "module": "sklearn._forest",
          "declaration": "RandomForestClassifier",
          "alias": null
        },
        {
          "module": "sklearn._forest",
          "declaration": "RandomForestRegressor",
          "alias": null
        },
        {
          "module": "sklearn._forest",
          "declaration": "RandomTreesEmbedding",
          "alias": null
        },
        {
          "module": "sklearn._gb",
          "declaration": "GradientBoostingClassifier",
          "alias": null
        },
        {
          "module": "sklearn._gb",
          "declaration": "GradientBoostingRegressor",
          "alias": null
        },
        {
          "module": "sklearn._hist_gradient_boosting.gradient_boosting",
          "declaration": "HistGradientBoostingClassifier",
          "alias": null
        },
        {
          "module": "sklearn._hist_gradient_boosting.gradient_boosting",
          "declaration": "HistGradientBoostingRegressor",
          "alias": null
        },
        {
          "module": "sklearn._iforest",
          "declaration": "IsolationForest",
          "alias": null
        },
        {
          "module": "sklearn._stacking",
          "declaration": "StackingClassifier",
          "alias": null
        },
        {
          "module": "sklearn._stacking",
          "declaration": "StackingRegressor",
          "alias": null
        },
        {
          "module": "sklearn._voting",
          "declaration": "VotingClassifier",
          "alias": null
        },
        {
          "module": "sklearn._voting",
          "declaration": "VotingRegressor",
          "alias": null
        },
        {
          "module": "sklearn._weight_boosting",
          "declaration": "AdaBoostClassifier",
          "alias": null
        },
        {
          "module": "sklearn._weight_boosting",
          "declaration": "AdaBoostRegressor",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._bagging",
      "imports": [
        {
          "module": "itertools",
          "alias": null
        },
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "BaseEnsemble",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "_partition_estimators",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "r2_score",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "DecisionTreeClassifier",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "DecisionTreeRegressor",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "indices_to_mask",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "if_delegate_has_method",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.random",
          "declaration": "sample_without_replacement",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "has_fit_parameter",
          "alias": null
        },
        {
          "module": "warnings",
          "declaration": "warn",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaggingClassifier",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "BaseBagging"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted classes."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class for X.\n\nThe predicted class of an input sample is computed as the class with\nthe highest mean predicted probability. If base estimators do not\nimplement a ``predict_proba`` method, then it resorts to voting.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted classes."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe mean predicted class probabilities of the base estimators in the\nensemble. If base estimators do not implement a ``predict_proba``\nmethod, then it resorts to voting and the predicted class probabilities\nof an input sample represents the proportion of estimators predicting\neach class.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class log-probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe log of the mean predicted class probabilities of the base\nestimators in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class log-probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": null,
                  "description": "The decision function of the input samples. The columns correspond\nto the classes in sorted order, as they appear in the attribute\n``classes_``. Regression and binary classification are special\ncases with ``k == 1``, otherwise ``k==n_classes``."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Average of the decision functions of the base classifiers.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\nscore : ndarray of shape (n_samples, k)\n    The decision function of the input samples. The columns correspond\n    to the classes in sorted order, as they appear in the attribute\n    ``classes_``. Regression and binary classification are special\n    cases with ``k == 1``, otherwise ``k==n_classes``."
            }
          ],
          "fullDocstring": "A Bagging classifier.\n\nA Bagging classifier is an ensemble meta-estimator that fits base\nclassifiers each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\nbase_estimator : object, default=None\n    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a\n    :class:`~sklearn.tree.DecisionTreeClassifier`.\n\nn_estimators : int, default=10\n    The number of base estimators in the ensemble.\n\nmax_samples : int or float, default=1.0\n    The number of samples to draw from X to train each base estimator (with\n    replacement by default, see `bootstrap` for more details).\n\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n\nmax_features : int or float, default=1.0\n    The number of features to draw from X to train each base estimator (\n    without replacement by default, see `bootstrap_features` for more\n    details).\n\n    - If int, then draw `max_features` features.\n    - If float, then draw `max_features * X.shape[1]` features.\n\nbootstrap : bool, default=True\n    Whether samples are drawn with replacement. If False, sampling\n    without replacement is performed.\n\nbootstrap_features : bool, default=False\n    Whether features are drawn with replacement.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate\n    the generalization error.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n    .. versionadded:: 0.17\n       *warm_start* constructor parameter.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random resampling of the original dataset\n    (sample wise and feature wise).\n    If the base estimator accepts a `random_state` attribute, a different\n    seed is generated for each instance in the ensemble.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nn_features_ : int\n    The number of features when :meth:`fit` is performed.\n\nestimators_ : list of estimators\n    The collection of fitted base estimators.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\nestimators_features_ : list of arrays\n    The subset of drawn features for each base estimator.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_classes_ : int or list\n    The number of classes.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nExamples\n--------\n>>> from sklearn.svm import SVC\n>>> from sklearn.ensemble import BaggingClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=100, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = BaggingClassifier(base_estimator=SVC(),\n...                         n_estimators=10, random_state=0).fit(X, y)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])\n\nReferences\n----------\n\n.. [1] L. Breiman, \"Pasting small votes for classification in large\n       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n       1996.\n\n.. [3] T. Ho, \"The random subspace method for constructing decision\n       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n       1998.\n\n.. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n       Learning and Knowledge Discovery in Databases, 346-361, 2012."
        },
        {
          "name": "BaggingRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseBagging"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the estimators in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted values."
            }
          ],
          "fullDocstring": "A Bagging regressor.\n\nA Bagging regressor is an ensemble meta-estimator that fits base\nregressors each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\nbase_estimator : object, default=None\n    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a\n    :class:`~sklearn.tree.DecisionTreeRegressor`.\n\nn_estimators : int, default=10\n    The number of base estimators in the ensemble.\n\nmax_samples : int or float, default=1.0\n    The number of samples to draw from X to train each base estimator (with\n    replacement by default, see `bootstrap` for more details).\n\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n\nmax_features : int or float, default=1.0\n    The number of features to draw from X to train each base estimator (\n    without replacement by default, see `bootstrap_features` for more\n    details).\n\n    - If int, then draw `max_features` features.\n    - If float, then draw `max_features * X.shape[1]` features.\n\nbootstrap : bool, default=True\n    Whether samples are drawn with replacement. If False, sampling\n    without replacement is performed.\n\nbootstrap_features : bool, default=False\n    Whether features are drawn with replacement.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate\n    the generalization error.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random resampling of the original dataset\n    (sample wise and feature wise).\n    If the base estimator accepts a `random_state` attribute, a different\n    seed is generated for each instance in the ensemble.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nn_features_ : int\n    The number of features when :meth:`fit` is performed.\n\nestimators_ : list of estimators\n    The collection of fitted sub-estimators.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\nestimators_features_ : list of arrays\n    The subset of drawn features for each base estimator.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_prediction_ : ndarray of shape (n_samples,)\n    Prediction computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_prediction_` might contain NaN. This attribute exists only\n    when ``oob_score`` is True.\n\nExamples\n--------\n>>> from sklearn.svm import SVR\n>>> from sklearn.ensemble import BaggingRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_samples=100, n_features=4,\n...                        n_informative=2, n_targets=1,\n...                        random_state=0, shuffle=False)\n>>> regr = BaggingRegressor(base_estimator=SVR(),\n...                         n_estimators=10, random_state=0).fit(X, y)\n>>> regr.predict([[0, 0, 0, 0]])\narray([-2.8720...])\n\nReferences\n----------\n\n.. [1] L. Breiman, \"Pasting small votes for classification in large\n       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n       1996.\n\n.. [3] T. Ho, \"The random subspace method for constructing decision\n       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n       1998.\n\n.. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n       Learning and Knowledge Discovery in Databases, 346-361, 2012."
        },
        {
          "name": "BaseBagging",
          "decorators": [],
          "superclasses": [
            "BaseEnsemble"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (class labels in classification, real numbers in\nregression)."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted.\nNote that this is supported only if the base estimator supports\nsample weighting."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Build a Bagging ensemble of estimators from the training\n   set (X, y).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\ny : array-like of shape (n_samples,)\n    The target values (class labels in classification, real numbers in\n    regression).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Note that this is supported only if the base estimator supports\n    sample weighting.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "estimators_samples_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "The subset of drawn samples for each base estimator.\n\nReturns a dynamically generated list of indices identifying\nthe samples used for fitting each member of the ensemble, i.e.,\nthe in-bag samples.\n\nNote: the list is re-created at each call to the property in order\nto reduce the object memory footprint by not storing the sampling\ndata. Thus fetching the property may be slower than expected."
            }
          ],
          "fullDocstring": "Base class for Bagging meta-estimator.\n\nWarning: This class should not be used directly. Use derived classes\ninstead."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._base",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_regressor",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_print_elapsed_time",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "_BaseComposition",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "List",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseEnsemble",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "BaseEstimator"
          ],
          "methods": [],
          "fullDocstring": "Base class for all ensemble classes.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.\n\nParameters\n----------\nbase_estimator : object\n    The base estimator from which the ensemble is built.\n\nn_estimators : int, default=10\n    The number of estimators in the ensemble.\n\nestimator_params : list of str, default=tuple()\n    The list of attributes to use as parameters when instantiating a\n    new base estimator. If none are given, default parameters are used.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nestimators_ : list of estimators\n    The collection of fitted base estimators."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._forest",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "threading",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "hstack",
          "alias": "sparse_hstack"
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "BaseEnsemble",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "_partition_estimators",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "DataConversionWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "r2_score",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "OneHotEncoder",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "DecisionTreeClassifier",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "DecisionTreeRegressor",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "ExtraTreeClassifier",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "ExtraTreeRegressor",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "DOUBLE",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "compute_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_joblib_parallel_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "warnings",
          "declaration": "catch_warnings",
          "alias": null
        },
        {
          "module": "warnings",
          "declaration": "simplefilter",
          "alias": null
        },
        {
          "module": "warnings",
          "declaration": "warn",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseForest",
          "decorators": [],
          "superclasses": [
            "BaseEnsemble",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "apply",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, its dtype will be converted to\n``dtype=np.float32``. If a sparse matrix is provided, it will be\nconverted into a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "X_leaves",
                  "type": null,
                  "description": "For each datapoint x in X and for each tree in the forest,\nreturn the index of the leaf x ends up in."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply trees in the forest to X, return leaf indices.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, its dtype will be converted to\n    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n    converted into a sparse ``csr_matrix``.\n\nReturns\n-------\nX_leaves : ndarray of shape (n_samples, n_estimators)\n    For each datapoint x in X and for each tree in the forest,\n    return the index of the leaf x ends up in."
            },
            {
              "name": "decision_path",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, its dtype will be converted to\n``dtype=np.float32``. If a sparse matrix is provided, it will be\nconverted into a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "indicator",
                  "type": null,
                  "description": "Return a node indicator matrix where non zero elements indicates\nthat the samples goes through the nodes. The matrix is of CSR\nformat."
                },
                {
                  "name": "n_nodes_ptr",
                  "type": null,
                  "description": "The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\ngives the indicator value for the i-th estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the decision path in the forest.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, its dtype will be converted to\n    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n    converted into a sparse ``csr_matrix``.\n\nReturns\n-------\nindicator : sparse matrix of shape (n_samples, n_nodes)\n    Return a node indicator matrix where non zero elements indicates\n    that the samples goes through the nodes. The matrix is of CSR\n    format.\n\nn_nodes_ptr : ndarray of shape (n_estimators + 1,)\n    The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n    gives the indicator value for the i-th estimator."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Internally, its dtype will be converted\nto ``dtype=np.float32``. If a sparse matrix is provided, it will be\nconverted into a sparse ``csc_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (class labels in classification, real numbers in\nregression)."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Build a forest of trees from the training set (X, y).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Internally, its dtype will be converted\n    to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n    converted into a sparse ``csc_matrix``.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    The target values (class labels in classification, real numbers in\n    regression).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node. In the case of\n    classification, splits are also ignored if they would result in any\n    single class carrying a negative weight in either child node.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "feature_importances_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "feature_importances_",
                  "type": null,
                  "description": "The values of this array sum to 1, unless all trees are single node\ntrees consisting of only the root node, in which case it will be an\narray of zeros."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "The impurity-based feature importances.\n\nThe higher, the more important the feature.\nThe importance of a feature is computed as the (normalized)\ntotal reduction of the criterion brought by that feature.  It is also\nknown as the Gini importance.\n\nWarning: impurity-based feature importances can be misleading for\nhigh cardinality features (many unique values). See\n:func:`sklearn.inspection.permutation_importance` as an alternative.\n\nReturns\n-------\nfeature_importances_ : ndarray of shape (n_features,)\n    The values of this array sum to 1, unless all trees are single node\n    trees consisting of only the root node, in which case it will be an\n    array of zeros."
            }
          ],
          "fullDocstring": "Base class for forests of trees.\n\nWarning: This class should not be used directly. Use derived classes\ninstead."
        },
        {
          "name": "ExtraTreesClassifier",
          "decorators": [],
          "superclasses": [
            "ForestClassifier"
          ],
          "methods": [],
          "fullDocstring": "An extra-trees classifier.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"gini\", \"entropy\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `round(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=sqrt(n_features)`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nbootstrap : bool, default=False\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate\n    the generalization accuracy.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls 3 sources of randomness:\n\n    - the bootstrapping of the samples used when building trees\n      (if ``bootstrap=True``)\n    - the sampling of the features to consider when looking for the best\n      split at each node (if ``max_features < n_features``)\n    - the draw of the splits for each of the `max_features`\n\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nbase_estimator_ : ExtraTreesClassifier\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\n\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nSee Also\n--------\nsklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\nRandomForestClassifier : Ensemble Classifier based on trees with optimal\n    splits.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n       trees\", Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.ensemble import ExtraTreesClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_features=4, random_state=0)\n>>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n>>> clf.fit(X, y)\nExtraTreesClassifier(random_state=0)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])"
        },
        {
          "name": "ExtraTreesRegressor",
          "decorators": [],
          "superclasses": [
            "ForestRegressor"
          ],
          "methods": [],
          "fullDocstring": "An extra-trees regressor.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"mse\", \"mae\"}, default=\"mse\"\n    The function to measure the quality of a split. Supported criteria\n    are \"mse\" for the mean squared error, which is equal to variance\n    reduction as feature selection criterion, and \"mae\" for the mean\n    absolute error.\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `round(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nbootstrap : bool, default=False\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate the R^2 on unseen data.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls 3 sources of randomness:\n\n    - the bootstrapping of the samples used when building trees\n      (if ``bootstrap=True``)\n    - the sampling of the features to consider when looking for the best\n      split at each node (if ``max_features < n_features``)\n    - the draw of the splits for each of the `max_features`\n\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nbase_estimator_ : ExtraTreeRegressor\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeRegressor\n    The collection of fitted sub-estimators.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_ : int\n    The number of features.\n\nn_outputs_ : int\n    The number of outputs.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_prediction_ : ndarray of shape (n_samples,)\n    Prediction computed with out-of-bag estimate on the training set.\n    This attribute exists only when ``oob_score`` is True.\n\nSee Also\n--------\nsklearn.tree.ExtraTreeRegressor : Base estimator for this ensemble.\nRandomForestRegressor : Ensemble regressor using trees with optimal splits.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.ensemble import ExtraTreesRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n...    X_train, y_train)\n>>> reg.score(X_test, y_test)\n0.2708..."
        },
        {
          "name": "ForestClassifier",
          "decorators": [],
          "superclasses": [
            "BaseForest",
            "ClassifierMixin"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, its dtype will be converted to\n``dtype=np.float32``. If a sparse matrix is provided, it will be\nconverted into a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted classes."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class for X.\n\nThe predicted class of an input sample is a vote by the trees in\nthe forest, weighted by their probability estimates. That is,\nthe predicted class is the one with highest mean probability\nestimate across the trees.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, its dtype will be converted to\n    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n    converted into a sparse ``csr_matrix``.\n\nReturns\n-------\ny : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n    The predicted classes."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, its dtype will be converted to\n``dtype=np.float32``. If a sparse matrix is provided, it will be\nconverted into a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "such arrays if n_outputs > 1.\nThe class probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample are computed as\nthe mean predicted class probabilities of the trees in the forest.\nThe class probability of a single tree is the fraction of samples of\nthe same class in a leaf.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, its dtype will be converted to\n    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n    converted into a sparse ``csr_matrix``.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n    such arrays if n_outputs > 1.\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, its dtype will be converted to\n``dtype=np.float32``. If a sparse matrix is provided, it will be\nconverted into a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "such arrays if n_outputs > 1.\nThe class probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe log of the mean predicted class probabilities of the trees in the\nforest.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, its dtype will be converted to\n    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n    converted into a sparse ``csr_matrix``.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n    such arrays if n_outputs > 1.\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            }
          ],
          "fullDocstring": "Base class for forest of trees-based classifiers.\n\nWarning: This class should not be used directly. Use derived classes\ninstead."
        },
        {
          "name": "ForestRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseForest"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, its dtype will be converted to\n``dtype=np.float32``. If a sparse matrix is provided, it will be\nconverted into a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the trees in the forest.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, its dtype will be converted to\n    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n    converted into a sparse ``csr_matrix``.\n\nReturns\n-------\ny : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n    The predicted values."
            }
          ],
          "fullDocstring": "Base class for forest of trees-based regressors.\n\nWarning: This class should not be used directly. Use derived classes\ninstead."
        },
        {
          "name": "RandomForestClassifier",
          "decorators": [],
          "superclasses": [
            "ForestClassifier"
          ],
          "methods": [],
          "fullDocstring": "A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"gini\", \"entropy\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n    Note: this parameter is tree-specific.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `round(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=sqrt(n_features)`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate\n    the generalization accuracy.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nbase_estimator_ : DecisionTreeClassifier\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\n\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nSee Also\n--------\nDecisionTreeClassifier, ExtraTreesClassifier\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\n\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n>>> clf.fit(X, y)\nRandomForestClassifier(...)\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]"
        },
        {
          "name": "RandomForestRegressor",
          "decorators": [],
          "superclasses": [
            "ForestRegressor"
          ],
          "methods": [],
          "fullDocstring": "A random forest regressor.\n\nA random forest is a meta estimator that fits a number of classifying\ndecision trees on various sub-samples of the dataset and uses averaging\nto improve the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"mse\", \"mae\"}, default=\"mse\"\n    The function to measure the quality of a split. Supported criteria\n    are \"mse\" for the mean squared error, which is equal to variance\n    reduction as feature selection criterion, and \"mae\" for the mean\n    absolute error.\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `round(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool, default=False\n    whether to use out-of-bag samples to estimate\n    the R^2 on unseen data.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nbase_estimator_ : DecisionTreeRegressor\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeRegressor\n    The collection of fitted sub-estimators.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_prediction_ : ndarray of shape (n_samples,)\n    Prediction computed with out-of-bag estimate on the training set.\n    This attribute exists only when ``oob_score`` is True.\n\nSee Also\n--------\nDecisionTreeRegressor, ExtraTreesRegressor\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\n\nThe default value ``max_features=\"auto\"`` uses ``n_features``\nrather than ``n_features / 3``. The latter was originally suggested in\n[1], whereas the former was more recently justified empirically in [2].\n\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n.. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n       trees\", Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=4, n_informative=2,\n...                        random_state=0, shuffle=False)\n>>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n>>> regr.fit(X, y)\nRandomForestRegressor(...)\n>>> print(regr.predict([[0, 0, 0, 0]]))\n[-8.32987858]"
        },
        {
          "name": "RandomTreesEmbedding",
          "decorators": [],
          "superclasses": [
            "BaseForest"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Use ``dtype=np.float32`` for maximum\nefficiency. Sparse matrices are also supported, use sparse\n``csc_matrix`` for maximum efficiency."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Use ``dtype=np.float32`` for maximum\n    efficiency. Sparse matrices are also supported, use sparse\n    ``csc_matrix`` for maximum efficiency.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node. In the case of\n    classification, splits are also ignored if they would result in any\n    single class carrying a negative weight in either child node.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data used to build forests. Use ``dtype=np.float32`` for\nmaximum efficiency."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node."
                }
              ],
              "results": [
                {
                  "name": "X_transformed",
                  "type": null,
                  "description": "Transformed dataset."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit estimator and transform dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data used to build forests. Use ``dtype=np.float32`` for\n    maximum efficiency.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node. In the case of\n    classification, splits are also ignored if they would result in any\n    single class carrying a negative weight in either child node.\n\nReturns\n-------\nX_transformed : sparse matrix of shape (n_samples, n_out)\n    Transformed dataset."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data to be transformed. Use ``dtype=np.float32`` for maximum\nefficiency. Sparse matrices are also supported, use sparse\n``csr_matrix`` for maximum efficiency."
                }
              ],
              "results": [
                {
                  "name": "X_transformed",
                  "type": null,
                  "description": "Transformed dataset."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data to be transformed. Use ``dtype=np.float32`` for maximum\n    efficiency. Sparse matrices are also supported, use sparse\n    ``csr_matrix`` for maximum efficiency.\n\nReturns\n-------\nX_transformed : sparse matrix of shape (n_samples, n_out)\n    Transformed dataset."
            }
          ],
          "fullDocstring": "An ensemble of totally random trees.\n\nAn unsupervised transformation of a dataset to a high-dimensional\nsparse representation. A datapoint is coded according to which leaf of\neach tree it is sorted into. Using a one-hot encoding of the leaves,\nthis leads to a binary coding with as many ones as there are trees in\nthe forest.\n\nThe dimensionality of the resulting representation is\n``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\nthe number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n\nRead more in the :ref:`User Guide <random_trees_embedding>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    Number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\nmax_depth : int, default=5\n    The maximum depth of each tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` is the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` is the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nsparse_output : bool, default=True\n    Whether or not to return a sparse CSR matrix, as default behavior,\n    or to return a dense array compatible with dense pipeline operators.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the generation of the random `y` used to fit the trees\n    and the draw of the splits for each feature at the trees' nodes.\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nAttributes\n----------\nbase_estimator_ : DecisionTreeClassifier instance\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeClassifier instances\n    The collection of fitted sub-estimators.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The feature importances (the higher, the more important the feature).\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\none_hot_encoder_ : OneHotEncoder instance\n    One-hot encoder used to create the sparse embedding.\n\nReferences\n----------\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n.. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n       visual codebooks using randomized clustering forests\"\n       NIPS 2007\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomTreesEmbedding\n>>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]\n>>> random_trees = RandomTreesEmbedding(\n...    n_estimators=5, random_state=0, max_depth=1).fit(X)\n>>> X_sparse_embedding = random_trees.transform(X)\n>>> X_sparse_embedding.toarray()\narray([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n       [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n       [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._gb",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csc_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csr_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.ensemble",
          "declaration": "_gb_losses",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "BaseEnsemble",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._gradient_boosting",
          "declaration": "_random_sample_mask",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._gradient_boosting",
          "declaration": "predict_stage",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._gradient_boosting",
          "declaration": "predict_stages",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "train_test_split",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "DecisionTreeRegressor",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "DOUBLE",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "time",
          "declaration": "time",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseGradientBoosting",
          "decorators": [],
          "superclasses": [
            "BaseEnsemble"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values (strings or integers in classification, real numbers\nin regression)\nFor classification, labels must correspond to classes."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node."
                },
                {
                  "name": "monitor",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The monitor is called after each iteration with the current\niteration, a reference to the estimator and the local variables of\n``_fit_stages`` as keyword arguments ``callable(i, self,\nlocals())``. If the callable returns ``True`` the fitting procedure\nis stopped. The monitor can be used for various things such as\ncomputing held-out estimates, early stopping, model introspect, and\nsnapshoting."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the gradient boosting model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\ny : array-like of shape (n_samples,)\n    Target values (strings or integers in classification, real numbers\n    in regression)\n    For classification, labels must correspond to classes.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node. In the case of\n    classification, splits are also ignored if they would result in any\n    single class carrying a negative weight in either child node.\n\nmonitor : callable, default=None\n    The monitor is called after each iteration with the current\n    iteration, a reference to the estimator and the local variables of\n    ``_fit_stages`` as keyword arguments ``callable(i, self,\n    locals())``. If the callable returns ``True`` the fitting procedure\n    is stopped. The monitor can be used for various things such as\n    computing held-out estimates, early stopping, model introspect, and\n    snapshoting.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "feature_importances_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "feature_importances_",
                  "type": null,
                  "description": "The values of this array sum to 1, unless all trees are single node\ntrees consisting of only the root node, in which case it will be an\narray of zeros."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "The impurity-based feature importances.\n\nThe higher, the more important the feature.\nThe importance of a feature is computed as the (normalized)\ntotal reduction of the criterion brought by that feature.  It is also\nknown as the Gini importance.\n\nWarning: impurity-based feature importances can be misleading for\nhigh cardinality features (many unique values). See\n:func:`sklearn.inspection.permutation_importance` as an alternative.\n\nReturns\n-------\nfeature_importances_ : ndarray of shape (n_features,)\n    The values of this array sum to 1, unless all trees are single node\n    trees consisting of only the root node, in which case it will be an\n    array of zeros."
            },
            {
              "name": "apply",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, its dtype will be converted to\n``dtype=np.float32``. If a sparse matrix is provided, it will\nbe converted to a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "X_leaves",
                  "type": null,
                  "description": "For each datapoint x in X and for each tree in the ensemble,\nreturn the index of the leaf x ends up in each estimator.\nIn the case of binary classification n_classes is 1."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply trees in the ensemble to X, return leaf indices.\n\n.. versionadded:: 0.17\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, its dtype will be converted to\n    ``dtype=np.float32``. If a sparse matrix is provided, it will\n    be converted to a sparse ``csr_matrix``.\n\nReturns\n-------\nX_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n    For each datapoint x in X and for each tree in the ensemble,\n    return the index of the leaf x ends up in each estimator.\n    In the case of binary classification n_classes is 1."
            }
          ],
          "fullDocstring": "Abstract base class for Gradient Boosting."
        },
        {
          "name": "GradientBoostingClassifier",
          "decorators": [],
          "superclasses": [
            "BaseGradientBoosting",
            "ClassifierMixin"
          ],
          "methods": [
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": null,
                  "description": "The decision function of the input samples, which corresponds to\nthe raw values predicted from the trees of the ensemble . The\norder of the classes corresponds to that in the attribute\n:term:`classes_`. Regression and binary classification produce an\narray of shape (n_samples,)."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the decision function of ``X``.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nscore : ndarray of shape (n_samples, n_classes) or (n_samples,)\n    The decision function of the input samples, which corresponds to\n    the raw values predicted from the trees of the ensemble . The\n    order of the classes corresponds to that in the attribute\n    :term:`classes_`. Regression and binary classification produce an\n    array of shape (n_samples,)."
            },
            {
              "name": "staged_decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": null,
                  "description": "The decision function of the input samples, which corresponds to\nthe raw values predicted from the trees of the ensemble . The\nclasses corresponds to that in the attribute :term:`classes_`.\nRegression and binary classification are special cases with\n``k == 1``, otherwise ``k==n_classes``."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute decision function of ``X`` for each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nscore : generator of ndarray of shape (n_samples, k)\n    The decision function of the input samples, which corresponds to\n    the raw values predicted from the trees of the ensemble . The\n    classes corresponds to that in the attribute :term:`classes_`.\n    Regression and binary classification are special cases with\n    ``k == 1``, otherwise ``k==n_classes``."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted values."
            },
            {
              "name": "staged_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted value of the input samples."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted value of the input samples."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nRaises\n------\nAttributeError\n    If the ``loss`` does not support probabilities.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class log-probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class log-probabilities for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nRaises\n------\nAttributeError\n    If the ``loss`` does not support probabilities.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class log-probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            },
            {
              "name": "staged_predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted value of the input samples."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted value of the input samples."
            }
          ],
          "fullDocstring": "Gradient Boosting for classification.\n\nGB builds an additive model in a\nforward stage-wise fashion; it allows for the optimization of\narbitrary differentiable loss functions. In each stage ``n_classes_``\nregression trees are fit on the negative gradient of the\nbinomial or multinomial deviance loss function. Binary classification\nis a special case where only a single regression tree is induced.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.\n\nParameters\n----------\nloss : {'deviance', 'exponential'}, default='deviance'\n    The loss function to be optimized. 'deviance' refers to\n    deviance (= logistic regression) for classification\n    with probabilistic outputs. For loss 'exponential' gradient\n    boosting recovers the AdaBoost algorithm.\n\nlearning_rate : float, default=0.1\n    Learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n\nn_estimators : int, default=100\n    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n\nsubsample : float, default=1.0\n    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n\ncriterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n    The function to measure the quality of a split. Supported criteria\n    are 'friedman_mse' for the mean squared error with improvement\n    score by Friedman, 'mse' for mean squared error, and 'mae' for\n    the mean absolute error. The default value of 'friedman_mse' is\n    generally the best as it can provide a better approximation in\n    some cases.\n\n    .. versionadded:: 0.18\n    .. deprecated:: 0.24\n        `criterion='mae'` is deprecated and will be removed in version\n        1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'mse'`\n        instead, as trees should use a least-square criterion in\n        Gradient Boosting.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_depth : int, default=3\n    The maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\ninit : estimator or 'zero', default=None\n    An estimator object that is used to compute the initial predictions.\n    ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n    'zero', the initial raw predictions are set to zero. By default, a\n    ``DummyEstimator`` predicting the classes priors is used.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given to each Tree estimator at each\n    boosting iteration.\n    In addition, it controls the random permutation of the features at\n    each split (see Notes for more details).\n    It also controls the random spliting of the training data to obtain a\n    validation set if `n_iter_no_change` is not None.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If 'auto', then `max_features=sqrt(n_features)`.\n    - If 'sqrt', then `max_features=sqrt(n_features)`.\n    - If 'log2', then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Choosing `max_features < n_features` leads to a reduction of variance\n    and an increase in bias.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints progress and performance\n    once in a while (the more trees the lower the frequency). If greater\n    than 1 then it prints progress and performance for every tree.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if ``n_iter_no_change`` is set to an integer.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=None\n    ``n_iter_no_change`` is used to decide if early stopping will be used\n    to terminate training when validation score is not improving. By\n    default it is set to None to disable early stopping. If set to a\n    number, it will set aside ``validation_fraction`` size of the training\n    data as validation and terminate training when validation score is not\n    improving in all of the previous ``n_iter_no_change`` numbers of\n    iterations. The split is stratified.\n\n    .. versionadded:: 0.20\n\ntol : float, default=1e-4\n    Tolerance for the early stopping. When the loss is not improving\n    by at least tol for ``n_iter_no_change`` iterations (if set to a\n    number), the training stops.\n\n    .. versionadded:: 0.20\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nn_estimators_ : int\n    The number of estimators as selected by early stopping (if\n    ``n_iter_no_change`` is specified). Otherwise it is set to\n    ``n_estimators``.\n\n    .. versionadded:: 0.20\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_improvement_ : ndarray of shape (n_estimators,)\n    The improvement in loss (= deviance) on the out-of-bag samples\n    relative to the previous iteration.\n    ``oob_improvement_[0]`` is the improvement in\n    loss of the first stage over the ``init`` estimator.\n    Only available if ``subsample < 1.0``\n\ntrain_score_ : ndarray of shape (n_estimators,)\n    The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n    model at iteration ``i`` on the in-bag sample.\n    If ``subsample == 1`` this is the deviance on the training data.\n\nloss_ : LossFunction\n    The concrete ``LossFunction`` object.\n\ninit_ : estimator\n    The estimator that provides the initial predictions.\n    Set via the ``init`` argument or ``loss.init_estimator``.\n\nestimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, ``loss_.K``)\n    The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n    classification, otherwise n_classes.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_features_ : int\n    The number of data features.\n\nn_classes_ : int\n    The number of classes.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nSee Also\n--------\nHistGradientBoostingClassifier : Histogram-based Gradient Boosting\n    Classification Tree.\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nRandomForestClassifier : A meta-estimator that fits a number of decision\n    tree classifiers on various sub-samples of the dataset and uses\n    averaging to improve the predictive accuracy and control over-fitting.\nAdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n    on the original dataset and then fits additional copies of the\n    classifier on the same dataset where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers\n    focus more on difficult cases.\n\nNotes\n-----\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data and\n``max_features=n_features``, if the improvement of the criterion is\nidentical for several splits enumerated during the search of the best\nsplit. To obtain a deterministic behaviour during fitting,\n``random_state`` has to be fixed.\n\nReferences\n----------\nJ. Friedman, Greedy Function Approximation: A Gradient Boosting\nMachine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\nJ. Friedman, Stochastic Gradient Boosting, 1999\n\nT. Hastie, R. Tibshirani and J. Friedman.\nElements of Statistical Learning Ed. 2, Springer, 2009.\n\nExamples\n--------\nThe following example shows how to fit a gradient boosting classifier with\n100 decision stumps as weak learners.\n\n>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> X_train, X_test = X[:2000], X[2000:]\n>>> y_train, y_test = y[:2000], y[2000:]\n\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)\n0.913..."
        },
        {
          "name": "GradientBoostingRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseGradientBoosting"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict regression target for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted values."
            },
            {
              "name": "staged_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted value of the input samples."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict regression target at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted value of the input samples."
            },
            {
              "name": "apply",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, its dtype will be converted to\n``dtype=np.float32``. If a sparse matrix is provided, it will\nbe converted to a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "X_leaves",
                  "type": null,
                  "description": "For each datapoint x in X and for each tree in the ensemble,\nreturn the index of the leaf x ends up in each estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply trees in the ensemble to X, return leaf indices.\n\n.. versionadded:: 0.17\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, its dtype will be converted to\n    ``dtype=np.float32``. If a sparse matrix is provided, it will\n    be converted to a sparse ``csr_matrix``.\n\nReturns\n-------\nX_leaves : array-like of shape (n_samples, n_estimators)\n    For each datapoint x in X and for each tree in the ensemble,\n    return the index of the leaf x ends up in each estimator."
            },
            {
              "name": "n_classes_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Gradient Boosting for regression.\n\nGB builds an additive model in a forward stage-wise fashion;\nit allows for the optimization of arbitrary differentiable loss functions.\nIn each stage a regression tree is fit on the negative gradient of the\ngiven loss function.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.\n\nParameters\n----------\nloss : {'ls', 'lad', 'huber', 'quantile'}, default='ls'\n    Loss function to be optimized. 'ls' refers to least squares\n    regression. 'lad' (least absolute deviation) is a highly robust\n    loss function solely based on order information of the input\n    variables. 'huber' is a combination of the two. 'quantile'\n    allows quantile regression (use `alpha` to specify the quantile).\n\nlearning_rate : float, default=0.1\n    Learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n\nn_estimators : int, default=100\n    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n\nsubsample : float, default=1.0\n    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n\ncriterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n    The function to measure the quality of a split. Supported criteria\n    are \"friedman_mse\" for the mean squared error with improvement\n    score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n    the mean absolute error. The default value of \"friedman_mse\" is\n    generally the best as it can provide a better approximation in\n    some cases.\n\n    .. versionadded:: 0.18\n    .. deprecated:: 0.24\n        `criterion='mae'` is deprecated and will be removed in version\n        1.1 (renaming of 0.26). The correct way of minimizing the absolute\n        error is to use `loss='lad'` instead.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_depth : int, default=3\n    Maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\ninit : estimator or 'zero', default=None\n    An estimator object that is used to compute the initial predictions.\n    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n    initial raw predictions are set to zero. By default a\n    ``DummyEstimator`` is used, predicting either the average target value\n    (for loss='ls'), or a quantile for the other losses.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given to each Tree estimator at each\n    boosting iteration.\n    In addition, it controls the random permutation of the features at\n    each split (see Notes for more details).\n    It also controls the random spliting of the training data to obtain a\n    validation set if `n_iter_no_change` is not None.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Choosing `max_features < n_features` leads to a reduction of variance\n    and an increase in bias.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nalpha : float, default=0.9\n    The alpha-quantile of the huber loss function and the quantile\n    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints progress and performance\n    once in a while (the more trees the lower the frequency). If greater\n    than 1 then it prints progress and performance for every tree.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if ``n_iter_no_change`` is set to an integer.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=None\n    ``n_iter_no_change`` is used to decide if early stopping will be used\n    to terminate training when validation score is not improving. By\n    default it is set to None to disable early stopping. If set to a\n    number, it will set aside ``validation_fraction`` size of the training\n    data as validation and terminate training when validation score is not\n    improving in all of the previous ``n_iter_no_change`` numbers of\n    iterations.\n\n    .. versionadded:: 0.20\n\ntol : float, default=1e-4\n    Tolerance for the early stopping. When the loss is not improving\n    by at least tol for ``n_iter_no_change`` iterations (if set to a\n    number), the training stops.\n\n    .. versionadded:: 0.20\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_improvement_ : ndarray of shape (n_estimators,)\n    The improvement in loss (= deviance) on the out-of-bag samples\n    relative to the previous iteration.\n    ``oob_improvement_[0]`` is the improvement in\n    loss of the first stage over the ``init`` estimator.\n    Only available if ``subsample < 1.0``\n\ntrain_score_ : ndarray of shape (n_estimators,)\n    The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n    model at iteration ``i`` on the in-bag sample.\n    If ``subsample == 1`` this is the deviance on the training data.\n\nloss_ : LossFunction\n    The concrete ``LossFunction`` object.\n\ninit_ : estimator\n    The estimator that provides the initial predictions.\n    Set via the ``init`` argument or ``loss.init_estimator``.\n\nestimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n    The collection of fitted sub-estimators.\n\nn_classes_ : int\n    The number of classes, set to 1 for regressors.\n\n    .. deprecated:: 0.24\n        Attribute ``n_classes_`` was deprecated in version 0.24 and\n        will be removed in 1.1 (renaming of 0.26).\n\nn_estimators_ : int\n    The number of estimators as selected by early stopping (if\n    ``n_iter_no_change`` is specified). Otherwise it is set to\n    ``n_estimators``.\n\nn_features_ : int\n    The number of data features.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nSee Also\n--------\nHistGradientBoostingRegressor : Histogram-based Gradient Boosting\n    Classification Tree.\nsklearn.tree.DecisionTreeRegressor : A decision tree regressor.\nsklearn.tree.RandomForestRegressor : A random forest regressor.\n\nNotes\n-----\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data and\n``max_features=n_features``, if the improvement of the criterion is\nidentical for several splits enumerated during the search of the best\nsplit. To obtain a deterministic behaviour during fitting,\n``random_state`` has to be fixed.\n\nExamples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.ensemble import GradientBoostingRegressor\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = make_regression(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> reg = GradientBoostingRegressor(random_state=0)\n>>> reg.fit(X_train, y_train)\nGradientBoostingRegressor(random_state=0)\n>>> reg.predict(X_test[1:2])\narray([-61...])\n>>> reg.score(X_test, y_test)\n0.4...\n\nReferences\n----------\nJ. Friedman, Greedy Function Approximation: A Gradient Boosting\nMachine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\nJ. Friedman, Stochastic Gradient Boosting, 1999\n\nT. Hastie, R. Tibshirani and J. Friedman.\nElements of Statistical Learning Ed. 2, Springer, 2009."
        },
        {
          "name": "VerboseReporter",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "init",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "est",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The estimator"
                },
                {
                  "name": "begin_at_stage",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "0",
                  "limitation": null,
                  "ignored": false,
                  "description": "stage at which to begin reporting"
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Initialize reporter\n\nParameters\n----------\nest : Estimator\n    The estimator\n\nbegin_at_stage : int, default=0\n    stage at which to begin reporting"
            },
            {
              "name": "update",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "j",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The new iteration."
                },
                {
                  "name": "est",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The estimator."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Update reporter with new iteration.\n\nParameters\n----------\nj : int\n    The new iteration.\nest : Estimator\n    The estimator."
            }
          ],
          "fullDocstring": "Reports verbose output to stdout.\n\nParameters\n----------\nverbose : int\n    Verbosity level. If ``verbose==1`` output is printed once in a while\n    (when iteration mod verbose_mod is zero).; if larger than 1 then output\n    is printed for each update."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._gb_losses",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "logsumexp",
          "alias": null
        },
        {
          "module": "sklearn.dummy",
          "declaration": "DummyClassifier",
          "alias": null
        },
        {
          "module": "sklearn.dummy",
          "declaration": "DummyRegressor",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "TREE_LEAF",
          "alias": null
        },
        {
          "module": "sklearn.utils.stats",
          "declaration": "_weighted_percentile",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BinomialDeviance",
          "decorators": [],
          "superclasses": [
            "ClassificationLossFunction"
          ],
          "methods": [
            {
              "name": "init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "negative_gradient",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True labels."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute half of the negative gradient.\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    True labels.\n\nraw_predictions : ndarray of shape (n_samples, K)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``."
            },
            {
              "name": "get_init_raw_predictions",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "estimator",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Binomial deviance loss function for binary classification.\n\nBinary classification is a special case; here, we only need to\nfit one tree instead of ``n_classes`` trees.\n\nParameters\n----------\nn_classes : int\n    Number of classes."
        },
        {
          "name": "ClassificationLossFunction",
          "decorators": [],
          "superclasses": [
            "LossFunction"
          ],
          "methods": [
            {
              "name": "check_init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "estimator",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The init estimator to check."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Make sure estimator has fit and predict_proba methods.\n\nParameters\n----------\nestimator : object\n    The init estimator to check."
            }
          ],
          "fullDocstring": "Base class for classification loss functions. "
        },
        {
          "name": "ExponentialLoss",
          "decorators": [],
          "superclasses": [
            "ClassificationLossFunction"
          ],
          "methods": [
            {
              "name": "init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "negative_gradient",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True labels."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the residual (= negative gradient).\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    True labels.\n\nraw_predictions : ndarray of shape (n_samples, K)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``."
            },
            {
              "name": "get_init_raw_predictions",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "estimator",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Exponential loss function for binary classification.\n\nSame loss as AdaBoost.\n\nParameters\n----------\nn_classes : int\n    Number of classes.\n\nReferences\n----------\nGreg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007"
        },
        {
          "name": "HuberLossFunction",
          "decorators": [],
          "superclasses": [
            "RegressionLossFunction"
          ],
          "methods": [
            {
              "name": "init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "negative_gradient",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target labels."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the negative gradient.\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    The target labels.\n\nraw_predictions : ndarray of shape (n_samples, K)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``.\n\nsample_weight : ndarray of shape (n_samples,), default=None\n    Sample weights."
            }
          ],
          "fullDocstring": "Huber loss function for robust regression.\n\nM-Regression proposed in Friedman 2001.\n\nParameters\n----------\nalpha : float, default=0.9\n    Percentile at which to extract score.\n\nReferences\n----------\nJ. Friedman, Greedy Function Approximation: A Gradient Boosting\nMachine, The Annals of Statistics, Vol. 29, No. 5, 2001."
        },
        {
          "name": "LeastAbsoluteError",
          "decorators": [],
          "superclasses": [
            "RegressionLossFunction"
          ],
          "methods": [
            {
              "name": "init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "negative_gradient",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target labels."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the negative gradient.\n\n1.0 if y - raw_predictions > 0.0 else -1.0\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    The target labels.\n\nraw_predictions : ndarray of shape (n_samples, K)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``."
            }
          ],
          "fullDocstring": "Loss function for least absolute deviation (LAD) regression.\n\nParameters\n----------\nn_classes : int\n    Number of classes"
        },
        {
          "name": "LeastSquaresError",
          "decorators": [],
          "superclasses": [
            "RegressionLossFunction"
          ],
          "methods": [
            {
              "name": "init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "negative_gradient",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target labels."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute half of the negative gradient.\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    The target labels.\n\nraw_predictions : ndarray of shape (n_samples,)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``."
            },
            {
              "name": "update_terminal_regions",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "tree",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The tree object."
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data array."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target labels."
                },
                {
                  "name": "residual",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The residuals (usually the negative gradient)."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weight of each sample."
                },
                {
                  "name": "sample_mask",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The sample mask to be used."
                },
                {
                  "name": "learning_rate",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "0.1",
                  "limitation": null,
                  "ignored": false,
                  "description": "Learning rate shrinks the contribution of each tree by\n ``learning_rate``."
                },
                {
                  "name": "k",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "0",
                  "limitation": null,
                  "ignored": false,
                  "description": "The index of the estimator being updated."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Least squares does not need to update terminal regions.\n\nBut it has to update the predictions.\n\nParameters\n----------\ntree : tree.Tree\n    The tree object.\nX : ndarray of shape (n_samples, n_features)\n    The data array.\ny : ndarray of shape (n_samples,)\n    The target labels.\nresidual : ndarray of shape (n_samples,)\n    The residuals (usually the negative gradient).\nraw_predictions : ndarray of shape (n_samples, K)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``.\nsample_weight : ndarray of shape (n,)\n    The weight of each sample.\nsample_mask : ndarray of shape (n,)\n    The sample mask to be used.\nlearning_rate : float, default=0.1\n    Learning rate shrinks the contribution of each tree by\n     ``learning_rate``.\nk : int, default=0\n    The index of the estimator being updated."
            }
          ],
          "fullDocstring": "Loss function for least squares (LS) estimation.\nTerminal regions do not need to be updated for least squares.\n\nParameters\n----------\nn_classes : int\n    Number of classes."
        },
        {
          "name": "LossFunction",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Default ``init`` estimator for loss function. "
            },
            {
              "name": "negative_gradient",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target labels."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the negative gradient.\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    The target labels.\n\nraw_predictions : ndarray of shape (n_samples, K)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``."
            },
            {
              "name": "update_terminal_regions",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "tree",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The tree object."
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data array."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target labels."
                },
                {
                  "name": "residual",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The residuals (usually the negative gradient)."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The weight of each sample."
                },
                {
                  "name": "sample_mask",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The sample mask to be used."
                },
                {
                  "name": "learning_rate",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "0.1",
                  "limitation": null,
                  "ignored": false,
                  "description": "Learning rate shrinks the contribution of each tree by\n ``learning_rate``."
                },
                {
                  "name": "k",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "0",
                  "limitation": null,
                  "ignored": false,
                  "description": "The index of the estimator being updated."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n\nParameters\n----------\ntree : tree.Tree\n    The tree object.\nX : ndarray of shape (n_samples, n_features)\n    The data array.\ny : ndarray of shape (n_samples,)\n    The target labels.\nresidual : ndarray of shape (n_samples,)\n    The residuals (usually the negative gradient).\nraw_predictions : ndarray of shape (n_samples, K)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``.\nsample_weight : ndarray of shape (n_samples,)\n    The weight of each sample.\nsample_mask : ndarray of shape (n_samples,)\n    The sample mask to be used.\nlearning_rate : float, default=0.1\n    Learning rate shrinks the contribution of each tree by\n     ``learning_rate``.\nk : int, default=0\n    The index of the estimator being updated."
            },
            {
              "name": "get_init_raw_predictions",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data array."
                },
                {
                  "name": "estimator",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The estimator to use to compute the predictions."
                }
              ],
              "results": [
                {
                  "name": "raw_predictions",
                  "type": null,
                  "description": "The initial raw predictions. K is equal to 1 for binary\nclassification and regression, and equal to the number of classes\nfor multiclass classification. ``raw_predictions`` is casted\ninto float64."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the initial raw predictions.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    The data array.\nestimator : object\n    The estimator to use to compute the predictions.\n\nReturns\n-------\nraw_predictions : ndarray of shape (n_samples, K)\n    The initial raw predictions. K is equal to 1 for binary\n    classification and regression, and equal to the number of classes\n    for multiclass classification. ``raw_predictions`` is casted\n    into float64."
            }
          ],
          "fullDocstring": "Abstract base class for various loss functions.\n\nParameters\n----------\nn_classes : int\n    Number of classes.\n\nAttributes\n----------\nK : int\n    The number of regression trees to be induced;\n    1 for regression and binary classification;\n    ``n_classes`` for multi-class classification."
        },
        {
          "name": "MultinomialDeviance",
          "decorators": [],
          "superclasses": [
            "ClassificationLossFunction"
          ],
          "methods": [
            {
              "name": "init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "negative_gradient",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target labels."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                },
                {
                  "name": "k",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "0",
                  "limitation": null,
                  "ignored": false,
                  "description": "The index of the class."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute negative gradient for the ``k``-th class.\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    The target labels.\n\nraw_predictions : ndarray of shape (n_samples, K)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``.\n\nk : int, default=0\n    The index of the class."
            },
            {
              "name": "get_init_raw_predictions",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "estimator",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Multinomial deviance loss function for multi-class classification.\n\nFor multi-class classification we need to fit ``n_classes`` trees at\neach stage.\n\nParameters\n----------\nn_classes : int\n    Number of classes."
        },
        {
          "name": "QuantileLossFunction",
          "decorators": [],
          "superclasses": [
            "RegressionLossFunction"
          ],
          "methods": [
            {
              "name": "init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "negative_gradient",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target labels."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw predictions (i.e. values from the tree leaves) of the\ntree ensemble at iteration ``i - 1``."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the negative gradient.\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    The target labels.\n\nraw_predictions : ndarray of shape (n_samples, K)\n    The raw predictions (i.e. values from the tree leaves) of the\n    tree ensemble at iteration ``i - 1``."
            }
          ],
          "fullDocstring": "Loss function for quantile regression.\n\nQuantile regression allows to estimate the percentiles\nof the conditional distribution of the target.\n\nParameters\n----------\nalpha : float, default=0.9\n    The percentile."
        },
        {
          "name": "RegressionLossFunction",
          "decorators": [],
          "superclasses": [
            "LossFunction"
          ],
          "methods": [
            {
              "name": "check_init_estimator",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "estimator",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The init estimator to check."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Make sure estimator has the required fit and predict methods.\n\nParameters\n----------\nestimator : object\n    The init estimator to check."
            },
            {
              "name": "get_init_raw_predictions",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "estimator",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Base class for regression loss functions."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._hist_gradient_boosting",
      "imports": [],
      "fromImports": [],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._hist_gradient_boosting.binning",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._binning",
          "declaration": "_map_to_bins",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._bitset",
          "declaration": "set_bitset_memoryview",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "ALMOST_INF",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "X_BINNED_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "X_BITSET_INNER_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "X_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABC",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._gradient_boosting",
          "declaration": "_update_raw_predictions",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.binning",
          "declaration": "_BinMapper",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "X_BINNED_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "X_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "Y_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.grower",
          "declaration": "TreeGrower",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.loss",
          "declaration": "BaseLoss",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.loss",
          "declaration": "_LOSSES",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "check_scoring",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "train_test_split",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "resample",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "timeit",
          "declaration": "default_timer",
          "alias": "time"
        }
      ],
      "classes": [
        {
          "name": "BaseHistGradientBoosting",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "ABC"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights of training data.\n\n.. versionadded:: 0.23"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the gradient boosting model.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,) default=None\n    Weights of training data.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\nself : object"
            },
            {
              "name": "n_iter_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Base class for histogram-based gradient boosting estimators."
        },
        {
          "name": "HistGradientBoostingClassifier",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "BaseHistGradientBoosting"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted classes."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict classes for X.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\ny : ndarray, shape (n_samples,)\n    The predicted classes."
            },
            {
              "name": "staged_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict classes at each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples.\n\nYields\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted classes of the input samples, for each iteration."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class probabilities of the input samples."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities for X.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\np : ndarray, shape (n_samples, n_classes)\n    The class probabilities of the input samples."
            },
            {
              "name": "staged_predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities at each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples.\n\nYields\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted class probabilities of the input samples,\n    for each iteration."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "decision",
                  "type": null,
                  "description": "The raw predicted values (i.e. the sum of the trees leaves) for\neach sample. n_trees_per_iteration is equal to the number of\nclasses in multiclass classification."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the decision function of ``X``.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\ndecision : ndarray, shape (n_samples,) or                 (n_samples, n_trees_per_iteration)\n    The raw predicted values (i.e. the sum of the trees leaves) for\n    each sample. n_trees_per_iteration is equal to the number of\n    classes in multiclass classification."
            },
            {
              "name": "staged_decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute decision function of ``X`` for each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples.\n\nYields\n-------\ndecision : generator of ndarray of shape (n_samples,) or                 (n_samples, n_trees_per_iteration)\n    The decision function of the input samples, which corresponds to\n    the raw values predicted from the trees of the ensemble . The\n    classes corresponds to that in the attribute :term:`classes_`."
            }
          ],
          "fullDocstring": "Histogram-based Gradient Boosting Classification Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_hist_gradient_boosting``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> # now you can import normally from ensemble\n    >>> from sklearn.ensemble import HistGradientBoostingClassifier\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\nloss : {'auto', 'binary_crossentropy', 'categorical_crossentropy'},             default='auto'\n    The loss function to use in the boosting process. 'binary_crossentropy'\n    (also known as logistic loss) is used for binary classification and\n    generalizes to 'categorical_crossentropy' for multiclass\n    classification. 'auto' will automatically choose either loss depending\n    on the nature of the problem.\nlearning_rate : float, default=0.1\n    The learning rate, also known as *shrinkage*. This is used as a\n    multiplicative factor for the leaves values. Use ``1`` for no\n    shrinkage.\nmax_iter : int, default=100\n    The maximum number of iterations of the boosting process, i.e. the\n    maximum number of trees for binary classification. For multiclass\n    classification, `n_classes` trees per iteration are built.\nmax_leaf_nodes : int or None, default=31\n    The maximum number of leaves for each tree. Must be strictly greater\n    than 1. If None, there is no maximum limit.\nmax_depth : int or None, default=None\n    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\nmin_samples_leaf : int, default=20\n    The minimum number of samples per leaf. For small datasets with less\n    than a few hundred samples, it is recommended to lower this value\n    since only very shallow trees would be built.\nl2_regularization : float, default=0\n    The L2 regularization parameter. Use 0 for no regularization.\nmax_bins : int, default=255\n    The maximum number of bins to use for non-missing values. Before\n    training, each feature of the input array `X` is binned into\n    integer-valued bins, which allows for a much faster training stage.\n    Features with a small number of unique values may use less than\n    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n    is always reserved for missing values. Must be no larger than 255.\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonic constraint to enforce on each feature. -1, 1\n    and 0 respectively correspond to a negative constraint, positive\n    constraint and no constraint. Read more in the :ref:`User Guide\n    <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 0.23\n\ncategorical_features : array-like of {bool, int} of shape (n_features)             or shape (n_categorical_features,), default=None.\n    Indicates the categorical features.\n\n    - None : no feature will be considered categorical.\n    - boolean array-like : boolean mask indicating categorical features.\n    - integer array-like : integer indices indicating categorical\n      features.\n\n    For each categorical feature, there must be at most `max_bins` unique\n    categories, and each categorical value must be in [0, max_bins -1].\n\n    Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n    .. versionadded:: 0.24\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble. For results to be valid, the\n    estimator should be re-trained on the same data only.\n    See :term:`the Glossary <warm_start>`.\nearly_stopping : 'auto' or bool, default='auto'\n    If 'auto', early stopping is enabled if the sample size is larger than\n    10000. If True, early stopping is enabled, otherwise early stopping is\n    disabled.\n\n    .. versionadded:: 0.23\n\nscoring : str or callable or None, default='loss'\n    Scoring parameter to use for early stopping. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer\n    is used. If ``scoring='loss'``, early stopping is checked\n    w.r.t the loss value. Only used if early stopping is performed.\nvalidation_fraction : int or float or None, default=0.1\n    Proportion (or absolute size) of training data to set aside as\n    validation data for early stopping. If None, early stopping is done on\n    the training data. Only used if early stopping is performed.\nn_iter_no_change : int, default=10\n    Used to determine when to \"early stop\". The fitting process is\n    stopped when none of the last ``n_iter_no_change`` scores are better\n    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n    tolerance. Only used if early stopping is performed.\ntol : float or None, default=1e-7\n    The absolute tolerance to use when comparing scores. The higher the\n    tolerance, the more likely we are to early stop: higher tolerance\n    means that it will be harder for subsequent iterations to be\n    considered an improvement upon the reference score.\nverbose : int, default=0\n    The verbosity level. If not zero, print some information about the\n    fitting process.\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the subsampling in the\n    binning process, and the train/validation data split if early stopping\n    is enabled.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nclasses_ : array, shape = (n_classes,)\n    Class labels.\ndo_early_stopping_ : bool\n    Indicates whether early stopping is used during training.\nn_iter_ : int\n    The number of iterations as selected by early stopping, depending on\n    the `early_stopping` parameter. Otherwise it corresponds to max_iter.\nn_trees_per_iteration_ : int\n    The number of tree that are built at each iteration. This is equal to 1\n    for binary classification, and to ``n_classes`` for multiclass\n    classification.\ntrain_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the training data. The first entry\n    is the score of the ensemble before the first iteration. Scores are\n    computed according to the ``scoring`` parameter. If ``scoring`` is\n    not 'loss', scores are computed on a subset of at most 10 000\n    samples. Empty if no early stopping.\nvalidation_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the held-out validation data. The\n    first entry is the score of the ensemble before the first iteration.\n    Scores are computed according to the ``scoring`` parameter. Empty if\n    no early stopping or if ``validation_fraction`` is None.\nis_categorical_ : ndarray, shape (n_features, ) or None\n    Boolean mask for the categorical features. ``None`` if there are no\n    categorical features.\n\nExamples\n--------\n>>> # To use this experimental feature, we need to explicitly ask for it:\n>>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = HistGradientBoostingClassifier().fit(X, y)\n>>> clf.score(X, y)\n1.0"
        },
        {
          "name": "HistGradientBoostingRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseHistGradientBoosting"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict values for X.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\ny : ndarray, shape (n_samples,)\n    The predicted values."
            },
            {
              "name": "staged_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict regression target for each iteration\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples.\n\nYields\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted values of the input samples, for each iteration."
            }
          ],
          "fullDocstring": "Histogram-based Gradient Boosting Regression Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_hist_gradient_boosting``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> # now you can import normally from ensemble\n    >>> from sklearn.ensemble import HistGradientBoostingRegressor\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\nloss : {'least_squares', 'least_absolute_deviation', 'poisson'},             default='least_squares'\n    The loss function to use in the boosting process. Note that the\n    \"least squares\" and \"poisson\" losses actually implement\n    \"half least squares loss\" and \"half poisson deviance\" to simplify the\n    computation of the gradient. Furthermore, \"poisson\" loss internally\n    uses a log-link and requires ``y >= 0``\n\n    .. versionchanged:: 0.23\n       Added option 'poisson'.\n\nlearning_rate : float, default=0.1\n    The learning rate, also known as *shrinkage*. This is used as a\n    multiplicative factor for the leaves values. Use ``1`` for no\n    shrinkage.\nmax_iter : int, default=100\n    The maximum number of iterations of the boosting process, i.e. the\n    maximum number of trees.\nmax_leaf_nodes : int or None, default=31\n    The maximum number of leaves for each tree. Must be strictly greater\n    than 1. If None, there is no maximum limit.\nmax_depth : int or None, default=None\n    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\nmin_samples_leaf : int, default=20\n    The minimum number of samples per leaf. For small datasets with less\n    than a few hundred samples, it is recommended to lower this value\n    since only very shallow trees would be built.\nl2_regularization : float, default=0\n    The L2 regularization parameter. Use ``0`` for no regularization\n    (default).\nmax_bins : int, default=255\n    The maximum number of bins to use for non-missing values. Before\n    training, each feature of the input array `X` is binned into\n    integer-valued bins, which allows for a much faster training stage.\n    Features with a small number of unique values may use less than\n    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n    is always reserved for missing values. Must be no larger than 255.\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonic constraint to enforce on each feature. -1, 1\n    and 0 respectively correspond to a negative constraint, positive\n    constraint and no constraint. Read more in the :ref:`User Guide\n    <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 0.23\n\ncategorical_features : array-like of {bool, int} of shape (n_features)             or shape (n_categorical_features,), default=None.\n    Indicates the categorical features.\n\n    - None : no feature will be considered categorical.\n    - boolean array-like : boolean mask indicating categorical features.\n    - integer array-like : integer indices indicating categorical\n      features.\n\n    For each categorical feature, there must be at most `max_bins` unique\n    categories, and each categorical value must be in [0, max_bins -1].\n\n    Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n    .. versionadded:: 0.24\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble. For results to be valid, the\n    estimator should be re-trained on the same data only.\n    See :term:`the Glossary <warm_start>`.\nearly_stopping : 'auto' or bool, default='auto'\n    If 'auto', early stopping is enabled if the sample size is larger than\n    10000. If True, early stopping is enabled, otherwise early stopping is\n    disabled.\n\n    .. versionadded:: 0.23\n\nscoring : str or callable or None, default='loss'\n    Scoring parameter to use for early stopping. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer is used. If\n    ``scoring='loss'``, early stopping is checked w.r.t the loss value.\n    Only used if early stopping is performed.\nvalidation_fraction : int or float or None, default=0.1\n    Proportion (or absolute size) of training data to set aside as\n    validation data for early stopping. If None, early stopping is done on\n    the training data. Only used if early stopping is performed.\nn_iter_no_change : int, default=10\n    Used to determine when to \"early stop\". The fitting process is\n    stopped when none of the last ``n_iter_no_change`` scores are better\n    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n    tolerance. Only used if early stopping is performed.\ntol : float or None, default=1e-7\n    The absolute tolerance to use when comparing scores during early\n    stopping. The higher the tolerance, the more likely we are to early\n    stop: higher tolerance means that it will be harder for subsequent\n    iterations to be considered an improvement upon the reference score.\nverbose : int, default=0\n    The verbosity level. If not zero, print some information about the\n    fitting process.\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the subsampling in the\n    binning process, and the train/validation data split if early stopping\n    is enabled.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ndo_early_stopping_ : bool\n    Indicates whether early stopping is used during training.\nn_iter_ : int\n    The number of iterations as selected by early stopping, depending on\n    the `early_stopping` parameter. Otherwise it corresponds to max_iter.\nn_trees_per_iteration_ : int\n    The number of tree that are built at each iteration. For regressors,\n    this is always 1.\ntrain_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the training data. The first entry\n    is the score of the ensemble before the first iteration. Scores are\n    computed according to the ``scoring`` parameter. If ``scoring`` is\n    not 'loss', scores are computed on a subset of at most 10 000\n    samples. Empty if no early stopping.\nvalidation_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the held-out validation data. The\n    first entry is the score of the ensemble before the first iteration.\n    Scores are computed according to the ``scoring`` parameter. Empty if\n    no early stopping or if ``validation_fraction`` is None.\nis_categorical_ : ndarray, shape (n_features, ) or None\n    Boolean mask for the categorical features. ``None`` if there are no\n    categorical features.\n\nExamples\n--------\n>>> # To use this experimental feature, we need to explicitly ask for it:\n>>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n>>> from sklearn.ensemble import HistGradientBoostingRegressor\n>>> from sklearn.datasets import load_diabetes\n>>> X, y = load_diabetes(return_X_y=True)\n>>> est = HistGradientBoostingRegressor().fit(X, y)\n>>> est.score(X, y)\n0.92..."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._hist_gradient_boosting.grower",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "heapq",
          "declaration": "heappop",
          "alias": null
        },
        {
          "module": "heapq",
          "declaration": "heappush",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._bitset",
          "declaration": "set_raw_bitset_from_binned_bitset",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "MonotonicConstraint",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "PREDICTOR_RECORD_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "X_BITSET_INNER_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "Y_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.histogram",
          "declaration": "HistogramBuilder",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.predictor",
          "declaration": "TreePredictor",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.splitting",
          "declaration": "Splitter",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.utils",
          "declaration": "sum_parallel",
          "alias": null
        },
        {
          "module": "timeit",
          "declaration": "default_timer",
          "alias": "time"
        }
      ],
      "classes": [
        {
          "name": "TreeGrower",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "grow",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Grow the tree, from root to leaves."
            },
            {
              "name": "split_next",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "left",
                  "type": null,
                  "description": "The resulting left child."
                },
                {
                  "name": "right",
                  "type": null,
                  "description": "The resulting right child."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Split the node with highest potential gain.\n\nReturns\n-------\nleft : TreeNode\n    The resulting left child.\nright : TreeNode\n    The resulting right child."
            },
            {
              "name": "make_predictor",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "binning_thresholds",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Corresponds to the bin_thresholds_ attribute of the BinMapper.\nFor each feature, this stores:\n\n- the bin frontiers for continuous features\n- the unique raw category values for categorical features"
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Make a TreePredictor object out of the current tree.\n\nParameters\n----------\nbinning_thresholds : array-like of floats\n    Corresponds to the bin_thresholds_ attribute of the BinMapper.\n    For each feature, this stores:\n\n    - the bin frontiers for continuous features\n    - the unique raw category values for categorical features\n\nReturns\n-------\nA TreePredictor object."
            }
          ],
          "fullDocstring": "Tree grower class used to build a tree.\n\nThe tree is fitted to predict the values of a Newton-Raphson step. The\nsplits are considered in a best-first fashion, and the quality of a\nsplit is defined in splitting._split_gain.\n\nParameters\n----------\nX_binned : ndarray of shape (n_samples, n_features), dtype=np.uint8\n    The binned input samples. Must be Fortran-aligned.\ngradients : ndarray of shape (n_samples,)\n    The gradients of each training sample. Those are the gradients of the\n    loss w.r.t the predictions, evaluated at iteration ``i - 1``.\nhessians : ndarray of shape (n_samples,)\n    The hessians of each training sample. Those are the hessians of the\n    loss w.r.t the predictions, evaluated at iteration ``i - 1``.\nmax_leaf_nodes : int, default=None\n    The maximum number of leaves for each tree. If None, there is no\n    maximum limit.\nmax_depth : int, default=None\n    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\nmin_samples_leaf : int, default=20\n    The minimum number of samples per leaf.\nmin_gain_to_split : float, default=0.\n    The minimum gain needed to split a node. Splits with lower gain will\n    be ignored.\nn_bins : int, default=256\n    The total number of bins, including the bin for missing values. Used\n    to define the shape of the histograms.\nn_bins_non_missing : ndarray, dtype=np.uint32, default=None\n    For each feature, gives the number of bins actually used for\n    non-missing values. For features with a lot of unique values, this\n    is equal to ``n_bins - 1``. If it's an int, all features are\n    considered to have the same number of bins. If None, all features\n    are considered to have ``n_bins - 1`` bins.\nhas_missing_values : bool or ndarray, dtype=bool, default=False\n    Whether each feature contains missing values (in the training data).\n    If it's a bool, the same value is used for all features.\nis_categorical : ndarray of bool of shape (n_features,), default=None\n    Indicates categorical features.\nmonotonic_cst : array-like of shape (n_features,), dtype=int, default=None\n    Indicates the monotonic constraint to enforce on each feature. -1, 1\n    and 0 respectively correspond to a positive constraint, negative\n    constraint and no constraint. Read more in the :ref:`User Guide\n    <monotonic_cst_gbdt>`.\nl2_regularization : float, default=0.\n    The L2 regularization parameter.\nmin_hessian_to_split : float, default=1e-3\n    The minimum sum of hessians needed in each node. Splits that result in\n    at least one child having a sum of hessians less than\n    ``min_hessian_to_split`` are discarded.\nshrinkage : float, default=1.\n    The shrinkage parameter to apply to the leaves values, also known as\n    learning rate."
        },
        {
          "name": "TreeNode",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "set_children_bounds",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "lower",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "upper",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Set children values bounds to respect monotonic constraints."
            }
          ],
          "fullDocstring": "Tree Node class used in TreeGrower.\n\nThis isn't used for prediction purposes, only for training (see\nTreePredictor).\n\nParameters\n----------\ndepth : int\n    The depth of the node, i.e. its distance from the root.\nsample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint\n    The indices of the samples at the node.\nsum_gradients : float\n    The sum of the gradients of the samples at the node.\nsum_hessians : float\n    The sum of the hessians of the samples at the node.\n\nAttributes\n----------\ndepth : int\n    The depth of the node, i.e. its distance from the root.\nsample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint\n    The indices of the samples at the node.\nsum_gradients : float\n    The sum of the gradients of the samples at the node.\nsum_hessians : float\n    The sum of the hessians of the samples at the node.\nsplit_info : SplitInfo or None\n    The result of the split evaluation.\nleft_child : TreeNode or None\n    The left child of the node. None for leaves.\nright_child : TreeNode or None\n    The right child of the node. None for leaves.\nvalue : float or None\n    The value of the leaf, as computed in finalize_leaf(). None for\n    non-leaf nodes.\npartition_start : int\n    start position of the node's sample_indices in splitter.partition.\npartition_stop : int\n    stop position of the node's sample_indices in splitter.partition."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._hist_gradient_boosting.loss",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABC",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "logsumexp",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "xlogy",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._loss",
          "declaration": "_update_gradients_hessians_binary_crossentropy",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._loss",
          "declaration": "_update_gradients_hessians_categorical_crossentropy",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._loss",
          "declaration": "_update_gradients_hessians_least_absolute_deviation",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._loss",
          "declaration": "_update_gradients_hessians_least_squares",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._loss",
          "declaration": "_update_gradients_hessians_poisson",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._loss",
          "declaration": "_update_gradients_least_absolute_deviation",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._loss",
          "declaration": "_update_gradients_least_squares",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "G_H_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "Y_DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.utils.stats",
          "declaration": "_weighted_percentile",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseLoss",
          "decorators": [],
          "superclasses": [
            "ABC"
          ],
          "methods": [
            {
              "name": "pointwise_loss",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return loss value for each input"
            },
            {
              "name": "init_gradients_and_hessians",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "n_samples",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The number of samples passed to `fit()`."
                },
                {
                  "name": "prediction_dim",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The dimension of a raw prediction, i.e. the number of trees\nbuilt at each iteration. Equals 1 for regression and binary\nclassification, or K where K is the number of classes for\nmulticlass classification."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights of training data."
                }
              ],
              "results": [
                {
                  "name": "gradients",
                  "type": null,
                  "description": "The initial gradients. The array is not initialized."
                },
                {
                  "name": "hessians",
                  "type": null,
                  "description": "If hessians are constant (e.g. for `LeastSquares` loss, the\narray is initialized to ``1``. Otherwise, the array is allocated\nwithout being initialized."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return initial gradients and hessians.\n\nUnless hessians are constant, arrays are initialized with undefined\nvalues.\n\nParameters\n----------\nn_samples : int\n    The number of samples passed to `fit()`.\n\nprediction_dim : int\n    The dimension of a raw prediction, i.e. the number of trees\n    built at each iteration. Equals 1 for regression and binary\n    classification, or K where K is the number of classes for\n    multiclass classification.\n\nsample_weight : array-like of shape(n_samples,) default=None\n    Weights of training data.\n\nReturns\n-------\ngradients : ndarray, shape (prediction_dim, n_samples)\n    The initial gradients. The array is not initialized.\nhessians : ndarray, shape (prediction_dim, n_samples)\n    If hessians are constant (e.g. for `LeastSquares` loss, the\n    array is initialized to ``1``. Otherwise, the array is allocated\n    without being initialized."
            },
            {
              "name": "get_baseline_prediction",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_train",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target training values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights of training data."
                },
                {
                  "name": "prediction_dim",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The dimension of one prediction: 1 for binary classification and\nregression, n_classes for multiclass classification."
                }
              ],
              "results": [
                {
                  "name": "baseline_prediction",
                  "type": null,
                  "description": "The baseline prediction."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return initial predictions (before the first iteration).\n\nParameters\n----------\ny_train : ndarray, shape (n_samples,)\n    The target training values.\n\nsample_weight : array-like of shape(n_samples,) default=None\n    Weights of training data.\n\nprediction_dim : int\n    The dimension of one prediction: 1 for binary classification and\n    regression, n_classes for multiclass classification.\n\nReturns\n-------\nbaseline_prediction : float or ndarray, shape (1, prediction_dim)\n    The baseline prediction."
            },
            {
              "name": "update_gradients_and_hessians",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "gradients",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The gradients (treated as OUT array)."
                },
                {
                  "name": "hessians",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The hessians (treated as OUT array)."
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The true target values or each training sample."
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The raw_predictions (i.e. values from the trees) of the tree\nensemble at iteration ``i - 1``."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights of training data."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Update gradients and hessians arrays, inplace.\n\nThe gradients (resp. hessians) are the first (resp. second) order\nderivatives of the loss for each sample with respect to the\npredictions of model, evaluated at iteration ``i - 1``.\n\nParameters\n----------\ngradients : ndarray, shape (prediction_dim, n_samples)\n    The gradients (treated as OUT array).\n\nhessians : ndarray, shape (prediction_dim, n_samples) or             (1,)\n    The hessians (treated as OUT array).\n\ny_true : ndarray, shape (n_samples,)\n    The true target values or each training sample.\n\nraw_predictions : ndarray, shape (prediction_dim, n_samples)\n    The raw_predictions (i.e. values from the trees) of the tree\n    ensemble at iteration ``i - 1``.\n\nsample_weight : array-like of shape(n_samples,) default=None\n    Weights of training data."
            }
          ],
          "fullDocstring": "Base class for a loss."
        },
        {
          "name": "BinaryCrossEntropy",
          "decorators": [],
          "superclasses": [
            "BaseLoss"
          ],
          "methods": [
            {
              "name": "pointwise_loss",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "get_baseline_prediction",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_train",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "prediction_dim",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "update_gradients_and_hessians",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "gradients",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "hessians",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Binary cross-entropy loss, for binary classification.\n\nFor a given sample x_i, the binary cross-entropy loss is defined as the\nnegative log-likelihood of the model which can be expressed as::\n\n    loss(x_i) = log(1 + exp(raw_pred_i)) - y_true_i * raw_pred_i\n\nSee The Elements of Statistical Learning, by Hastie, Tibshirani, Friedman,\nsection 4.4.1 (about logistic regression)."
        },
        {
          "name": "CategoricalCrossEntropy",
          "decorators": [],
          "superclasses": [
            "BaseLoss"
          ],
          "methods": [
            {
              "name": "pointwise_loss",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "get_baseline_prediction",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_train",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "prediction_dim",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "update_gradients_and_hessians",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "gradients",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "hessians",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Categorical cross-entropy loss, for multiclass classification.\n\nFor a given sample x_i, the categorical cross-entropy loss is defined as\nthe negative log-likelihood of the model and generalizes the binary\ncross-entropy to more than 2 classes."
        },
        {
          "name": "LeastAbsoluteDeviation",
          "decorators": [],
          "superclasses": [
            "BaseLoss"
          ],
          "methods": [
            {
              "name": "pointwise_loss",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "get_baseline_prediction",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_train",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "prediction_dim",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "inverse_link_function",
              "decorators": [
                "staticmethod"
              ],
              "parameters": [
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "update_gradients_and_hessians",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "gradients",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "hessians",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "update_leaves_values",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "grower",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Least absolute deviation, for regression.\n\nFor a given sample x_i, the loss is defined as::\n\n    loss(x_i) = |y_true_i - raw_pred_i|"
        },
        {
          "name": "LeastSquares",
          "decorators": [],
          "superclasses": [
            "BaseLoss"
          ],
          "methods": [
            {
              "name": "pointwise_loss",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "get_baseline_prediction",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_train",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "prediction_dim",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "inverse_link_function",
              "decorators": [
                "staticmethod"
              ],
              "parameters": [
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "update_gradients_and_hessians",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "gradients",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "hessians",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Least squares loss, for regression.\n\nFor a given sample x_i, least squares loss is defined as::\n\n    loss(x_i) = 0.5 * (y_true_i - raw_pred_i)**2\n\nThis actually computes the half least squares loss to simplify\nthe computation of the gradients and get a unit hessian (and be consistent\nwith what is done in LightGBM)."
        },
        {
          "name": "Poisson",
          "decorators": [],
          "superclasses": [
            "BaseLoss"
          ],
          "methods": [
            {
              "name": "pointwise_loss",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "get_baseline_prediction",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_train",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "prediction_dim",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "update_gradients_and_hessians",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "gradients",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "hessians",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_true",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_predictions",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Poisson deviance loss with log-link, for regression.\n\nFor a given sample x_i, Poisson deviance loss is defined as::\n\n    loss(x_i) = y_true_i * log(y_true_i/exp(raw_pred_i))\n                - y_true_i + exp(raw_pred_i))\n\nThis actually computes half the Poisson deviance to simplify\nthe computation of the gradients."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._hist_gradient_boosting.predictor",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._predictor",
          "declaration": "_compute_partial_dependence",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._predictor",
          "declaration": "_predict_from_binned_data",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting._predictor",
          "declaration": "_predict_from_raw_data",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.common",
          "declaration": "Y_DTYPE",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "TreePredictor",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "get_n_leaf_nodes",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return number of leaves."
            },
            {
              "name": "get_max_depth",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return maximum depth among all leaves."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                },
                {
                  "name": "known_cat_bitsets",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of bitsets of known categories, for each categorical feature."
                },
                {
                  "name": "f_idx_map",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Map from original feature index to the corresponding index in the\nknown_cat_bitsets array."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The raw predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict raw values for non-binned data.\n\nParameters\n----------\nX : ndarray, shape (n_samples, n_features)\n    The input samples.\n\nknown_cat_bitsets : ndarray of shape (n_categorical_features, 8)\n    Array of bitsets of known categories, for each categorical feature.\n\nf_idx_map : ndarray of shape (n_features,)\n    Map from original feature index to the corresponding index in the\n    known_cat_bitsets array.\n\nReturns\n-------\ny : ndarray, shape (n_samples,)\n    The raw predicted values."
            },
            {
              "name": "predict_binned",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                },
                {
                  "name": "missing_values_bin_idx",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Index of the bin that is used for missing values. This is the\nindex of the last bin and is always equal to max_bins (as passed\nto the GBDT classes), or equivalently to n_bins - 1."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The raw predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict raw values for binned data.\n\nParameters\n----------\nX : ndarray, shape (n_samples, n_features)\n    The input samples.\nmissing_values_bin_idx : uint8\n    Index of the bin that is used for missing values. This is the\n    index of the last bin and is always equal to max_bins (as passed\n    to the GBDT classes), or equivalently to n_bins - 1.\n\nReturns\n-------\ny : ndarray, shape (n_samples,)\n    The raw predicted values."
            },
            {
              "name": "compute_partial_dependence",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "grid",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The grid points on which the partial dependence should be\nevaluated."
                },
                {
                  "name": "target_features",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The set of target features for which the partial dependence\nshould be evaluated."
                },
                {
                  "name": "out",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The value of the partial dependence function on each grid\npoint."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fast partial dependence computation.\n\nParameters\n----------\ngrid : ndarray, shape (n_samples, n_target_features)\n    The grid points on which the partial dependence should be\n    evaluated.\ntarget_features : ndarray, shape (n_target_features)\n    The set of target features for which the partial dependence\n    should be evaluated.\nout : ndarray, shape (n_samples)\n    The value of the partial dependence function on each grid\n    point."
            }
          ],
          "fullDocstring": "Tree class used for predictions.\n\nParameters\n----------\nnodes : ndarray of PREDICTOR_RECORD_DTYPE\n    The nodes of the tree.\nbinned_left_cat_bitsets : ndarray of shape (n_categorical_splits, 8),             dtype=uint32\n    Array of bitsets for binned categories used in predict_binned when a\n    split is categorical.\nraw_left_cat_bitsets : ndarray of shape (n_categorical_splits, 8),             dtype=uint32\n    Array of bitsets for raw categories used in predict when a split is\n    categorical."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._iforest",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "OutlierMixin",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._bagging",
          "declaration": "BaseBagging",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "ExtraTreeRegressor",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_batches",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "get_chunk_n_rows",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_joblib_parallel_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "warnings",
          "declaration": "warn",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "IsolationForest",
          "decorators": [],
          "superclasses": [
            "OutlierMixin",
            "BaseBagging"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Use ``dtype=np.float32`` for maximum\nefficiency. Sparse matrices are also supported, use sparse\n``csc_matrix`` for maximum efficiency."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Use ``dtype=np.float32`` for maximum\n    efficiency. Sparse matrices are also supported, use sparse\n    ``csc_matrix`` for maximum efficiency.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n\nReturns\n-------\nself : object\n    Fitted estimator."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "is_inlier",
                  "type": null,
                  "description": "For each observation, tells whether or not (+1 or -1) it should\nbe considered as an inlier according to the fitted model."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict if a particular sample is an outlier or not.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nis_inlier : ndarray of shape (n_samples,)\n    For each observation, tells whether or not (+1 or -1) it should\n    be considered as an inlier according to the fitted model."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "scores",
                  "type": null,
                  "description": "The anomaly score of the input samples.\nThe lower, the more abnormal. Negative scores represent outliers,\npositive scores represent inliers."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Average anomaly score of X of the base classifiers.\n\nThe anomaly score of an input sample is computed as\nthe mean anomaly score of the trees in the forest.\n\nThe measure of normality of an observation given a tree is the depth\nof the leaf containing this observation, which is equivalent to\nthe number of splittings required to isolate this point. In case of\nseveral observations n_left in the leaf, the average path length of\na n_left samples isolation tree is added.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nscores : ndarray of shape (n_samples,)\n    The anomaly score of the input samples.\n    The lower, the more abnormal. Negative scores represent outliers,\n    positive scores represent inliers."
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "scores",
                  "type": null,
                  "description": "The anomaly score of the input samples.\nThe lower, the more abnormal."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Opposite of the anomaly score defined in the original paper.\n\nThe anomaly score of an input sample is computed as\nthe mean anomaly score of the trees in the forest.\n\nThe measure of normality of an observation given a tree is the depth\nof the leaf containing this observation, which is equivalent to\nthe number of splittings required to isolate this point. In case of\nseveral observations n_left in the leaf, the average path length of\na n_left samples isolation tree is added.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\nscores : ndarray of shape (n_samples,)\n    The anomaly score of the input samples.\n    The lower, the more abnormal."
            }
          ],
          "fullDocstring": "Isolation Forest Algorithm.\n\nReturn the anomaly score of each sample using the IsolationForest algorithm\n\nThe IsolationForest 'isolates' observations by randomly selecting a feature\nand then randomly selecting a split value between the maximum and minimum\nvalues of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the\nnumber of splittings required to isolate a sample is equivalent to the path\nlength from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a\nmeasure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies.\nHence, when a forest of random trees collectively produce shorter path\nlengths for particular samples, they are highly likely to be anomalies.\n\nRead more in the :ref:`User Guide <isolation_forest>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of base estimators in the ensemble.\n\nmax_samples : \"auto\", int or float, default=\"auto\"\n    The number of samples to draw from X to train each base estimator.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n        - If \"auto\", then `max_samples=min(256, n_samples)`.\n\n    If max_samples is larger than the number of samples provided,\n    all samples will be used for all trees (no sampling).\n\ncontamination : 'auto' or float, default='auto'\n    The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. Used when fitting to define the threshold\n    on the scores of the samples.\n\n        - If 'auto', the threshold is determined as in the\n          original paper.\n        - If float, the contamination should be in the range [0, 0.5].\n\n    .. versionchanged:: 0.22\n       The default value of ``contamination`` changed from 0.1\n       to ``'auto'``.\n\nmax_features : int or float, default=1.0\n    The number of features to draw from X to train each base estimator.\n\n        - If int, then draw `max_features` features.\n        - If float, then draw `max_features * X.shape[1]` features.\n\nbootstrap : bool, default=False\n    If True, individual trees are fit on random subsets of the training\n    data sampled with replacement. If False, sampling without replacement\n    is performed.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo-randomness of the selection of the feature\n    and split values for each branching step and each tree in the forest.\n\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    Controls the verbosity of the tree building process.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\n    .. versionadded:: 0.21\n\nAttributes\n----------\nbase_estimator_ : ExtraTreeRegressor instance\n    The child estimator template used to create the collection of\n    fitted sub-estimators.\n\nestimators_ : list of ExtraTreeRegressor instances\n    The collection of fitted sub-estimators.\n\nestimators_features_ : list of ndarray\n    The subset of drawn features for each base estimator.\n\nestimators_samples_ : list of ndarray\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator.\n\nmax_samples_ : int\n    The actual number of samples.\n\noffset_ : float\n    Offset used to define the decision function from the raw scores. We\n    have the relation: ``decision_function = score_samples - offset_``.\n    ``offset_`` is defined as follows. When the contamination parameter is\n    set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n    close to 0 and the scores of outliers are close to -1. When a\n    contamination parameter different than \"auto\" is provided, the offset\n    is defined in such a way we obtain the expected number of outliers\n    (samples with decision function < 0) in training.\n\n    .. versionadded:: 0.20\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nNotes\n-----\nThe implementation is based on an ensemble of ExtraTreeRegressor. The\nmaximum depth of each tree is set to ``ceil(log_2(n))`` where\n:math:`n` is the number of samples used to build the tree\n(see (Liu et al., 2008) for more details).\n\nReferences\n----------\n.. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n       Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n.. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n       anomaly detection.\" ACM Transactions on Knowledge Discovery from\n       Data (TKDD) 6.1 (2012): 3.\n\nSee Also\n----------\nsklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n    Gaussian distributed dataset.\nsklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n    Estimate the support of a high-dimensional distribution.\n    The implementation is based on libsvm.\nsklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n    using Local Outlier Factor (LOF).\n\nExamples\n--------\n>>> from sklearn.ensemble import IsolationForest\n>>> X = [[-1.1], [0.3], [0.5], [100]]\n>>> clf = IsolationForest(random_state=0).fit(X)\n>>> clf.predict([[0.1], [0], [90]])\narray([ 1,  1, -1])"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._stacking",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sparse"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "copy",
          "declaration": "deepcopy",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_regressor",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "_BaseHeterogeneousEnsemble",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "_fit_single_estimator",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "LogisticRegression",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "RidgeCV",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "cross_val_predict",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils._estimator_html_repr",
          "declaration": "_VisualBlock",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "if_delegate_has_method",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "column_or_1d",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "StackingClassifier",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "_BaseStacking"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where `n_samples` is the number of samples and\n`n_features` is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted.\nNote that this is supported only if all underlying estimators\nsupport sample weights."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the estimators.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Note that this is supported only if all underlying estimators\n    support sample weights.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
                }
              ],
              "results": [
                {
                  "name": "y_pred",
                  "type": null,
                  "description": "Predicted targets."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict target for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\n**predict_params : dict of str -> obj\n    Parameters to the `predict` called by the `final_estimator`. Note\n    that this may be used to return uncertainties from some estimators\n    with `return_std` or `return_cov`. Be aware that it will only\n    accounts for uncertainty in the final estimator.\n\nReturns\n-------\ny_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n    Predicted targets."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
                }
              ],
              "results": [
                {
                  "name": "probabilities",
                  "type": null,
                  "description": "The class probabilities of the input samples."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities for X using\n`final_estimator_.predict_proba`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n-------\nprobabilities : ndarray of shape (n_samples, n_classes) or             list of ndarray of shape (n_output,)\n    The class probabilities of the input samples."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
                }
              ],
              "results": [
                {
                  "name": "decisions",
                  "type": null,
                  "description": "The decision function computed the final estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict decision function for samples in X using\n`final_estimator_.decision_function`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n-------\ndecisions : ndarray of shape (n_samples,), (n_samples, n_classes),             or (n_samples, n_classes * (n_classes-1) / 2)\n    The decision function computed the final estimator."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where `n_samples` is the number of samples and\n`n_features` is the number of features."
                }
              ],
              "results": [
                {
                  "name": "y_preds",
                  "type": null,
                  "description": "Prediction outputs for each estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return class labels or probabilities for X for each estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n\nReturns\n-------\ny_preds : ndarray of shape (n_samples, n_estimators) or                 (n_samples, n_classes * n_estimators)\n    Prediction outputs for each estimator."
            }
          ],
          "fullDocstring": "Stack of estimators with a final classifier.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a classifier to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nestimators : list of (str, estimator)\n    Base estimators which will be stacked together. Each element of the\n    list is defined as a tuple of string (i.e. name) and an estimator\n    instance. An estimator can be set to 'drop' using `set_params`.\n\nfinal_estimator : estimator, default=None\n    A classifier which will be used to combine the base estimators.\n    The default classifier is a\n    :class:`~sklearn.linear_model.LogisticRegression`.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy used in\n    `cross_val_predict` to train `final_estimator`. Possible inputs for\n    cv are:\n\n    * None, to use the default 5-fold cross validation,\n    * integer, to specify the number of folds in a (Stratified) KFold,\n    * An object to be used as a cross-validation generator,\n    * An iterable yielding train, test splits.\n\n    For integer/None inputs, if the estimator is a classifier and y is\n    either binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used.\n    In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. note::\n       A larger number of split will provide no benefits if the number\n       of training samples is large enough. Indeed, the training time\n       will increase. ``cv`` is not used for model evaluation but for\n       prediction.\n\nstack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'\n    Methods called for each base estimator. It can be:\n\n    * if 'auto', it will try to invoke, for each estimator,\n      `'predict_proba'`, `'decision_function'` or `'predict'` in that\n      order.\n    * otherwise, one of `'predict_proba'`, `'decision_function'` or\n      `'predict'`. If the method is not implemented by the estimator, it\n      will raise an error.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel all `estimators` `fit`.\n    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n    using all processors. See Glossary for more details.\n\npassthrough : bool, default=False\n    When False, only the predictions of estimators will be used as\n    training data for `final_estimator`. When True, the\n    `final_estimator` is trained on the predictions as well as the\n    original training data.\n\nverbose : int, default=0\n    Verbosity level.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    Class labels.\n\nestimators_ : list of estimators\n    The elements of the estimators parameter, having been fitted on the\n    training data. If an estimator has been set to `'drop'`, it\n    will not appear in `estimators_`.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\nfinal_estimator_ : estimator\n    The classifier which predicts given the output of `estimators_`.\n\nstack_method_ : list of str\n    The method used by each base estimator.\n\nNotes\n-----\nWhen `predict_proba` is used by each estimator (i.e. most of the time for\n`stack_method='auto'` or specifically for `stack_method='predict_proba'`),\nThe first column predicted by each estimator will be dropped in the case\nof a binary classification problem. Indeed, both feature will be perfectly\ncollinear.\n\nReferences\n----------\n.. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n   (1992): 241-259.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.svm import LinearSVC\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.ensemble import StackingClassifier\n>>> X, y = load_iris(return_X_y=True)\n>>> estimators = [\n...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n...     ('svr', make_pipeline(StandardScaler(),\n...                           LinearSVC(random_state=42)))\n... ]\n>>> clf = StackingClassifier(\n...     estimators=estimators, final_estimator=LogisticRegression()\n... )\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, stratify=y, random_state=42\n... )\n>>> clf.fit(X_train, y_train).score(X_test, y_test)\n0.9..."
        },
        {
          "name": "StackingRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "_BaseStacking"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted.\nNote that this is supported only if all underlying estimators\nsupport sample weights."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the estimators.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Note that this is supported only if all underlying estimators\n    support sample weights.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where `n_samples` is the number of samples and\n`n_features` is the number of features."
                }
              ],
              "results": [
                {
                  "name": "y_preds",
                  "type": null,
                  "description": "Prediction outputs for each estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the predictions for X for each estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n\nReturns\n-------\ny_preds : ndarray of shape (n_samples, n_estimators)\n    Prediction outputs for each estimator."
            }
          ],
          "fullDocstring": "Stack of estimators with a final regressor.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a regressor to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nestimators : list of (str, estimator)\n    Base estimators which will be stacked together. Each element of the\n    list is defined as a tuple of string (i.e. name) and an estimator\n    instance. An estimator can be set to 'drop' using `set_params`.\n\nfinal_estimator : estimator, default=None\n    A regressor which will be used to combine the base estimators.\n    The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy used in\n    `cross_val_predict` to train `final_estimator`. Possible inputs for\n    cv are:\n\n    * None, to use the default 5-fold cross validation,\n    * integer, to specify the number of folds in a (Stratified) KFold,\n    * An object to be used as a cross-validation generator,\n    * An iterable yielding train, test splits.\n\n    For integer/None inputs, if the estimator is a classifier and y is\n    either binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used.\n    In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. note::\n       A larger number of split will provide no benefits if the number\n       of training samples is large enough. Indeed, the training time\n       will increase. ``cv`` is not used for model evaluation but for\n       prediction.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for `fit` of all `estimators`.\n    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n    using all processors. See Glossary for more details.\n\npassthrough : bool, default=False\n    When False, only the predictions of estimators will be used as\n    training data for `final_estimator`. When True, the\n    `final_estimator` is trained on the predictions as well as the\n    original training data.\n\nverbose : int, default=0\n    Verbosity level.\n\nAttributes\n----------\nestimators_ : list of estimator\n    The elements of the estimators parameter, having been fitted on the\n    training data. If an estimator has been set to `'drop'`, it\n    will not appear in `estimators_`.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\n\nfinal_estimator_ : estimator\n    The regressor to stacked the base estimators fitted.\n\nReferences\n----------\n.. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n   (1992): 241-259.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.linear_model import RidgeCV\n>>> from sklearn.svm import LinearSVR\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import StackingRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> estimators = [\n...     ('lr', RidgeCV()),\n...     ('svr', LinearSVR(random_state=42))\n... ]\n>>> reg = StackingRegressor(\n...     estimators=estimators,\n...     final_estimator=RandomForestRegressor(n_estimators=10,\n...                                           random_state=42)\n... )\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=42\n... )\n>>> reg.fit(X_train, y_train).score(X_test, y_test)\n0.3..."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._voting",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "_BaseHeterogeneousEnsemble",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "_fit_single_estimator",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils._estimator_html_repr",
          "declaration": "_VisualBlock",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "column_or_1d",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "VotingClassifier",
          "decorators": [],
          "superclasses": [
            "_BaseVoting",
            "ClassifierMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted.\nNote that this is supported only if all underlying estimators\nsupport sample weights.\n\n.. versionadded:: 0.18"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the estimators.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Note that this is supported only if all underlying estimators\n    support sample weights.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nself : object"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "maj",
                  "type": null,
                  "description": "Predicted class labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class labels for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\nmaj : array-like of shape (n_samples,)\n    Predicted class labels."
            },
            {
              "name": "predict_proba",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "avg",
                  "type": null,
                  "description": "Weighted average probability for each class per sample."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute probabilities of possible outcomes for samples in X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\navg : array-like of shape (n_samples, n_classes)\n    Weighted average probability for each class per sample."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "If `voting='soft'` and `flatten_transform=True`:\n    returns ndarray of shape (n_classifiers, n_samples *\n    n_classes), being class probabilities calculated by each\n    classifier.\nIf `voting='soft' and `flatten_transform=False`:\n    ndarray of shape (n_classifiers, n_samples, n_classes)\nIf `voting='hard'`:\n    ndarray of shape (n_samples, n_classifiers), being\n    class labels predicted by each classifier."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return class labels or probabilities for X for each estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n-------\nprobabilities_or_labels\n    If `voting='soft'` and `flatten_transform=True`:\n        returns ndarray of shape (n_classifiers, n_samples *\n        n_classes), being class probabilities calculated by each\n        classifier.\n    If `voting='soft' and `flatten_transform=False`:\n        ndarray of shape (n_classifiers, n_samples, n_classes)\n    If `voting='hard'`:\n        ndarray of shape (n_samples, n_classifiers), being\n        class labels predicted by each classifier."
            }
          ],
          "fullDocstring": "Soft Voting/Majority Rule classifier for unfitted estimators.\n\nRead more in the :ref:`User Guide <voting_classifier>`.\n\n.. versionadded:: 0.17\n\nParameters\n----------\nestimators : list of (str, estimator) tuples\n    Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n    of those original estimators that will be stored in the class attribute\n    ``self.estimators_``. An estimator can be set to ``'drop'``\n    using ``set_params``.\n\n    .. versionchanged:: 0.21\n        ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n        support was removed in 0.24.\n\nvoting : {'hard', 'soft'}, default='hard'\n    If 'hard', uses predicted class labels for majority rule voting.\n    Else if 'soft', predicts the class label based on the argmax of\n    the sums of the predicted probabilities, which is recommended for\n    an ensemble of well-calibrated classifiers.\n\nweights : array-like of shape (n_classifiers,), default=None\n    Sequence of weights (`float` or `int`) to weight the occurrences of\n    predicted class labels (`hard` voting) or class probabilities\n    before averaging (`soft` voting). Uses uniform weights if `None`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for ``fit``.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18\n\nflatten_transform : bool, default=True\n    Affects shape of transform output only when voting='soft'\n    If voting='soft' and flatten_transform=True, transform method returns\n    matrix with shape (n_samples, n_classifiers * n_classes). If\n    flatten_transform=False, it returns\n    (n_classifiers, n_samples, n_classes).\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting will be printed as it\n    is completed.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nestimators_ : list of classifiers\n    The collection of fitted sub-estimators as defined in ``estimators``\n    that are not 'drop'.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\n    .. versionadded:: 0.20\n\nclasses_ : array-like of shape (n_predictions,)\n    The classes labels.\n\nSee Also\n--------\nVotingRegressor : Prediction voting regressor.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n>>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = VotingClassifier(estimators=[\n...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n...                eclf1.named_estimators_['lr'].predict(X))\nTrue\n>>> eclf2 = VotingClassifier(estimators=[\n...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...         voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = VotingClassifier(estimators=[\n...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...        voting='soft', weights=[2,1,1],\n...        flatten_transform=True)\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>> print(eclf3.transform(X).shape)\n(6, 6)"
        },
        {
          "name": "VotingRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "_BaseVoting"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted.\nNote that this is supported only if all underlying estimators\nsupport sample weights."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the estimators.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Note that this is supported only if all underlying estimators\n    support sample weights.\n\nReturns\n-------\nself : object\n    Fitted estimator."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the estimators in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted values."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Values predicted by each regressor."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return predictions for X for each estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\npredictions: ndarray of shape (n_samples, n_classifiers)\n    Values predicted by each regressor."
            }
          ],
          "fullDocstring": "Prediction voting regressor for unfitted estimators.\n\nA voting regressor is an ensemble meta-estimator that fits several base\nregressors, each on the whole dataset. Then it averages the individual\npredictions to form a final prediction.\n\nRead more in the :ref:`User Guide <voting_regressor>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\nestimators : list of (str, estimator) tuples\n    Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n    of those original estimators that will be stored in the class attribute\n    ``self.estimators_``. An estimator can be set to ``'drop'`` using\n    ``set_params``.\n\n    .. versionchanged:: 0.21\n        ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n        support was removed in 0.24.\n\nweights : array-like of shape (n_regressors,), default=None\n    Sequence of weights (`float` or `int`) to weight the occurrences of\n    predicted values before averaging. Uses uniform weights if `None`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for ``fit``.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting will be printed as it\n    is completed.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nestimators_ : list of regressors\n    The collection of fitted sub-estimators as defined in ``estimators``\n    that are not 'drop'.\n\nnamed_estimators_ : Bunch\n    Attribute to access any fitted sub-estimators by name.\n\n    .. versionadded:: 0.20\n\nSee Also\n--------\nVotingClassifier : Soft Voting/Majority Rule classifier.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import VotingRegressor\n>>> r1 = LinearRegression()\n>>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n>>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n>>> y = np.array([2, 6, 12, 20, 30, 42])\n>>> er = VotingRegressor([('lr', r1), ('rf', r2)])\n>>> print(er.fit(X, y).predict(X))\n[ 3.3  5.7 11.8 19.7 28.  40.3]"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble._weight_boosting",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "xlogy",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_regressor",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._base",
          "declaration": "BaseEnsemble",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "r2_score",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "DecisionTreeClassifier",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "DecisionTreeRegressor",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "softmax",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "stable_cumsum",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "has_fit_parameter",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "AdaBoostClassifier",
          "decorators": [],
          "superclasses": [
            "BaseWeightBoosting",
            "ClassifierMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (class labels)."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, the sample weights are initialized to\n``1 / n_samples``."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Build a boosted classifier from the training set (X, y).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\ny : array-like of shape (n_samples,)\n    The target values (class labels).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, the sample weights are initialized to\n    ``1 / n_samples``.\n\nReturns\n-------\nself : object\n    Fitted estimator."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted classes."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict classes for X.\n\nThe predicted class of an input sample is computed as the weighted mean\nprediction of the classifiers in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted classes."
            },
            {
              "name": "staged_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return staged predictions for X.\n\nThe predicted class of an input sample is computed as the weighted mean\nprediction of the classifiers in the ensemble.\n\nThis generator method yields the ensemble prediction after each\niteration of boosting and therefore allows monitoring, such as to\ndetermine the prediction on a test set after each boost.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nYields\n------\ny : generator of ndarray of shape (n_samples,)\n    The predicted classes."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": null,
                  "description": "The decision function of the input samples. The order of\noutputs is the same of that of the :term:`classes_` attribute.\nBinary classification is a special cases with ``k == 1``,\notherwise ``k==n_classes``. For binary classification,\nvalues closer to -1 or 1 mean more like the first or second\nclass in ``classes_``, respectively."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the decision function of ``X``.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\nscore : ndarray of shape of (n_samples, k)\n    The decision function of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute.\n    Binary classification is a special cases with ``k == 1``,\n    otherwise ``k==n_classes``. For binary classification,\n    values closer to -1 or 1 mean more like the first or second\n    class in ``classes_``, respectively."
            },
            {
              "name": "staged_decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute decision function of ``X`` for each boosting iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each boosting iteration.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nYields\n------\nscore : generator of ndarray of shape (n_samples, k)\n    The decision function of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute.\n    Binary classification is a special cases with ``k == 1``,\n    otherwise ``k==n_classes``. For binary classification,\n    values closer to -1 or 1 mean more like the first or second\n    class in ``classes_``, respectively."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class probabilities of the input samples. The order of\noutputs is the same of that of the :term:`classes_` attribute."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe weighted mean predicted class probabilities of the classifiers\nin the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute."
            },
            {
              "name": "staged_predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe weighted mean predicted class probabilities of the classifiers\nin the ensemble.\n\nThis generator method yields the ensemble predicted class probabilities\nafter each iteration of boosting and therefore allows monitoring, such\nas to determine the predicted class probabilities on a test set after\neach boost.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nYields\n-------\np : generator of ndarray of shape (n_samples,)\n    The class probabilities of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class probabilities of the input samples. The order of\noutputs is the same of that of the :term:`classes_` attribute."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe weighted mean predicted class log-probabilities of the classifiers\nin the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute."
            }
          ],
          "fullDocstring": "An AdaBoost classifier.\n\nAn AdaBoost [1] classifier is a meta-estimator that begins by fitting a\nclassifier on the original dataset and then fits additional copies of the\nclassifier on the same dataset but where the weights of incorrectly\nclassified instances are adjusted such that subsequent classifiers focus\nmore on difficult cases.\n\nThis class implements the algorithm known as AdaBoost-SAMME [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14\n\nParameters\n----------\nbase_estimator : object, default=None\n    The base estimator from which the boosted ensemble is built.\n    Support for sample weighting is required, as well as proper\n    ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n    the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n    initialized with `max_depth=1`.\n\nn_estimators : int, default=50\n    The maximum number of estimators at which boosting is terminated.\n    In case of perfect fit, the learning procedure is stopped early.\n\nlearning_rate : float, default=1.\n    Learning rate shrinks the contribution of each classifier by\n    ``learning_rate``. There is a trade-off between ``learning_rate`` and\n    ``n_estimators``.\n\nalgorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n    If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n    ``base_estimator`` must support calculation of class probabilities.\n    If 'SAMME' then use the SAMME discrete boosting algorithm.\n    The SAMME.R algorithm typically converges faster than SAMME,\n    achieving a lower test error with fewer boosting iterations.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given at each `base_estimator` at each\n    boosting iteration.\n    Thus, it is only used when `base_estimator` exposes a `random_state`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nestimators_ : list of classifiers\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_classes_ : int\n    The number of classes.\n\nestimator_weights_ : ndarray of floats\n    Weights for each estimator in the boosted ensemble.\n\nestimator_errors_ : ndarray of floats\n    Classification error for each estimator in the boosted\n    ensemble.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances if supported by the\n    ``base_estimator`` (when based on decision trees).\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nSee Also\n--------\nAdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n    regressor on the original dataset and then fits additional copies of\n    the regressor on the same dataset but where the weights of instances\n    are adjusted according to the error of the current prediction.\n\nGradientBoostingClassifier : GB builds an additive model in a forward\n    stage-wise fashion. Regression trees are fit on the negative gradient\n    of the binomial or multinomial deviance loss function. Binary\n    classification is a special case where only a single regression tree is\n    induced.\n\nsklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n    method used for classification.\n    Creates a model that predicts the value of a target variable by\n    learning simple decision rules inferred from the data features.\n\nReferences\n----------\n.. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n       on-Line Learning and an Application to Boosting\", 1995.\n\n.. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n\nExamples\n--------\n>>> from sklearn.ensemble import AdaBoostClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n>>> clf.fit(X, y)\nAdaBoostClassifier(n_estimators=100, random_state=0)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])\n>>> clf.score(X, y)\n0.983..."
        },
        {
          "name": "AdaBoostRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseWeightBoosting"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (real numbers)."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, the sample weights are initialized to\n1 / n_samples."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Build a boosted regressor from the training set (X, y).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\ny : array-like of shape (n_samples,)\n    The target values (real numbers).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, the sample weights are initialized to\n    1 / n_samples.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted regression values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict regression value for X.\n\nThe predicted regression value of an input sample is computed\nas the weighted median prediction of the classifiers in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted regression values."
            },
            {
              "name": "staged_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return staged predictions for X.\n\nThe predicted regression value of an input sample is computed\nas the weighted median prediction of the classifiers in the ensemble.\n\nThis generator method yields the ensemble prediction after each\niteration of boosting and therefore allows monitoring, such as to\ndetermine the prediction on a test set after each boost.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples.\n\nYields\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted regression values."
            }
          ],
          "fullDocstring": "An AdaBoost regressor.\n\nAn AdaBoost [1] regressor is a meta-estimator that begins by fitting a\nregressor on the original dataset and then fits additional copies of the\nregressor on the same dataset but where the weights of instances are\nadjusted according to the error of the current prediction. As such,\nsubsequent regressors focus more on difficult cases.\n\nThis class implements the algorithm known as AdaBoost.R2 [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14\n\nParameters\n----------\nbase_estimator : object, default=None\n    The base estimator from which the boosted ensemble is built.\n    If ``None``, then the base estimator is\n    :class:`~sklearn.tree.DecisionTreeRegressor` initialized with\n    `max_depth=3`.\n\nn_estimators : int, default=50\n    The maximum number of estimators at which boosting is terminated.\n    In case of perfect fit, the learning procedure is stopped early.\n\nlearning_rate : float, default=1.\n    Learning rate shrinks the contribution of each regressor by\n    ``learning_rate``. There is a trade-off between ``learning_rate`` and\n    ``n_estimators``.\n\nloss : {'linear', 'square', 'exponential'}, default='linear'\n    The loss function to use when updating the weights after each\n    boosting iteration.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given at each `base_estimator` at each\n    boosting iteration.\n    Thus, it is only used when `base_estimator` exposes a `random_state`.\n    In addition, it controls the bootstrap of the weights used to train the\n    `base_estimator` at each boosting iteration.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nestimators_ : list of classifiers\n    The collection of fitted sub-estimators.\n\nestimator_weights_ : ndarray of floats\n    Weights for each estimator in the boosted ensemble.\n\nestimator_errors_ : ndarray of floats\n    Regression error for each estimator in the boosted ensemble.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances if supported by the\n    ``base_estimator`` (when based on decision trees).\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nExamples\n--------\n>>> from sklearn.ensemble import AdaBoostRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=4, n_informative=2,\n...                        random_state=0, shuffle=False)\n>>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n>>> regr.fit(X, y)\nAdaBoostRegressor(n_estimators=100, random_state=0)\n>>> regr.predict([[0, 0, 0, 0]])\narray([4.7972...])\n>>> regr.score(X, y)\n0.9771...\n\nSee Also\n--------\nAdaBoostClassifier, GradientBoostingRegressor,\nsklearn.tree.DecisionTreeRegressor\n\nReferences\n----------\n.. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n       on-Line Learning and an Application to Boosting\", 1995.\n\n.. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997."
        },
        {
          "name": "BaseWeightBoosting",
          "decorators": [],
          "superclasses": [
            "BaseEnsemble"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (class labels in classification, real numbers in\nregression)."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, the sample weights are initialized to\n1 / n_samples."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Build a boosted classifier/regressor from the training set (X, y).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\ny : array-like of shape (n_samples,)\n    The target values (class labels in classification, real numbers in\n    regression).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, the sample weights are initialized to\n    1 / n_samples.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "staged_score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Labels for X."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return staged scores for X, y.\n\nThis generator method yields the ensemble score after each iteration of\nboosting and therefore allows monitoring, such as to determine the\nscore on a test set after each boost.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\ny : array-like of shape (n_samples,)\n    Labels for X.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nYields\n------\nz : float"
            },
            {
              "name": "feature_importances_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "feature_importances_",
                  "type": null,
                  "description": "The feature importances."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "The impurity-based feature importances.\n\nThe higher, the more important the feature.\nThe importance of a feature is computed as the (normalized)\ntotal reduction of the criterion brought by that feature.  It is also\nknown as the Gini importance.\n\nWarning: impurity-based feature importances can be misleading for\nhigh cardinality features (many unique values). See\n:func:`sklearn.inspection.permutation_importance` as an alternative.\n\nReturns\n-------\nfeature_importances_ : ndarray of shape (n_features,)\n    The feature importances."
            }
          ],
          "fullDocstring": "Base class for AdaBoost estimators.\n\nWarning: This class should not be used directly. Use derived classes\ninstead."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.ensemble.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "numpy.distutils.misc_util",
          "declaration": "Configuration",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.exceptions",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "ChangedBehaviorWarning",
          "decorators": [],
          "superclasses": [
            "UserWarning"
          ],
          "methods": [],
          "fullDocstring": "Warning class used to notify the user of any change in the behavior.\n\n.. versionchanged:: 0.18\n   Moved from sklearn.base."
        },
        {
          "name": "ConvergenceWarning",
          "decorators": [],
          "superclasses": [
            "UserWarning"
          ],
          "methods": [],
          "fullDocstring": "Custom warning to capture convergence problems\n\n.. versionchanged:: 0.18\n   Moved from sklearn.utils."
        },
        {
          "name": "DataConversionWarning",
          "decorators": [],
          "superclasses": [
            "UserWarning"
          ],
          "methods": [],
          "fullDocstring": "Warning used to notify implicit data conversions happening in the code.\n\nThis warning occurs when some input data needs to be converted or\ninterpreted in a way that may not match the user's expectations.\n\nFor example, this warning may occur when the user\n    - passes an integer array to a function which expects float input and\n      will convert the input\n    - requests a non-copying operation, but a copy is required to meet the\n      implementation's data-type expectations;\n    - passes an input whose shape can be interpreted ambiguously.\n\n.. versionchanged:: 0.18\n   Moved from sklearn.utils.validation."
        },
        {
          "name": "DataDimensionalityWarning",
          "decorators": [],
          "superclasses": [
            "UserWarning"
          ],
          "methods": [],
          "fullDocstring": "Custom warning to notify potential issues with data dimensionality.\n\nFor example, in random projection, this warning is raised when the\nnumber of components, which quantifies the dimensionality of the target\nprojection space, is higher than the number of features, which quantifies\nthe dimensionality of the original source space, to imply that the\ndimensionality of the problem will not be reduced.\n\n.. versionchanged:: 0.18\n   Moved from sklearn.utils."
        },
        {
          "name": "EfficiencyWarning",
          "decorators": [],
          "superclasses": [
            "UserWarning"
          ],
          "methods": [],
          "fullDocstring": "Warning used to notify the user of inefficient computation.\n\nThis warning notifies the user that the efficiency may not be optimal due\nto some reason which may be included as a part of the warning message.\nThis may be subclassed into a more specific Warning class.\n\n.. versionadded:: 0.18"
        },
        {
          "name": "FitFailedWarning",
          "decorators": [],
          "superclasses": [
            "RuntimeWarning"
          ],
          "methods": [],
          "fullDocstring": "Warning class used if there is an error while fitting the estimator.\n\nThis Warning is used in meta estimators GridSearchCV and RandomizedSearchCV\nand the cross-validation helper function cross_val_score to warn when there\nis an error while fitting the estimator.\n\n.. versionchanged:: 0.18\n   Moved from sklearn.cross_validation."
        },
        {
          "name": "NonBLASDotWarning",
          "decorators": [],
          "superclasses": [
            "EfficiencyWarning"
          ],
          "methods": [],
          "fullDocstring": "Warning used when the dot operation does not use BLAS.\n\nThis warning is used to notify the user that BLAS was not used for dot\noperation and hence the efficiency may be affected.\n\n.. versionchanged:: 0.18\n   Moved from sklearn.utils.validation, extends EfficiencyWarning."
        },
        {
          "name": "NotFittedError",
          "decorators": [],
          "superclasses": [
            "ValueError",
            "AttributeError"
          ],
          "methods": [],
          "fullDocstring": "Exception class to raise if estimator is used before fitting.\n\nThis class inherits from both ValueError and AttributeError to help with\nexception handling and backward compatibility.\n\nExamples\n--------\n>>> from sklearn.svm import LinearSVC\n>>> from sklearn.exceptions import NotFittedError\n>>> try:\n...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])\n... except NotFittedError as e:\n...     print(repr(e))\nNotFittedError(\"This LinearSVC instance is not fitted yet. Call 'fit' with\nappropriate arguments before using this estimator.\"...)\n\n.. versionchanged:: 0.18\n   Moved from sklearn.utils.validation."
        },
        {
          "name": "PositiveSpectrumWarning",
          "decorators": [],
          "superclasses": [
            "UserWarning"
          ],
          "methods": [],
          "fullDocstring": "Warning raised when the eigenvalues of a PSD matrix have issues\n\nThis warning is typically raised by ``_check_psd_eigenvalues`` when the\neigenvalues of a positive semidefinite (PSD) matrix such as a gram matrix\n(kernel) present significant negative eigenvalues, or bad conditioning i.e.\nvery small non-zero eigenvalues compared to the largest eigenvalue.\n\n.. versionadded:: 0.22"
        },
        {
          "name": "SkipTestWarning",
          "decorators": [],
          "superclasses": [
            "UserWarning"
          ],
          "methods": [],
          "fullDocstring": "Warning class used to notify the user of a test that was skipped.\n\nFor example, one of the estimator checks requires a pandas import.\nIf the pandas package cannot be imported, the test will be skipped rather\nthan register as a failure."
        },
        {
          "name": "UndefinedMetricWarning",
          "decorators": [],
          "superclasses": [
            "UserWarning"
          ],
          "methods": [],
          "fullDocstring": "Warning used when the metric is invalid\n\n.. versionchanged:: 0.18\n   Moved from sklearn.base."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.experimental",
      "imports": [],
      "fromImports": [],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.experimental.enable_halving_search_cv",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn",
          "declaration": "model_selection",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._search_successive_halving",
          "declaration": "HalvingGridSearchCV",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._search_successive_halving",
          "declaration": "HalvingRandomSearchCV",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.experimental.enable_hist_gradient_boosting",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn",
          "declaration": "ensemble",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting",
          "declaration": "HistGradientBoostingClassifier",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting",
          "declaration": "HistGradientBoostingRegressor",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.experimental.enable_iterative_imputer",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn",
          "declaration": "impute",
          "alias": null
        },
        {
          "module": "sklearn.impute._iterative",
          "declaration": "IterativeImputer",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.externals",
      "imports": [],
      "fromImports": [],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.externals._arff",
      "imports": [
        {
          "module": "csv",
          "alias": null
        },
        {
          "module": "re",
          "alias": null
        },
        {
          "module": "sys",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "itertools",
          "declaration": "izip",
          "alias": "zip"
        },
        {
          "module": "typing",
          "declaration": "Any",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Dict",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Iterator",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "List",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Optional",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "TYPE_CHECKING",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Tuple",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Union",
          "alias": null
        },
        {
          "module": "typing_extensions",
          "declaration": "TypedDict",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "ArffContainerType",
          "decorators": [],
          "superclasses": [
            "TypedDict"
          ],
          "methods": [],
          "fullDocstring": null
        },
        {
          "name": "ArffDecoder",
          "decorators": [],
          "superclasses": [
            "object"
          ],
          "methods": [
            {
              "name": "decode",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "s",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "encode_nominal",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "return_type",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the Python representation of a given ARFF file.\n\nWhen a file object is passed as an argument, this method reads lines\niteratively, avoiding to load unnecessary information to the memory.\n\n:param s: a string or file object with the ARFF file.\n:param encode_nominal: boolean, if True perform a label encoding\n    while reading the .arff file.\n:param return_type: determines the data structure used to store the\n    dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,\n    `arff.DENSE_GEN` or `arff.LOD_GEN`.\n    Consult the sections on `working with sparse data`_ and `loading\n    progressively`_."
            }
          ],
          "fullDocstring": "An ARFF decoder."
        },
        {
          "name": "ArffEncoder",
          "decorators": [],
          "superclasses": [
            "object"
          ],
          "methods": [
            {
              "name": "encode",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "obj",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Encodes a given object to an ARFF file.\n\n:param obj: the object containing the ARFF information.\n:return: the ARFF file as an unicode string."
            },
            {
              "name": "iter_encode",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "obj",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "The iterative version of `arff.ArffEncoder.encode`.\n\nThis encodes iteratively a given object and return, one-by-one, the\nlines of the ARFF file.\n\n:param obj: the object containing the ARFF information.\n:return: (yields) the ARFF file as unicode strings."
            }
          ],
          "fullDocstring": "An ARFF encoder."
        },
        {
          "name": "ArffException",
          "decorators": [],
          "superclasses": [
            "Exception"
          ],
          "methods": [],
          "fullDocstring": null
        },
        {
          "name": "BadAttributeFormat",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when some attribute declaration is in an invalid format."
        },
        {
          "name": "BadAttributeName",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when an attribute name is provided twice the attribute\ndeclaration."
        },
        {
          "name": "BadAttributeType",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when some invalid type is provided into the attribute\ndeclaration."
        },
        {
          "name": "BadDataFormat",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when some data instance is in an invalid format."
        },
        {
          "name": "BadLayout",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when the layout of the ARFF file has something wrong."
        },
        {
          "name": "BadNominalFormatting",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when a nominal value with space is not properly quoted."
        },
        {
          "name": "BadNominalValue",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when a value in used in some data instance but is not\ndeclared into it respective attribute declaration."
        },
        {
          "name": "BadNumericalValue",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when and invalid numerical value is used in some data\ninstance."
        },
        {
          "name": "BadObject",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when the object representing the ARFF file has something\nwrong."
        },
        {
          "name": "BadRelationFormat",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raised when the relation declaration is in an invalid format."
        },
        {
          "name": "BadStringValue",
          "decorators": [],
          "superclasses": [
            "ArffException"
          ],
          "methods": [],
          "fullDocstring": "Error raise when a string contains space but is not quoted."
        },
        {
          "name": "COOData",
          "decorators": [],
          "superclasses": [
            "object"
          ],
          "methods": [
            {
              "name": "decode_rows",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "stream",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "conversors",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "encode_data",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "data",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "attributes",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": null
        },
        {
          "name": "Data",
          "decorators": [],
          "superclasses": [
            "DenseGeneratorData",
            "_DataListMixin"
          ],
          "methods": [],
          "fullDocstring": null
        },
        {
          "name": "DenseGeneratorData",
          "decorators": [],
          "superclasses": [
            "object"
          ],
          "methods": [
            {
              "name": "decode_rows",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "stream",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "conversors",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "encode_data",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "data",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "attributes",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "(INTERNAL) Encodes a line of data.\n\nData instances follow the csv format, i.e, attribute values are\ndelimited by commas. After converted from csv.\n\n:param data: a list of values.\n:param attributes: a list of attributes. Used to check if data is valid.\n:return: a string with the encoded data line."
            }
          ],
          "fullDocstring": "Internal helper class to allow for different matrix types without\nmaking the code a huge collection of if statements."
        },
        {
          "name": "EncodedNominalConversor",
          "decorators": [],
          "superclasses": [
            "object"
          ],
          "methods": [],
          "fullDocstring": null
        },
        {
          "name": "LODData",
          "decorators": [],
          "superclasses": [
            "_DataListMixin",
            "LODGeneratorData"
          ],
          "methods": [],
          "fullDocstring": null
        },
        {
          "name": "LODGeneratorData",
          "decorators": [],
          "superclasses": [
            "object"
          ],
          "methods": [
            {
              "name": "decode_rows",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "stream",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "conversors",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "encode_data",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "data",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "attributes",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": null
        },
        {
          "name": "NominalConversor",
          "decorators": [],
          "superclasses": [
            "object"
          ],
          "methods": [],
          "fullDocstring": null
        }
      ],
      "functions": [
        {
          "name": "dump",
          "decorators": [],
          "parameters": [
            {
              "name": "obj",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "fp",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Serialize an object representing the ARFF document to a given file-like\nobject.\n\n:param obj: a dictionary.\n:param fp: a file-like object."
        },
        {
          "name": "dumps",
          "decorators": [],
          "parameters": [
            {
              "name": "obj",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Serialize an object representing the ARFF document, returning a string.\n\n:param obj: a dictionary.\n:return: a string with the ARFF document."
        },
        {
          "name": "encode_string",
          "decorators": [],
          "parameters": [
            {
              "name": "s",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "load",
          "decorators": [],
          "parameters": [
            {
              "name": "fp",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "encode_nominal",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "return_type",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Load a file-like object containing the ARFF document and convert it into\na Python object.\n\n:param fp: a file-like object.\n:param encode_nominal: boolean, if True perform a label encoding\n    while reading the .arff file.\n:param return_type: determines the data structure used to store the\n    dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,\n    `arff.DENSE_GEN` or `arff.LOD_GEN`.\n    Consult the sections on `working with sparse data`_ and `loading\n    progressively`_.\n:return: a dictionary.\n "
        },
        {
          "name": "loads",
          "decorators": [],
          "parameters": [
            {
              "name": "s",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "encode_nominal",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "return_type",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Convert a string instance containing the ARFF document into a Python\nobject.\n\n:param s: a string object.\n:param encode_nominal: boolean, if True perform a label encoding\n    while reading the .arff file.\n:param return_type: determines the data structure used to store the\n    dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,\n    `arff.DENSE_GEN` or `arff.LOD_GEN`.\n    Consult the sections on `working with sparse data`_ and `loading\n    progressively`_.\n:return: a dictionary."
        }
      ]
    },
    {
      "name": "sklearn.externals._lobpcg",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "__future__",
          "declaration": "absolute_import",
          "alias": null
        },
        {
          "module": "__future__",
          "declaration": "division",
          "alias": null
        },
        {
          "module": "__future__",
          "declaration": "print_function",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "LinAlgError",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "cho_factor",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "cho_solve",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "cholesky",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "eigh",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "inv",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "orth",
          "alias": null
        },
        {
          "module": "scipy.sparse.linalg",
          "declaration": "aslinearoperator",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "bmat",
          "decorators": [],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "lobpcg",
          "decorators": [],
          "parameters": [
            {
              "name": "A",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The symmetric linear operator of the problem, usually a\nsparse matrix.  Often called the \"stiffness matrix\"."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Initial approximation to the ``k`` eigenvectors (non-sparse). If `A`\nhas ``shape=(n,n)`` then `X` should have shape ``shape=(n,k)``."
            },
            {
              "name": "B",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The right hand side operator in a generalized eigenproblem.\nBy default, ``B = Identity``.  Often called the \"mass matrix\"."
            },
            {
              "name": "M",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Preconditioner to `A`; by default ``M = Identity``.\n`M` should approximate the inverse of `A`."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "n-by-sizeY matrix of constraints (non-sparse), sizeY < n\nThe iterations will be performed in the B-orthogonal complement\nof the column-space of Y. Y must be full rank."
            },
            {
              "name": "tol",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Solver tolerance (stopping criterion).\nThe default is ``tol=n*sqrt(eps)``."
            },
            {
              "name": "maxiter",
              "type": "Any",
              "hasDefault": true,
              "default": "20",
              "limitation": null,
              "ignored": false,
              "description": "Maximum number of iterations.  The default is ``maxiter=min(n, 20)``."
            },
            {
              "name": "largest",
              "type": "Any",
              "hasDefault": true,
              "default": "True",
              "limitation": null,
              "ignored": false,
              "description": "When True, solve for the largest eigenvalues, otherwise the smallest."
            },
            {
              "name": "verbosityLevel",
              "type": "Any",
              "hasDefault": false,
              "default": "0",
              "limitation": null,
              "ignored": false,
              "description": "Controls solver output.  The default is ``verbosityLevel=0``."
            },
            {
              "name": "retLambdaHistory",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "Whether to return eigenvalue history.  Default is False."
            },
            {
              "name": "retResidualNormsHistory",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "Whether to return history of residual norms.  Default is False."
            }
          ],
          "results": [
            {
              "name": "w",
              "type": "NDArray",
              "description": "Array of ``k`` eigenvalues"
            },
            {
              "name": "v",
              "type": "NDArray",
              "description": "An array of ``k`` eigenvectors.  `v` has the same shape as `X`."
            },
            {
              "name": "lambdas",
              "type": null,
              "description": "The eigenvalue history, if `retLambdaHistory` is True."
            },
            {
              "name": "rnorms",
              "type": null,
              "description": "The history of residual norms, if `retResidualNormsHistory` is True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Locally Optimal Block Preconditioned Conjugate Gradient Method (LOBPCG)\n\nLOBPCG is a preconditioned eigensolver for large symmetric positive\ndefinite (SPD) generalized eigenproblems.\n\nParameters\n----------\nA : {sparse matrix, dense matrix, LinearOperator}\n    The symmetric linear operator of the problem, usually a\n    sparse matrix.  Often called the \"stiffness matrix\".\nX : ndarray, float32 or float64\n    Initial approximation to the ``k`` eigenvectors (non-sparse). If `A`\n    has ``shape=(n,n)`` then `X` should have shape ``shape=(n,k)``.\nB : {dense matrix, sparse matrix, LinearOperator}, optional\n    The right hand side operator in a generalized eigenproblem.\n    By default, ``B = Identity``.  Often called the \"mass matrix\".\nM : {dense matrix, sparse matrix, LinearOperator}, optional\n    Preconditioner to `A`; by default ``M = Identity``.\n    `M` should approximate the inverse of `A`.\nY : ndarray, float32 or float64, optional\n    n-by-sizeY matrix of constraints (non-sparse), sizeY < n\n    The iterations will be performed in the B-orthogonal complement\n    of the column-space of Y. Y must be full rank.\ntol : scalar, optional\n    Solver tolerance (stopping criterion).\n    The default is ``tol=n*sqrt(eps)``.\nmaxiter : int, optional\n    Maximum number of iterations.  The default is ``maxiter=min(n, 20)``.\nlargest : bool, optional\n    When True, solve for the largest eigenvalues, otherwise the smallest.\nverbosityLevel : int, optional\n    Controls solver output.  The default is ``verbosityLevel=0``.\nretLambdaHistory : bool, optional\n    Whether to return eigenvalue history.  Default is False.\nretResidualNormsHistory : bool, optional\n    Whether to return history of residual norms.  Default is False.\n\nReturns\n-------\nw : ndarray\n    Array of ``k`` eigenvalues\nv : ndarray\n    An array of ``k`` eigenvectors.  `v` has the same shape as `X`.\nlambdas : list of ndarray, optional\n    The eigenvalue history, if `retLambdaHistory` is True.\nrnorms : list of ndarray, optional\n    The history of residual norms, if `retResidualNormsHistory` is True.\n\nNotes\n-----\nIf both ``retLambdaHistory`` and ``retResidualNormsHistory`` are True,\nthe return tuple has the following format\n``(lambda, V, lambda history, residual norms history)``.\n\nIn the following ``n`` denotes the matrix size and ``m`` the number\nof required eigenvalues (smallest or largest).\n\nThe LOBPCG code internally solves eigenproblems of the size ``3m`` on every\niteration by calling the \"standard\" dense eigensolver, so if ``m`` is not\nsmall enough compared to ``n``, it does not make sense to call the LOBPCG\ncode, but rather one should use the \"standard\" eigensolver, e.g. numpy or\nscipy function in this case.\nIf one calls the LOBPCG algorithm for ``5m > n``, it will most likely break\ninternally, so the code tries to call the standard function instead.\n\nIt is not that ``n`` should be large for the LOBPCG to work, but rather the\nratio ``n / m`` should be large. It you call LOBPCG with ``m=1``\nand ``n=10``, it works though ``n`` is small. The method is intended\nfor extremely large ``n / m``, see e.g., reference [28] in\nhttps://arxiv.org/abs/0705.2626\n\nThe convergence speed depends basically on two factors:\n\n1. How well relatively separated the seeking eigenvalues are from the rest\n   of the eigenvalues. One can try to vary ``m`` to make this better.\n\n2. How well conditioned the problem is. This can be changed by using proper\n   preconditioning. For example, a rod vibration test problem (under tests\n   directory) is ill-conditioned for large ``n``, so convergence will be\n   slow, unless efficient preconditioning is used. For this specific\n   problem, a good simple preconditioner function would be a linear solve\n   for `A`, which is easy to code since A is tridiagonal.\n\nReferences\n----------\n.. [1] A. V. Knyazev (2001),\n       Toward the Optimal Preconditioned Eigensolver: Locally Optimal\n       Block Preconditioned Conjugate Gradient Method.\n       SIAM Journal on Scientific Computing 23, no. 2,\n       pp. 517-541. http://dx.doi.org/10.1137/S1064827500366124\n\n.. [2] A. V. Knyazev, I. Lashuk, M. E. Argentati, and E. Ovchinnikov\n       (2007), Block Locally Optimal Preconditioned Eigenvalue Xolvers\n       (BLOPEX) in hypre and PETSc. https://arxiv.org/abs/0705.2626\n\n.. [3] A. V. Knyazev's C and MATLAB implementations:\n       https://bitbucket.org/joseroman/blopex\n\nExamples\n--------\n\nSolve ``A x = lambda x`` with constraints and preconditioning.\n\n>>> import numpy as np\n>>> from scipy.sparse import spdiags, issparse\n>>> from scipy.sparse.linalg import lobpcg, LinearOperator\n>>> n = 100\n>>> vals = np.arange(1, n + 1)\n>>> A = spdiags(vals, 0, n, n)\n>>> A.toarray()\narray([[  1.,   0.,   0., ...,   0.,   0.,   0.],\n       [  0.,   2.,   0., ...,   0.,   0.,   0.],\n       [  0.,   0.,   3., ...,   0.,   0.,   0.],\n       ...,\n       [  0.,   0.,   0., ...,  98.,   0.,   0.],\n       [  0.,   0.,   0., ...,   0.,  99.,   0.],\n       [  0.,   0.,   0., ...,   0.,   0., 100.]])\n\nConstraints:\n\n>>> Y = np.eye(n, 3)\n\nInitial guess for eigenvectors, should have linearly independent\ncolumns. Column dimension = number of requested eigenvalues.\n\n>>> X = np.random.rand(n, 3)\n\nPreconditioner in the inverse of A in this example:\n\n>>> invA = spdiags([1./vals], 0, n, n)\n\nThe preconditiner must be defined by a function:\n\n>>> def precond( x ):\n...     return invA @ x\n\nThe argument x of the preconditioner function is a matrix inside `lobpcg`,\nthus the use of matrix-matrix product ``@``.\n\nThe preconditioner function is passed to lobpcg as a `LinearOperator`:\n\n>>> M = LinearOperator(matvec=precond, matmat=precond,\n...                    shape=(n, n), dtype=float)\n\nLet us now solve the eigenvalue problem for the matrix A:\n\n>>> eigenvalues, _ = lobpcg(A, X, Y=Y, M=M, largest=False)\n>>> eigenvalues\narray([4., 5., 6.])\n\nNote that the vectors passed in Y are the eigenvectors of the 3 smallest\neigenvalues. The results returned are orthogonal to those."
        }
      ]
    },
    {
      "name": "sklearn.externals._pep562",
      "imports": [
        {
          "module": "sys",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "__future__",
          "declaration": "unicode_literals",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "Pep562",
          "decorators": [],
          "superclasses": [
            "object"
          ],
          "methods": [],
          "fullDocstring": "Backport of PEP 562 <https://pypi.org/search/?q=pep562>.\n\nWraps the module in a class that exposes the mechanics to override `__dir__` and `__getattr__`.\nThe given module will be searched for overrides of `__dir__` and `__getattr__` and use them when needed."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.externals._pilutil",
      "imports": [
        {
          "module": "Image",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "PIL",
          "declaration": "Image",
          "alias": null
        },
        {
          "module": "__future__",
          "declaration": "absolute_import",
          "alias": null
        },
        {
          "module": "__future__",
          "declaration": "division",
          "alias": null
        },
        {
          "module": "__future__",
          "declaration": "print_function",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "amax",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "amin",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "arange",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "array",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "asarray",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "iscomplexobj",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "issubdtype",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "newaxis",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "ones",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "ravel",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "transpose",
          "alias": null
        },
        {
          "module": "numpy",
          "declaration": "uint8",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "bytescale",
          "decorators": [],
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "PIL image data array."
            },
            {
              "name": "cmin",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Bias scaling of small values. Default is ``data.min()``."
            },
            {
              "name": "cmax",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Bias scaling of large values. Default is ``data.max()``."
            },
            {
              "name": "high",
              "type": "Any",
              "hasDefault": true,
              "default": "255",
              "limitation": null,
              "ignored": false,
              "description": "Scale max value to `high`.  Default is 255."
            },
            {
              "name": "low",
              "type": "Any",
              "hasDefault": false,
              "default": "0",
              "limitation": null,
              "ignored": false,
              "description": "Scale min value to `low`.  Default is 0."
            }
          ],
          "results": [
            {
              "name": "img_array",
              "type": null,
              "description": "The byte-scaled array."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Byte scales an array (image).\n\nByte scaling means converting the input image to uint8 dtype and scaling\nthe range to ``(low, high)`` (default 0-255).\nIf the input image already has dtype uint8, no scaling is done.\n\nThis function is only available if Python Imaging Library (PIL) is installed.\n\nParameters\n----------\ndata : ndarray\n    PIL image data array.\ncmin : scalar, default=None\n    Bias scaling of small values. Default is ``data.min()``.\ncmax : scalar, default=None\n    Bias scaling of large values. Default is ``data.max()``.\nhigh : scalar, default=None\n    Scale max value to `high`.  Default is 255.\nlow : scalar, default=None\n    Scale min value to `low`.  Default is 0.\n\nReturns\n-------\nimg_array : uint8 ndarray\n    The byte-scaled array.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.misc import bytescale\n>>> img = np.array([[ 91.06794177,   3.39058326,  84.4221549 ],\n...                 [ 73.88003259,  80.91433048,   4.88878881],\n...                 [ 51.53875334,  34.45808177,  27.5873488 ]])\n>>> bytescale(img)\narray([[255,   0, 236],\n       [205, 225,   4],\n       [140,  90,  70]], dtype=uint8)\n>>> bytescale(img, high=200, low=100)\narray([[200, 100, 192],\n       [180, 188, 102],\n       [155, 135, 128]], dtype=uint8)\n>>> bytescale(img, cmin=0, cmax=255)\narray([[91,  3, 84],\n       [74, 81,  5],\n       [52, 34, 28]], dtype=uint8)"
        },
        {
          "name": "fromimage",
          "decorators": [],
          "parameters": [
            {
              "name": "im",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input image."
            },
            {
              "name": "flatten",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "If true, convert the output to grey-scale."
            },
            {
              "name": "mode",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Mode to convert image to, e.g. ``'RGB'``.  See the Notes of the\n`imread` docstring for more details."
            }
          ],
          "results": [
            {
              "name": "fromimage",
              "type": "NDArray",
              "description": "The different colour bands/channels are stored in the\nthird dimension, such that a grey-image is MxN, an\nRGB-image MxNx3 and an RGBA-image MxNx4."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Return a copy of a PIL image as a numpy array.\n\nThis function is only available if Python Imaging Library (PIL) is installed.\n\nParameters\n----------\nim : PIL image\n    Input image.\nflatten : bool, default=False\n    If true, convert the output to grey-scale.\nmode : str, default=None\n    Mode to convert image to, e.g. ``'RGB'``.  See the Notes of the\n    `imread` docstring for more details.\n\nReturns\n-------\nfromimage : ndarray\n    The different colour bands/channels are stored in the\n    third dimension, such that a grey-image is MxN, an\n    RGB-image MxNx3 and an RGBA-image MxNx4."
        },
        {
          "name": "imread",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The file name or file object to be read."
            },
            {
              "name": "flatten",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "If True, flattens the color layers into a single gray-scale layer."
            },
            {
              "name": "mode",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Mode to convert image to, e.g. ``'RGB'``.  See the Notes for more\ndetails."
            }
          ],
          "results": [
            {
              "name": "imread",
              "type": "NDArray",
              "description": "The array obtained by reading the image."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Read an image from a file as an array.\n\nThis function is only available if Python Imaging Library (PIL) is installed.\n\nParameters\n----------\nname : str or file object\n    The file name or file object to be read.\nflatten : bool, default=False\n    If True, flattens the color layers into a single gray-scale layer.\nmode : str, default=None\n    Mode to convert image to, e.g. ``'RGB'``.  See the Notes for more\n    details.\n\nReturns\n-------\nimread : ndarray\n    The array obtained by reading the image.\n\nNotes\n-----\n`imread` uses the Python Imaging Library (PIL) to read an image.\nThe following notes are from the PIL documentation.\n\n`mode` can be one of the following strings:\n\n* 'L' (8-bit pixels, black and white)\n* 'P' (8-bit pixels, mapped to any other mode using a color palette)\n* 'RGB' (3x8-bit pixels, true color)\n* 'RGBA' (4x8-bit pixels, true color with transparency mask)\n* 'CMYK' (4x8-bit pixels, color separation)\n* 'YCbCr' (3x8-bit pixels, color video format)\n* 'I' (32-bit signed integer pixels)\n* 'F' (32-bit floating point pixels)\n\nPIL also provides limited support for a few special modes, including\n'LA' ('L' with alpha), 'RGBX' (true color with padding) and 'RGBa'\n(true color with premultiplied alpha).\n\nWhen translating a color image to black and white (mode 'L', 'I' or\n'F'), the library uses the ITU-R 601-2 luma transform::\n\n    L = R * 299/1000 + G * 587/1000 + B * 114/1000\n\nWhen `flatten` is True, the image is converted using mode 'F'.\nWhen `mode` is not None and `flatten` is True, the image is first\nconverted according to `mode`, and the result is then flattened using\nmode 'F'."
        },
        {
          "name": "imresize",
          "decorators": [],
          "parameters": [
            {
              "name": "arr",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The array of image to be resized."
            },
            {
              "name": "size",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "* int   - Percentage of current size.\n* float - Fraction of current size.\n* tuple - Size of the output image (height, width)."
            },
            {
              "name": "interp",
              "type": "Any",
              "hasDefault": true,
              "default": "bilinear",
              "limitation": null,
              "ignored": false,
              "description": "Interpolation to use for re-sizing ('nearest', 'lanczos', 'bilinear',\n'bicubic' or 'cubic')."
            },
            {
              "name": "mode",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The PIL image mode ('P', 'L', etc.) to convert `arr` before resizing.\nIf ``mode=None`` (the default), 2-D images will be treated like\n``mode='L'``, i.e. casting to long integer.  For 3-D and 4-D arrays,\n`mode` will be set to ``'RGB'`` and ``'RGBA'`` respectively."
            }
          ],
          "results": [
            {
              "name": "imresize",
              "type": "NDArray",
              "description": "The resized array of image."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Resize an image.\n\nThis function is only available if Python Imaging Library (PIL) is installed.\n\n.. warning::\n\n    This function uses `bytescale` under the hood to rescale images to use\n    the full (0, 255) range if ``mode`` is one of ``None, 'L', 'P', 'l'``.\n    It will also cast data for 2-D images to ``uint32`` for ``mode=None``\n    (which is the default).\n\nParameters\n----------\narr : ndarray\n    The array of image to be resized.\nsize : int, float or tuple\n    * int   - Percentage of current size.\n    * float - Fraction of current size.\n    * tuple - Size of the output image (height, width).\n\ninterp : str, default='bilinear'\n    Interpolation to use for re-sizing ('nearest', 'lanczos', 'bilinear',\n    'bicubic' or 'cubic').\nmode : str, default=None\n    The PIL image mode ('P', 'L', etc.) to convert `arr` before resizing.\n    If ``mode=None`` (the default), 2-D images will be treated like\n    ``mode='L'``, i.e. casting to long integer.  For 3-D and 4-D arrays,\n    `mode` will be set to ``'RGB'`` and ``'RGBA'`` respectively.\n\nReturns\n-------\nimresize : ndarray\n    The resized array of image.\n\nSee Also\n--------\ntoimage : Implicitly used to convert `arr` according to `mode`.\nscipy.ndimage.zoom : More generic implementation that does not use PIL."
        },
        {
          "name": "imsave",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Output file name or file object."
            },
            {
              "name": "arr",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array containing image values.  If the shape is ``MxN``, the array\nrepresents a grey-level image.  Shape ``MxNx3`` stores the red, green\nand blue bands along the last dimension.  An alpha layer may be\nincluded, specified as the last colour band of an ``MxNx4`` array."
            },
            {
              "name": "format",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Image format. If omitted, the format to use is determined from the\nfile name extension. If a file object was used instead of a file name,\nthis parameter should always be used."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Save an array as an image.\n\nThis function is only available if Python Imaging Library (PIL) is installed.\n\n.. warning::\n\n    This function uses `bytescale` under the hood to rescale images to use\n    the full (0, 255) range if ``mode`` is one of ``None, 'L', 'P', 'l'``.\n    It will also cast data for 2-D images to ``uint32`` for ``mode=None``\n    (which is the default).\n\nParameters\n----------\nname : str or file object\n    Output file name or file object.\narr : ndarray, MxN or MxNx3 or MxNx4\n    Array containing image values.  If the shape is ``MxN``, the array\n    represents a grey-level image.  Shape ``MxNx3`` stores the red, green\n    and blue bands along the last dimension.  An alpha layer may be\n    included, specified as the last colour band of an ``MxNx4`` array.\nformat : str, default=None\n    Image format. If omitted, the format to use is determined from the\n    file name extension. If a file object was used instead of a file name,\n    this parameter should always be used.\n\nExamples\n--------\nConstruct an array of gradient intensity values and save to file:\n\n>>> import numpy as np\n>>> from scipy.misc import imsave\n>>> x = np.zeros((255, 255))\n>>> x = np.zeros((255, 255), dtype=np.uint8)\n>>> x[:] = np.arange(255)\n>>> imsave('gradient.png', x)\n\nConstruct an array with three colour bands (R, G, B) and store to file:\n\n>>> rgb = np.zeros((255, 255, 3), dtype=np.uint8)\n>>> rgb[..., 0] = np.arange(255)\n>>> rgb[..., 1] = 55\n>>> rgb[..., 2] = 1 - np.arange(255)\n>>> imsave('rgb_gradient.png', rgb)"
        },
        {
          "name": "toimage",
          "decorators": [],
          "parameters": [
            {
              "name": "arr",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "high",
              "type": "Any",
              "hasDefault": true,
              "default": "255",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "low",
              "type": "Any",
              "hasDefault": false,
              "default": "0",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "cmin",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "cmax",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "pal",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "mode",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "channel_axis",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Takes a numpy array and returns a PIL image.\n\nThis function is only available if Python Imaging Library (PIL) is installed.\n\nThe mode of the PIL image depends on the array shape and the `pal` and\n`mode` keywords.\n\nFor 2-D arrays, if `pal` is a valid (N,3) byte-array giving the RGB values\n(from 0 to 255) then ``mode='P'``, otherwise ``mode='L'``, unless mode\nis given as 'F' or 'I' in which case a float and/or integer array is made.\n\n.. warning::\n\n    This function uses `bytescale` under the hood to rescale images to use\n    the full (0, 255) range if ``mode`` is one of ``None, 'L', 'P', 'l'``.\n    It will also cast data for 2-D images to ``uint32`` for ``mode=None``\n    (which is the default).\n\nNotes\n-----\nFor 3-D arrays, the `channel_axis` argument tells which dimension of the\narray holds the channel data.\n\nFor 3-D arrays if one of the dimensions is 3, the mode is 'RGB'\nby default or 'YCbCr' if selected.\n\nThe numpy array must be either 2 dimensional or 3 dimensional."
        }
      ]
    },
    {
      "name": "sklearn.externals.conftest",
      "imports": [],
      "fromImports": [],
      "classes": [],
      "functions": [
        {
          "name": "pytest_ignore_collect",
          "decorators": [],
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "config",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.feature_extraction",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn",
          "declaration": "text",
          "alias": null
        },
        {
          "module": "sklearn._dict_vectorizer",
          "declaration": "DictVectorizer",
          "alias": null
        },
        {
          "module": "sklearn._hash",
          "declaration": "FeatureHasher",
          "alias": null
        },
        {
          "module": "sklearn.image",
          "declaration": "grid_to_graph",
          "alias": null
        },
        {
          "module": "sklearn.image",
          "declaration": "img_to_graph",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.feature_extraction._dict_vectorizer",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "array",
          "declaration": "array",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Iterable",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Mapping",
          "alias": null
        },
        {
          "module": "numbers",
          "declaration": "Number",
          "alias": null
        },
        {
          "module": "operator",
          "declaration": "itemgetter",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "tosequence",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "DictVectorizer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Dict(s) or Mapping(s) from feature names (arbitrary Python\nobjects) to feature values (strings or convertible to dtype).\n\n.. versionchanged:: 0.24\n   Accepts multiple string values for one categorical feature."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn a list of feature name -> indices mappings.\n\nParameters\n----------\nX : Mapping or iterable over Mappings\n    Dict(s) or Mapping(s) from feature names (arbitrary Python\n    objects) to feature values (strings or convertible to dtype).\n\n    .. versionchanged:: 0.24\n       Accepts multiple string values for one categorical feature.\n\ny : (ignored)\n\nReturns\n-------\nself"
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Dict(s) or Mapping(s) from feature names (arbitrary Python\nobjects) to feature values (strings or convertible to dtype).\n\n.. versionchanged:: 0.24\n   Accepts multiple string values for one categorical feature."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Xa",
                  "type": null,
                  "description": "Feature vectors; always 2-d."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn a list of feature name -> indices mappings and transform X.\n\nLike fit(X) followed by transform(X), but does not require\nmaterializing X in memory.\n\nParameters\n----------\nX : Mapping or iterable over Mappings\n    Dict(s) or Mapping(s) from feature names (arbitrary Python\n    objects) to feature values (strings or convertible to dtype).\n\n    .. versionchanged:: 0.24\n       Accepts multiple string values for one categorical feature.\n\ny : (ignored)\n\nReturns\n-------\nXa : {array, sparse matrix}\n    Feature vectors; always 2-d."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample matrix."
                },
                {
                  "name": "dict_type",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Constructor for feature mappings. Must conform to the\ncollections.Mapping API."
                }
              ],
              "results": [
                {
                  "name": "D",
                  "type": null,
                  "description": "Feature mappings for the samples in X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform array or sparse matrix X back to feature mappings.\n\nX must have been produced by this DictVectorizer's transform or\nfit_transform method; it may only have passed through transformers\nthat preserve the number of features and their order.\n\nIn the case of one-hot/one-of-K coding, the constructed feature\nnames and values are returned rather than the original ones.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Sample matrix.\ndict_type : type, default=dict\n    Constructor for feature mappings. Must conform to the\n    collections.Mapping API.\n\nReturns\n-------\nD : list of dict_type objects of shape (n_samples,)\n    Feature mappings for the samples in X."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Dict(s) or Mapping(s) from feature names (arbitrary Python\nobjects) to feature values (strings or convertible to dtype)."
                }
              ],
              "results": [
                {
                  "name": "Xa",
                  "type": null,
                  "description": "Feature vectors; always 2-d."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform feature->value dicts to array or sparse matrix.\n\nNamed features not encountered during fit or fit_transform will be\nsilently ignored.\n\nParameters\n----------\nX : Mapping or iterable over Mappings of shape (n_samples,)\n    Dict(s) or Mapping(s) from feature names (arbitrary Python\n    objects) to feature values (strings or convertible to dtype).\n\nReturns\n-------\nXa : {array, sparse matrix}\n    Feature vectors; always 2-d."
            },
            {
              "name": "get_feature_names",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns a list of feature names, ordered by their indices.\n\nIf one-of-K coding is applied to categorical features, this will\ninclude the constructed feature names but not the original ones."
            },
            {
              "name": "restrict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "support",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Boolean mask or list of indices (as returned by the get_support\nmember of feature selectors)."
                },
                {
                  "name": "indices",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "Whether support is a list of indices."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Restrict the features to those in support using feature selection.\n\nThis function modifies the estimator in-place.\n\nParameters\n----------\nsupport : array-like\n    Boolean mask or list of indices (as returned by the get_support\n    member of feature selectors).\nindices : bool, default=False\n    Whether support is a list of indices.\n\nReturns\n-------\nself\n\nExamples\n--------\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> from sklearn.feature_selection import SelectKBest, chi2\n>>> v = DictVectorizer()\n>>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n>>> X = v.fit_transform(D)\n>>> support = SelectKBest(chi2, k=2).fit(X, [0, 1])\n>>> v.get_feature_names()\n['bar', 'baz', 'foo']\n>>> v.restrict(support.get_support())\nDictVectorizer()\n>>> v.get_feature_names()\n['bar', 'foo']"
            }
          ],
          "fullDocstring": "Transforms lists of feature-value mappings to vectors.\n\nThis transformer turns lists of mappings (dict-like objects) of feature\nnames to feature values into Numpy arrays or scipy.sparse matrices for use\nwith scikit-learn estimators.\n\nWhen feature values are strings, this transformer will do a binary one-hot\n(aka one-of-K) coding: one boolean-valued feature is constructed for each\nof the possible string values that the feature can take on. For instance,\na feature \"f\" that can take on the values \"ham\" and \"spam\" will become two\nfeatures in the output, one signifying \"f=ham\", the other \"f=spam\".\n\nIf a feature value is a sequence or set of strings, this transformer\nwill iterate over the values and will count the occurrences of each string\nvalue.\n\nHowever, note that this transformer will only do a binary one-hot encoding\nwhen feature values are of type string. If categorical features are\nrepresented as numeric values such as int or iterables of strings, the\nDictVectorizer can be followed by\n:class:`~sklearn.preprocessing.OneHotEncoder` to complete\nbinary one-hot encoding.\n\nFeatures that do not occur in a sample (mapping) will have a zero value\nin the resulting array/matrix.\n\nRead more in the :ref:`User Guide <dict_feature_extraction>`.\n\nParameters\n----------\ndtype : dtype, default=np.float64\n    The type of feature values. Passed to Numpy array/scipy.sparse matrix\n    constructors as the dtype argument.\nseparator : str, default=\"=\"\n    Separator string used when constructing new features for one-hot\n    coding.\nsparse : bool, default=True\n    Whether transform should produce scipy.sparse matrices.\nsort : bool, default=True\n    Whether ``feature_names_`` and ``vocabulary_`` should be\n    sorted when fitting.\n\nAttributes\n----------\nvocabulary_ : dict\n    A dictionary mapping feature names to feature indices.\n\nfeature_names_ : list\n    A list of length n_features containing the feature names (e.g., \"f=ham\"\n    and \"f=spam\").\n\nExamples\n--------\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> v = DictVectorizer(sparse=False)\n>>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n>>> X = v.fit_transform(D)\n>>> X\narray([[2., 0., 1.],\n       [0., 1., 3.]])\n>>> v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0},\n...                            {'baz': 1.0, 'foo': 3.0}]\nTrue\n>>> v.transform({'foo': 4, 'unseen_feature': 3})\narray([[0., 0., 4.]])\n\nSee Also\n--------\nFeatureHasher : Performs vectorization using only a hash function.\nsklearn.preprocessing.OrdinalEncoder : Handles nominal/categorical\n    features encoded as columns of arbitrary data types."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.feature_extraction._hash",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.feature_extraction._hashing_fast",
          "declaration": "transform",
          "alias": "_hashing_transform"
        },
        {
          "module": "sklearn.utils",
          "declaration": "IS_PYPY",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "FeatureHasher",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "No-op.\n\nThis method doesn't do anything. It exists purely for compatibility\nwith the scikit-learn transformer API.\n\nParameters\n----------\nX : ndarray\n\nReturns\n-------\nself : FeatureHasher"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples. Each sample must be iterable an (e.g., a list or tuple)\ncontaining/generating feature names (and optionally values, see\nthe input_type constructor argument) which will be hashed.\nraw_X need not support the len function, so it can be the result\nof a generator; n_samples is determined on the fly."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Feature matrix, for use with estimators or further transformers."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform a sequence of instances to a scipy.sparse matrix.\n\nParameters\n----------\nraw_X : iterable over iterable over raw features, length = n_samples\n    Samples. Each sample must be iterable an (e.g., a list or tuple)\n    containing/generating feature names (and optionally values, see\n    the input_type constructor argument) which will be hashed.\n    raw_X need not support the len function, so it can be the result\n    of a generator; n_samples is determined on the fly.\n\nReturns\n-------\nX : sparse matrix of shape (n_samples, n_features)\n    Feature matrix, for use with estimators or further transformers."
            }
          ],
          "fullDocstring": "Implements feature hashing, aka the hashing trick.\n\nThis class turns sequences of symbolic feature names (strings) into\nscipy.sparse matrices, using a hash function to compute the matrix column\ncorresponding to a name. The hash function employed is the signed 32-bit\nversion of Murmurhash3.\n\nFeature names of type byte string are used as-is. Unicode strings are\nconverted to UTF-8 first, but no Unicode normalization is done.\nFeature values must be (finite) numbers.\n\nThis class is a low-memory alternative to DictVectorizer and\nCountVectorizer, intended for large-scale (online) learning and situations\nwhere memory is tight, e.g. when running prediction code on embedded\ndevices.\n\nRead more in the :ref:`User Guide <feature_hashing>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nn_features : int, default=2**20\n    The number of features (columns) in the output matrices. Small numbers\n    of features are likely to cause hash collisions, but large numbers\n    will cause larger coefficient dimensions in linear learners.\ninput_type : {\"dict\", \"pair\", \"string\"}, default=\"dict\"\n    Either \"dict\" (the default) to accept dictionaries over\n    (feature_name, value); \"pair\" to accept pairs of (feature_name, value);\n    or \"string\" to accept single strings.\n    feature_name should be a string, while value should be a number.\n    In the case of \"string\", a value of 1 is implied.\n    The feature_name is hashed to find the appropriate column for the\n    feature. The value's sign might be flipped in the output (but see\n    non_negative, below).\ndtype : numpy dtype, default=np.float64\n    The type of feature values. Passed to scipy.sparse matrix constructors\n    as the dtype argument. Do not set this to bool, np.boolean or any\n    unsigned integer type.\nalternate_sign : bool, default=True\n    When True, an alternating sign is added to the features as to\n    approximately conserve the inner product in the hashed space even for\n    small n_features. This approach is similar to sparse random projection.\n\n.. versionchanged:: 0.19\n    ``alternate_sign`` replaces the now deprecated ``non_negative``\n    parameter.\n\nExamples\n--------\n>>> from sklearn.feature_extraction import FeatureHasher\n>>> h = FeatureHasher(n_features=10)\n>>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n>>> f = h.transform(D)\n>>> f.toarray()\narray([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\n       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])\n\nSee Also\n--------\nDictVectorizer : Vectorizes string-valued features using a hash table.\nsklearn.preprocessing.OneHotEncoder : Handles nominal/categorical features."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.feature_extraction._stop_words",
      "imports": [],
      "fromImports": [],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.feature_extraction.image",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "itertools",
          "declaration": "product",
          "alias": null
        },
        {
          "module": "numpy.lib.stride_tricks",
          "declaration": "as_strided",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "PatchExtractor",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Do nothing and return the estimator unchanged.\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of images from which to extract patches. For color images,\nthe last dimension specifies the channel: a RGB image would have\n`n_channels=3`."
                }
              ],
              "results": [
                {
                  "name": "patches",
                  "type": null,
                  "description": "The collection of patches extracted from the images, where\n`n_patches` is either `n_samples * max_patches` or the total\nnumber of patches that can be extracted."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transforms the image samples in X into a matrix of patch data.\n\nParameters\n----------\nX : ndarray of shape (n_samples, image_height, image_width) or             (n_samples, image_height, image_width, n_channels)\n    Array of images from which to extract patches. For color images,\n    the last dimension specifies the channel: a RGB image would have\n    `n_channels=3`.\n\nReturns\n-------\npatches : array of shape (n_patches, patch_height, patch_width) or              (n_patches, patch_height, patch_width, n_channels)\n     The collection of patches extracted from the images, where\n     `n_patches` is either `n_samples * max_patches` or the total\n     number of patches that can be extracted."
            }
          ],
          "fullDocstring": "Extracts patches from a collection of images\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\n.. versionadded:: 0.9\n\nParameters\n----------\npatch_size : tuple of int (patch_height, patch_width), default=None\n    The dimensions of one patch.\n\nmax_patches : int or float, default=None\n    The maximum number of patches per image to extract. If max_patches is a\n    float in (0, 1), it is taken to mean a proportion of the total number\n    of patches.\n\nrandom_state : int, RandomState instance, default=None\n    Determines the random number generator used for random sampling when\n    `max_patches` is not None. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> from sklearn.datasets import load_sample_images\n>>> from sklearn.feature_extraction import image\n>>> # Use the array data from the second image in this dataset:\n>>> X = load_sample_images().images[1]\n>>> print('Image shape: {}'.format(X.shape))\nImage shape: (427, 640, 3)\n>>> pe = image.PatchExtractor(patch_size=(2, 2))\n>>> pe_fit = pe.fit(X)\n>>> pe_trans = pe.transform(X)\n>>> print('Patches shape: {}'.format(pe_trans.shape))\nPatches shape: (545706, 2, 2)"
        }
      ],
      "functions": [
        {
          "name": "extract_patches_2d",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "image",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The original image data. For color images, the last dimension specifies\nthe channel: a RGB image would have `n_channels=3`."
            },
            {
              "name": "patch_size",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The dimensions of one patch."
            }
          ],
          "results": [
            {
              "name": "patches",
              "type": null,
              "description": "The collection of patches extracted from the image, where `n_patches`\nis either `max_patches` or the total number of patches that can be\nextracted."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Reshape a 2D image into a collection of patches\n\nThe resulting patches are allocated in a dedicated array.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\nParameters\n----------\nimage : ndarray of shape (image_height, image_width) or         (image_height, image_width, n_channels)\n    The original image data. For color images, the last dimension specifies\n    the channel: a RGB image would have `n_channels=3`.\n\npatch_size : tuple of int (patch_height, patch_width)\n    The dimensions of one patch.\n\nmax_patches : int or float, default=None\n    The maximum number of patches to extract. If `max_patches` is a float\n    between 0 and 1, it is taken to be a proportion of the total number\n    of patches.\n\nrandom_state : int, RandomState instance, default=None\n    Determines the random number generator used for random sampling when\n    `max_patches` is not None. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\npatches : array of shape (n_patches, patch_height, patch_width) or         (n_patches, patch_height, patch_width, n_channels)\n    The collection of patches extracted from the image, where `n_patches`\n    is either `max_patches` or the total number of patches that can be\n    extracted.\n\nExamples\n--------\n>>> from sklearn.datasets import load_sample_image\n>>> from sklearn.feature_extraction import image\n>>> # Use the array data from the first image in this dataset:\n>>> one_image = load_sample_image(\"china.jpg\")\n>>> print('Image shape: {}'.format(one_image.shape))\nImage shape: (427, 640, 3)\n>>> patches = image.extract_patches_2d(one_image, (2, 2))\n>>> print('Patches shape: {}'.format(patches.shape))\nPatches shape: (272214, 2, 2, 3)\n>>> # Here are just two of these patches:\n>>> print(patches[1])\n[[[174 201 231]\n  [174 201 231]]\n [[173 200 230]\n  [173 200 230]]]\n>>> print(patches[800])\n[[[187 214 243]\n  [188 215 244]]\n [[187 214 243]\n  [188 215 244]]]"
        },
        {
          "name": "grid_to_graph",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_x",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Dimension in x axis"
            },
            {
              "name": "n_y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Dimension in y axis"
            },
            {
              "name": "n_z",
              "type": "Any",
              "hasDefault": true,
              "default": "1",
              "limitation": null,
              "ignored": false,
              "description": "Dimension in z axis"
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Graph of the pixel-to-pixel connections\n\nEdges exist if 2 voxels are connected.\n\nParameters\n----------\nn_x : int\n    Dimension in x axis\nn_y : int\n    Dimension in y axis\nn_z : int, default=1\n    Dimension in z axis\nmask : ndarray of shape (n_x, n_y, n_z), dtype=bool, default=None\n    An optional mask of the image, to consider only part of the\n    pixels.\nreturn_as : np.ndarray or a sparse matrix class,             default=sparse.coo_matrix\n    The class to use to build the returned adjacency matrix.\ndtype : dtype, default=int\n    The data of the returned sparse matrix. By default it is int\n\nNotes\n-----\nFor scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was\nhandled by returning a dense np.matrix instance.  Going forward, np.ndarray\nreturns an np.ndarray, as expected.\n\nFor compatibility, user code relying on this method should wrap its\ncalls in ``np.asarray`` to avoid type issues."
        },
        {
          "name": "img_to_graph",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "img",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "2D or 3D image."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Graph of the pixel-to-pixel gradient connections\n\nEdges are weighted with the gradient values.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\nParameters\n----------\nimg : ndarray of shape (height, width) or (height, width, channel)\n    2D or 3D image.\nmask : ndarray of shape (height, width) or             (height, width, channel), dtype=bool, default=None\n    An optional mask of the image, to consider only part of the\n    pixels.\nreturn_as : np.ndarray or a sparse matrix class,             default=sparse.coo_matrix\n    The class to use to build the returned adjacency matrix.\ndtype : dtype, default=None\n    The data of the returned sparse matrix. By default it is the\n    dtype of img\n\nNotes\n-----\nFor scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was\nhandled by returning a dense np.matrix instance.  Going forward, np.ndarray\nreturns an np.ndarray, as expected.\n\nFor compatibility, user code relying on this method should wrap its\ncalls in ``np.asarray`` to avoid type issues."
        },
        {
          "name": "reconstruct_from_patches_2d",
          "decorators": [],
          "parameters": [
            {
              "name": "patches",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The complete set of patches. If the patches contain colour information,\nchannels are indexed along the last dimension: RGB patches would\nhave `n_channels=3`."
            },
            {
              "name": "image_size",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The size of the image that will be reconstructed."
            }
          ],
          "results": [
            {
              "name": "image",
              "type": null,
              "description": "The reconstructed image."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Reconstruct the image from all of its patches.\n\nPatches are assumed to overlap and the image is constructed by filling in\nthe patches from left to right, top to bottom, averaging the overlapping\nregions.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\nParameters\n----------\npatches : ndarray of shape (n_patches, patch_height, patch_width) or         (n_patches, patch_height, patch_width, n_channels)\n    The complete set of patches. If the patches contain colour information,\n    channels are indexed along the last dimension: RGB patches would\n    have `n_channels=3`.\n\nimage_size : tuple of int (image_height, image_width) or         (image_height, image_width, n_channels)\n    The size of the image that will be reconstructed.\n\nReturns\n-------\nimage : ndarray of shape image_size\n    The reconstructed image."
        }
      ]
    },
    {
      "name": "sklearn.feature_extraction.setup",
      "imports": [
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "platform",
          "alias": null
        }
      ],
      "fromImports": [],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.feature_extraction.text",
      "imports": [
        {
          "module": "array",
          "alias": null
        },
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "re",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "unicodedata",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "defaultdict",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Mapping",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "operator",
          "declaration": "itemgetter",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.feature_extraction._hash",
          "declaration": "FeatureHasher",
          "alias": null
        },
        {
          "module": "sklearn.feature_extraction._stop_words",
          "declaration": "ENGLISH_STOP_WORDS",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "normalize",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_IS_32BIT",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_astype_copy_false",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "FLOAT_DTYPES",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "CountVectorizer",
          "decorators": [],
          "superclasses": [
            "_VectorizerMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_documents",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "An iterable which yields either str, unicode or file objects."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn a vocabulary dictionary of all tokens in the raw documents.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\n\nReturns\n-------\nself"
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_documents",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "An iterable which yields either str, unicode or file objects."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Document-term matrix."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn the vocabulary dictionary and return document-term matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\n\nReturns\n-------\nX : array of shape (n_samples, n_features)\n    Document-term matrix."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_documents",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "An iterable which yields either str, unicode or file objects."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Document-term matrix."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform documents to document-term matrix.\n\nExtract token counts out of raw text documents using the vocabulary\nfitted with fit or the one provided to the constructor.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\n\nReturns\n-------\nX : sparse matrix of shape (n_samples, n_features)\n    Document-term matrix."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Document-term matrix."
                }
              ],
              "results": [
                {
                  "name": "X_inv",
                  "type": null,
                  "description": "List of arrays of terms."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return terms per document with nonzero entries in X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document-term matrix.\n\nReturns\n-------\nX_inv : list of arrays of shape (n_samples,)\n    List of arrays of terms."
            },
            {
              "name": "get_feature_names",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "feature_names",
                  "type": "List",
                  "description": "A list of feature names."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Array mapping from feature integer indices to feature name.\n\nReturns\n-------\nfeature_names : list\n    A list of feature names."
            }
          ],
          "fullDocstring": "Convert a collection of text documents to a matrix of token counts\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : string {'filename', 'file', 'content'}, default='content'\n    If 'filename', the sequence passed as an argument to fit is\n    expected to be a list of filenames that need reading to fetch\n    the raw content to analyze.\n\n    If 'file', the sequence items must have a 'read' method (file-like\n    object) that is called to fetch the bytes in memory.\n\n    Otherwise the input is expected to be a sequence of items that\n    can be of type string or byte.\n\nencoding : string, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'}, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    an direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) does nothing.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (strip_accents and lowercase) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer is not callable``.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : string {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. max_df can be set to a value\n    in the range [0.7, 1.0) to automatically detect and filter stop\n    words based on intra corpus document frequency of terms.\n\ntoken_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp select tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    word n-grams or char n-grams to be extracted. All values of n such\n    such that min_n <= n <= max_n will be used. For example an\n    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n    Only applies if ``analyzer is not callable``.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word n-gram or character\n    n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nmax_df : float in range [0.0, 1.0] or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float in range [0.0, 1.0] or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    max_features ordered by term frequency across the corpus.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents. Indices\n    in the mapping should not be repeated and should not have any gap\n    between 0 and the largest index.\n\nbinary : bool, default=False\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\ndtype : type, default=np.int64\n    Type of the matrix returned by fit_transform() or transform().\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_: boolean\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = CountVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> print(vectorizer.get_feature_names())\n['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n>>> print(X.toarray())\n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n>>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n>>> X2 = vectorizer2.fit_transform(corpus)\n>>> print(vectorizer2.get_feature_names())\n['and this', 'document is', 'first document', 'is the', 'is this',\n'second document', 'the first', 'the second', 'the third', 'third one',\n 'this document', 'this is', 'this the']\n >>> print(X2.toarray())\n [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n [0 1 0 1 0 1 0 1 0 0 1 0 0]\n [1 0 0 1 0 0 0 0 1 1 0 1 0]\n [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n\nSee Also\n--------\nHashingVectorizer, TfidfVectorizer\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling."
        },
        {
          "name": "HashingVectorizer",
          "decorators": [],
          "superclasses": [
            "_VectorizerMixin",
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Does nothing: this transformer is stateless.\n\nThis method is just there to mark the fact that this transformer\ncan work in a streaming setup.\n\nParameters\n----------\nX : ndarray of shape [n_samples, n_features]\n    Training data."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Does nothing: this transformer is stateless.\n\nParameters\n----------\nX : ndarray of shape [n_samples, n_features]\n    Training data."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples. Each sample must be a text document (either bytes or\nunicode strings, file name or file object depending on the\nconstructor argument) which will be tokenized and hashed."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Document-term matrix."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform a sequence of documents to a document-term matrix.\n\nParameters\n----------\nX : iterable over raw text documents, length = n_samples\n    Samples. Each sample must be a text document (either bytes or\n    unicode strings, file name or file object depending on the\n    constructor argument) which will be tokenized and hashed.\n\nReturns\n-------\nX : sparse matrix of shape (n_samples, n_features)\n    Document-term matrix."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples. Each sample must be a text document (either bytes or\nunicode strings, file name or file object depending on the\nconstructor argument) which will be tokenized and hashed."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored. This parameter exists only for compatibility with\nsklearn.pipeline.Pipeline."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Document-term matrix."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform a sequence of documents to a document-term matrix.\n\nParameters\n----------\nX : iterable over raw text documents, length = n_samples\n    Samples. Each sample must be a text document (either bytes or\n    unicode strings, file name or file object depending on the\n    constructor argument) which will be tokenized and hashed.\ny : any\n    Ignored. This parameter exists only for compatibility with\n    sklearn.pipeline.Pipeline.\n\nReturns\n-------\nX : sparse matrix of shape (n_samples, n_features)\n    Document-term matrix."
            }
          ],
          "fullDocstring": "Convert a collection of text documents to a matrix of token occurrences\n\nIt turns a collection of text documents into a scipy.sparse matrix holding\ntoken occurrence counts (or binary occurrence information), possibly\nnormalized as token frequencies if norm='l1' or projected on the euclidean\nunit sphere if norm='l2'.\n\nThis text vectorizer implementation uses the hashing trick to find the\ntoken string name to feature integer index mapping.\n\nThis strategy has several advantages:\n\n- it is very low memory scalable to large datasets as there is no need to\n  store a vocabulary dictionary in memory\n\n- it is fast to pickle and un-pickle as it holds no state besides the\n  constructor parameters\n\n- it can be used in a streaming (partial fit) or parallel pipeline as there\n  is no state computed during fit.\n\nThere are also a couple of cons (vs using a CountVectorizer with an\nin-memory vocabulary):\n\n- there is no way to compute the inverse transform (from feature indices to\n  string feature names) which can be a problem when trying to introspect\n  which features are most important to a model.\n\n- there can be collisions: distinct tokens can be mapped to the same\n  feature index. However in practice this is rarely an issue if n_features\n  is large enough (e.g. 2 ** 18 for text classification problems).\n\n- no IDF weighting as this would render the transformer stateful.\n\nThe hash function employed is the signed 32-bit version of Murmurhash3.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\n\ninput : string {'filename', 'file', 'content'}, default='content'\n    If 'filename', the sequence passed as an argument to fit is\n    expected to be a list of filenames that need reading to fetch\n    the raw content to analyze.\n\n    If 'file', the sequence items must have a 'read' method (file-like\n    object) that is called to fetch the bytes in memory.\n\n    Otherwise the input is expected to be a sequence of items that\n    can be of type string or byte.\n\nencoding : string, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'}, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    an direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) does nothing.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (string transformation) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer is not callable``.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : string {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\ntoken_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n    only bigrams.\n    Only applies if ``analyzer is not callable``.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word or character n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nn_features : int, default=(2 ** 20)\n    The number of features (columns) in the output matrices. Small numbers\n    of features are likely to cause hash collisions, but large numbers\n    will cause larger coefficient dimensions in linear learners.\n\nbinary : bool, default=False.\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\nnorm : {'l1', 'l2'}, default='l2'\n    Norm used to normalize term vectors. None for no normalization.\n\nalternate_sign : bool, default=True\n    When True, an alternating sign is added to the features as to\n    approximately conserve the inner product in the hashed space even for\n    small n_features. This approach is similar to sparse random projection.\n\n    .. versionadded:: 0.19\n\ndtype : type, default=np.float64\n    Type of the matrix returned by fit_transform() or transform().\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import HashingVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = HashingVectorizer(n_features=2**4)\n>>> X = vectorizer.fit_transform(corpus)\n>>> print(X.shape)\n(4, 16)\n\nSee Also\n--------\nCountVectorizer, TfidfVectorizer"
        },
        {
          "name": "TfidfTransformer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "A matrix of term/token counts."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn the idf vector (global term weights).\n\nParameters\n----------\nX : sparse matrix of shape n_samples, n_features)\n    A matrix of term/token counts."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "a matrix of term/token counts"
                },
                {
                  "name": "copy",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Whether to copy X and operate on the copy or perform in-place\noperations."
                }
              ],
              "results": [
                {
                  "name": "vectors",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform a count matrix to a tf or tf-idf representation\n\nParameters\n----------\nX : sparse matrix of (n_samples, n_features)\n    a matrix of term/token counts\n\ncopy : bool, default=True\n    Whether to copy X and operate on the copy or perform in-place\n    operations.\n\nReturns\n-------\nvectors : sparse matrix of shape (n_samples, n_features)"
            },
            {
              "name": "idf_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "value",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Transform a count matrix to a normalized tf or tf-idf representation\n\nTf means term-frequency while tf-idf means term-frequency times inverse\ndocument-frequency. This is a common term weighting scheme in information\nretrieval, that has also found good use in document classification.\n\nThe goal of using tf-idf instead of the raw frequencies of occurrence of a\ntoken in a given document is to scale down the impact of tokens that occur\nvery frequently in a given corpus and that are hence empirically less\ninformative than features that occur in a small fraction of the training\ncorpus.\n\nThe formula that is used to compute the tf-idf for a term t of a document d\nin a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\ncomputed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\nn is the total number of documents in the document set and df(t) is the\ndocument frequency of t; the document frequency is the number of documents\nin the document set that contain the term t. The effect of adding \"1\" to\nthe idf in the equation above is that terms with zero idf, i.e., terms\nthat occur in all documents in a training set, will not be entirely\nignored.\n(Note that the idf formula above differs from the standard textbook\nnotation that defines the idf as\nidf(t) = log [ n / (df(t) + 1) ]).\n\nIf ``smooth_idf=True`` (the default), the constant \"1\" is added to the\nnumerator and denominator of the idf as if an extra document was seen\ncontaining every term in the collection exactly once, which prevents\nzero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.\n\nFurthermore, the formulas used to compute tf and idf depend\non parameter settings that correspond to the SMART notation used in IR\nas follows:\n\nTf is \"n\" (natural) by default, \"l\" (logarithmic) when\n``sublinear_tf=True``.\nIdf is \"t\" when use_idf is given, \"n\" (none) otherwise.\nNormalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\nwhen ``norm=None``.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\nnorm : {'l1', 'l2'}, default='l2'\n    Each output row will have unit norm, either:\n    * 'l2': Sum of squares of vector elements is 1. The cosine\n    similarity between two vectors is their dot product when l2 norm has\n    been applied.\n    * 'l1': Sum of absolute values of vector elements is 1.\n    See :func:`preprocessing.normalize`\n\nuse_idf : bool, default=True\n    Enable inverse-document-frequency reweighting.\n\nsmooth_idf : bool, default=True\n    Smooth idf weights by adding one to document frequencies, as if an\n    extra document was seen containing every term in the collection\n    exactly once. Prevents zero divisions.\n\nsublinear_tf : bool, default=False\n    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\nAttributes\n----------\nidf_ : array of shape (n_features)\n    The inverse document frequency (IDF) vector; only defined\n    if  ``use_idf`` is True.\n\n    .. versionadded:: 0.20\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import TfidfTransformer\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> from sklearn.pipeline import Pipeline\n>>> import numpy as np\n>>> corpus = ['this is the first document',\n...           'this document is the second document',\n...           'and this is the third one',\n...           'is this the first document']\n>>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n...               'and', 'one']\n>>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n...                  ('tfid', TfidfTransformer())]).fit(corpus)\n>>> pipe['count'].transform(corpus).toarray()\narray([[1, 1, 1, 1, 0, 1, 0, 0],\n       [1, 2, 0, 1, 1, 1, 0, 0],\n       [1, 0, 0, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 0, 1, 0, 0]])\n>>> pipe['tfid'].idf_\narray([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n       1.        , 1.91629073, 1.91629073])\n>>> pipe.transform(corpus).shape\n(4, 8)\n\nReferences\n----------\n\n.. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n               Information Retrieval. Addison Wesley, pp. 68-74.\n\n.. [MRS2008] C.D. Manning, P. Raghavan and H. Sch\u00fctze  (2008).\n               Introduction to Information Retrieval. Cambridge University\n               Press, pp. 118-120."
        },
        {
          "name": "TfidfVectorizer",
          "decorators": [],
          "superclasses": [
            "CountVectorizer"
          ],
          "methods": [
            {
              "name": "norm",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "value",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "use_idf",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "value",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "smooth_idf",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "value",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "sublinear_tf",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "value",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "idf_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "value",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_documents",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "An iterable which yields either str, unicode or file objects."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "This parameter is not needed to compute tfidf."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted vectorizer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn vocabulary and idf from training set.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\ny : None\n    This parameter is not needed to compute tfidf.\n\nReturns\n-------\nself : object\n    Fitted vectorizer."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_documents",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "An iterable which yields either str, unicode or file objects."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "This parameter is ignored."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Tf-idf-weighted document-term matrix."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn vocabulary and idf, return document-term matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\ny : None\n    This parameter is ignored.\n\nReturns\n-------\nX : sparse matrix of (n_samples, n_features)\n    Tf-idf-weighted document-term matrix."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "raw_documents",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "An iterable which yields either str, unicode or file objects."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Tf-idf-weighted document-term matrix."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform documents to document-term matrix.\n\nUses the vocabulary and document frequencies (df) learned by fit (or\nfit_transform).\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\n\nReturns\n-------\nX : sparse matrix of (n_samples, n_features)\n    Tf-idf-weighted document-term matrix."
            }
          ],
          "fullDocstring": "Convert a collection of raw documents to a matrix of TF-IDF features.\n\nEquivalent to :class:`CountVectorizer` followed by\n:class:`TfidfTransformer`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    If 'filename', the sequence passed as an argument to fit is\n    expected to be a list of filenames that need reading to fetch\n    the raw content to analyze.\n\n    If 'file', the sequence items must have a 'read' method (file-like\n    object) that is called to fetch the bytes in memory.\n\n    Otherwise the input is expected to be a sequence of items that\n    can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'}, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    an direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) does nothing.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (string transformation) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer is not callable``.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word or character n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nstop_words : {'english'}, list, default=None\n    If a string, it is passed to _check_stop_list and the appropriate stop\n    list is returned. 'english' is currently the only supported string\n    value.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. max_df can be set to a value\n    in the range [0.7, 1.0) to automatically detect and filter stop\n    words based on intra corpus document frequency of terms.\n\ntoken_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n    only bigrams.\n    Only applies if ``analyzer is not callable``.\n\nmax_df : float or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float in range [0.0, 1.0], the parameter represents a proportion of\n    documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float in range of [0.0, 1.0], the parameter represents a proportion\n    of documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    max_features ordered by term frequency across the corpus.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents.\n\nbinary : bool, default=False\n    If True, all non-zero term counts are set to 1. This does not mean\n    outputs will have only 0/1 values, only that the tf term in tf-idf\n    is binary. (Set idf and normalization to False to get 0/1 outputs).\n\ndtype : dtype, default=float64\n    Type of the matrix returned by fit_transform() or transform().\n\nnorm : {'l1', 'l2'}, default='l2'\n    Each output row will have unit norm, either:\n    * 'l2': Sum of squares of vector elements is 1. The cosine\n    similarity between two vectors is their dot product when l2 norm has\n    been applied.\n    * 'l1': Sum of absolute values of vector elements is 1.\n    See :func:`preprocessing.normalize`.\n\nuse_idf : bool, default=True\n    Enable inverse-document-frequency reweighting.\n\nsmooth_idf : bool, default=True\n    Smooth idf weights by adding one to document frequencies, as if an\n    extra document was seen containing every term in the collection\n    exactly once. Prevents zero divisions.\n\nsublinear_tf : bool, default=False\n    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_: bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user\n\nidf_ : array of shape (n_features,)\n    The inverse document frequency (IDF) vector; only defined\n    if ``use_idf`` is True.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nCountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n\nTfidfTransformer : Performs the TF-IDF transformation from a provided\n    matrix of counts.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = TfidfVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> print(vectorizer.get_feature_names())\n['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n>>> print(X.shape)\n(4, 9)"
        }
      ],
      "functions": [
        {
          "name": "strip_accents_ascii",
          "decorators": [],
          "parameters": [
            {
              "name": "s",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The string to strip"
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Transform accentuated unicode symbols into ascii or nothing\n\nWarning: this solution is only suited for languages that have a direct\ntransliteration to ASCII symbols.\n\nParameters\n----------\ns : string\n    The string to strip\n\nSee Also\n--------\nstrip_accents_unicode : Remove accentuated char for any unicode symbol."
        },
        {
          "name": "strip_accents_unicode",
          "decorators": [],
          "parameters": [
            {
              "name": "s",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The string to strip"
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Transform accentuated unicode symbols into their simple counterpart\n\nWarning: the python-level loop and join operations make this\nimplementation 20 times slower than the strip_accents_ascii basic\nnormalization.\n\nParameters\n----------\ns : string\n    The string to strip\n\nSee Also\n--------\nstrip_accents_ascii : Remove accentuated char for any unicode symbol that\n    has a direct ASCII equivalent."
        },
        {
          "name": "strip_tags",
          "decorators": [],
          "parameters": [
            {
              "name": "s",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The string to strip"
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Basic regexp based HTML / XML tag stripper function\n\nFor serious HTML/XML preprocessing you should rather use an external\nlibrary such as lxml or BeautifulSoup.\n\nParameters\n----------\ns : string\n    The string to strip"
        }
      ]
    },
    {
      "name": "sklearn.feature_selection",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._base",
          "declaration": "SelectorMixin",
          "alias": null
        },
        {
          "module": "sklearn._from_model",
          "declaration": "SelectFromModel",
          "alias": null
        },
        {
          "module": "sklearn._mutual_info",
          "declaration": "mutual_info_classif",
          "alias": null
        },
        {
          "module": "sklearn._mutual_info",
          "declaration": "mutual_info_regression",
          "alias": null
        },
        {
          "module": "sklearn._rfe",
          "declaration": "RFE",
          "alias": null
        },
        {
          "module": "sklearn._rfe",
          "declaration": "RFECV",
          "alias": null
        },
        {
          "module": "sklearn._sequential",
          "declaration": "SequentialFeatureSelector",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "GenericUnivariateSelect",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "SelectFdr",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "SelectFpr",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "SelectFwe",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "SelectKBest",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "SelectPercentile",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "chi2",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "f_classif",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "f_oneway",
          "alias": null
        },
        {
          "module": "sklearn._univariate_selection",
          "declaration": "f_regression",
          "alias": null
        },
        {
          "module": "sklearn._variance_threshold",
          "declaration": "VarianceThreshold",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.feature_selection._base",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "operator",
          "declaration": "attrgetter",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csc_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "safe_mask",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "safe_sqr",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_safe_tags",
          "alias": null
        },
        {
          "module": "warnings",
          "declaration": "warn",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "SelectorMixin",
          "decorators": [],
          "superclasses": [
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "get_support",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "indices",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, the return value will be an array of integers, rather\nthan a boolean mask."
                }
              ],
              "results": [
                {
                  "name": "support",
                  "type": "Array",
                  "description": "An index that selects the retained features from a feature vector.\nIf `indices` is False, this is a boolean array of shape\n[# input features], in which an element is True iff its\ncorresponding feature is selected for retention. If `indices` is\nTrue, this is an integer array of shape [# output features] whose\nvalues are indices into the input feature vector."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get a mask, or integer index, of the features selected\n\nParameters\n----------\nindices : bool, default=False\n    If True, the return value will be an array of integers, rather\n    than a boolean mask.\n\nReturns\n-------\nsupport : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "X_r",
                  "type": null,
                  "description": "The input samples with only the selected features."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Reduce X to the selected features.\n\nParameters\n----------\nX : array of shape [n_samples, n_features]\n    The input samples.\n\nReturns\n-------\nX_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "X_r",
                  "type": null,
                  "description": "`X` with columns of zeros inserted where features would have\nbeen removed by :meth:`transform`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Reverse the transformation operation\n\nParameters\n----------\nX : array of shape [n_samples, n_selected_features]\n    The input samples.\n\nReturns\n-------\nX_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`."
            }
          ],
          "fullDocstring": "Transformer mixin that performs feature selection given a support mask\n\nThis mixin provides a feature selector implementation with `transform` and\n`inverse_transform` functionality given an implementation of\n`_get_support_mask`."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.feature_selection._from_model",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.feature_selection._base",
          "declaration": "SelectorMixin",
          "alias": null
        },
        {
          "module": "sklearn.feature_selection._base",
          "declaration": "_get_feature_importances",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_safe_tags",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "if_delegate_has_method",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "SelectFromModel",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "SelectorMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (integers that correspond to classes in\nclassification, real numbers in regression)."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the SelectFromModel meta-transformer.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The training input samples.\n\ny : array-like of shape (n_samples,), default=None\n    The target values (integers that correspond to classes in\n    classification, real numbers in regression).\n\n**fit_params : Other estimator specific parameters\n\nReturns\n-------\nself : object"
            },
            {
              "name": "threshold_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (integers that correspond to classes in\nclassification, real numbers in regression)."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the SelectFromModel meta-transformer only once.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The training input samples.\n\ny : array-like of shape (n_samples,), default=None\n    The target values (integers that correspond to classes in\n    classification, real numbers in regression).\n\n**fit_params : Other estimator specific parameters\n\nReturns\n-------\nself : object"
            },
            {
              "name": "n_features_in_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Meta-transformer for selecting features based on importance weights.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <select_from_model>`.\n\nParameters\n----------\nestimator : object\n    The base estimator from which the transformer is built.\n    This can be both a fitted (if ``prefit`` is set to True)\n    or a non-fitted estimator. The estimator must have either a\n    ``feature_importances_`` or ``coef_`` attribute after fitting.\n\nthreshold : string or float, default=None\n    The threshold value to use for feature selection. Features whose\n    importance is greater or equal are kept while the others are\n    discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value is\n    the median (resp. the mean) of the feature importances. A scaling\n    factor (e.g., \"1.25*mean\") may also be used. If None and if the\n    estimator has a parameter penalty set to l1, either explicitly\n    or implicitly (e.g, Lasso), the threshold used is 1e-5.\n    Otherwise, \"mean\" is used by default.\n\nprefit : bool, default=False\n    Whether a prefit model is expected to be passed into the constructor\n    directly or not. If True, ``transform`` must be called directly\n    and SelectFromModel cannot be used with ``cross_val_score``,\n    ``GridSearchCV`` and similar utilities that clone the estimator.\n    Otherwise train the model using ``fit`` and then ``transform`` to do\n    feature selection.\n\nnorm_order : non-zero int, inf, -inf, default=1\n    Order of the norm used to filter the vectors of coefficients below\n    ``threshold`` in the case where the ``coef_`` attribute of the\n    estimator is of dimension 2.\n\nmax_features : int, default=None\n    The maximum number of features to select.\n    To only select based on ``max_features``, set ``threshold=-np.inf``.\n\n    .. versionadded:: 0.20\n\nimportance_getter : str or callable, default='auto'\n    If 'auto', uses the feature importance either through a ``coef_``\n    attribute or ``feature_importances_`` attribute of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nestimator_ : an estimator\n    The base estimator from which the transformer is built.\n    This is stored only when a non-fitted estimator is passed to the\n    ``SelectFromModel``, i.e when prefit is False.\n\nthreshold_ : float\n    The threshold value used for feature selection.\n\nNotes\n-----\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nExamples\n--------\n>>> from sklearn.feature_selection import SelectFromModel\n>>> from sklearn.linear_model import LogisticRegression\n>>> X = [[ 0.87, -1.34,  0.31 ],\n...      [-2.79, -0.02, -0.85 ],\n...      [-1.34, -0.48, -2.55 ],\n...      [ 1.92,  1.48,  0.65 ]]\n>>> y = [0, 1, 0, 1]\n>>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n>>> selector.estimator_.coef_\narray([[-0.3252302 ,  0.83462377,  0.49750423]])\n>>> selector.threshold_\n0.55245...\n>>> selector.get_support()\narray([False,  True, False])\n>>> selector.transform(X)\narray([[-1.34],\n       [-0.02],\n       [-0.48],\n       [ 1.48]])\n\nSee Also\n--------\nRFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.feature_selection._mutual_info",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "digamma",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "KDTree",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "scale",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_astype_copy_false",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "mutual_info_classif",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Feature matrix."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target vector."
            }
          ],
          "results": [
            {
              "name": "mi",
              "type": null,
              "description": "Estimated mutual information between each feature and the target."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Estimate mutual information for a discrete target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Feature matrix.\n\ny : array-like of shape (n_samples,)\n    Target vector.\n\ndiscrete_features : {'auto', bool, array-like}, default='auto'\n    If bool, then determines whether to consider all features discrete\n    or continuous. If array, then it should be either a boolean mask\n    with shape (n_features,) or array with indices of discrete features.\n    If 'auto', it is assigned to False for dense `X` and to True for\n    sparse `X`.\n\nn_neighbors : int, default=3\n    Number of neighbors to use for MI estimation for continuous variables,\n    see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n    could introduce a bias.\n\ncopy : bool, default=True\n    Whether to make a copy of the given data. If set to False, the initial\n    data will be overwritten.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for adding small noise to\n    continuous variables in order to remove repeated values.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nmi : ndarray, shape (n_features,)\n    Estimated mutual information between each feature and the target.\n\nNotes\n-----\n1. The term \"discrete features\" is used instead of naming them\n   \"categorical\", because it describes the essence more accurately.\n   For example, pixel intensities of an image are discrete features\n   (but hardly categorical) and you will get better results if mark them\n   as such. Also note, that treating a continuous variable as discrete and\n   vice versa will usually give incorrect results, so be attentive about\n   that.\n2. True mutual information can't be negative. If its estimate turns out\n   to be negative, it is replaced by zero.\n\nReferences\n----------\n.. [1] `Mutual Information\n       <https://en.wikipedia.org/wiki/Mutual_information>`_\n       on Wikipedia.\n.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n       information\". Phys. Rev. E 69, 2004.\n.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n       of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16"
        },
        {
          "name": "mutual_info_regression",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Feature matrix."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target vector."
            }
          ],
          "results": [
            {
              "name": "mi",
              "type": null,
              "description": "Estimated mutual information between each feature and the target."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Estimate mutual information for a continuous target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Feature matrix.\n\ny : array-like of shape (n_samples,)\n    Target vector.\n\ndiscrete_features : {'auto', bool, array-like}, default='auto'\n    If bool, then determines whether to consider all features discrete\n    or continuous. If array, then it should be either a boolean mask\n    with shape (n_features,) or array with indices of discrete features.\n    If 'auto', it is assigned to False for dense `X` and to True for\n    sparse `X`.\n\nn_neighbors : int, default=3\n    Number of neighbors to use for MI estimation for continuous variables,\n    see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n    could introduce a bias.\n\ncopy : bool, default=True\n    Whether to make a copy of the given data. If set to False, the initial\n    data will be overwritten.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for adding small noise to\n    continuous variables in order to remove repeated values.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nmi : ndarray, shape (n_features,)\n    Estimated mutual information between each feature and the target.\n\nNotes\n-----\n1. The term \"discrete features\" is used instead of naming them\n   \"categorical\", because it describes the essence more accurately.\n   For example, pixel intensities of an image are discrete features\n   (but hardly categorical) and you will get better results if mark them\n   as such. Also note, that treating a continuous variable as discrete and\n   vice versa will usually give incorrect results, so be attentive about\n   that.\n2. True mutual information can't be negative. If its estimate turns out\n   to be negative, it is replaced by zero.\n\nReferences\n----------\n.. [1] `Mutual Information\n       <https://en.wikipedia.org/wiki/Mutual_information>`_\n       on Wikipedia.\n.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n       information\". Phys. Rev. E 69, 2004.\n.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n       of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16"
        }
      ]
    },
    {
      "name": "sklearn.feature_selection._rfe",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.feature_selection._base",
          "declaration": "SelectorMixin",
          "alias": null
        },
        {
          "module": "sklearn.feature_selection._base",
          "declaration": "_get_feature_importances",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "check_scoring",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._validation",
          "declaration": "_score",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_safe_tags",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "_safe_split",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "if_delegate_has_method",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "RFE",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "SelectorMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "classes_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the RFE model and then the underlying estimator on the selected\n   features.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples.\n\ny : array-like of shape (n_samples,)\n    The target values."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted target values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Reduce X to the selected features and then predict using the\n   underlying estimator.\n\nParameters\n----------\nX : array of shape [n_samples, n_features]\n    The input samples.\n\nReturns\n-------\ny : array of shape [n_samples]\n    The predicted target values."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Reduce X to the selected features and then return the score of the\n   underlying estimator.\n\nParameters\n----------\nX : array of shape [n_samples, n_features]\n    The input samples.\n\ny : array of shape [n_samples]\n    The target values."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": null,
                  "description": "The decision function of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`.\nRegression and binary classification produce an array of shape\n[n_samples]."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the decision function of ``X``.\n\nParameters\n----------\nX : {array-like or sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nscore : array, shape = [n_samples, n_classes] or [n_samples]\n    The decision function of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.\n    Regression and binary classification produce an array of shape\n    [n_samples]."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities for X.\n\nParameters\n----------\nX : {array-like or sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\np : array of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class log-probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class log-probabilities for X.\n\nParameters\n----------\nX : array of shape [n_samples, n_features]\n    The input samples.\n\nReturns\n-------\np : array of shape (n_samples, n_classes)\n    The class log-probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            }
          ],
          "fullDocstring": "Feature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination\n(RFE) is to select features by recursively considering smaller and smaller\nsets of features. First, the estimator is trained on the initial set of\nfeatures and the importance of each feature is obtained either through\nany specific attribute or callable.\nThen, the least important features are pruned from current set of features.\nThat procedure is recursively repeated on the pruned set until the desired\nnumber of features to select is eventually reached.\n\nRead more in the :ref:`User Guide <rfe>`.\n\nParameters\n----------\nestimator : ``Estimator`` instance\n    A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance\n    (e.g. `coef_`, `feature_importances_`).\n\nn_features_to_select : int or float, default=None\n    The number of features to select. If `None`, half of the features are\n    selected. If integer, the parameter is the absolute number of features\n    to select. If float between 0 and 1, it is the fraction of features to\n    select.\n\n    .. versionchanged:: 0.24\n       Added float values for fractions.\n\nstep : int or float, default=1\n    If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n\nverbose : int, default=0\n    Controls verbosity of output.\n\nimportance_getter : str or callable, default='auto'\n    If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nestimator_ : ``Estimator`` instance\n    The fitted estimator used to select features.\n\nn_features_ : int\n    The number of selected features.\n\nranking_ : ndarray of shape (n_features,)\n    The feature ranking, such that ``ranking_[i]`` corresponds to the\n    ranking position of the i-th feature. Selected (i.e., estimated\n    best) features are assigned rank 1.\n\nsupport_ : ndarray of shape (n_features,)\n    The mask of selected features.\n\nExamples\n--------\nThe following example shows how to retrieve the 5 most informative\nfeatures in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFE\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFE(estimator, n_features_to_select=5, step=1)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\nNotes\n-----\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nSee Also\n--------\nRFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002."
        },
        {
          "name": "RFECV",
          "decorators": [],
          "superclasses": [
            "RFE"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where `n_samples` is the number of samples and\n`n_features` is the total number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values (integers for classification, real numbers for\nregression)."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set. Only used in conjunction with a \"Group\" :term:`cv`\ninstance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n\n.. versionadded:: 0.20"
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the RFE model and automatically tune the number of selected\n   features.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where `n_samples` is the number of samples and\n    `n_features` is the total number of features.\n\ny : array-like of shape (n_samples,)\n    Target values (integers for classification, real numbers for\n    regression).\n\ngroups : array-like of shape (n_samples,) or None, default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n\n    .. versionadded:: 0.20"
            }
          ],
          "fullDocstring": "Feature ranking with recursive feature elimination and cross-validated\nselection of the best number of features.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <rfe>`.\n\nParameters\n----------\nestimator : ``Estimator`` instance\n    A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance either through a ``coef_``\n    attribute or through a ``feature_importances_`` attribute.\n\nstep : int or float, default=1\n    If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n    Note that the last iteration may remove fewer than ``step`` features in\n    order to reach ``min_features_to_select``.\n\nmin_features_to_select : int, default=1\n    The minimum number of features to be selected. This number of features\n    will always be scored, even if the difference between the original\n    feature count and ``min_features_to_select`` isn't divisible by\n    ``step``.\n\n    .. versionadded:: 0.20\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used. If the\n    estimator is a classifier or if ``y`` is neither binary nor multiclass,\n    :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value of None changed from 3-fold to 5-fold.\n\nscoring : string, callable or None, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\nverbose : int, default=0\n    Controls verbosity of output.\n\nn_jobs : int or None, default=None\n    Number of cores to run in parallel while fitting across folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18\n\nimportance_getter : str or callable, default='auto'\n    If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance.\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nestimator_ : ``Estimator`` instance\n    The fitted estimator used to select features.\n\ngrid_scores_ : ndarray of shape (n_subsets_of_features,)\n    The cross-validation scores such that\n    ``grid_scores_[i]`` corresponds to\n    the CV score of the i-th subset of features.\n\nn_features_ : int\n    The number of selected features with cross-validation.\n\nranking_ : narray of shape (n_features,)\n    The feature ranking, such that `ranking_[i]`\n    corresponds to the ranking\n    position of the i-th feature.\n    Selected (i.e., estimated best)\n    features are assigned rank 1.\n\nsupport_ : ndarray of shape (n_features,)\n    The mask of selected features.\n\nNotes\n-----\nThe size of ``grid_scores_`` is equal to\n``ceil((n_features - min_features_to_select) / step) + 1``,\nwhere step is the number of features removed at each iteration.\n\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nExamples\n--------\nThe following example shows how to retrieve the a-priori not known 5\ninformative features in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFECV\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFECV(estimator, step=1, cv=5)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\nSee Also\n--------\nRFE : Recursive feature elimination.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.feature_selection._sequential",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.feature_selection._base",
          "declaration": "SelectorMixin",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "cross_val_score",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_safe_tags",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "SequentialFeatureSelector",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "SelectorMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn the features to select.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vectors.\ny : array-like of shape (n_samples,)\n    Target values.\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "Transformer that performs Sequential Feature Selection.\n\nThis Sequential Feature Selector adds (forward selection) or\nremoves (backward selection) features to form a feature subset in a\ngreedy fashion. At each stage, this estimator chooses the best feature to\nadd or remove based on the cross-validation score of an estimator.\n\nRead more in the :ref:`User Guide <sequential_feature_selection>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nestimator : estimator instance\n    An unfitted estimator.\n\nn_features_to_select : int or float, default=None\n    The number of features to select. If `None`, half of the features are\n    selected. If integer, the parameter is the absolute number of features\n    to select. If float between 0 and 1, it is the fraction of features to\n    select.\n\ndirection: {'forward', 'backward'}, default='forward'\n    Whether to perform forward selection or backward selection.\n\nscoring : str, callable, list/tuple or dict, default=None\n    A single str (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n    NOTE that when using custom scorers, each scorer should return a single\n    value. Metric functions returning a list/array of values can be wrapped\n    into multiple scorers that return one value each.\n\n    If None, the estimator's score method is used.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel. When evaluating a new feature to\n    add or remove, the cross-validation procedure is parallel over the\n    folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nn_features_to_select_ : int\n    The number of features that were selected.\n\nsupport_ : ndarray of shape (n_features,), dtype=bool\n    The mask of selected features.\n\nSee Also\n--------\nRFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination based on importance weights, with\n    automatic selection of the number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.\n\nExamples\n--------\n>>> from sklearn.feature_selection import SequentialFeatureSelector\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n>>> sfs.fit(X, y)\nSequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                          n_features_to_select=3)\n>>> sfs.get_support()\narray([ True, False,  True,  True])\n>>> sfs.transform(X).shape\n(150, 3)"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.feature_selection._univariate_selection",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "special",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "stats",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.feature_selection._base",
          "declaration": "SelectorMixin",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "as_float_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "safe_mask",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "safe_sqr",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "GenericUnivariateSelect",
          "decorators": [],
          "superclasses": [
            "_BaseFilter"
          ],
          "methods": [],
          "fullDocstring": "Univariate feature selector with configurable strategy.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues). For modes 'percentile' or 'kbest' it can return\n    a single array scores.\n\nmode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'\n    Feature selection mode.\n\nparam : float or int depending on the feature selection mode, default=1e-5\n    Parameter of the corresponding mode.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores, None if `score_func` returned scores only.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import GenericUnivariateSelect, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n>>> X_new = transformer.fit_transform(X, y)\n>>> X_new.shape\n(569, 20)\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate."
        },
        {
          "name": "SelectFdr",
          "decorators": [],
          "superclasses": [
            "_BaseFilter"
          ],
          "methods": [],
          "fullDocstring": "Filter: Select the p-values for an estimated false discovery rate\n\nThis uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\non the expected false discovery rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\nalpha : float, default=5e-2\n    The highest uncorrected p-value for features to keep.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFdr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores.\n\nReferences\n----------\nhttps://en.wikipedia.org/wiki/False_discovery_rate\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a contnuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode."
        },
        {
          "name": "SelectFpr",
          "decorators": [],
          "superclasses": [
            "_BaseFilter"
          ],
          "methods": [],
          "fullDocstring": "Filter: Select the pvalues below alpha based on a FPR test.\n\nFPR test stands for False Positive Rate test. It controls the total\namount of false detections.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\nalpha : float, default=5e-2\n    The highest p-value for features to be kept.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFpr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nmutual_info_classif: Mutual information for a discrete target.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode."
        },
        {
          "name": "SelectFwe",
          "decorators": [],
          "superclasses": [
            "_BaseFilter"
          ],
          "methods": [],
          "fullDocstring": "Filter: Select the p-values corresponding to Family-wise error rate\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\nalpha : float, default=5e-2\n    The highest uncorrected p-value for features to keep.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFwe, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 15)\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores.\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode."
        },
        {
          "name": "SelectKBest",
          "decorators": [],
          "superclasses": [
            "_BaseFilter"
          ],
          "methods": [],
          "fullDocstring": "Select features according to the k highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18\n\nk : int or \"all\", default=10\n    Number of top features to select.\n    The \"all\" option bypasses selection, for use in a parameter search.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores, None if `score_func` returned only scores.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectKBest, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n>>> X_new.shape\n(1797, 20)\n\nNotes\n-----\nTies between features with equal scores will be broken in an unspecified\nway.\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode."
        },
        {
          "name": "SelectPercentile",
          "decorators": [],
          "superclasses": [
            "_BaseFilter"
          ],
          "methods": [],
          "fullDocstring": "Select features according to a percentile of the highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18\n\npercentile : int, default=10\n    Percent of features to keep.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores, None if `score_func` returned only scores.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectPercentile, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n>>> X_new.shape\n(1797, 7)\n\nNotes\n-----\nTies between features with equal scores will be broken in an unspecified\nway.\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode."
        }
      ],
      "functions": [
        {
          "name": "chi2",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Sample vectors."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target vector (class labels)."
            }
          ],
          "results": [
            {
              "name": "chi2",
              "type": null,
              "description": "chi2 statistics of each feature."
            },
            {
              "name": "pval",
              "type": null,
              "description": "p-values of each feature."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute chi-squared stats between each non-negative feature and class.\n\nThis score can be used to select the n_features features with the\nhighest values for the test chi-squared statistic from X, which must\ncontain only non-negative features such as booleans or frequencies\n(e.g., term counts in document classification), relative to the classes.\n\nRecall that the chi-square test measures dependence between stochastic\nvariables, so using this function \"weeds out\" the features that are the\nmost likely to be independent of class and therefore irrelevant for\nclassification.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Sample vectors.\n\ny : array-like of shape (n_samples,)\n    Target vector (class labels).\n\nReturns\n-------\nchi2 : array, shape = (n_features,)\n    chi2 statistics of each feature.\npval : array, shape = (n_features,)\n    p-values of each feature.\n\nNotes\n-----\nComplexity of this algorithm is O(n_classes * n_features).\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nf_regression : F-value between label/feature for regression tasks."
        },
        {
          "name": "f_classif",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The set of regressors that will be tested sequentially."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data matrix."
            }
          ],
          "results": [
            {
              "name": "F",
              "type": null,
              "description": "The set of F values."
            },
            {
              "name": "pval",
              "type": null,
              "description": "The set of p-values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the ANOVA F-value for the provided sample.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} shape = [n_samples, n_features]\n    The set of regressors that will be tested sequentially.\n\ny : array of shape(n_samples)\n    The data matrix.\n\nReturns\n-------\nF : array, shape = [n_features,]\n    The set of F values.\n\npval : array, shape = [n_features,]\n    The set of p-values.\n\nSee Also\n--------\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks."
        },
        {
          "name": "f_oneway",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "F-value",
              "type": "float",
              "description": "The computed F-value of the test."
            },
            {
              "name": "p-value",
              "type": "float",
              "description": "The associated p-value from the F-distribution."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Performs a 1-way ANOVA.\n\nThe one-way ANOVA tests the null hypothesis that 2 or more groups have\nthe same population mean. The test is applied to samples from two or\nmore groups, possibly with differing sizes.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\n*args : array-like, sparse matrices\n    sample1, sample2... The sample measurements should be given as\n    arguments.\n\nReturns\n-------\nF-value : float\n    The computed F-value of the test.\np-value : float\n    The associated p-value from the F-distribution.\n\nNotes\n-----\nThe ANOVA test has important assumptions that must be satisfied in order\nfor the associated p-value to be valid.\n\n1. The samples are independent\n2. Each sample is from a normally distributed population\n3. The population standard deviations of the groups are all equal. This\n   property is known as homoscedasticity.\n\nIf these assumptions are not true for a given set of data, it may still be\npossible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`_) although\nwith some loss of power.\n\nThe algorithm is from Heiman[2], pp.394-7.\n\nSee ``scipy.stats.f_oneway`` that should give the same results while\nbeing less efficient.\n\nReferences\n----------\n\n.. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n       Statistics\". Chapter 14.\n       http://faculty.vassar.edu/lowry/ch14pt1.html\n\n.. [2] Heiman, G.W.  Research Methods in Statistics. 2002."
        },
        {
          "name": "f_regression",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The set of regressors that will be tested sequentially."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data matrix"
            }
          ],
          "results": [
            {
              "name": "F",
              "type": null,
              "description": "F values of features."
            },
            {
              "name": "pval",
              "type": null,
              "description": "p-values of F-scores."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Univariate linear regression tests.\n\nLinear model for testing the individual effect of each of many regressors.\nThis is a scoring function to be used in a feature selection procedure, not\na free standing feature selection procedure.\n\nThis is done in 2 steps:\n\n1. The correlation between each regressor and the target is computed,\n   that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) *\n   std(y)).\n2. It is converted to an F score then to a p-value.\n\nFor more on usage see the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix}  shape = (n_samples, n_features)\n    The set of regressors that will be tested sequentially.\n\ny : array of shape(n_samples).\n    The data matrix\n\ncenter : bool, default=True\n    If true, X and y will be centered.\n\nReturns\n-------\nF : array, shape=(n_features,)\n    F values of features.\n\npval : array, shape=(n_features,)\n    p-values of F-scores.\n\nSee Also\n--------\nmutual_info_regression : Mutual information for a continuous target.\nf_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nSelectPercentile : Select features based on percentile of the highest\n    scores."
        }
      ]
    },
    {
      "name": "sklearn.feature_selection._variance_threshold",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.feature_selection._base",
          "declaration": "SelectorMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "mean_variance_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "min_max_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "VarianceThreshold",
          "decorators": [],
          "superclasses": [
            "SelectorMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample vectors from which to compute variances."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored. This parameter exists only for compatibility with\nsklearn.pipeline.Pipeline."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Learn empirical variances from X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Sample vectors from which to compute variances.\n\ny : any, default=None\n    Ignored. This parameter exists only for compatibility with\n    sklearn.pipeline.Pipeline.\n\nReturns\n-------\nself"
            }
          ],
          "fullDocstring": "Feature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the\ndesired outputs (y), and can thus be used for unsupervised learning.\n\nRead more in the :ref:`User Guide <variance_threshold>`.\n\nParameters\n----------\nthreshold : float, default=0\n    Features with a training-set variance lower than this threshold will\n    be removed. The default is to keep all features with non-zero variance,\n    i.e. remove the features that have the same value in all samples.\n\nAttributes\n----------\nvariances_ : array, shape (n_features,)\n    Variances of individual features.\n\nNotes\n-----\nAllows NaN in the input.\nRaises ValueError if no feature in X meets the variance threshold.\n\nExamples\n--------\nThe following dataset has integer features, two of which are the same\nin every sample. These are removed with the default setting for threshold::\n\n    >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n    >>> selector = VarianceThreshold()\n    >>> selector.fit_transform(X)\n    array([[2, 0],\n           [1, 4],\n           [1, 1]])"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.gaussian_process",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn",
          "declaration": "kernels",
          "alias": null
        },
        {
          "module": "sklearn._gpc",
          "declaration": "GaussianProcessClassifier",
          "alias": null
        },
        {
          "module": "sklearn._gpr",
          "declaration": "GaussianProcessRegressor",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.gaussian_process._gpc",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.optimize",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "operator",
          "declaration": "itemgetter",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "cho_solve",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "cholesky",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "solve",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "erf",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.gaussian_process.kernels",
          "declaration": "CompoundKernel",
          "alias": null
        },
        {
          "module": "sklearn.gaussian_process.kernels",
          "declaration": "ConstantKernel",
          "alias": "C"
        },
        {
          "module": "sklearn.gaussian_process.kernels",
          "declaration": "RBF",
          "alias": null
        },
        {
          "module": "sklearn.multiclass",
          "declaration": "OneVsOneClassifier",
          "alias": null
        },
        {
          "module": "sklearn.multiclass",
          "declaration": "OneVsRestClassifier",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.optimize",
          "declaration": "_check_optimize_result",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "GaussianProcessClassifier",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Feature vectors or other representations of training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values, must be binary"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit Gaussian process classification model\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or list of object\n    Feature vectors or other representations of training data.\n\ny : array-like of shape (n_samples,)\n    Target values, must be binary\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Query points where the GP is evaluated for classification."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Predicted target values for X, values are from ``classes_``"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform classification on an array of test vectors X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or list of object\n    Query points where the GP is evaluated for classification.\n\nReturns\n-------\nC : ndarray of shape (n_samples,)\n    Predicted target values for X, values are from ``classes_``"
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Query points where the GP is evaluated for classification."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Returns the probability of the samples for each class in\nthe model. The columns correspond to the classes in sorted\norder, as they appear in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return probability estimates for the test vector X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or list of object\n    Query points where the GP is evaluated for classification.\n\nReturns\n-------\nC : array-like of shape (n_samples, n_classes)\n    Returns the probability of the samples for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`."
            },
            {
              "name": "kernel_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "log_marginal_likelihood",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "theta",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Kernel hyperparameters for which the log-marginal likelihood is\nevaluated. In the case of multi-class classification, theta may\nbe the  hyperparameters of the compound kernel or of an individual\nkernel. In the latter case, all individual kernel get assigned the\nsame theta values. If None, the precomputed log_marginal_likelihood\nof ``self.kernel_.theta`` is returned."
                },
                {
                  "name": "eval_gradient",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, the gradient of the log-marginal likelihood with respect\nto the kernel hyperparameters at position theta is returned\nadditionally. Note that gradient computation is not supported\nfor non-binary classification. If True, theta must not be None."
                },
                {
                  "name": "clone_kernel",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, the kernel attribute is copied. If False, the kernel\nattribute is modified, but may result in a performance improvement."
                }
              ],
              "results": [
                {
                  "name": "log_likelihood",
                  "type": "float",
                  "description": "Log-marginal likelihood of theta for training data."
                },
                {
                  "name": "log_likelihood_gradient",
                  "type": null,
                  "description": "Gradient of the log-marginal likelihood with respect to the kernel\nhyperparameters at position theta.\nOnly returned when `eval_gradient` is True."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns log-marginal likelihood of theta for training data.\n\nIn the case of multi-class classification, the mean log-marginal\nlikelihood of the one-versus-rest classifiers are returned.\n\nParameters\n----------\ntheta : array-like of shape (n_kernel_params,), default=None\n    Kernel hyperparameters for which the log-marginal likelihood is\n    evaluated. In the case of multi-class classification, theta may\n    be the  hyperparameters of the compound kernel or of an individual\n    kernel. In the latter case, all individual kernel get assigned the\n    same theta values. If None, the precomputed log_marginal_likelihood\n    of ``self.kernel_.theta`` is returned.\n\neval_gradient : bool, default=False\n    If True, the gradient of the log-marginal likelihood with respect\n    to the kernel hyperparameters at position theta is returned\n    additionally. Note that gradient computation is not supported\n    for non-binary classification. If True, theta must not be None.\n\nclone_kernel : bool, default=True\n    If True, the kernel attribute is copied. If False, the kernel\n    attribute is modified, but may result in a performance improvement.\n\nReturns\n-------\nlog_likelihood : float\n    Log-marginal likelihood of theta for training data.\n\nlog_likelihood_gradient : ndarray of shape (n_kernel_params,), optional\n    Gradient of the log-marginal likelihood with respect to the kernel\n    hyperparameters at position theta.\n    Only returned when `eval_gradient` is True."
            }
          ],
          "fullDocstring": "Gaussian process classification (GPC) based on Laplace approximation.\n\nThe implementation is based on Algorithm 3.1, 3.2, and 5.1 of\nGaussian Processes for Machine Learning (GPML) by Rasmussen and\nWilliams.\n\nInternally, the Laplace approximation is used for approximating the\nnon-Gaussian posterior by a Gaussian.\n\nCurrently, the implementation is restricted to using the logistic link\nfunction. For multi-class classification, several binary one-versus rest\nclassifiers are fitted. Note that this class thus does not implement\na true multi-class Laplace approximation.\n\nRead more in the :ref:`User Guide <gaussian_process>`.\n\nParameters\n----------\nkernel : kernel instance, default=None\n    The kernel specifying the covariance function of the GP. If None is\n    passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n    the kernel's hyperparameters are optimized during fitting.\n\noptimizer : 'fmin_l_bfgs_b' or callable, default='fmin_l_bfgs_b'\n    Can either be one of the internally supported optimizers for optimizing\n    the kernel's parameters, specified by a string, or an externally\n    defined optimizer passed as a callable. If a callable is passed, it\n    must have the  signature::\n\n        def optimizer(obj_func, initial_theta, bounds):\n            # * 'obj_func' is the objective function to be maximized, which\n            #   takes the hyperparameters theta as parameter and an\n            #   optional flag eval_gradient, which determines if the\n            #   gradient is returned additionally to the function value\n            # * 'initial_theta': the initial value for theta, which can be\n            #   used by local optimizers\n            # * 'bounds': the bounds on the values of theta\n            ....\n            # Returned are the best found hyperparameters theta and\n            # the corresponding value of the target function.\n            return theta_opt, func_min\n\n    Per default, the 'L-BFGS-B' algorithm from scipy.optimize.minimize\n    is used. If None is passed, the kernel's parameters are kept fixed.\n    Available internal optimizers are::\n\n        'fmin_l_bfgs_b'\n\nn_restarts_optimizer : int, default=0\n    The number of restarts of the optimizer for finding the kernel's\n    parameters which maximize the log-marginal likelihood. The first run\n    of the optimizer is performed from the kernel's initial parameters,\n    the remaining ones (if any) from thetas sampled log-uniform randomly\n    from the space of allowed theta-values. If greater than 0, all bounds\n    must be finite. Note that n_restarts_optimizer=0 implies that one\n    run is performed.\n\nmax_iter_predict : int, default=100\n    The maximum number of iterations in Newton's method for approximating\n    the posterior during predict. Smaller values will reduce computation\n    time at the cost of worse results.\n\nwarm_start : bool, default=False\n    If warm-starts are enabled, the solution of the last Newton iteration\n    on the Laplace approximation of the posterior mode is used as\n    initialization for the next call of _posterior_mode(). This can speed\n    up convergence when _posterior_mode is called several times on similar\n    problems as in hyperparameter optimization. See :term:`the Glossary\n    <warm_start>`.\n\ncopy_X_train : bool, default=True\n    If True, a persistent copy of the training data is stored in the\n    object. Otherwise, just a reference to the training data is stored,\n    which might cause predictions to change if the data is modified\n    externally.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation used to initialize the centers.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nmulti_class : {'one_vs_rest', 'one_vs_one'}, default='one_vs_rest'\n    Specifies how multi-class classification problems are handled.\n    Supported are 'one_vs_rest' and 'one_vs_one'. In 'one_vs_rest',\n    one binary Gaussian process classifier is fitted for each class, which\n    is trained to separate this class from the rest. In 'one_vs_one', one\n    binary Gaussian process classifier is fitted for each pair of classes,\n    which is trained to separate these two classes. The predictions of\n    these binary predictors are combined into multi-class predictions.\n    Note that 'one_vs_one' does not support predicting probability\n    estimates.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation: the specified\n    multiclass problems are computed in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nbase_estimator_ : ``Estimator`` instance\n    The estimator instance that defines the likelihood function\n    using the observed data.\n\nkernel_ : kernel instance\n    The kernel used for prediction. In case of binary classification,\n    the structure of the kernel is the same as the one passed as parameter\n    but with optimized hyperparameters. In case of multi-class\n    classification, a CompoundKernel is returned which consists of the\n    different kernels used in the one-versus-rest classifiers.\n\nlog_marginal_likelihood_value_ : float\n    The log-marginal-likelihood of ``self.kernel_.theta``\n\nclasses_ : array-like of shape (n_classes,)\n    Unique class labels.\n\nn_classes_ : int\n    The number of classes in the training data\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.gaussian_process import GaussianProcessClassifier\n>>> from sklearn.gaussian_process.kernels import RBF\n>>> X, y = load_iris(return_X_y=True)\n>>> kernel = 1.0 * RBF(1.0)\n>>> gpc = GaussianProcessClassifier(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpc.score(X, y)\n0.9866...\n>>> gpc.predict_proba(X[:2,:])\narray([[0.83548752, 0.03228706, 0.13222543],\n       [0.79064206, 0.06525643, 0.14410151]])\n\n.. versionadded:: 0.18"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.gaussian_process._gpr",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.optimize",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "operator",
          "declaration": "itemgetter",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "cho_solve",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "cholesky",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "solve_triangular",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.gaussian_process.kernels",
          "declaration": "ConstantKernel",
          "alias": "C"
        },
        {
          "module": "sklearn.gaussian_process.kernels",
          "declaration": "RBF",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.optimize",
          "declaration": "_check_optimize_result",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "GaussianProcessRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseEstimator",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Feature vectors or other representations of training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit Gaussian process regression model.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or list of object\n    Feature vectors or other representations of training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Query points where the GP is evaluated."
                },
                {
                  "name": "return_std",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, the standard-deviation of the predictive distribution at\nthe query points is returned along with the mean."
                },
                {
                  "name": "return_cov",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, the covariance of the joint predictive distribution at\nthe query points is returned along with the mean."
                }
              ],
              "results": [
                {
                  "name": "y_mean",
                  "type": null,
                  "description": "Mean of predictive distribution a query points."
                },
                {
                  "name": "y_std",
                  "type": null,
                  "description": "Standard deviation of predictive distribution at query points.\nOnly returned when `return_std` is True."
                },
                {
                  "name": "y_cov",
                  "type": null,
                  "description": "Covariance of joint predictive distribution a query points.\nOnly returned when `return_cov` is True."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the Gaussian process regression model\n\nWe can also predict based on an unfitted model by using the GP prior.\nIn addition to the mean of the predictive distribution, also its\nstandard deviation (return_std=True) or covariance (return_cov=True).\nNote that at most one of the two can be requested.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or list of object\n    Query points where the GP is evaluated.\n\nreturn_std : bool, default=False\n    If True, the standard-deviation of the predictive distribution at\n    the query points is returned along with the mean.\n\nreturn_cov : bool, default=False\n    If True, the covariance of the joint predictive distribution at\n    the query points is returned along with the mean.\n\nReturns\n-------\ny_mean : ndarray of shape (n_samples, [n_output_dims])\n    Mean of predictive distribution a query points.\n\ny_std : ndarray of shape (n_samples,), optional\n    Standard deviation of predictive distribution at query points.\n    Only returned when `return_std` is True.\n\ny_cov : ndarray of shape (n_samples, n_samples), optional\n    Covariance of joint predictive distribution a query points.\n    Only returned when `return_cov` is True."
            },
            {
              "name": "sample_y",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Query points where the GP is evaluated."
                },
                {
                  "name": "n_samples",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "1",
                  "limitation": null,
                  "ignored": false,
                  "description": "The number of samples drawn from the Gaussian process"
                },
                {
                  "name": "random_state",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "0",
                  "limitation": null,
                  "ignored": false,
                  "description": "Determines random number generation to randomly draw samples.\nPass an int for reproducible results across multiple function\ncalls.\nSee :term: `Glossary <random_state>`."
                }
              ],
              "results": [
                {
                  "name": "y_samples",
                  "type": null,
                  "description": "Values of n_samples samples drawn from Gaussian process and\nevaluated at query points."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Draw samples from Gaussian process and evaluate at X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or list of object\n    Query points where the GP is evaluated.\n\nn_samples : int, default=1\n    The number of samples drawn from the Gaussian process\n\nrandom_state : int, RandomState instance or None, default=0\n    Determines random number generation to randomly draw samples.\n    Pass an int for reproducible results across multiple function\n    calls.\n    See :term: `Glossary <random_state>`.\n\nReturns\n-------\ny_samples : ndarray of shape (n_samples_X, [n_output_dims], n_samples)\n    Values of n_samples samples drawn from Gaussian process and\n    evaluated at query points."
            },
            {
              "name": "log_marginal_likelihood",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "theta",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Kernel hyperparameters for which the log-marginal likelihood is\nevaluated. If None, the precomputed log_marginal_likelihood\nof ``self.kernel_.theta`` is returned."
                },
                {
                  "name": "eval_gradient",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, the gradient of the log-marginal likelihood with respect\nto the kernel hyperparameters at position theta is returned\nadditionally. If True, theta must not be None."
                },
                {
                  "name": "clone_kernel",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, the kernel attribute is copied. If False, the kernel\nattribute is modified, but may result in a performance improvement."
                }
              ],
              "results": [
                {
                  "name": "log_likelihood",
                  "type": "float",
                  "description": "Log-marginal likelihood of theta for training data."
                },
                {
                  "name": "log_likelihood_gradient",
                  "type": null,
                  "description": "Gradient of the log-marginal likelihood with respect to the kernel\nhyperparameters at position theta.\nOnly returned when eval_gradient is True."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns log-marginal likelihood of theta for training data.\n\nParameters\n----------\ntheta : array-like of shape (n_kernel_params,) default=None\n    Kernel hyperparameters for which the log-marginal likelihood is\n    evaluated. If None, the precomputed log_marginal_likelihood\n    of ``self.kernel_.theta`` is returned.\n\neval_gradient : bool, default=False\n    If True, the gradient of the log-marginal likelihood with respect\n    to the kernel hyperparameters at position theta is returned\n    additionally. If True, theta must not be None.\n\nclone_kernel : bool, default=True\n    If True, the kernel attribute is copied. If False, the kernel\n    attribute is modified, but may result in a performance improvement.\n\nReturns\n-------\nlog_likelihood : float\n    Log-marginal likelihood of theta for training data.\n\nlog_likelihood_gradient : ndarray of shape (n_kernel_params,), optional\n    Gradient of the log-marginal likelihood with respect to the kernel\n    hyperparameters at position theta.\n    Only returned when eval_gradient is True."
            }
          ],
          "fullDocstring": "Gaussian process regression (GPR).\n\nThe implementation is based on Algorithm 2.1 of Gaussian Processes\nfor Machine Learning (GPML) by Rasmussen and Williams.\n\nIn addition to standard scikit-learn estimator API,\nGaussianProcessRegressor:\n\n   * allows prediction without prior fitting (based on the GP prior)\n   * provides an additional method sample_y(X), which evaluates samples\n     drawn from the GPR (prior or posterior) at given inputs\n   * exposes a method log_marginal_likelihood(theta), which can be used\n     externally for other ways of selecting hyperparameters, e.g., via\n     Markov chain Monte Carlo.\n\nRead more in the :ref:`User Guide <gaussian_process>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nkernel : kernel instance, default=None\n    The kernel specifying the covariance function of the GP. If None is\n    passed, the kernel ``ConstantKernel(1.0, constant_value_bounds=\"fixed\"\n    * RBF(1.0, length_scale_bounds=\"fixed\")`` is used as default. Note that\n    the kernel hyperparameters are optimized during fitting unless the\n    bounds are marked as \"fixed\".\n\nalpha : float or ndarray of shape (n_samples,), default=1e-10\n    Value added to the diagonal of the kernel matrix during fitting.\n    This can prevent a potential numerical issue during fitting, by\n    ensuring that the calculated values form a positive definite matrix.\n    It can also be interpreted as the variance of additional Gaussian\n    measurement noise on the training observations. Note that this is\n    different from using a `WhiteKernel`. If an array is passed, it must\n    have the same number of entries as the data used for fitting and is\n    used as datapoint-dependent noise level. Allowing to specify the\n    noise level directly as a parameter is mainly for convenience and\n    for consistency with Ridge.\n\noptimizer : \"fmin_l_bfgs_b\" or callable, default=\"fmin_l_bfgs_b\"\n    Can either be one of the internally supported optimizers for optimizing\n    the kernel's parameters, specified by a string, or an externally\n    defined optimizer passed as a callable. If a callable is passed, it\n    must have the signature::\n\n        def optimizer(obj_func, initial_theta, bounds):\n            # * 'obj_func' is the objective function to be minimized, which\n            #   takes the hyperparameters theta as parameter and an\n            #   optional flag eval_gradient, which determines if the\n            #   gradient is returned additionally to the function value\n            # * 'initial_theta': the initial value for theta, which can be\n            #   used by local optimizers\n            # * 'bounds': the bounds on the values of theta\n            ....\n            # Returned are the best found hyperparameters theta and\n            # the corresponding value of the target function.\n            return theta_opt, func_min\n\n    Per default, the 'L-BGFS-B' algorithm from scipy.optimize.minimize\n    is used. If None is passed, the kernel's parameters are kept fixed.\n    Available internal optimizers are::\n\n        'fmin_l_bfgs_b'\n\nn_restarts_optimizer : int, default=0\n    The number of restarts of the optimizer for finding the kernel's\n    parameters which maximize the log-marginal likelihood. The first run\n    of the optimizer is performed from the kernel's initial parameters,\n    the remaining ones (if any) from thetas sampled log-uniform randomly\n    from the space of allowed theta-values. If greater than 0, all bounds\n    must be finite. Note that n_restarts_optimizer == 0 implies that one\n    run is performed.\n\nnormalize_y : bool, default=False\n    Whether the target values y are normalized, the mean and variance of\n    the target values are set equal to 0 and 1 respectively. This is\n    recommended for cases where zero-mean, unit-variance priors are used.\n    Note that, in this implementation, the normalisation is reversed\n    before the GP predictions are reported.\n\n    .. versionchanged:: 0.23\n\ncopy_X_train : bool, default=True\n    If True, a persistent copy of the training data is stored in the\n    object. Otherwise, just a reference to the training data is stored,\n    which might cause predictions to change if the data is modified\n    externally.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation used to initialize the centers.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nAttributes\n----------\nX_train_ : array-like of shape (n_samples, n_features) or list of object\n    Feature vectors or other representations of training data (also\n    required for prediction).\n\ny_train_ : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values in training data (also required for prediction)\n\nkernel_ : kernel instance\n    The kernel used for prediction. The structure of the kernel is the\n    same as the one passed as parameter but with optimized hyperparameters\n\nL_ : array-like of shape (n_samples, n_samples)\n    Lower-triangular Cholesky decomposition of the kernel in ``X_train_``\n\nalpha_ : array-like of shape (n_samples,)\n    Dual coefficients of training data points in kernel space\n\nlog_marginal_likelihood_value_ : float\n    The log-marginal-likelihood of ``self.kernel_.theta``\n\nExamples\n--------\n>>> from sklearn.datasets import make_friedman2\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\n>>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n>>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n>>> kernel = DotProduct() + WhiteKernel()\n>>> gpr = GaussianProcessRegressor(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpr.score(X, y)\n0.3680...\n>>> gpr.predict(X[:2,:], return_std=True)\n(array([653.0..., 592.1...]), array([316.6..., 316.6...]))"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.gaussian_process.kernels",
      "imports": [
        {
          "module": "math",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "collections",
          "declaration": "namedtuple",
          "alias": null
        },
        {
          "module": "inspect",
          "declaration": "signature",
          "alias": null
        },
        {
          "module": "scipy.spatial.distance",
          "declaration": "cdist",
          "alias": null
        },
        {
          "module": "scipy.spatial.distance",
          "declaration": "pdist",
          "alias": null
        },
        {
          "module": "scipy.spatial.distance",
          "declaration": "squareform",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "gamma",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "kv",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_kernels",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "CompoundKernel",
          "decorators": [],
          "superclasses": [
            "Kernel"
          ],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
                }
              ],
              "results": [
                {
                  "name": "params",
                  "type": "Dict",
                  "description": "Parameter names mapped to their values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get parameters of this kernel.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : dict\n    Parameter names mapped to their values."
            },
            {
              "name": "theta",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "theta",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The non-fixed, log-transformed hyperparameters of the kernel"
                }
              ],
              "results": [
                {
                  "name": "theta",
                  "type": null,
                  "description": "The non-fixed, log-transformed hyperparameters of the kernel"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Sets the (flattened, log-transformed) non-fixed hyperparameters.\n\nParameters\n----------\ntheta : array of shape (n_dims,)\n    The non-fixed, log-transformed hyperparameters of the kernel"
            },
            {
              "name": "bounds",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "bounds",
                  "type": null,
                  "description": "The log-transformed bounds on the kernel's hyperparameters theta"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the log-transformed bounds on the theta.\n\nReturns\n-------\nbounds : array of shape (n_dims, 2)\n    The log-transformed bounds on the kernel's hyperparameters theta"
            },
            {
              "name": "is_stationary",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is stationary. "
            },
            {
              "name": "requires_vector_input",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is defined on discrete structures. "
            },
            {
              "name": "diag",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Argument to the kernel."
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to `np.diag(self(X))`; however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features) or list of object\n    Argument to the kernel.\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X, n_kernels)\n    Diagonal of kernel k(X, X)"
            }
          ],
          "fullDocstring": "Kernel which is composed of a set of other kernels.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nkernels : list of Kernels\n    The other kernels\n\nExamples\n--------\n>>> from sklearn.gaussian_process.kernels import WhiteKernel\n>>> from sklearn.gaussian_process.kernels import RBF\n>>> from sklearn.gaussian_process.kernels import CompoundKernel\n>>> kernel = CompoundKernel(\n...     [WhiteKernel(noise_level=3.0), RBF(length_scale=2.0)])\n>>> print(kernel.bounds)\n[[-11.51292546  11.51292546]\n [-11.51292546  11.51292546]]\n>>> print(kernel.n_dims)\n2\n>>> print(kernel.theta)\n[1.09861229 0.69314718]"
        },
        {
          "name": "ConstantKernel",
          "decorators": [],
          "superclasses": [
            "Kernel",
            "GenericKernelMixin",
            "StationaryKernelMixin"
          ],
          "methods": [
            {
              "name": "hyperparameter_constant_value",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "diag",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Argument to the kernel."
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features) or list of object\n    Argument to the kernel.\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X,)\n    Diagonal of kernel k(X, X)"
            }
          ],
          "fullDocstring": "Constant kernel.\n\nCan be used as part of a product-kernel where it scales the magnitude of\nthe other factor (kernel) or as part of a sum-kernel, where it modifies\nthe mean of the Gaussian process.\n\n.. math::\n    k(x_1, x_2) = constant\\_value \\;\\forall\\; x_1, x_2\n\nAdding a constant kernel is equivalent to adding a constant::\n\n        kernel = RBF() + ConstantKernel(constant_value=2)\n\nis the same as::\n\n        kernel = RBF() + 2\n\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nconstant_value : float, default=1.0\n    The constant value which defines the covariance:\n    k(x_1, x_2) = constant_value\n\nconstant_value_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on `constant_value`.\n    If set to \"fixed\", `constant_value` cannot be changed during\n    hyperparameter tuning.\n\nExamples\n--------\n>>> from sklearn.datasets import make_friedman2\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\n>>> from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n>>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n>>> kernel = RBF() + ConstantKernel(constant_value=2)\n>>> gpr = GaussianProcessRegressor(kernel=kernel, alpha=5,\n...         random_state=0).fit(X, y)\n>>> gpr.score(X, y)\n0.3696...\n>>> gpr.predict(X[:1,:], return_std=True)\n(array([606.1...]), array([0.24...]))"
        },
        {
          "name": "DotProduct",
          "decorators": [],
          "superclasses": [
            "Kernel"
          ],
          "methods": [
            {
              "name": "hyperparameter_sigma_0",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "diag",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Left argument of the returned kernel k(X, Y)."
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_features)\n    Left argument of the returned kernel k(X, Y).\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X,)\n    Diagonal of kernel k(X, X)."
            },
            {
              "name": "is_stationary",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is stationary. "
            }
          ],
          "fullDocstring": "Dot-Product kernel.\n\nThe DotProduct kernel is non-stationary and can be obtained from linear\nregression by putting :math:`N(0, 1)` priors on the coefficients\nof :math:`x_d (d = 1, . . . , D)` and a prior of :math:`N(0, \\sigma_0^2)`\non the bias. The DotProduct kernel is invariant to a rotation of\nthe coordinates about the origin, but not translations.\nIt is parameterized by a parameter sigma_0 :math:`\\sigma`\nwhich controls the inhomogenity of the kernel. For :math:`\\sigma_0^2 =0`,\nthe kernel is called the homogeneous linear kernel, otherwise\nit is inhomogeneous. The kernel is given by\n\n.. math::\n    k(x_i, x_j) = \\sigma_0 ^ 2 + x_i \\cdot x_j\n\nThe DotProduct kernel is commonly combined with exponentiation.\n\nSee [1]_, Chapter 4, Section 4.2, for further details regarding the\nDotProduct kernel.\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nsigma_0 : float >= 0, default=1.0\n    Parameter controlling the inhomogenity of the kernel. If sigma_0=0,\n    the kernel is homogenous.\n\nsigma_0_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on 'sigma_0'.\n    If set to \"fixed\", 'sigma_0' cannot be changed during\n    hyperparameter tuning.\n\nReferences\n----------\n.. [1] `Carl Edward Rasmussen, Christopher K. I. Williams (2006).\n    \"Gaussian Processes for Machine Learning\". The MIT Press.\n    <http://www.gaussianprocess.org/gpml/>`_\n\nExamples\n--------\n>>> from sklearn.datasets import make_friedman2\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\n>>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n>>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n>>> kernel = DotProduct() + WhiteKernel()\n>>> gpr = GaussianProcessRegressor(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpr.score(X, y)\n0.3680...\n>>> gpr.predict(X[:2,:], return_std=True)\n(array([653.0..., 592.1...]), array([316.6..., 316.6...]))"
        },
        {
          "name": "ExpSineSquared",
          "decorators": [],
          "superclasses": [
            "Kernel",
            "NormalizedKernelMixin",
            "StationaryKernelMixin"
          ],
          "methods": [
            {
              "name": "hyperparameter_length_scale",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the length scale"
            },
            {
              "name": "hyperparameter_periodicity",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Exp-Sine-Squared kernel (aka periodic kernel).\n\nThe ExpSineSquared kernel allows one to model functions which repeat\nthemselves exactly. It is parameterized by a length scale\nparameter :math:`l>0` and a periodicity parameter :math:`p>0`.\nOnly the isotropic variant where :math:`l` is a scalar is\nsupported at the moment. The kernel is given by:\n\n.. math::\n    k(x_i, x_j) = \\text{exp}\\left(-\n    \\frac{ 2\\sin^2(\\pi d(x_i, x_j)/p) }{ l^ 2} \\right)\n\nwhere :math:`l` is the length scale of the kernel, :math:`p` the\nperiodicity of the kernel and :math:`d(\\\\cdot,\\\\cdot)` is the\nEuclidean distance.\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\n\nlength_scale : float > 0, default=1.0\n    The length scale of the kernel.\n\nperiodicity : float > 0, default=1.0\n    The periodicity of the kernel.\n\nlength_scale_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on 'length_scale'.\n    If set to \"fixed\", 'length_scale' cannot be changed during\n    hyperparameter tuning.\n\nperiodicity_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on 'periodicity'.\n    If set to \"fixed\", 'periodicity' cannot be changed during\n    hyperparameter tuning.\n\nExamples\n--------\n>>> from sklearn.datasets import make_friedman2\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\n>>> from sklearn.gaussian_process.kernels import ExpSineSquared\n>>> X, y = make_friedman2(n_samples=50, noise=0, random_state=0)\n>>> kernel = ExpSineSquared(length_scale=1, periodicity=1)\n>>> gpr = GaussianProcessRegressor(kernel=kernel, alpha=5,\n...         random_state=0).fit(X, y)\n>>> gpr.score(X, y)\n0.0144...\n>>> gpr.predict(X[:2,:], return_std=True)\n(array([425.6..., 457.5...]), array([0.3894..., 0.3467...]))"
        },
        {
          "name": "Exponentiation",
          "decorators": [],
          "superclasses": [
            "Kernel"
          ],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
                }
              ],
              "results": [
                {
                  "name": "params",
                  "type": "Dict",
                  "description": "Parameter names mapped to their values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get parameters of this kernel.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : dict\n    Parameter names mapped to their values."
            },
            {
              "name": "hyperparameters",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns a list of all hyperparameter."
            },
            {
              "name": "theta",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "theta",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The non-fixed, log-transformed hyperparameters of the kernel"
                }
              ],
              "results": [
                {
                  "name": "theta",
                  "type": null,
                  "description": "The non-fixed, log-transformed hyperparameters of the kernel"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Sets the (flattened, log-transformed) non-fixed hyperparameters.\n\nParameters\n----------\ntheta : ndarray of shape (n_dims,)\n    The non-fixed, log-transformed hyperparameters of the kernel"
            },
            {
              "name": "bounds",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "bounds",
                  "type": null,
                  "description": "The log-transformed bounds on the kernel's hyperparameters theta"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the log-transformed bounds on the theta.\n\nReturns\n-------\nbounds : ndarray of shape (n_dims, 2)\n    The log-transformed bounds on the kernel's hyperparameters theta"
            },
            {
              "name": "diag",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Argument to the kernel."
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features) or list of object\n    Argument to the kernel.\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X,)\n    Diagonal of kernel k(X, X)"
            },
            {
              "name": "is_stationary",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is stationary. "
            },
            {
              "name": "requires_vector_input",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is defined on discrete structures. "
            }
          ],
          "fullDocstring": "The Exponentiation kernel takes one base kernel and a scalar parameter\n:math:`p` and combines them via\n\n.. math::\n    k_{exp}(X, Y) = k(X, Y) ^p\n\nNote that the `__pow__` magic method is overridden, so\n`Exponentiation(RBF(), 2)` is equivalent to using the ** operator\nwith `RBF() ** 2`.\n\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nkernel : Kernel\n    The base kernel\n\nexponent : float\n    The exponent for the base kernel\n\n\nExamples\n--------\n>>> from sklearn.datasets import make_friedman2\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\n>>> from sklearn.gaussian_process.kernels import (RationalQuadratic,\n...            Exponentiation)\n>>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n>>> kernel = Exponentiation(RationalQuadratic(), exponent=2)\n>>> gpr = GaussianProcessRegressor(kernel=kernel, alpha=5,\n...         random_state=0).fit(X, y)\n>>> gpr.score(X, y)\n0.419...\n>>> gpr.predict(X[:1,:], return_std=True)\n(array([635.5...]), array([0.559...]))"
        },
        {
          "name": "GenericKernelMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "requires_vector_input",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Whether the kernel works only on fixed-length feature vectors."
            }
          ],
          "fullDocstring": "Mixin for kernels which operate on generic objects such as variable-\nlength sequences, trees, and graphs.\n\n.. versionadded:: 0.22"
        },
        {
          "name": "Hyperparameter",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "A kernel hyperparameter's specification in form of a namedtuple.\n\n.. versionadded:: 0.18\n\nAttributes\n----------\nname : str\n    The name of the hyperparameter. Note that a kernel using a\n    hyperparameter with name \"x\" must have the attributes self.x and\n    self.x_bounds\n\nvalue_type : str\n    The type of the hyperparameter. Currently, only \"numeric\"\n    hyperparameters are supported.\n\nbounds : pair of floats >= 0 or \"fixed\"\n    The lower and upper bound on the parameter. If n_elements>1, a pair\n    of 1d array with n_elements each may be given alternatively. If\n    the string \"fixed\" is passed as bounds, the hyperparameter's value\n    cannot be changed.\n\nn_elements : int, default=1\n    The number of elements of the hyperparameter value. Defaults to 1,\n    which corresponds to a scalar hyperparameter. n_elements > 1\n    corresponds to a hyperparameter which is vector-valued,\n    such as, e.g., anisotropic length-scales.\n\nfixed : bool, default=None\n    Whether the value of this hyperparameter is fixed, i.e., cannot be\n    changed during hyperparameter tuning. If None is passed, the \"fixed\" is\n    derived based on the given bounds.\n\nExamples\n--------\n>>> from sklearn.gaussian_process.kernels import ConstantKernel\n>>> from sklearn.datasets import make_friedman2\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\n>>> from sklearn.gaussian_process.kernels import Hyperparameter\n>>> X, y = make_friedman2(n_samples=50, noise=0, random_state=0)\n>>> kernel = ConstantKernel(constant_value=1.0,\n...    constant_value_bounds=(0.0, 10.0))\n\nWe can access each hyperparameter:\n\n>>> for hyperparameter in kernel.hyperparameters:\n...    print(hyperparameter)\nHyperparameter(name='constant_value', value_type='numeric',\nbounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n\n>>> params = kernel.get_params()\n>>> for key in sorted(params): print(f\"{key} : {params[key]}\")\nconstant_value : 1.0\nconstant_value_bounds : (0.0, 10.0)"
        },
        {
          "name": "Kernel",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
                }
              ],
              "results": [
                {
                  "name": "params",
                  "type": "Dict",
                  "description": "Parameter names mapped to their values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get parameters of this kernel.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : dict\n    Parameter names mapped to their values."
            },
            {
              "name": "set_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n\nReturns\n-------\nself"
            },
            {
              "name": "clone_with_theta",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "theta",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The hyperparameters"
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns a clone of self with given hyperparameters theta.\n\nParameters\n----------\ntheta : ndarray of shape (n_dims,)\n    The hyperparameters"
            },
            {
              "name": "n_dims",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the number of non-fixed hyperparameters of the kernel."
            },
            {
              "name": "hyperparameters",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns a list of all hyperparameter specifications."
            },
            {
              "name": "theta",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "theta",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The non-fixed, log-transformed hyperparameters of the kernel"
                }
              ],
              "results": [
                {
                  "name": "theta",
                  "type": null,
                  "description": "The non-fixed, log-transformed hyperparameters of the kernel"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Sets the (flattened, log-transformed) non-fixed hyperparameters.\n\nParameters\n----------\ntheta : ndarray of shape (n_dims,)\n    The non-fixed, log-transformed hyperparameters of the kernel"
            },
            {
              "name": "bounds",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "bounds",
                  "type": null,
                  "description": "The log-transformed bounds on the kernel's hyperparameters theta"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the log-transformed bounds on the theta.\n\nReturns\n-------\nbounds : ndarray of shape (n_dims, 2)\n    The log-transformed bounds on the kernel's hyperparameters theta"
            },
            {
              "name": "diag",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Left argument of the returned kernel k(X, Y)"
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : array-like of shape (n_samples,)\n    Left argument of the returned kernel k(X, Y)\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X,)\n    Diagonal of kernel k(X, X)"
            },
            {
              "name": "is_stationary",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is stationary. "
            },
            {
              "name": "requires_vector_input",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is defined on fixed-length feature\nvectors or generic objects. Defaults to True for backward\ncompatibility."
            }
          ],
          "fullDocstring": "Base class for all kernels.\n\n.. versionadded:: 0.18"
        },
        {
          "name": "KernelOperator",
          "decorators": [],
          "superclasses": [
            "Kernel"
          ],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
                }
              ],
              "results": [
                {
                  "name": "params",
                  "type": "Dict",
                  "description": "Parameter names mapped to their values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get parameters of this kernel.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : dict\n    Parameter names mapped to their values."
            },
            {
              "name": "hyperparameters",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns a list of all hyperparameter."
            },
            {
              "name": "theta",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "theta",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The non-fixed, log-transformed hyperparameters of the kernel"
                }
              ],
              "results": [
                {
                  "name": "theta",
                  "type": null,
                  "description": "The non-fixed, log-transformed hyperparameters of the kernel"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Sets the (flattened, log-transformed) non-fixed hyperparameters.\n\nParameters\n----------\ntheta : ndarray of shape (n_dims,)\n    The non-fixed, log-transformed hyperparameters of the kernel"
            },
            {
              "name": "bounds",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "bounds",
                  "type": null,
                  "description": "The log-transformed bounds on the kernel's hyperparameters theta"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the log-transformed bounds on the theta.\n\nReturns\n-------\nbounds : ndarray of shape (n_dims, 2)\n    The log-transformed bounds on the kernel's hyperparameters theta"
            },
            {
              "name": "is_stationary",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is stationary. "
            },
            {
              "name": "requires_vector_input",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is stationary. "
            }
          ],
          "fullDocstring": "Base class for all kernel operators.\n\n.. versionadded:: 0.18"
        },
        {
          "name": "Matern",
          "decorators": [],
          "superclasses": [
            "RBF"
          ],
          "methods": [],
          "fullDocstring": "Matern kernel.\n\nThe class of Matern kernels is a generalization of the :class:`RBF`.\nIt has an additional parameter :math:`\\nu` which controls the\nsmoothness of the resulting function. The smaller :math:`\\nu`,\nthe less smooth the approximated function is.\nAs :math:`\\nu\\rightarrow\\infty`, the kernel becomes equivalent to\nthe :class:`RBF` kernel. When :math:`\\nu = 1/2`, the Mat\u00e9rn kernel\nbecomes identical to the absolute exponential kernel.\nImportant intermediate values are\n:math:`\\nu=1.5` (once differentiable functions)\nand :math:`\\nu=2.5` (twice differentiable functions).\n\nThe kernel is given by:\n\n.. math::\n     k(x_i, x_j) =  \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\n     \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\n     \\Bigg)^\\nu K_\\nu\\Bigg(\n     \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)\n\n\n\nwhere :math:`d(\\cdot,\\cdot)` is the Euclidean distance,\n:math:`K_{\\nu}(\\cdot)` is a modified Bessel function and\n:math:`\\Gamma(\\cdot)` is the gamma function.\nSee [1]_, Chapter 4, Section 4.2, for details regarding the different\nvariants of the Matern kernel.\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nlength_scale : float or ndarray of shape (n_features,), default=1.0\n    The length scale of the kernel. If a float, an isotropic kernel is\n    used. If an array, an anisotropic kernel is used where each dimension\n    of l defines the length-scale of the respective feature dimension.\n\nlength_scale_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on 'length_scale'.\n    If set to \"fixed\", 'length_scale' cannot be changed during\n    hyperparameter tuning.\n\nnu : float, default=1.5\n    The parameter nu controlling the smoothness of the learned function.\n    The smaller nu, the less smooth the approximated function is.\n    For nu=inf, the kernel becomes equivalent to the RBF kernel and for\n    nu=0.5 to the absolute exponential kernel. Important intermediate\n    values are nu=1.5 (once differentiable functions) and nu=2.5\n    (twice differentiable functions). Note that values of nu not in\n    [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost\n    (appr. 10 times higher) since they require to evaluate the modified\n    Bessel function. Furthermore, in contrast to l, nu is kept fixed to\n    its initial value and not optimized.\n\nReferences\n----------\n.. [1] `Carl Edward Rasmussen, Christopher K. I. Williams (2006).\n    \"Gaussian Processes for Machine Learning\". The MIT Press.\n    <http://www.gaussianprocess.org/gpml/>`_\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.gaussian_process import GaussianProcessClassifier\n>>> from sklearn.gaussian_process.kernels import Matern\n>>> X, y = load_iris(return_X_y=True)\n>>> kernel = 1.0 * Matern(length_scale=1.0, nu=1.5)\n>>> gpc = GaussianProcessClassifier(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpc.score(X, y)\n0.9866...\n>>> gpc.predict_proba(X[:2,:])\narray([[0.8513..., 0.0368..., 0.1117...],\n        [0.8086..., 0.0693..., 0.1220...]])"
        },
        {
          "name": "NormalizedKernelMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "diag",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Left argument of the returned kernel k(X, Y)"
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_features)\n    Left argument of the returned kernel k(X, Y)\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X,)\n    Diagonal of kernel k(X, X)"
            }
          ],
          "fullDocstring": "Mixin for kernels which are normalized: k(X, X)=1.\n\n.. versionadded:: 0.18"
        },
        {
          "name": "PairwiseKernel",
          "decorators": [],
          "superclasses": [
            "Kernel"
          ],
          "methods": [
            {
              "name": "hyperparameter_gamma",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "diag",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Left argument of the returned kernel k(X, Y)"
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_features)\n    Left argument of the returned kernel k(X, Y)\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X,)\n    Diagonal of kernel k(X, X)"
            },
            {
              "name": "is_stationary",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is stationary. "
            }
          ],
          "fullDocstring": "Wrapper for kernels in sklearn.metrics.pairwise.\n\nA thin wrapper around the functionality of the kernels in\nsklearn.metrics.pairwise.\n\nNote: Evaluation of eval_gradient is not analytic but numeric and all\n      kernels support only isotropic distances. The parameter gamma is\n      considered to be a hyperparameter and may be optimized. The other\n      kernel parameters are set directly at initialization and are kept\n      fixed.\n\n.. versionadded:: 0.18\n\nParameters\n----------\ngamma : float, default=1.0\n    Parameter gamma of the pairwise kernel specified by metric. It should\n    be positive.\n\ngamma_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on 'gamma'.\n    If set to \"fixed\", 'gamma' cannot be changed during\n    hyperparameter tuning.\n\nmetric : {\"linear\", \"additive_chi2\", \"chi2\", \"poly\", \"polynomial\",               \"rbf\", \"laplacian\", \"sigmoid\", \"cosine\"} or callable,               default=\"linear\"\n    The metric to use when calculating kernel between instances in a\n    feature array. If metric is a string, it must be one of the metrics\n    in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n    If metric is \"precomputed\", X is assumed to be a kernel matrix.\n    Alternatively, if metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays from X as input and return a value indicating\n    the distance between them.\n\npairwise_kernels_kwargs : dict, default=None\n    All entries of this dict (if any) are passed as keyword arguments to\n    the pairwise kernel function.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.gaussian_process import GaussianProcessClassifier\n>>> from sklearn.gaussian_process.kernels import PairwiseKernel\n>>> X, y = load_iris(return_X_y=True)\n>>> kernel = PairwiseKernel(metric='rbf')\n>>> gpc = GaussianProcessClassifier(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpc.score(X, y)\n0.9733...\n>>> gpc.predict_proba(X[:2,:])\narray([[0.8880..., 0.05663..., 0.05532...],\n       [0.8676..., 0.07073..., 0.06165...]])"
        },
        {
          "name": "Product",
          "decorators": [],
          "superclasses": [
            "KernelOperator"
          ],
          "methods": [
            {
              "name": "diag",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Argument to the kernel."
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features) or list of object\n    Argument to the kernel.\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X,)\n    Diagonal of kernel k(X, X)"
            }
          ],
          "fullDocstring": "The `Product` kernel takes two kernels :math:`k_1` and :math:`k_2`\nand combines them via\n\n.. math::\n    k_{prod}(X, Y) = k_1(X, Y) * k_2(X, Y)\n\nNote that the `__mul__` magic method is overridden, so\n`Product(RBF(), RBF())` is equivalent to using the * operator\nwith `RBF() * RBF()`.\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nk1 : Kernel\n    The first base-kernel of the product-kernel\n\nk2 : Kernel\n    The second base-kernel of the product-kernel\n\n\nExamples\n--------\n>>> from sklearn.datasets import make_friedman2\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\n>>> from sklearn.gaussian_process.kernels import (RBF, Product,\n...            ConstantKernel)\n>>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n>>> kernel = Product(ConstantKernel(2), RBF())\n>>> gpr = GaussianProcessRegressor(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpr.score(X, y)\n1.0\n>>> kernel\n1.41**2 * RBF(length_scale=1)"
        },
        {
          "name": "RBF",
          "decorators": [],
          "superclasses": [
            "Kernel",
            "NormalizedKernelMixin",
            "StationaryKernelMixin"
          ],
          "methods": [
            {
              "name": "anisotropic",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "hyperparameter_length_scale",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Radial-basis function kernel (aka squared-exponential kernel).\n\nThe RBF kernel is a stationary kernel. It is also known as the\n\"squared exponential\" kernel. It is parameterized by a length scale\nparameter :math:`l>0`, which can either be a scalar (isotropic variant\nof the kernel) or a vector with the same number of dimensions as the inputs\nX (anisotropic variant of the kernel). The kernel is given by:\n\n.. math::\n    k(x_i, x_j) = \\exp\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)\n\nwhere :math:`l` is the length scale of the kernel and\n:math:`d(\\cdot,\\cdot)` is the Euclidean distance.\nFor advice on how to set the length scale parameter, see e.g. [1]_.\n\nThis kernel is infinitely differentiable, which implies that GPs with this\nkernel as covariance function have mean square derivatives of all orders,\nand are thus very smooth.\nSee [2]_, Chapter 4, Section 4.2, for further details of the RBF kernel.\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nlength_scale : float or ndarray of shape (n_features,), default=1.0\n    The length scale of the kernel. If a float, an isotropic kernel is\n    used. If an array, an anisotropic kernel is used where each dimension\n    of l defines the length-scale of the respective feature dimension.\n\nlength_scale_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on 'length_scale'.\n    If set to \"fixed\", 'length_scale' cannot be changed during\n    hyperparameter tuning.\n\nReferences\n----------\n.. [1] `David Duvenaud (2014). \"The Kernel Cookbook:\n    Advice on Covariance functions\".\n    <https://www.cs.toronto.edu/~duvenaud/cookbook/>`_\n\n.. [2] `Carl Edward Rasmussen, Christopher K. I. Williams (2006).\n    \"Gaussian Processes for Machine Learning\". The MIT Press.\n    <http://www.gaussianprocess.org/gpml/>`_\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.gaussian_process import GaussianProcessClassifier\n>>> from sklearn.gaussian_process.kernels import RBF\n>>> X, y = load_iris(return_X_y=True)\n>>> kernel = 1.0 * RBF(1.0)\n>>> gpc = GaussianProcessClassifier(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpc.score(X, y)\n0.9866...\n>>> gpc.predict_proba(X[:2,:])\narray([[0.8354..., 0.03228..., 0.1322...],\n       [0.7906..., 0.0652..., 0.1441...]])"
        },
        {
          "name": "RationalQuadratic",
          "decorators": [],
          "superclasses": [
            "Kernel",
            "NormalizedKernelMixin",
            "StationaryKernelMixin"
          ],
          "methods": [
            {
              "name": "hyperparameter_length_scale",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "hyperparameter_alpha",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Rational Quadratic kernel.\n\nThe RationalQuadratic kernel can be seen as a scale mixture (an infinite\nsum) of RBF kernels with different characteristic length scales. It is\nparameterized by a length scale parameter :math:`l>0` and a scale\nmixture parameter :math:`\\alpha>0`. Only the isotropic variant\nwhere length_scale :math:`l` is a scalar is supported at the moment.\nThe kernel is given by:\n\n.. math::\n    k(x_i, x_j) = \\left(\n    1 + \\frac{d(x_i, x_j)^2 }{ 2\\alpha  l^2}\\right)^{-\\alpha}\n\nwhere :math:`\\alpha` is the scale mixture parameter, :math:`l` is\nthe length scale of the kernel and :math:`d(\\cdot,\\cdot)` is the\nEuclidean distance.\nFor advice on how to set the parameters, see e.g. [1]_.\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nlength_scale : float > 0, default=1.0\n    The length scale of the kernel.\n\nalpha : float > 0, default=1.0\n    Scale mixture parameter\n\nlength_scale_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on 'length_scale'.\n    If set to \"fixed\", 'length_scale' cannot be changed during\n    hyperparameter tuning.\n\nalpha_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on 'alpha'.\n    If set to \"fixed\", 'alpha' cannot be changed during\n    hyperparameter tuning.\n\nReferences\n----------\n.. [1] `David Duvenaud (2014). \"The Kernel Cookbook:\n    Advice on Covariance functions\".\n    <https://www.cs.toronto.edu/~duvenaud/cookbook/>`_\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.gaussian_process import GaussianProcessClassifier\n>>> from sklearn.gaussian_process.kernels import Matern\n>>> X, y = load_iris(return_X_y=True)\n>>> kernel = RationalQuadratic(length_scale=1.0, alpha=1.5)\n>>> gpc = GaussianProcessClassifier(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpc.score(X, y)\n0.9733...\n>>> gpc.predict_proba(X[:2,:])\narray([[0.8881..., 0.0566..., 0.05518...],\n        [0.8678..., 0.0707... , 0.0614...]])"
        },
        {
          "name": "StationaryKernelMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "is_stationary",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns whether the kernel is stationary. "
            }
          ],
          "fullDocstring": "Mixin for kernels which are stationary: k(X, Y)= f(X-Y).\n\n.. versionadded:: 0.18"
        },
        {
          "name": "Sum",
          "decorators": [],
          "superclasses": [
            "KernelOperator"
          ],
          "methods": [
            {
              "name": "diag",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Argument to the kernel."
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to `np.diag(self(X))`; however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features) or list of object\n    Argument to the kernel.\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X,)\n    Diagonal of kernel k(X, X)"
            }
          ],
          "fullDocstring": "The `Sum` kernel takes two kernels :math:`k_1` and :math:`k_2`\nand combines them via\n\n.. math::\n    k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)\n\nNote that the `__add__` magic method is overridden, so\n`Sum(RBF(), RBF())` is equivalent to using the + operator\nwith `RBF() + RBF()`.\n\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nk1 : Kernel\n    The first base-kernel of the sum-kernel\n\nk2 : Kernel\n    The second base-kernel of the sum-kernel\n\nExamples\n--------\n>>> from sklearn.datasets import make_friedman2\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\n>>> from sklearn.gaussian_process.kernels import RBF, Sum, ConstantKernel\n>>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n>>> kernel = Sum(ConstantKernel(2), RBF())\n>>> gpr = GaussianProcessRegressor(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpr.score(X, y)\n1.0\n>>> kernel\n1.41**2 + RBF(length_scale=1)"
        },
        {
          "name": "WhiteKernel",
          "decorators": [],
          "superclasses": [
            "Kernel",
            "GenericKernelMixin",
            "StationaryKernelMixin"
          ],
          "methods": [
            {
              "name": "hyperparameter_noise_level",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "diag",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Argument to the kernel."
                }
              ],
              "results": [
                {
                  "name": "K_diag",
                  "type": null,
                  "description": "Diagonal of kernel k(X, X)"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features) or list of object\n    Argument to the kernel.\n\nReturns\n-------\nK_diag : ndarray of shape (n_samples_X,)\n    Diagonal of kernel k(X, X)"
            }
          ],
          "fullDocstring": "White kernel.\n\nThe main use-case of this kernel is as part of a sum-kernel where it\nexplains the noise of the signal as independently and identically\nnormally-distributed. The parameter noise_level equals the variance of this\nnoise.\n\n.. math::\n    k(x_1, x_2) = noise\\_level \\text{ if } x_i == x_j \\text{ else } 0\n\n\nRead more in the :ref:`User Guide <gp_kernels>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nnoise_level : float, default=1.0\n    Parameter controlling the noise level (variance)\n\nnoise_level_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)\n    The lower and upper bound on 'noise_level'.\n    If set to \"fixed\", 'noise_level' cannot be changed during\n    hyperparameter tuning.\n\nExamples\n--------\n>>> from sklearn.datasets import make_friedman2\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\n>>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n>>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n>>> kernel = DotProduct() + WhiteKernel(noise_level=0.5)\n>>> gpr = GaussianProcessRegressor(kernel=kernel,\n...         random_state=0).fit(X, y)\n>>> gpr.score(X, y)\n0.3680...\n>>> gpr.predict(X[:2,:], return_std=True)\n(array([653.0..., 592.1... ]), array([316.6..., 316.6...]))"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.impute",
      "imports": [
        {
          "module": "typing",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn._base",
          "declaration": "MissingIndicator",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "SimpleImputer",
          "alias": null
        },
        {
          "module": "sklearn._iterative",
          "declaration": "IterativeImputer",
          "alias": null
        },
        {
          "module": "sklearn._knn",
          "declaration": "KNNImputer",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.impute._base",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "numpy.ma",
          "alias": "ma"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "Counter",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": "sp"
        },
        {
          "module": "scipy",
          "declaration": "stats",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "is_scalar_nan",
          "alias": null
        },
        {
          "module": "sklearn.utils._mask",
          "declaration": "_get_mask",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "_get_median",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "FLOAT_DTYPES",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "MissingIndicator",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, where ``n_samples`` is the number of samples and\n``n_features`` is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the transformer on X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Input data, where ``n_samples`` is the number of samples and\n    ``n_features`` is the number of features.\n\nReturns\n-------\nself : object\n    Returns self."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data to complete."
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "The missing indicator for input data. The data type of ``Xt``\nwill be boolean."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate missing values indicator for X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The input data to complete.\n\nReturns\n-------\nXt : {ndarray or sparse matrix}, shape (n_samples, n_features)         or (n_samples, n_features_with_missing)\n    The missing indicator for input data. The data type of ``Xt``\n    will be boolean."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data to complete."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "The missing indicator for input data. The data type of ``Xt``\nwill be boolean."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate missing values indicator for X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The input data to complete.\n\nReturns\n-------\nXt : {ndarray or sparse matrix}, shape (n_samples, n_features)         or (n_samples, n_features_with_missing)\n    The missing indicator for input data. The data type of ``Xt``\n    will be boolean."
            }
          ],
          "fullDocstring": "Binary indicators for missing values.\n\nNote that this component typically should not be used in a vanilla\n:class:`Pipeline` consisting of transformers and a classifier, but rather\ncould be added using a :class:`FeatureUnion` or :class:`ColumnTransformer`.\n\nRead more in the :ref:`User Guide <impute>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nmissing_values : int, float, string, np.nan or None, default=np.nan\n    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n\nfeatures : {'missing-only', 'all'}, default='missing-only'\n    Whether the imputer mask should represent all or a subset of\n    features.\n\n    - If 'missing-only' (default), the imputer mask will only represent\n      features containing missing values during fit time.\n    - If 'all', the imputer mask will represent all features.\n\nsparse : bool or 'auto', default='auto'\n    Whether the imputer mask format should be sparse or dense.\n\n    - If 'auto' (default), the imputer mask will be of same type as\n      input.\n    - If True, the imputer mask will be a sparse matrix.\n    - If False, the imputer mask will be a numpy array.\n\nerror_on_new : bool, default=True\n    If True, transform will raise an error when there are features with\n    missing values in transform that have no missing values in fit. This is\n    applicable only when `features='missing-only'`.\n\nAttributes\n----------\nfeatures_ : ndarray, shape (n_missing_features,) or (n_features,)\n    The features indices which will be returned when calling ``transform``.\n    They are computed during ``fit``. For ``features='all'``, it is\n    to ``range(n_features)``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.impute import MissingIndicator\n>>> X1 = np.array([[np.nan, 1, 3],\n...                [4, 0, np.nan],\n...                [8, 1, 0]])\n>>> X2 = np.array([[5, 1, np.nan],\n...                [np.nan, 2, 3],\n...                [2, 4, 0]])\n>>> indicator = MissingIndicator()\n>>> indicator.fit(X1)\nMissingIndicator()\n>>> X2_tr = indicator.transform(X2)\n>>> X2_tr\narray([[False,  True],\n       [ True, False],\n       [False, False]])"
        },
        {
          "name": "SimpleImputer",
          "decorators": [],
          "superclasses": [
            "_BaseImputer"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, where ``n_samples`` is the number of samples and\n``n_features`` is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the imputer on X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Input data, where ``n_samples`` is the number of samples and\n    ``n_features`` is the number of features.\n\nReturns\n-------\nself : SimpleImputer"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data to complete."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Impute all missing values in X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The input data to complete."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The imputed data to be reverted to original data. It has to be\nan augmented array of imputed data and the missing indicator mask."
                }
              ],
              "results": [
                {
                  "name": "X_original",
                  "type": null,
                  "description": "The original X with missing values as it was prior\nto imputation."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Convert the data back to the original representation.\n\nInverts the `transform` operation performed on an array.\nThis operation can only be performed after :class:`SimpleImputer` is\ninstantiated with `add_indicator=True`.\n\nNote that ``inverse_transform`` can only invert the transform in\nfeatures that have binary indicators for missing values. If a feature\nhas no missing values at ``fit`` time, the feature won't have a binary\nindicator, and the imputation done at ``transform`` time won't be\ninverted.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nX : array-like of shape                 (n_samples, n_features + n_features_missing_indicator)\n    The imputed data to be reverted to original data. It has to be\n    an augmented array of imputed data and the missing indicator mask.\n\nReturns\n-------\nX_original : ndarray of shape (n_samples, n_features)\n    The original X with missing values as it was prior\n    to imputation."
            }
          ],
          "fullDocstring": "Imputation transformer for completing missing values.\n\nRead more in the :ref:`User Guide <impute>`.\n\n.. versionadded:: 0.20\n   `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`\n   estimator which is now removed.\n\nParameters\n----------\nmissing_values : int, float, str, np.nan or None, default=np.nan\n    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n\nstrategy : string, default='mean'\n    The imputation strategy.\n\n    - If \"mean\", then replace missing values using the mean along\n      each column. Can only be used with numeric data.\n    - If \"median\", then replace missing values using the median along\n      each column. Can only be used with numeric data.\n    - If \"most_frequent\", then replace missing using the most frequent\n      value along each column. Can be used with strings or numeric data.\n      If there is more than one such value, only the smallest is returned.\n    - If \"constant\", then replace missing values with fill_value. Can be\n      used with strings or numeric data.\n\n    .. versionadded:: 0.20\n       strategy=\"constant\" for fixed value imputation.\n\nfill_value : string or numerical value, default=None\n    When strategy == \"constant\", fill_value is used to replace all\n    occurrences of missing_values.\n    If left to the default, fill_value will be 0 when imputing numerical\n    data and \"missing_value\" for strings or object data types.\n\nverbose : integer, default=0\n    Controls the verbosity of the imputer.\n\ncopy : boolean, default=True\n    If True, a copy of X will be created. If False, imputation will\n    be done in-place whenever possible. Note that, in the following cases,\n    a new copy will always be made, even if `copy=False`:\n\n    - If X is not an array of floating values;\n    - If X is encoded as a CSR matrix;\n    - If add_indicator=True.\n\nadd_indicator : boolean, default=False\n    If True, a :class:`MissingIndicator` transform will stack onto output\n    of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on\n    the missing indicator even if there are missing values at\n    transform/test time.\n\nAttributes\n----------\nstatistics_ : array of shape (n_features,)\n    The imputation fill value for each feature.\n    Computing statistics can result in `np.nan` values.\n    During :meth:`transform`, features corresponding to `np.nan`\n    statistics will be discarded.\n\nindicator_ : :class:`~sklearn.impute.MissingIndicator`\n    Indicator used to add binary indicators for missing values.\n    ``None`` if add_indicator is False.\n\nSee Also\n--------\nIterativeImputer : Multivariate imputation of missing values.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.impute import SimpleImputer\n>>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n>>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\nSimpleImputer()\n>>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n>>> print(imp_mean.transform(X))\n[[ 7.   2.   3. ]\n [ 4.   3.5  6. ]\n [10.   3.5  9. ]]\n\nNotes\n-----\nColumns which only contained missing values at :meth:`fit` are discarded\nupon :meth:`transform` if strategy is not \"constant\"."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.impute._iterative",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "namedtuple",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "stats",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.impute._base",
          "declaration": "SimpleImputer",
          "alias": null
        },
        {
          "module": "sklearn.impute._base",
          "declaration": "_BaseImputer",
          "alias": null
        },
        {
          "module": "sklearn.impute._base",
          "declaration": "_check_inputs_dtype",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "normalize",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "is_scalar_nan",
          "alias": null
        },
        {
          "module": "sklearn.utils._mask",
          "declaration": "_get_mask",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "FLOAT_DTYPES",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "time",
          "declaration": "time",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "IterativeImputer",
          "decorators": [],
          "superclasses": [
            "_BaseImputer"
          ],
          "methods": [
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, where \"n_samples\" is the number of samples and\n\"n_features\" is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "The imputed input data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fits the imputer on X and return the transformed X.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    Input data, where \"n_samples\" is the number of samples and\n    \"n_features\" is the number of features.\n\ny : ignored.\n\nReturns\n-------\nXt : array-like, shape (n_samples, n_features)\n    The imputed input data."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data to complete."
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "The imputed input data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Imputes all missing values in X.\n\nNote that this is stochastic, and that if random_state is not fixed,\nrepeated calls, or permuted input, will yield different results.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input data to complete.\n\nReturns\n-------\nXt : array-like, shape (n_samples, n_features)\n     The imputed input data."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, where \"n_samples\" is the number of samples and\n\"n_features\" is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fits the imputer on X and return self.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    Input data, where \"n_samples\" is the number of samples and\n    \"n_features\" is the number of features.\n\ny : ignored\n\nReturns\n-------\nself : object\n    Returns self."
            }
          ],
          "fullDocstring": "Multivariate imputer that estimates each feature from all the others.\n\nA strategy for imputing missing values by modeling each feature with\nmissing values as a function of other features in a round-robin fashion.\n\nRead more in the :ref:`User Guide <iterative_imputer>`.\n\n.. versionadded:: 0.21\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_iterative_imputer``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_iterative_imputer  # noqa\n    >>> # now you can import normally from sklearn.impute\n    >>> from sklearn.impute import IterativeImputer\n\nParameters\n----------\nestimator : estimator object, default=BayesianRidge()\n    The estimator to use at each step of the round-robin imputation.\n    If ``sample_posterior`` is True, the estimator must support\n    ``return_std`` in its ``predict`` method.\n\nmissing_values : int, np.nan, default=np.nan\n    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n\nsample_posterior : boolean, default=False\n    Whether to sample from the (Gaussian) predictive posterior of the\n    fitted estimator for each imputation. Estimator must support\n    ``return_std`` in its ``predict`` method if set to ``True``. Set to\n    ``True`` if using ``IterativeImputer`` for multiple imputations.\n\nmax_iter : int, default=10\n    Maximum number of imputation rounds to perform before returning the\n    imputations computed during the final round. A round is a single\n    imputation of each feature with missing values. The stopping criterion\n    is met once `max(abs(X_t - X_{t-1}))/max(abs(X[known_vals]))` < tol,\n    where `X_t` is `X` at iteration `t. Note that early stopping is only\n    applied if ``sample_posterior=False``.\n\ntol : float, default=1e-3\n    Tolerance of the stopping condition.\n\nn_nearest_features : int, default=None\n    Number of other features to use to estimate the missing values of\n    each feature column. Nearness between features is measured using\n    the absolute correlation coefficient between each feature pair (after\n    initial imputation). To ensure coverage of features throughout the\n    imputation process, the neighbor features are not necessarily nearest,\n    but are drawn with probability proportional to correlation for each\n    imputed target feature. Can provide significant speed-up when the\n    number of features is huge. If ``None``, all features will be used.\n\ninitial_strategy : str, default='mean'\n    Which strategy to use to initialize the missing values. Same as the\n    ``strategy`` parameter in :class:`~sklearn.impute.SimpleImputer`\n    Valid values: {\"mean\", \"median\", \"most_frequent\", or \"constant\"}.\n\nimputation_order : str, default='ascending'\n    The order in which the features will be imputed. Possible values:\n\n    \"ascending\"\n        From features with fewest missing values to most.\n    \"descending\"\n        From features with most missing values to fewest.\n    \"roman\"\n        Left to right.\n    \"arabic\"\n        Right to left.\n    \"random\"\n        A random order for each round.\n\nskip_complete : boolean, default=False\n    If ``True`` then features with missing values during ``transform``\n    which did not have any missing values during ``fit`` will be imputed\n    with the initial imputation method only. Set to ``True`` if you have\n    many features with no missing values at both ``fit`` and ``transform``\n    time to save compute.\n\nmin_value : float or array-like of shape (n_features,), default=-np.inf\n    Minimum possible imputed value. Broadcast to shape (n_features,) if\n    scalar. If array-like, expects shape (n_features,), one min value for\n    each feature. The default is `-np.inf`.\n\n    .. versionchanged:: 0.23\n       Added support for array-like.\n\nmax_value : float or array-like of shape (n_features,), default=np.inf\n    Maximum possible imputed value. Broadcast to shape (n_features,) if\n    scalar. If array-like, expects shape (n_features,), one max value for\n    each feature. The default is `np.inf`.\n\n    .. versionchanged:: 0.23\n       Added support for array-like.\n\nverbose : int, default=0\n    Verbosity flag, controls the debug messages that are issued\n    as functions are evaluated. The higher, the more verbose. Can be 0, 1,\n    or 2.\n\nrandom_state : int, RandomState instance or None, default=None\n    The seed of the pseudo random number generator to use. Randomizes\n    selection of estimator features if n_nearest_features is not None, the\n    ``imputation_order`` if ``random``, and the sampling from posterior if\n    ``sample_posterior`` is True. Use an integer for determinism.\n    See :term:`the Glossary <random_state>`.\n\nadd_indicator : boolean, default=False\n    If True, a :class:`MissingIndicator` transform will stack onto output\n    of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on\n    the missing indicator even if there are missing values at\n    transform/test time.\n\nAttributes\n----------\ninitial_imputer_ : object of type :class:`~sklearn.impute.SimpleImputer`\n    Imputer used to initialize the missing values.\n\nimputation_sequence_ : list of tuples\n    Each tuple has ``(feat_idx, neighbor_feat_idx, estimator)``, where\n    ``feat_idx`` is the current feature to be imputed,\n    ``neighbor_feat_idx`` is the array of other features used to impute the\n    current feature, and ``estimator`` is the trained estimator used for\n    the imputation. Length is ``self.n_features_with_missing_ *\n    self.n_iter_``.\n\nn_iter_ : int\n    Number of iteration rounds that occurred. Will be less than\n    ``self.max_iter`` if early stopping criterion was reached.\n\nn_features_with_missing_ : int\n    Number of features with missing values.\n\nindicator_ : :class:`~sklearn.impute.MissingIndicator`\n    Indicator used to add binary indicators for missing values.\n    ``None`` if add_indicator is False.\n\nrandom_state_ : RandomState instance\n    RandomState instance that is generated either from a seed, the random\n    number generator or by `np.random`.\n\nSee Also\n--------\nSimpleImputer : Univariate imputation of missing values.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.experimental import enable_iterative_imputer\n>>> from sklearn.impute import IterativeImputer\n>>> imp_mean = IterativeImputer(random_state=0)\n>>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\nIterativeImputer(random_state=0)\n>>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n>>> imp_mean.transform(X)\narray([[ 6.9584...,  2.       ,  3.        ],\n       [ 4.       ,  2.6000...,  6.        ],\n       [10.       ,  4.9999...,  9.        ]])\n\nNotes\n-----\nTo support imputation in inductive mode we store each feature's estimator\nduring the ``fit`` phase, and predict without refitting (in order) during\nthe ``transform`` phase.\n\nFeatures which contain all missing values at ``fit`` are discarded upon\n``transform``.\n\nReferences\n----------\n.. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \"mice:\n    Multivariate Imputation by Chained Equations in R\". Journal of\n    Statistical Software 45: 1-67.\n    <https://www.jstatsoft.org/article/view/v045i03>`_\n\n.. [2] `S. F. Buck, (1960). \"A Method of Estimation of Missing Values in\n    Multivariate Data Suitable for use with an Electronic Computer\".\n    Journal of the Royal Statistical Society 22(2): 302-306.\n    <https://www.jstor.org/stable/2984099>`_"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.impute._knn",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.impute._base",
          "declaration": "_BaseImputer",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "pairwise_distances_chunked",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "_NAN_METRICS",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "_check_weights",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "_get_weights",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "is_scalar_nan",
          "alias": null
        },
        {
          "module": "sklearn.utils._mask",
          "declaration": "_get_mask",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "FLOAT_DTYPES",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KNNImputer",
          "decorators": [],
          "superclasses": [
            "_BaseImputer"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, where `n_samples` is the number of samples and\n`n_features` is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the imputer on X.\n\nParameters\n----------\nX : array-like shape of (n_samples, n_features)\n    Input data, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data to complete."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "The imputed dataset. `n_output_features` is the number of features\nthat is not always missing during `fit`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Impute all missing values in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input data to complete.\n\nReturns\n-------\nX : array-like of shape (n_samples, n_output_features)\n    The imputed dataset. `n_output_features` is the number of features\n    that is not always missing during `fit`."
            }
          ],
          "fullDocstring": "Imputation for completing missing values using k-Nearest Neighbors.\n\nEach sample's missing values are imputed using the mean value from\n`n_neighbors` nearest neighbors found in the training set. Two samples are\nclose if the features that neither is missing are close.\n\nRead more in the :ref:`User Guide <knnimpute>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nmissing_values : int, float, str, np.nan or None, default=np.nan\n    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to np.nan, since `pd.NA` will be converted to np.nan.\n\nn_neighbors : int, default=5\n    Number of neighboring samples to use for imputation.\n\nweights : {'uniform', 'distance'} or callable, default='uniform'\n    Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights. All points in each neighborhood are\n      weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - callable : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\nmetric : {'nan_euclidean'} or callable, default='nan_euclidean'\n    Distance metric for searching neighbors. Possible values:\n\n    - 'nan_euclidean'\n    - callable : a user-defined function which conforms to the definition\n      of ``_pairwise_callable(X, Y, metric, **kwds)``. The function\n      accepts two arrays, X and Y, and a `missing_values` keyword in\n      `kwds` and returns a scalar distance value.\n\ncopy : bool, default=True\n    If True, a copy of X will be created. If False, imputation will\n    be done in-place whenever possible.\n\nadd_indicator : bool, default=False\n    If True, a :class:`MissingIndicator` transform will stack onto the\n    output of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on the\n    missing indicator even if there are missing values at transform/test\n    time.\n\nAttributes\n----------\nindicator_ : :class:`~sklearn.impute.MissingIndicator`\n    Indicator used to add binary indicators for missing values.\n    ``None`` if add_indicator is False.\n\nReferences\n----------\n* Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor\n  Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing\n  value estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17\n  no. 6, 2001 Pages 520-525.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.impute import KNNImputer\n>>> X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n>>> imputer = KNNImputer(n_neighbors=2)\n>>> imputer.fit_transform(X)\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.inspection",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._partial_dependence",
          "declaration": "partial_dependence",
          "alias": null
        },
        {
          "module": "sklearn._permutation_importance",
          "declaration": "permutation_importance",
          "alias": null
        },
        {
          "module": "sklearn._plot.partial_dependence",
          "declaration": "PartialDependenceDisplay",
          "alias": null
        },
        {
          "module": "sklearn._plot.partial_dependence",
          "declaration": "plot_partial_dependence",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.inspection._partial_dependence",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections.abc",
          "declaration": "Iterable",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy.stats.mstats",
          "declaration": "mquantiles",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_regressor",
          "alias": null
        },
        {
          "module": "sklearn.ensemble",
          "declaration": "RandomForestRegressor",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._gb",
          "declaration": "BaseGradientBoosting",
          "alias": null
        },
        {
          "module": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting",
          "declaration": "BaseHistGradientBoosting",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.pipeline",
          "declaration": "Pipeline",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "DecisionTreeRegressor",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_determine_key_type",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_get_column_indices",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_matplotlib_support",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "cartesian",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "partial_dependence",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A fitted estimator object implementing :term:`predict`,\n:term:`predict_proba`, or :term:`decision_function`.\nMultioutput-multiclass classifiers are not supported."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "``X`` is used to generate a grid of values for the target\n``features`` (where the partial dependence will be evaluated), and\nalso to generate values for the complement features when the\n`method` is 'brute'."
            },
            {
              "name": "features",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The feature (e.g. `[0]`) or pair of interacting features\n(e.g. `[(0, 1)]`) for which the partial dependency should be computed."
            }
          ],
          "results": [
            {
              "name": "predictions",
              "type": null,
              "description": "- if `kind='legacy'`, return value is ndarray of shape (n_outputs,                 len(values[0]), len(values[1]), ...)\n    The predictions for all the points in the grid, averaged\n    over all samples in X (or over the training data if ``method``\n    is 'recursion').\n\n- if `kind='individual'`, `'average'` or `'both'`, return value is                 :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    individual : ndarray of shape (n_outputs, n_instances,                     len(values[0]), len(values[1]), ...)\n        The predictions for all the points in the grid for all\n        samples in X. This is also known as Individual\n        Conditional Expectation (ICE)\n\n    average : ndarray of shape (n_outputs, len(values[0]),                     len(values[1]), ...)\n        The predictions for all the points in the grid, averaged\n        over all samples in X (or over the training data if\n        ``method`` is 'recursion').\n        Only available when kind='both'.\n\n    values : seq of 1d ndarrays\n        The values with which the grid has been created. The generated\n        grid is a cartesian product of the arrays in ``values``.\n        ``len(values) == len(features)``. The size of each array\n        ``values[j]`` is either ``grid_resolution``, or the number of\n        unique values in ``X[:, j]``, whichever is smaller.\n\n``n_outputs`` corresponds to the number of classes in a multi-class\nsetting, or to the number of tasks for multi-output regression.\nFor classical regression and binary classification ``n_outputs==1``.\n``n_values_feature_j`` corresponds to the size ``values[j]``."
            },
            {
              "name": "values",
              "type": null,
              "description": "The values with which the grid has been created. The generated grid\nis a cartesian product of the arrays in ``values``. ``len(values) ==\nlen(features)``. The size of each array ``values[j]`` is either\n``grid_resolution``, or the number of unique values in ``X[:, j]``,\nwhichever is smaller. Only available when `kind=\"legacy\"`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Partial dependence of ``features``.\n\nPartial dependence of a feature (or a set of features) corresponds to\nthe average response of an estimator for each possible value of the\nfeature.\n\nRead more in the :ref:`User Guide <partial_dependence>`.\n\n.. warning::\n\n    For :class:`~sklearn.ensemble.GradientBoostingClassifier` and\n    :class:`~sklearn.ensemble.GradientBoostingRegressor`, the\n    `'recursion'` method (used by default) will not account for the `init`\n    predictor of the boosting process. In practice, this will produce\n    the same values as `'brute'` up to a constant offset in the target\n    response, provided that `init` is a constant estimator (which is the\n    default). However, if `init` is not a constant estimator, the\n    partial dependence values are incorrect for `'recursion'` because the\n    offset will be sample-dependent. It is preferable to use the `'brute'`\n    method. Note that this only applies to\n    :class:`~sklearn.ensemble.GradientBoostingClassifier` and\n    :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to\n    :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n    :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.\n\nParameters\n----------\nestimator : BaseEstimator\n    A fitted estimator object implementing :term:`predict`,\n    :term:`predict_proba`, or :term:`decision_function`.\n    Multioutput-multiclass classifiers are not supported.\n\nX : {array-like or dataframe} of shape (n_samples, n_features)\n    ``X`` is used to generate a grid of values for the target\n    ``features`` (where the partial dependence will be evaluated), and\n    also to generate values for the complement features when the\n    `method` is 'brute'.\n\nfeatures : array-like of {int, str}\n    The feature (e.g. `[0]`) or pair of interacting features\n    (e.g. `[(0, 1)]`) for which the partial dependency should be computed.\n\nresponse_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'\n    Specifies whether to use :term:`predict_proba` or\n    :term:`decision_function` as the target response. For regressors\n    this parameter is ignored and the response is always the output of\n    :term:`predict`. By default, :term:`predict_proba` is tried first\n    and we revert to :term:`decision_function` if it doesn't exist. If\n    ``method`` is 'recursion', the response is always the output of\n    :term:`decision_function`.\n\npercentiles : tuple of float, default=(0.05, 0.95)\n    The lower and upper percentile used to create the extreme values\n    for the grid. Must be in [0, 1].\n\ngrid_resolution : int, default=100\n    The number of equally spaced points on the grid, for each target\n    feature.\n\nmethod : {'auto', 'recursion', 'brute'}, default='auto'\n    The method used to calculate the averaged predictions:\n\n    - `'recursion'` is only supported for some tree-based estimators\n      (namely\n      :class:`~sklearn.ensemble.GradientBoostingClassifier`,\n      :class:`~sklearn.ensemble.GradientBoostingRegressor`,\n      :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,\n      :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,\n      :class:`~sklearn.tree.DecisionTreeRegressor`,\n      :class:`~sklearn.ensemble.RandomForestRegressor`,\n      ) when `kind='average'`.\n      This is more efficient in terms of speed.\n      With this method, the target response of a\n      classifier is always the decision function, not the predicted\n      probabilities. Since the `'recursion'` method implicitely computes\n      the average of the Individual Conditional Expectation (ICE) by\n      design, it is not compatible with ICE and thus `kind` must be\n      `'average'`.\n\n    - `'brute'` is supported for any estimator, but is more\n      computationally intensive.\n\n    - `'auto'`: the `'recursion'` is used for estimators that support it,\n      and `'brute'` is used otherwise.\n\n    Please see :ref:`this note <pdp_method_differences>` for\n    differences between the `'brute'` and `'recursion'` method.\n\nkind : {'legacy', 'average', 'individual', 'both'}, default='legacy'\n    Whether to return the partial dependence averaged across all the\n    samples in the dataset or one line per sample or both.\n    See Returns below.\n\n    Note that the fast `method='recursion'` option is only available for\n    `kind='average'`. Plotting individual dependencies requires using the\n    slower `method='brute'` option.\n\n    .. versionadded:: 0.24\n    .. deprecated:: 0.24\n        `kind='legacy'` is deprecated and will be removed in version 1.1.\n        `kind='average'` will be the new default. It is intended to migrate\n        from the ndarray output to :class:`~sklearn.utils.Bunch` output.\n\n\nReturns\n-------\npredictions : ndarray or :class:`~sklearn.utils.Bunch`\n\n    - if `kind='legacy'`, return value is ndarray of shape (n_outputs,                 len(values[0]), len(values[1]), ...)\n        The predictions for all the points in the grid, averaged\n        over all samples in X (or over the training data if ``method``\n        is 'recursion').\n\n    - if `kind='individual'`, `'average'` or `'both'`, return value is                 :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        individual : ndarray of shape (n_outputs, n_instances,                     len(values[0]), len(values[1]), ...)\n            The predictions for all the points in the grid for all\n            samples in X. This is also known as Individual\n            Conditional Expectation (ICE)\n\n        average : ndarray of shape (n_outputs, len(values[0]),                     len(values[1]), ...)\n            The predictions for all the points in the grid, averaged\n            over all samples in X (or over the training data if\n            ``method`` is 'recursion').\n            Only available when kind='both'.\n\n        values : seq of 1d ndarrays\n            The values with which the grid has been created. The generated\n            grid is a cartesian product of the arrays in ``values``.\n            ``len(values) == len(features)``. The size of each array\n            ``values[j]`` is either ``grid_resolution``, or the number of\n            unique values in ``X[:, j]``, whichever is smaller.\n\n    ``n_outputs`` corresponds to the number of classes in a multi-class\n    setting, or to the number of tasks for multi-output regression.\n    For classical regression and binary classification ``n_outputs==1``.\n    ``n_values_feature_j`` corresponds to the size ``values[j]``.\n\nvalues : seq of 1d ndarrays\n    The values with which the grid has been created. The generated grid\n    is a cartesian product of the arrays in ``values``. ``len(values) ==\n    len(features)``. The size of each array ``values[j]`` is either\n    ``grid_resolution``, or the number of unique values in ``X[:, j]``,\n    whichever is smaller. Only available when `kind=\"legacy\"`.\n\nSee Also\n--------\nplot_partial_dependence : Plot Partial Dependence.\nPartialDependenceDisplay : Partial Dependence visualization.\n\nExamples\n--------\n>>> X = [[0, 0, 2], [1, 0, 0]]\n>>> y = [0, 1]\n>>> from sklearn.ensemble import GradientBoostingClassifier\n>>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)\n>>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),\n...                    grid_resolution=2) # doctest: +SKIP\n(array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])"
        }
      ]
    },
    {
      "name": "sklearn.inspection._permutation_importance",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "check_scoring",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "permutation_importance",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "An estimator that has already been :term:`fitted` and is compatible\nwith :term:`scorer`."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data on which permutation importance will be computed."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Targets for supervised or `None` for unsupervised."
            }
          ],
          "results": [
            {
              "name": "result",
              "type": null,
              "description": "Dictionary-like object, with the following attributes.\n\nimportances_mean : ndarray, shape (n_features, )\n    Mean of feature importance over `n_repeats`.\nimportances_std : ndarray, shape (n_features, )\n    Standard deviation over `n_repeats`.\nimportances : ndarray, shape (n_features, n_repeats)\n    Raw permutation importance scores."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Permutation importance for feature evaluation [BRE]_.\n\nThe :term:`estimator` is required to be a fitted estimator. `X` can be the\ndata set used to train the estimator or a hold-out set. The permutation\nimportance of a feature is calculated as follows. First, a baseline metric,\ndefined by :term:`scoring`, is evaluated on a (potentially different)\ndataset defined by the `X`. Next, a feature column from the validation set\nis permuted and the metric is evaluated again. The permutation importance\nis defined to be the difference between the baseline metric and metric from\npermutating the feature column.\n\nRead more in the :ref:`User Guide <permutation_importance>`.\n\nParameters\n----------\nestimator : object\n    An estimator that has already been :term:`fitted` and is compatible\n    with :term:`scorer`.\n\nX : ndarray or DataFrame, shape (n_samples, n_features)\n    Data on which permutation importance will be computed.\n\ny : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n    Targets for supervised or `None` for unsupervised.\n\nscoring : string, callable or None, default=None\n    Scorer to use. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer is used.\n\nn_repeats : int, default=5\n    Number of times to permute a feature.\n\nn_jobs : int or None, default=None\n    Number of jobs to run in parallel. The computation is done by computing\n    permutation score for each columns and parallelized over the columns.\n    `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    `-1` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    Pseudo-random number generator to control the permutations of each\n    feature.\n    Pass an int to get reproducible results across function calls.\n    See :term: `Glossary <random_state>`.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights used in scoring.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nresult : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    importances_mean : ndarray, shape (n_features, )\n        Mean of feature importance over `n_repeats`.\n    importances_std : ndarray, shape (n_features, )\n        Standard deviation over `n_repeats`.\n    importances : ndarray, shape (n_features, n_repeats)\n        Raw permutation importance scores.\n\nReferences\n----------\n.. [BRE] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\n         2001. https://doi.org/10.1023/A:1010933404324\n\nExamples\n--------\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.inspection import permutation_importance\n>>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\n...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\n>>> y = [1, 1, 1, 0, 0, 0]\n>>> clf = LogisticRegression().fit(X, y)\n>>> result = permutation_importance(clf, X, y, n_repeats=10,\n...                                 random_state=0)\n>>> result.importances_mean\narray([0.4666..., 0.       , 0.       ])\n>>> result.importances_std\narray([0.2211..., 0.       , 0.       ])"
        }
      ]
    },
    {
      "name": "sklearn.inspection._plot",
      "imports": [],
      "fromImports": [],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.inspection._plot.partial_dependence",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "itertools",
          "declaration": "chain",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "ceil",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy.stats.mstats",
          "declaration": "mquantiles",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_regressor",
          "alias": null
        },
        {
          "module": "sklearn.inspection",
          "declaration": "partial_dependence",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_matplotlib_support",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "PartialDependenceDisplay",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "plot",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "display",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Plot partial dependence plots.\n\nParameters\n----------\nax : Matplotlib axes or array-like of Matplotlib axes, default=None\n    - If a single axis is passed in, it is treated as a bounding axes\n        and a grid of partial dependence plots will be drawn within\n        these bounds. The `n_cols` parameter controls the number of\n        columns in the grid.\n    - If an array-like of axes are passed in, the partial dependence\n        plots will be drawn directly into these axes.\n    - If `None`, a figure and a bounding axes is created and treated\n        as the single axes case.\n\nn_cols : int, default=3\n    The maximum number of columns in the grid plot. Only active when\n    `ax` is a single axes or `None`.\n\nline_kw : dict, default=None\n    Dict with keywords passed to the `matplotlib.pyplot.plot` call.\n    For one-way partial dependence plots.\n\ncontour_kw : dict, default=None\n    Dict with keywords passed to the `matplotlib.pyplot.contourf`\n    call for two-way partial dependence plots.\n\nReturns\n-------\ndisplay : :class:`~sklearn.inspection.PartialDependenceDisplay`"
            }
          ],
          "fullDocstring": "Partial Dependence Plot (PDP).\n\nThis can also display individual partial dependencies which are often\nreferred to as: Individual Condition Expectation (ICE).\n\nIt is recommended to use\n:func:`~sklearn.inspection.plot_partial_dependence` to create a\n:class:`~sklearn.inspection.PartialDependenceDisplay`. All parameters are\nstored as attributes.\n\nRead more in\n:ref:`sphx_glr_auto_examples_miscellaneous_plot_partial_dependence_visualization_api.py`\nand the :ref:`User Guide <visualizations>`.\n\n    .. versionadded:: 0.22\n\nParameters\n----------\npd_results : list of Bunch\n    Results of :func:`~sklearn.inspection.partial_dependence` for\n    ``features``.\n\nfeatures : list of (int,) or list of (int, int)\n    Indices of features for a given plot. A tuple of one integer will plot\n    a partial dependence curve of one feature. A tuple of two integers will\n    plot a two-way partial dependence curve as a contour plot.\n\nfeature_names : list of str\n    Feature names corresponding to the indices in ``features``.\n\ntarget_idx : int\n\n    - In a multiclass setting, specifies the class for which the PDPs\n      should be computed. Note that for binary classification, the\n      positive class (index 1) is always used.\n    - In a multioutput setting, specifies the task for which the PDPs\n      should be computed.\n\n    Ignored in binary classification or classical regression settings.\n\npdp_lim : dict\n    Global min and max average predictions, such that all plots will have\n    the same scale and y limits. `pdp_lim[1]` is the global min and max for\n    single partial dependence curves. `pdp_lim[2]` is the global min and\n    max for two-way partial dependence curves.\n\ndeciles : dict\n    Deciles for feature indices in ``features``.\n\nkind : {'average', 'individual', 'both'}, default='average'\n    Whether to plot the partial dependence averaged across all the samples\n    in the dataset or one line per sample or both.\n\n    - ``kind='average'`` results in the traditional PD plot;\n    - ``kind='individual'`` results in the ICE plot.\n\n   Note that the fast ``method='recursion'`` option is only available for\n   ``kind='average'``. Plotting individual dependencies requires using the\n   slower ``method='brute'`` option.\n\n    .. versionadded:: 0.24\n\nsubsample : float, int or None, default=1000\n    Sampling for ICE curves when `kind` is 'individual' or 'both'.\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to be used to plot ICE curves. If int, represents the\n    maximum absolute number of samples to use.\n\n    Note that the full dataset is still used to calculate partial\n    dependence when `kind='both'`.\n\n    .. versionadded:: 0.24\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the selected samples when subsamples is not\n    `None`. See :term:`Glossary <random_state>` for details.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nbounding_ax_ : matplotlib Axes or None\n    If `ax` is an axes or None, the `bounding_ax_` is the axes where the\n    grid of partial dependence plots are drawn. If `ax` is a list of axes\n    or a numpy array of axes, `bounding_ax_` is None.\n\naxes_ : ndarray of matplotlib Axes\n    If `ax` is an axes or None, `axes_[i, j]` is the axes on the i-th row\n    and j-th column. If `ax` is a list of axes, `axes_[i]` is the i-th item\n    in `ax`. Elements that are None correspond to a nonexisting axes in\n    that position.\n\nlines_ : ndarray of matplotlib Artists\n    If `ax` is an axes or None, `lines_[i, j]` is the partial dependence\n    curve on the i-th row and j-th column. If `ax` is a list of axes,\n    `lines_[i]` is the partial dependence curve corresponding to the i-th\n    item in `ax`. Elements that are None correspond to a nonexisting axes\n    or an axes that does not include a line plot.\n\ndeciles_vlines_ : ndarray of matplotlib LineCollection\n    If `ax` is an axes or None, `vlines_[i, j]` is the line collection\n    representing the x axis deciles of the i-th row and j-th column. If\n    `ax` is a list of axes, `vlines_[i]` corresponds to the i-th item in\n    `ax`. Elements that are None correspond to a nonexisting axes or an\n    axes that does not include a PDP plot.\n\n    .. versionadded:: 0.23\n\ndeciles_hlines_ : ndarray of matplotlib LineCollection\n    If `ax` is an axes or None, `vlines_[i, j]` is the line collection\n    representing the y axis deciles of the i-th row and j-th column. If\n    `ax` is a list of axes, `vlines_[i]` corresponds to the i-th item in\n    `ax`. Elements that are None correspond to a nonexisting axes or an\n    axes that does not include a 2-way plot.\n\n    .. versionadded:: 0.23\n\ncontours_ : ndarray of matplotlib Artists\n    If `ax` is an axes or None, `contours_[i, j]` is the partial dependence\n    plot on the i-th row and j-th column. If `ax` is a list of axes,\n    `contours_[i]` is the partial dependence plot corresponding to the i-th\n    item in `ax`. Elements that are None correspond to a nonexisting axes\n    or an axes that does not include a contour plot.\n\nfigure_ : matplotlib Figure\n    Figure containing partial dependence plots.\n\nSee Also\n--------\npartial_dependence : Compute Partial Dependence values.\nplot_partial_dependence : Plot Partial Dependence."
        }
      ],
      "functions": [
        {
          "name": "plot_partial_dependence",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A fitted estimator object implementing :term:`predict`,\n:term:`predict_proba`, or :term:`decision_function`.\nMultioutput-multiclass classifiers are not supported."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "``X`` is used to generate a grid of values for the target\n``features`` (where the partial dependence will be evaluated), and\nalso to generate values for the complement features when the\n`method` is `'brute'`."
            },
            {
              "name": "features",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The target features for which to create the PDPs.\nIf `features[i]` is an integer or a string, a one-way PDP is created;\nif `features[i]` is a tuple, a two-way PDP is created (only supported\nwith `kind='average'`). Each tuple must be of size 2.\nif any entry is a string, then it must be in ``feature_names``."
            }
          ],
          "results": [
            {
              "name": "display",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Partial dependence (PD) and individual conditional expectation (ICE)\nplots.\n\nPartial dependence plots, individual conditional expectation plots or an\noverlay of both of them can be plotted by setting the ``kind``\nparameter.\nThe ``len(features)`` plots are arranged in a grid with ``n_cols``\ncolumns. Two-way partial dependence plots are plotted as contour plots. The\ndeciles of the feature values will be shown with tick marks on the x-axes\nfor one-way plots, and on both axes for two-way plots.\n\nRead more in the :ref:`User Guide <partial_dependence>`.\n\n.. note::\n\n    :func:`plot_partial_dependence` does not support using the same axes\n    with multiple calls. To plot the the partial dependence for multiple\n    estimators, please pass the axes created by the first call to the\n    second call::\n\n      >>> from sklearn.inspection import plot_partial_dependence\n      >>> from sklearn.datasets import make_friedman1\n      >>> from sklearn.linear_model import LinearRegression\n      >>> from sklearn.ensemble import RandomForestRegressor\n      >>> X, y = make_friedman1()\n      >>> est1 = LinearRegression().fit(X, y)\n      >>> est2 = RandomForestRegressor().fit(X, y)\n      >>> disp1 = plot_partial_dependence(est1, X,\n      ...                                 [1, 2])  # doctest: +SKIP\n      >>> disp2 = plot_partial_dependence(est2, X, [1, 2],\n      ...                                 ax=disp1.axes_)  # doctest: +SKIP\n\n.. warning::\n\n    For :class:`~sklearn.ensemble.GradientBoostingClassifier` and\n    :class:`~sklearn.ensemble.GradientBoostingRegressor`, the\n    `'recursion'` method (used by default) will not account for the `init`\n    predictor of the boosting process. In practice, this will produce\n    the same values as `'brute'` up to a constant offset in the target\n    response, provided that `init` is a constant estimator (which is the\n    default). However, if `init` is not a constant estimator, the\n    partial dependence values are incorrect for `'recursion'` because the\n    offset will be sample-dependent. It is preferable to use the `'brute'`\n    method. Note that this only applies to\n    :class:`~sklearn.ensemble.GradientBoostingClassifier` and\n    :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to\n    :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n    :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.\n\nParameters\n----------\nestimator : BaseEstimator\n    A fitted estimator object implementing :term:`predict`,\n    :term:`predict_proba`, or :term:`decision_function`.\n    Multioutput-multiclass classifiers are not supported.\n\nX : {array-like or dataframe} of shape (n_samples, n_features)\n    ``X`` is used to generate a grid of values for the target\n    ``features`` (where the partial dependence will be evaluated), and\n    also to generate values for the complement features when the\n    `method` is `'brute'`.\n\nfeatures : list of {int, str, pair of int, pair of str}\n    The target features for which to create the PDPs.\n    If `features[i]` is an integer or a string, a one-way PDP is created;\n    if `features[i]` is a tuple, a two-way PDP is created (only supported\n    with `kind='average'`). Each tuple must be of size 2.\n    if any entry is a string, then it must be in ``feature_names``.\n\nfeature_names : array-like of shape (n_features,), dtype=str, default=None\n    Name of each feature; `feature_names[i]` holds the name of the feature\n    with index `i`.\n    By default, the name of the feature corresponds to their numerical\n    index for NumPy array and their column name for pandas dataframe.\n\ntarget : int, default=None\n    - In a multiclass setting, specifies the class for which the PDPs\n      should be computed. Note that for binary classification, the\n      positive class (index 1) is always used.\n    - In a multioutput setting, specifies the task for which the PDPs\n      should be computed.\n\n    Ignored in binary classification or classical regression settings.\n\nresponse_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'\n    Specifies whether to use :term:`predict_proba` or\n    :term:`decision_function` as the target response. For regressors\n    this parameter is ignored and the response is always the output of\n    :term:`predict`. By default, :term:`predict_proba` is tried first\n    and we revert to :term:`decision_function` if it doesn't exist. If\n    ``method`` is `'recursion'`, the response is always the output of\n    :term:`decision_function`.\n\nn_cols : int, default=3\n    The maximum number of columns in the grid plot. Only active when `ax`\n    is a single axis or `None`.\n\ngrid_resolution : int, default=100\n    The number of equally spaced points on the axes of the plots, for each\n    target feature.\n\npercentiles : tuple of float, default=(0.05, 0.95)\n    The lower and upper percentile used to create the extreme values\n    for the PDP axes. Must be in [0, 1].\n\nmethod : str, default='auto'\n    The method used to calculate the averaged predictions:\n\n    - `'recursion'` is only supported for some tree-based estimators\n      (namely\n      :class:`~sklearn.ensemble.GradientBoostingClassifier`,\n      :class:`~sklearn.ensemble.GradientBoostingRegressor`,\n      :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,\n      :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,\n      :class:`~sklearn.tree.DecisionTreeRegressor`,\n      :class:`~sklearn.ensemble.RandomForestRegressor`\n      but is more efficient in terms of speed.\n      With this method, the target response of a\n      classifier is always the decision function, not the predicted\n      probabilities. Since the `'recursion'` method implicitely computes\n      the average of the ICEs by design, it is not compatible with ICE and\n      thus `kind` must be `'average'`.\n\n    - `'brute'` is supported for any estimator, but is more\n      computationally intensive.\n\n    - `'auto'`: the `'recursion'` is used for estimators that support it,\n      and `'brute'` is used otherwise.\n\n    Please see :ref:`this note <pdp_method_differences>` for\n    differences between the `'brute'` and `'recursion'` method.\n\nn_jobs : int, default=None\n    The number of CPUs to use to compute the partial dependences.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int, default=0\n    Verbose output during PD computations.\n\nline_kw : dict, default=None\n    Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.\n    For one-way partial dependence plots.\n\ncontour_kw : dict, default=None\n    Dict with keywords passed to the ``matplotlib.pyplot.contourf`` call.\n    For two-way partial dependence plots.\n\nax : Matplotlib axes or array-like of Matplotlib axes, default=None\n    - If a single axis is passed in, it is treated as a bounding axes\n      and a grid of partial dependence plots will be drawn within\n      these bounds. The `n_cols` parameter controls the number of\n      columns in the grid.\n    - If an array-like of axes are passed in, the partial dependence\n      plots will be drawn directly into these axes.\n    - If `None`, a figure and a bounding axes is created and treated\n      as the single axes case.\n\n    .. versionadded:: 0.22\n\nkind : {'average', 'individual', 'both'}, default='average'\n    Whether to plot the partial dependence averaged across all the samples\n    in the dataset or one line per sample or both.\n\n    - ``kind='average'`` results in the traditional PD plot;\n    - ``kind='individual'`` results in the ICE plot.\n\n   Note that the fast ``method='recursion'`` option is only available for\n   ``kind='average'``. Plotting individual dependencies requires using the\n   slower ``method='brute'`` option.\n\n    .. versionadded:: 0.24\n\nsubsample : float, int or None, default=1000\n    Sampling for ICE curves when `kind` is 'individual' or 'both'.\n    If `float`, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to be used to plot ICE curves. If `int`, represents the\n    absolute number samples to use.\n\n    Note that the full dataset is still used to calculate averaged partial\n    dependence when `kind='both'`.\n\n    .. versionadded:: 0.24\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the selected samples when subsamples is not\n    `None` and `kind` is either `'both'` or `'individual'`.\n    See :term:`Glossary <random_state>` for details.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ndisplay : :class:`~sklearn.inspection.PartialDependenceDisplay`\n\nSee Also\n--------\npartial_dependence : Compute Partial Dependence values.\nPartialDependenceDisplay : Partial Dependence visualization.\n\nExamples\n--------\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.ensemble import GradientBoostingRegressor\n>>> X, y = make_friedman1()\n>>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n>>> plot_partial_dependence(clf, X, [0, (0, 1)]) #doctest: +SKIP"
        }
      ]
    },
    {
      "name": "sklearn.inspection.setup",
      "imports": [],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "numpy.distutils.misc_util",
          "declaration": "Configuration",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.isotonic",
      "imports": [
        {
          "module": "math",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "interpolate",
          "alias": null
        },
        {
          "module": "scipy.stats",
          "declaration": "spearmanr",
          "alias": null
        },
        {
          "module": "sklearn._isotonic",
          "declaration": "_inplace_contiguous_isotonic_regression",
          "alias": null
        },
        {
          "module": "sklearn._isotonic",
          "declaration": "_make_unique",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "IsotonicRegression",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data.\n\n.. versionchanged:: 0.24\n   Also accepts 2d array with 1 feature."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training target."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights. If set to None, all weights will be set to 1 (equal\nweights)."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model using X, y as training data.\n\nParameters\n----------\nX : array-like of shape (n_samples,) or (n_samples, 1)\n    Training data.\n\n    .. versionchanged:: 0.24\n       Also accepts 2d array with 1 feature.\n\ny : array-like of shape (n_samples,)\n    Training target.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weights. If set to None, all weights will be set to 1 (equal\n    weights).\n\nReturns\n-------\nself : object\n    Returns an instance of self.\n\nNotes\n-----\nX is stored for future use, as :meth:`transform` needs X to interpolate\nnew input data."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "T",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to transform.\n\n.. versionchanged:: 0.24\n   Also accepts 2d array with 1 feature."
                }
              ],
              "results": [
                {
                  "name": "y_pred",
                  "type": null,
                  "description": "The transformed data"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform new data by linear interpolation\n\nParameters\n----------\nT : array-like of shape (n_samples,) or (n_samples, 1)\n    Data to transform.\n\n    .. versionchanged:: 0.24\n       Also accepts 2d array with 1 feature.\n\nReturns\n-------\ny_pred : ndarray of shape (n_samples,)\n    The transformed data"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "T",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to transform."
                }
              ],
              "results": [
                {
                  "name": "y_pred",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict new data by linear interpolation.\n\nParameters\n----------\nT : array-like of shape (n_samples,) or (n_samples, 1)\n    Data to transform.\n\nReturns\n-------\ny_pred : ndarray of shape (n_samples,)\n    Transformed data."
            }
          ],
          "fullDocstring": "Isotonic regression model.\n\nRead more in the :ref:`User Guide <isotonic>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\ny_min : float, default=None\n    Lower bound on the lowest predicted value (the minimum value may\n    still be higher). If not set, defaults to -inf.\n\ny_max : float, default=None\n    Upper bound on the highest predicted value (the maximum may still be\n    lower). If not set, defaults to +inf.\n\nincreasing : bool or 'auto', default=True\n    Determines whether the predictions should be constrained to increase\n    or decrease with `X`. 'auto' will decide based on the Spearman\n    correlation estimate's sign.\n\nout_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n    Handles how `X` values outside of the training domain are handled\n    during prediction.\n\n    - 'nan', predictions will be NaN.\n    - 'clip', predictions will be set to the value corresponding to\n      the nearest train interval endpoint.\n    - 'raise', a `ValueError` is raised.\n\nAttributes\n----------\nX_min_ : float\n    Minimum value of input array `X_` for left bound.\n\nX_max_ : float\n    Maximum value of input array `X_` for right bound.\n\nX_thresholds_ : ndarray of shape (n_thresholds,)\n    Unique ascending `X` values used to interpolate\n    the y = f(X) monotonic function.\n\n    .. versionadded:: 0.24\n\ny_thresholds_ : ndarray of shape (n_thresholds,)\n    De-duplicated `y` values suitable to interpolate the y = f(X)\n    monotonic function.\n\n    .. versionadded:: 0.24\n\nf_ : function\n    The stepwise interpolating function that covers the input domain ``X``.\n\nincreasing_ : bool\n    Inferred value for ``increasing``.\n\nNotes\n-----\nTies are broken using the secondary method from de Leeuw, 1977.\n\nReferences\n----------\nIsotonic Median Regression: A Linear Programming Approach\nNilotpal Chakravarti\nMathematics of Operations Research\nVol. 14, No. 2 (May, 1989), pp. 303-308\n\nIsotone Optimization in R : Pool-Adjacent-Violators\nAlgorithm (PAVA) and Active Set Methods\nde Leeuw, Hornik, Mair\nJournal of Statistical Software 2009\n\nCorrectness of Kruskal's algorithms for monotone regression with ties\nde Leeuw, Psychometrica, 1977\n\nExamples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.isotonic import IsotonicRegression\n>>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n>>> iso_reg = IsotonicRegression().fit(X, y)\n>>> iso_reg.predict([.1, .2])\narray([1.8628..., 3.7256...])"
        }
      ],
      "functions": [
        {
          "name": "check_increasing",
          "decorators": [],
          "parameters": [
            {
              "name": "x",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training data."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training target."
            }
          ],
          "results": [
            {
              "name": "increasing_bool",
              "type": "bool",
              "description": "Whether the relationship is increasing or decreasing."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Determine whether y is monotonically correlated with x.\n\ny is found increasing or decreasing with respect to x based on a Spearman\ncorrelation test.\n\nParameters\n----------\nx : array-like of shape (n_samples,)\n        Training data.\n\ny : array-like of shape (n_samples,)\n    Training target.\n\nReturns\n-------\nincreasing_bool : boolean\n    Whether the relationship is increasing or decreasing.\n\nNotes\n-----\nThe Spearman correlation coefficient is estimated from the data, and the\nsign of the resulting estimate is used as the result.\n\nIn the event that the 95% confidence interval based on Fisher transform\nspans zero, a warning is raised.\n\nReferences\n----------\nFisher transformation. Wikipedia.\nhttps://en.wikipedia.org/wiki/Fisher_transformation"
        },
        {
          "name": "isotonic_regression",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data."
            }
          ],
          "results": [
            {
              "name": "y_",
              "type": null,
              "description": "Isotonic fit of y."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Solve the isotonic regression model.\n\nRead more in the :ref:`User Guide <isotonic>`.\n\nParameters\n----------\ny : array-like of shape (n_samples,)\n    The data.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weights on each point of the regression.\n    If None, weight is set to 1 (equal weights).\n\ny_min : float, default=None\n    Lower bound on the lowest predicted value (the minimum value may\n    still be higher). If not set, defaults to -inf.\n\ny_max : float, default=None\n    Upper bound on the highest predicted value (the maximum may still be\n    lower). If not set, defaults to +inf.\n\nincreasing : bool, default=True\n    Whether to compute ``y_`` is increasing (if set to True) or decreasing\n    (if set to False)\n\nReturns\n-------\ny_ : list of floats\n    Isotonic fit of y.\n\nReferences\n----------\n\"Active set algorithms for isotonic regression; A unifying framework\"\nby Michael J. Best and Nilotpal Chakravarti, section 3."
        }
      ]
    },
    {
      "name": "sklearn.kernel_approximation",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy.fft",
          "declaration": "fft",
          "alias": null
        },
        {
          "module": "scipy.fft",
          "declaration": "ifft",
          "alias": null
        },
        {
          "module": "scipy.fftpack",
          "declaration": "fft",
          "alias": null
        },
        {
          "module": "scipy.fftpack",
          "declaration": "ifft",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "svd",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "KERNEL_PARAMS",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_kernels",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "as_float_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_non_negative",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "AdditiveChi2Sampler",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Set the parameters\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    Training data, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nself : object\n    Returns the transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Whether the return value is an array of sparse matrix depends on\nthe type of the input X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply approximate feature map to X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n\nReturns\n-------\nX_new : {ndarray, sparse matrix},                shape = (n_samples, n_features * (2*sample_steps + 1))\n    Whether the return value is an array of sparse matrix depends on\n    the type of the input X."
            }
          ],
          "fullDocstring": "Approximate feature map for additive chi2 kernel.\n\nUses sampling the fourier transform of the kernel characteristic\nat regular intervals.\n\nSince the kernel that is to be approximated is additive, the components of\nthe input vectors can be treated separately.  Each entry in the original\nspace is transformed into 2*sample_steps+1 features, where sample_steps is\na parameter of the method. Typical values of sample_steps include 1, 2 and\n3.\n\nOptimal choices for the sampling interval for certain data ranges can be\ncomputed (see the reference). The default values should be reasonable.\n\nRead more in the :ref:`User Guide <additive_chi_kernel_approx>`.\n\nParameters\n----------\nsample_steps : int, default=2\n    Gives the number of (complex) sampling points.\nsample_interval : float, default=None\n    Sampling interval. Must be specified when sample_steps not in {1,2,3}.\n\nAttributes\n----------\nsample_interval_ : float\n    Stored sampling interval. Specified as a parameter if sample_steps not\n    in {1,2,3}.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.linear_model import SGDClassifier\n>>> from sklearn.kernel_approximation import AdditiveChi2Sampler\n>>> X, y = load_digits(return_X_y=True)\n>>> chi2sampler = AdditiveChi2Sampler(sample_steps=2)\n>>> X_transformed = chi2sampler.fit_transform(X, y)\n>>> clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3)\n>>> clf.fit(X_transformed, y)\nSGDClassifier(max_iter=5, random_state=0)\n>>> clf.score(X_transformed, y)\n0.9499...\n\nNotes\n-----\nThis estimator approximates a slightly different version of the additive\nchi squared kernel then ``metric.additive_chi2`` computes.\n\nSee Also\n--------\nSkewedChi2Sampler : A Fourier-approximation to a non-additive variant of\n    the chi squared kernel.\n\nsklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.\n\nsklearn.metrics.pairwise.additive_chi2_kernel : The exact additive chi\n    squared kernel.\n\nReferences\n----------\nSee `\"Efficient additive kernels via explicit feature maps\"\n<http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>`_\nA. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,\n2011"
        },
        {
          "name": "Nystroem",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit estimator to data.\n\nSamples a subset of training points, computes kernel\non these and computes normalization matrix.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to transform."
                }
              ],
              "results": [
                {
                  "name": "X_transformed",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply feature map to X.\n\nComputes an approximate feature map using the kernel\nbetween some training points and X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data to transform.\n\nReturns\n-------\nX_transformed : ndarray of shape (n_samples, n_components)\n    Transformed data."
            }
          ],
          "fullDocstring": "Approximate a kernel map using a subset of the training data.\n\nConstructs an approximate feature map for an arbitrary kernel\nusing a subset of the data as basis.\n\nRead more in the :ref:`User Guide <nystroem_kernel_approx>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nkernel : string or callable, default='rbf'\n    Kernel map to be approximated. A callable should accept two arguments\n    and the keyword arguments passed to this object as kernel_params, and\n    should return a floating point number.\n\ngamma : float, default=None\n    Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\n    and sigmoid kernels. Interpretation of the default value is left to\n    the kernel; see the documentation for sklearn.metrics.pairwise.\n    Ignored by other kernels.\n\ncoef0 : float, default=None\n    Zero coefficient for polynomial and sigmoid kernels.\n    Ignored by other kernels.\n\ndegree : float, default=None\n    Degree of the polynomial kernel. Ignored by other kernels.\n\nkernel_params : dict, default=None\n    Additional parameters (keyword arguments) for kernel function passed\n    as callable object.\n\nn_components : int, default=100\n    Number of features to construct.\n    How many data points will be used to construct the mapping.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the uniform sampling without\n    replacement of n_components of the training data to construct the basis\n    kernel.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the kernel matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Subset of training points used to construct the feature map.\n\ncomponent_indices_ : ndarray of shape (n_components)\n    Indices of ``components_`` in the training set.\n\nnormalization_ : ndarray of shape (n_components, n_components)\n    Normalization matrix needed for embedding.\n    Square root of the kernel matrix on ``components_``.\n\nExamples\n--------\n>>> from sklearn import datasets, svm\n>>> from sklearn.kernel_approximation import Nystroem\n>>> X, y = datasets.load_digits(n_class=9, return_X_y=True)\n>>> data = X / 16.\n>>> clf = svm.LinearSVC()\n>>> feature_map_nystroem = Nystroem(gamma=.2,\n...                                 random_state=1,\n...                                 n_components=300)\n>>> data_transformed = feature_map_nystroem.fit_transform(data)\n>>> clf.fit(data_transformed, y)\nLinearSVC()\n>>> clf.score(data_transformed, y)\n0.9987...\n\nReferences\n----------\n* Williams, C.K.I. and Seeger, M.\n  \"Using the Nystroem method to speed up kernel machines\",\n  Advances in neural information processing systems 2001\n\n* T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou\n  \"Nystroem Method vs Random Fourier Features: A Theoretical and Empirical\n  Comparison\",\n  Advances in Neural Information Processing Systems 2012\n\n\nSee Also\n--------\nRBFSampler : An approximation to the RBF kernel using random Fourier\n    features.\n\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels."
        },
        {
          "name": "PolynomialCountSketch",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model with X.\n\nInitializes the internal variables. The method needs no information\nabout the distribution of data, so we only care about n_features in X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nself : object\n    Returns the transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data, where n_samples in the number of samples\nand n_features is the number of features."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate the feature map approximation for X.\n\nParameters\n----------\nX : {array-like}, shape (n_samples, n_features)\n    New data, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nX_new : array-like, shape (n_samples, n_components)"
            }
          ],
          "fullDocstring": "Polynomial kernel approximation via Tensor Sketch.\n\nImplements Tensor Sketch, which approximates the feature map\nof the polynomial kernel::\n\n    K(X, Y) = (gamma * <X, Y> + coef0)^degree\n\nby efficiently computing a Count Sketch of the outer product of a\nvector with itself using Fast Fourier Transforms (FFT). Read more in the\n:ref:`User Guide <polynomial_kernel_approx>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\ngamma : float, default=1.0\n    Parameter of the polynomial kernel whose feature map\n    will be approximated.\n\ndegree : int, default=2\n    Degree of the polynomial kernel whose feature map\n    will be approximated.\n\ncoef0 : int, default=0\n    Constant term of the polynomial kernel whose feature map\n    will be approximated.\n\nn_components : int, default=100\n    Dimensionality of the output feature space. Usually, n_components\n    should be greater than the number of features in input samples in\n    order to achieve good performance. The optimal score / run time\n    balance is typically achieved around n_components = 10 * n_features,\n    but this depends on the specific dataset being used.\n\nrandom_state : int, RandomState instance, default=None\n    Determines random number generation for indexHash and bitHash\n    initialization. Pass an int for reproducible results across multiple\n    function calls. See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nindexHash_ : ndarray of shape (degree, n_features), dtype=int64\n    Array of indexes in range [0, n_components) used to represent\n    the 2-wise independent hash functions for Count Sketch computation.\n\nbitHash_ : ndarray of shape (degree, n_features), dtype=float32\n    Array with random entries in {+1, -1}, used to represent\n    the 2-wise independent hash functions for Count Sketch computation.\n\nExamples\n--------\n>>> from sklearn.kernel_approximation import PolynomialCountSketch\n>>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n>>> y = [0, 0, 1, 1]\n>>> ps = PolynomialCountSketch(degree=3, random_state=1)\n>>> X_features = ps.fit_transform(X)\n>>> clf = SGDClassifier(max_iter=10, tol=1e-3)\n>>> clf.fit(X_features, y)\nSGDClassifier(max_iter=10)\n>>> clf.score(X_features, y)\n1.0"
        },
        {
          "name": "RBFSampler",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model with X.\n\nSamples random projection according to n_features.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Training data, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nself : object\n    Returns the transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data, where n_samples in the number of samples\nand n_features is the number of features."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply the approximate feature map to X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    New data, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nX_new : array-like, shape (n_samples, n_components)"
            }
          ],
          "fullDocstring": "Approximates feature map of an RBF kernel by Monte Carlo approximation\nof its Fourier transform.\n\nIt implements a variant of Random Kitchen Sinks.[1]\n\nRead more in the :ref:`User Guide <rbf_kernel_approx>`.\n\nParameters\n----------\ngamma : float, default=1.0\n    Parameter of RBF kernel: exp(-gamma * x^2)\n\nn_components : int, default=100\n    Number of Monte Carlo samples per original feature.\n    Equals the dimensionality of the computed feature space.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the generation of the random\n    weights and random offset when fitting the training data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrandom_offset_ : ndarray of shape (n_components,), dtype=float64\n    Random offset used to compute the projection in the `n_components`\n    dimensions of the feature space.\n\nrandom_weights_ : ndarray of shape (n_features, n_components),        dtype=float64\n    Random projection directions drawn from the Fourier transform\n    of the RBF kernel.\n\n\nExamples\n--------\n>>> from sklearn.kernel_approximation import RBFSampler\n>>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n>>> y = [0, 0, 1, 1]\n>>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n>>> X_features = rbf_feature.fit_transform(X)\n>>> clf = SGDClassifier(max_iter=5, tol=1e-3)\n>>> clf.fit(X_features, y)\nSGDClassifier(max_iter=5)\n>>> clf.score(X_features, y)\n1.0\n\nNotes\n-----\nSee \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\nBenjamin Recht.\n\n[1] \"Weighted Sums of Random Kitchen Sinks: Replacing\nminimization with randomization in learning\" by A. Rahimi and\nBenjamin Recht.\n(https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)"
        },
        {
          "name": "SkewedChi2Sampler",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model with X.\n\nSamples random projection according to n_features.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    Training data, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nself : object\n    Returns the transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "New data, where n_samples in the number of samples\nand n_features is the number of features. All values of X must be\nstrictly greater than \"-skewedness\"."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply the approximate feature map to X.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    New data, where n_samples in the number of samples\n    and n_features is the number of features. All values of X must be\n    strictly greater than \"-skewedness\".\n\nReturns\n-------\nX_new : array-like, shape (n_samples, n_components)"
            }
          ],
          "fullDocstring": "Approximates feature map of the \"skewed chi-squared\" kernel by Monte\nCarlo approximation of its Fourier transform.\n\nRead more in the :ref:`User Guide <skewed_chi_kernel_approx>`.\n\nParameters\n----------\nskewedness : float, default=1.0\n    \"skewedness\" parameter of the kernel. Needs to be cross-validated.\n\nn_components : int, default=100\n    number of Monte Carlo samples per original feature.\n    Equals the dimensionality of the computed feature space.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the generation of the random\n    weights and random offset when fitting the training data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrandom_weights_ : ndarray of shape (n_features, n_components)\n    Weight array, sampled from a secant hyperbolic distribution, which will\n    be used to linearly transform the log of the data.\n\nrandom_offset_ : ndarray of shape (n_features, n_components)\n    Bias term, which will be added to the data. It is uniformly distributed\n    between 0 and 2*pi.\n\nExamples\n--------\n>>> from sklearn.kernel_approximation import SkewedChi2Sampler\n>>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n>>> y = [0, 0, 1, 1]\n>>> chi2_feature = SkewedChi2Sampler(skewedness=.01,\n...                                  n_components=10,\n...                                  random_state=0)\n>>> X_features = chi2_feature.fit_transform(X, y)\n>>> clf = SGDClassifier(max_iter=10, tol=1e-3)\n>>> clf.fit(X_features, y)\nSGDClassifier(max_iter=10)\n>>> clf.score(X_features, y)\n1.0\n\nReferences\n----------\nSee \"Random Fourier Approximations for Skewed Multiplicative Histogram\nKernels\" by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.\n\nSee Also\n--------\nAdditiveChi2Sampler : A different approach for approximating an additive\n    variant of the chi squared kernel.\n\nsklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.kernel_ridge",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._ridge",
          "declaration": "_solve_cholesky_kernel",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_kernels",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KernelRidge",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseEstimator",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data. If kernel == \"precomputed\" this is instead\na precomputed kernel matrix, of shape (n_samples, n_samples)."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values"
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Individual weights for each sample, ignored if None is passed."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit Kernel Ridge regression model\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data. If kernel == \"precomputed\" this is instead\n    a precomputed kernel matrix, of shape (n_samples, n_samples).\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values\n\nsample_weight : float or array-like of shape (n_samples,), default=None\n    Individual weights for each sample, ignored if None is passed.\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples. If kernel == \"precomputed\" this is instead a\nprecomputed kernel matrix, shape = [n_samples,\nn_samples_fitted], where n_samples_fitted is the number of\nsamples used in the fitting for this estimator."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Returns predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the kernel ridge model\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Samples. If kernel == \"precomputed\" this is instead a\n    precomputed kernel matrix, shape = [n_samples,\n    n_samples_fitted], where n_samples_fitted is the number of\n    samples used in the fitting for this estimator.\n\nReturns\n-------\nC : ndarray of shape (n_samples,) or (n_samples, n_targets)\n    Returns predicted values."
            }
          ],
          "fullDocstring": "Kernel ridge regression.\n\nKernel ridge regression (KRR) combines ridge regression (linear least\nsquares with l2-norm regularization) with the kernel trick. It thus\nlearns a linear function in the space induced by the respective kernel and\nthe data. For non-linear kernels, this corresponds to a non-linear\nfunction in the original space.\n\nThe form of the model learned by KRR is identical to support vector\nregression (SVR). However, different loss functions are used: KRR uses\nsquared error loss while support vector regression uses epsilon-insensitive\nloss, both combined with l2 regularization. In contrast to SVR, fitting a\nKRR model can be done in closed-form and is typically faster for\nmedium-sized datasets. On the other hand, the learned model is non-sparse\nand thus slower than SVR, which learns a sparse model for epsilon > 0, at\nprediction-time.\n\nThis estimator has built-in support for multi-variate regression\n(i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n\nRead more in the :ref:`User Guide <kernel_ridge>`.\n\nParameters\n----------\nalpha : float or array-like of shape (n_targets,), default=1.0\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number. See :ref:`ridge_regression` for formula.\n\nkernel : string or callable, default=\"linear\"\n    Kernel mapping used internally. This parameter is directly passed to\n    :class:`~sklearn.metrics.pairwise.pairwise_kernel`.\n    If `kernel` is a string, it must be one of the metrics\n    in `pairwise.PAIRWISE_KERNEL_FUNCTIONS`.\n    If `kernel` is \"precomputed\", X is assumed to be a kernel matrix.\n    Alternatively, if `kernel` is a callable function, it is called on\n    each pair of instances (rows) and the resulting value recorded. The\n    callable should take two rows from X as input and return the\n    corresponding kernel value as a single number. This means that\n    callables from :mod:`sklearn.metrics.pairwise` are not allowed, as\n    they operate on matrices, not single samples. Use the string\n    identifying the kernel instead.\n\ngamma : float, default=None\n    Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\n    and sigmoid kernels. Interpretation of the default value is left to\n    the kernel; see the documentation for sklearn.metrics.pairwise.\n    Ignored by other kernels.\n\ndegree : float, default=3\n    Degree of the polynomial kernel. Ignored by other kernels.\n\ncoef0 : float, default=1\n    Zero coefficient for polynomial and sigmoid kernels.\n    Ignored by other kernels.\n\nkernel_params : mapping of string to any, default=None\n    Additional parameters (keyword arguments) for kernel function passed\n    as callable object.\n\nAttributes\n----------\ndual_coef_ : ndarray of shape (n_samples,) or (n_samples, n_targets)\n    Representation of weight vector(s) in kernel space\n\nX_fit_ : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Training data, which is also required for prediction. If\n    kernel == \"precomputed\" this is instead the precomputed\n    training matrix, of shape (n_samples, n_samples).\n\nReferences\n----------\n* Kevin P. Murphy\n  \"Machine Learning: A Probabilistic Perspective\", The MIT Press\n  chapter 14.4.3, pp. 492-493\n\nSee Also\n--------\nsklearn.linear_model.Ridge : Linear ridge regression.\nsklearn.svm.SVR : Support Vector Regression implemented using libsvm.\n\nExamples\n--------\n>>> from sklearn.kernel_ridge import KernelRidge\n>>> import numpy as np\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> clf = KernelRidge(alpha=1.0)\n>>> clf.fit(X, y)\nKernelRidge(alpha=1.0)"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._base",
          "declaration": "LinearRegression",
          "alias": null
        },
        {
          "module": "sklearn._bayes",
          "declaration": "ARDRegression",
          "alias": null
        },
        {
          "module": "sklearn._bayes",
          "declaration": "BayesianRidge",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "ElasticNet",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "ElasticNetCV",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "Lasso",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "LassoCV",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "MultiTaskElasticNet",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "MultiTaskElasticNetCV",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "MultiTaskLasso",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "MultiTaskLassoCV",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "enet_path",
          "alias": null
        },
        {
          "module": "sklearn._coordinate_descent",
          "declaration": "lasso_path",
          "alias": null
        },
        {
          "module": "sklearn._glm",
          "declaration": "GammaRegressor",
          "alias": null
        },
        {
          "module": "sklearn._glm",
          "declaration": "PoissonRegressor",
          "alias": null
        },
        {
          "module": "sklearn._glm",
          "declaration": "TweedieRegressor",
          "alias": null
        },
        {
          "module": "sklearn._huber",
          "declaration": "HuberRegressor",
          "alias": null
        },
        {
          "module": "sklearn._least_angle",
          "declaration": "Lars",
          "alias": null
        },
        {
          "module": "sklearn._least_angle",
          "declaration": "LarsCV",
          "alias": null
        },
        {
          "module": "sklearn._least_angle",
          "declaration": "LassoLars",
          "alias": null
        },
        {
          "module": "sklearn._least_angle",
          "declaration": "LassoLarsCV",
          "alias": null
        },
        {
          "module": "sklearn._least_angle",
          "declaration": "LassoLarsIC",
          "alias": null
        },
        {
          "module": "sklearn._least_angle",
          "declaration": "lars_path",
          "alias": null
        },
        {
          "module": "sklearn._least_angle",
          "declaration": "lars_path_gram",
          "alias": null
        },
        {
          "module": "sklearn._logistic",
          "declaration": "LogisticRegression",
          "alias": null
        },
        {
          "module": "sklearn._logistic",
          "declaration": "LogisticRegressionCV",
          "alias": null
        },
        {
          "module": "sklearn._omp",
          "declaration": "OrthogonalMatchingPursuit",
          "alias": null
        },
        {
          "module": "sklearn._omp",
          "declaration": "OrthogonalMatchingPursuitCV",
          "alias": null
        },
        {
          "module": "sklearn._omp",
          "declaration": "orthogonal_mp",
          "alias": null
        },
        {
          "module": "sklearn._omp",
          "declaration": "orthogonal_mp_gram",
          "alias": null
        },
        {
          "module": "sklearn._passive_aggressive",
          "declaration": "PassiveAggressiveClassifier",
          "alias": null
        },
        {
          "module": "sklearn._passive_aggressive",
          "declaration": "PassiveAggressiveRegressor",
          "alias": null
        },
        {
          "module": "sklearn._perceptron",
          "declaration": "Perceptron",
          "alias": null
        },
        {
          "module": "sklearn._ransac",
          "declaration": "RANSACRegressor",
          "alias": null
        },
        {
          "module": "sklearn._ridge",
          "declaration": "Ridge",
          "alias": null
        },
        {
          "module": "sklearn._ridge",
          "declaration": "RidgeCV",
          "alias": null
        },
        {
          "module": "sklearn._ridge",
          "declaration": "RidgeClassifier",
          "alias": null
        },
        {
          "module": "sklearn._ridge",
          "declaration": "RidgeClassifierCV",
          "alias": null
        },
        {
          "module": "sklearn._ridge",
          "declaration": "ridge_regression",
          "alias": null
        },
        {
          "module": "sklearn._sgd_fast",
          "declaration": "Hinge",
          "alias": null
        },
        {
          "module": "sklearn._sgd_fast",
          "declaration": "Huber",
          "alias": null
        },
        {
          "module": "sklearn._sgd_fast",
          "declaration": "Log",
          "alias": null
        },
        {
          "module": "sklearn._sgd_fast",
          "declaration": "ModifiedHuber",
          "alias": null
        },
        {
          "module": "sklearn._sgd_fast",
          "declaration": "SquaredLoss",
          "alias": null
        },
        {
          "module": "sklearn._stochastic_gradient",
          "declaration": "SGDClassifier",
          "alias": null
        },
        {
          "module": "sklearn._stochastic_gradient",
          "declaration": "SGDRegressor",
          "alias": null
        },
        {
          "module": "sklearn._theil_sen",
          "declaration": "TheilSenRegressor",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._base",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "optimize",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "normalize",
          "alias": "f_normalize"
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils._seq_dataset",
          "declaration": "ArrayDataset32",
          "alias": null
        },
        {
          "module": "sklearn.utils._seq_dataset",
          "declaration": "ArrayDataset64",
          "alias": null
        },
        {
          "module": "sklearn.utils._seq_dataset",
          "declaration": "CSRDataset32",
          "alias": null
        },
        {
          "module": "sklearn.utils._seq_dataset",
          "declaration": "CSRDataset64",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "sparse_lsqr",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "inplace_column_scale",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "mean_variance_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "FLOAT_DTYPES",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "LinearClassifierMixin",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin"
          ],
          "methods": [
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Confidence scores per (sample, class) combination. In the binary\ncase, confidence score for self.classes_[1] where >0 means this\nclass would be predicted."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict confidence scores for samples.\n\nThe confidence score for a sample is proportional to the signed\ndistance of that sample to the hyperplane.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Samples.\n\nReturns\n-------\narray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n    Confidence scores per (sample, class) combination. In the binary\n    case, confidence score for self.classes_[1] where >0 means this\n    class would be predicted."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Predicted class label per sample."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class labels for samples in X.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Samples.\n\nReturns\n-------\nC : array, shape [n_samples]\n    Predicted class label per sample."
            }
          ],
          "fullDocstring": "Mixin for linear classifiers.\n\nHandles prediction for sparse and dense X."
        },
        {
          "name": "LinearModel",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit model."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples."
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": "Returns predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the linear model.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Samples.\n\nReturns\n-------\nC : array, shape (n_samples,)\n    Returns predicted values."
            }
          ],
          "fullDocstring": "Base class for Linear Models"
        },
        {
          "name": "LinearRegression",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "MultiOutputMixin",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values. Will be cast to X's dtype if necessary"
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Individual weights for each sample\n\n.. versionadded:: 0.17\n   parameter *sample_weight* support to LinearRegression."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit linear model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values. Will be cast to X's dtype if necessary\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample\n\n    .. versionadded:: 0.17\n       parameter *sample_weight* support to LinearRegression.\n\nReturns\n-------\nself : returns an instance of self."
            }
          ],
          "fullDocstring": "Ordinary least squares Linear Regression.\n\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup for n_targets > 1 and sufficient large problems.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\n\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\n\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\n\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\n\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\n\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n>>> # y = 1 * x_0 + 2 * x_1 + 3\n>>> y = np.dot(X, np.array([1, 2])) + 3\n>>> reg = LinearRegression().fit(X, y)\n>>> reg.score(X, y)\n1.0\n>>> reg.coef_\narray([1., 2.])\n>>> reg.intercept_\n3.0000...\n>>> reg.predict(np.array([[3, 5]]))\narray([16.])"
        },
        {
          "name": "SparseCoefMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "densify",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n\nReturns\n-------\nself\n    Fitted estimator."
            },
            {
              "name": "sparsify",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nReturns\n-------\nself\n    Fitted estimator.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify."
            }
          ],
          "fullDocstring": "Mixin for converting coef_ to and from CSR format.\n\nL1-regularizing estimators should inherit this."
        }
      ],
      "functions": [
        {
          "name": "make_dataset",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training data"
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values."
            },
            {
              "name": "sample_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The weight of each sample"
            },
            {
              "name": "random_state",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Determines random number generation for dataset shuffling and noise.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
            }
          ],
          "results": [
            {
              "name": "",
              "type": "Any",
              "description": "The intercept decay"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Create ``Dataset`` abstraction for sparse and dense inputs.\n\nThis also returns the ``intercept_decay`` which is different\nfor sparse datasets.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    Training data\n\ny : array-like, shape (n_samples, )\n    Target values.\n\nsample_weight : numpy array of shape (n_samples,)\n    The weight of each sample\n\nrandom_state : int, RandomState instance or None (default)\n    Determines random number generation for dataset shuffling and noise.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\ndataset\n    The ``Dataset`` abstraction\nintercept_decay\n    The intercept decay"
        }
      ]
    },
    {
      "name": "sklearn.linear_model._bayes",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "math",
          "declaration": "log",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "pinvh",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearModel",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "_rescale_data",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "fast_logdet",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "ARDRegression",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values (integers). Will be cast to X's dtype if necessary"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the ARDRegression model according to the given training data\nand parameters.\n\nIterative procedure to maximize the evidence\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples and\n    n_features is the number of features.\ny : array-like of shape (n_samples,)\n    Target values (integers). Will be cast to X's dtype if necessary\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples."
                },
                {
                  "name": "return_std",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "Whether to return the standard deviation of posterior prediction."
                }
              ],
              "results": [
                {
                  "name": "y_mean",
                  "type": null,
                  "description": "Mean of predictive distribution of query points."
                },
                {
                  "name": "y_std",
                  "type": null,
                  "description": "Standard deviation of predictive distribution of query points."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the linear model.\n\nIn addition to the mean of the predictive distribution, also its\nstandard deviation can be returned.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Samples.\n\nreturn_std : bool, default=False\n    Whether to return the standard deviation of posterior prediction.\n\nReturns\n-------\ny_mean : array-like of shape (n_samples,)\n    Mean of predictive distribution of query points.\n\ny_std : array-like of shape (n_samples,)\n    Standard deviation of predictive distribution of query points."
            }
          ],
          "fullDocstring": "Bayesian ARD regression.\n\nFit the weights of a regression model, using an ARD prior. The weights of\nthe regression model are assumed to be in Gaussian distributions.\nAlso estimate the parameters lambda (precisions of the distributions of the\nweights) and alpha (precision of the distribution of the noise).\nThe estimation is done by an iterative procedures (Evidence Maximization)\n\nRead more in the :ref:`User Guide <bayesian_regression>`.\n\nParameters\n----------\nn_iter : int, default=300\n    Maximum number of iterations.\n\ntol : float, default=1e-3\n    Stop the algorithm if w has converged.\n\nalpha_1 : float, default=1e-6\n    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the alpha parameter.\n\nalpha_2 : float, default=1e-6\n    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the alpha parameter.\n\nlambda_1 : float, default=1e-6\n    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the lambda parameter.\n\nlambda_2 : float, default=1e-6\n    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the lambda parameter.\n\ncompute_score : bool, default=False\n    If True, compute the objective function at each step of the model.\n\nthreshold_lambda : float, default=10 000\n    threshold for removing (pruning) weights with high precision from\n    the computation.\n\nfit_intercept : bool, default=True\n    whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nverbose : bool, default=False\n    Verbose mode when fitting the model.\n\nAttributes\n----------\ncoef_ : array-like of shape (n_features,)\n    Coefficients of the regression model (mean of distribution)\n\nalpha_ : float\n   estimated precision of the noise.\n\nlambda_ : array-like of shape (n_features,)\n   estimated precisions of the weights.\n\nsigma_ : array-like of shape (n_features, n_features)\n    estimated variance-covariance matrix of the weights\n\nscores_ : float\n    if computed, value of the objective function (to be maximized)\n\nintercept_ : float\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nX_offset_ : float\n    If `normalize=True`, offset subtracted for centering data to a\n    zero mean.\n\nX_scale_ : float\n    If `normalize=True`, parameter used to scale data to a unit\n    standard deviation.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.ARDRegression()\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\nARDRegression()\n>>> clf.predict([[1, 1]])\narray([1.])\n\nNotes\n-----\nFor an example, see :ref:`examples/linear_model/plot_ard.py\n<sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n\nReferences\n----------\nD. J. C. MacKay, Bayesian nonlinear modeling for the prediction\ncompetition, ASHRAE Transactions, 1994.\n\nR. Salakhutdinov, Lecture notes on Statistical Machine Learning,\nhttp://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\nTheir beta is our ``self.alpha_``\nTheir alpha is our ``self.lambda_``\nARD is a little different than the slide: only dimensions/features for\nwhich ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\ndiscarded."
        },
        {
          "name": "BayesianRidge",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values. Will be cast to X's dtype if necessary"
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Individual weights for each sample\n\n.. versionadded:: 0.20\n   parameter *sample_weight* support to BayesianRidge."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Training data\ny : ndarray of shape (n_samples,)\n    Target values. Will be cast to X's dtype if necessary\n\nsample_weight : ndarray of shape (n_samples,), default=None\n    Individual weights for each sample\n\n    .. versionadded:: 0.20\n       parameter *sample_weight* support to BayesianRidge.\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples."
                },
                {
                  "name": "return_std",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "Whether to return the standard deviation of posterior prediction."
                }
              ],
              "results": [
                {
                  "name": "y_mean",
                  "type": null,
                  "description": "Mean of predictive distribution of query points."
                },
                {
                  "name": "y_std",
                  "type": null,
                  "description": "Standard deviation of predictive distribution of query points."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the linear model.\n\nIn addition to the mean of the predictive distribution, also its\nstandard deviation can be returned.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Samples.\n\nreturn_std : bool, default=False\n    Whether to return the standard deviation of posterior prediction.\n\nReturns\n-------\ny_mean : array-like of shape (n_samples,)\n    Mean of predictive distribution of query points.\n\ny_std : array-like of shape (n_samples,)\n    Standard deviation of predictive distribution of query points."
            }
          ],
          "fullDocstring": "Bayesian ridge regression.\n\nFit a Bayesian ridge model. See the Notes section for details on this\nimplementation and the optimization of the regularization parameters\nlambda (precision of the weights) and alpha (precision of the noise).\n\nRead more in the :ref:`User Guide <bayesian_regression>`.\n\nParameters\n----------\nn_iter : int, default=300\n    Maximum number of iterations. Should be greater than or equal to 1.\n\ntol : float, default=1e-3\n    Stop the algorithm if w has converged.\n\nalpha_1 : float, default=1e-6\n    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the alpha parameter.\n\nalpha_2 : float, default=1e-6\n    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the alpha parameter.\n\nlambda_1 : float, default=1e-6\n    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the lambda parameter.\n\nlambda_2 : float, default=1e-6\n    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the lambda parameter.\n\nalpha_init : float, default=None\n    Initial value for alpha (precision of the noise).\n    If not set, alpha_init is 1/Var(y).\n\n        .. versionadded:: 0.22\n\nlambda_init : float, default=None\n    Initial value for lambda (precision of the weights).\n    If not set, lambda_init is 1.\n\n        .. versionadded:: 0.22\n\ncompute_score : bool, default=False\n    If True, compute the log marginal likelihood at each iteration of the\n    optimization.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model.\n    The intercept is not treated as a probabilistic parameter\n    and thus has no associated variance. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nverbose : bool, default=False\n    Verbose mode when fitting the model.\n\n\nAttributes\n----------\ncoef_ : array-like of shape (n_features,)\n    Coefficients of the regression model (mean of distribution)\n\nintercept_ : float\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nalpha_ : float\n   Estimated precision of the noise.\n\nlambda_ : float\n   Estimated precision of the weights.\n\nsigma_ : array-like of shape (n_features, n_features)\n    Estimated variance-covariance matrix of the weights\n\nscores_ : array-like of shape (n_iter_+1,)\n    If computed_score is True, value of the log marginal likelihood (to be\n    maximized) at each iteration of the optimization. The array starts\n    with the value of the log marginal likelihood obtained for the initial\n    values of alpha and lambda and ends with the value obtained for the\n    estimated alpha and lambda.\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n\nX_offset_ : float\n    If `normalize=True`, offset subtracted for centering data to a\n    zero mean.\n\nX_scale_ : float\n    If `normalize=True`, parameter used to scale data to a unit\n    standard deviation.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.BayesianRidge()\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\nBayesianRidge()\n>>> clf.predict([[1, 1]])\narray([1.])\n\nNotes\n-----\nThere exist several strategies to perform Bayesian ridge regression. This\nimplementation is based on the algorithm described in Appendix A of\n(Tipping, 2001) where updates of the regularization parameters are done as\nsuggested in (MacKay, 1992). Note that according to A New\nView of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\nupdate rules do not guarantee that the marginal likelihood is increasing\nbetween two consecutive iterations of the optimization.\n\nReferences\n----------\nD. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\nVol. 4, No. 3, 1992.\n\nM. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\nJournal of Machine Learning Research, Vol. 1, 2001."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._coordinate_descent",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "sys",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "_cd_fast",
          "alias": "cd_fast"
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearModel",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "_pre_fit",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "_preprocess_data",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_astype_copy_false",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_joblib_parallel_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "column_or_1d",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "ElasticNet",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "MultiOutputMixin",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target. Will be cast to X's dtype if necessary."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weight.\n\n.. versionadded:: 0.23"
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Allow to bypass several input checking.\nDon't use this parameter unless you know what you do."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit model with coordinate descent.\n\nParameters\n----------\nX : {ndarray, sparse matrix} of (n_samples, n_features)\n    Data.\n\ny : {ndarray, sparse matrix} of shape (n_samples,) or             (n_samples, n_targets)\n    Target. Will be cast to X's dtype if necessary.\n\nsample_weight : float or array-like of shape (n_samples,), default=None\n    Sample weight.\n\n    .. versionadded:: 0.23\n\ncheck_input : bool, default=True\n    Allow to bypass several input checking.\n    Don't use this parameter unless you know what you do.\n\nNotes\n-----\n\nCoordinate descent is an algorithm that considers each column of\ndata at a time hence it will automatically convert the X input\nas a Fortran-contiguous numpy array if necessary.\n\nTo avoid memory re-allocation it is advised to allocate the\ninitial data in memory directly using that format."
            },
            {
              "name": "sparse_coef_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Sparse representation of the fitted `coef_`."
            }
          ],
          "fullDocstring": "Linear regression with combined L1 and L2 priors as regularizer.\n\nMinimizes the objective function::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\nIf you are interested in controlling the L1 and L2 penalty\nseparately, keep in mind that this is equivalent to::\n\n        a * L1 + b * L2\n\nwhere::\n\n        alpha = a + b and l1_ratio = a / (a + b)\n\nThe parameter l1_ratio corresponds to alpha in the glmnet R package while\nalpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\nunless you supply your own sequence of alpha.\n\nRead more in the :ref:`User Guide <elastic_net>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the penalty terms. Defaults to 1.0.\n    See the notes for the exact mathematical meaning of this\n    parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n    solved by the :class:`LinearRegression` object. For numerical\n    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n    Given this, you should use the :class:`LinearRegression` object.\n\nl1_ratio : float, default=0.5\n    The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n    is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n    combination of L1 and L2.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If ``False``, the\n    data is assumed to be already centered.\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : bool or array-like of shape (n_features, n_features),                 default=False\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. The Gram matrix can also be passed as argument.\n    For sparse input this option is always ``True`` to preserve sparsity.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the cost function formula).\n\nsparse_coef_ : sparse matrix of shape (n_features,) or             (n_tasks, n_features)\n    Sparse representation of the `coef_`.\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : list of int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance.\n\ndual_gap_ : float or ndarray of shape (n_targets,)\n    Given param alpha, the dual gaps at the end of the optimization,\n    same shape as each observation of y.\n\nExamples\n--------\n>>> from sklearn.linear_model import ElasticNet\n>>> from sklearn.datasets import make_regression\n\n>>> X, y = make_regression(n_features=2, random_state=0)\n>>> regr = ElasticNet(random_state=0)\n>>> regr.fit(X, y)\nElasticNet(random_state=0)\n>>> print(regr.coef_)\n[18.83816048 64.55968825]\n>>> print(regr.intercept_)\n1.451...\n>>> print(regr.predict([[0, 0]]))\n[1.451...]\n\n\nNotes\n-----\nTo avoid unnecessary memory duplication the X argument of the fit method\nshould be directly passed as a Fortran-contiguous numpy array.\n\nSee Also\n--------\nElasticNetCV : Elastic net model with best model selection by\n    cross-validation.\nSGDRegressor : Implements elastic net regression with incremental training.\nSGDClassifier : Implements logistic regression with elastic net penalty\n    (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``)."
        },
        {
          "name": "ElasticNetCV",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "LinearModelCV"
          ],
          "methods": [],
          "fullDocstring": "Elastic Net model with iterative fitting along a regularization path.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <elastic_net>`.\n\nParameters\n----------\nl1_ratio : float or list of float, default=0.5\n    float between 0 and 1 passed to ElasticNet (scaling between\n    l1 and l2 penalties). For ``l1_ratio = 0``\n    the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n    For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n    This parameter can be a list, in which case the different\n    values are tested by cross-validation and the one giving the best\n    prediction score is used. Note that a good choice of list of\n    values for l1_ratio is often to put more values close to 1\n    (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n    .9, .95, .99, 1]``.\n\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``.\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path, used for each l1_ratio.\n\nalphas : ndarray, default=None\n    List of alphas where to compute the models.\n    If None alphas are set automatically.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - int, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nverbose : bool or int, default=0\n    Amount of verbosity.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nalpha_ : float\n    The amount of penalization chosen by cross validation.\n\nl1_ratio_ : float\n    The compromise between l1 and l2 penalization chosen by\n    cross validation.\n\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the cost function formula).\n\nintercept_ : float or ndarray of shape (n_targets, n_features)\n    Independent term in the decision function.\n\nmse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)\n    Mean square error for the test set on each fold, varying l1_ratio and\n    alpha.\n\nalphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n    The grid of alphas used for fitting, for each l1_ratio.\n\ndual_gap_ : float\n    The dual gaps at the end of the optimization for the optimal alpha.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance for the optimal alpha.\n\nExamples\n--------\n>>> from sklearn.linear_model import ElasticNetCV\n>>> from sklearn.datasets import make_regression\n\n>>> X, y = make_regression(n_features=2, random_state=0)\n>>> regr = ElasticNetCV(cv=5, random_state=0)\n>>> regr.fit(X, y)\nElasticNetCV(cv=5, random_state=0)\n>>> print(regr.alpha_)\n0.199...\n>>> print(regr.intercept_)\n0.398...\n>>> print(regr.predict([[0, 0]]))\n[0.398...]\n\n\nNotes\n-----\nFor an example, see\n:ref:`examples/linear_model/plot_lasso_model_selection.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\nTo avoid unnecessary memory duplication the X argument of the fit method\nshould be directly passed as a Fortran-contiguous numpy array.\n\nThe parameter l1_ratio corresponds to alpha in the glmnet R package\nwhile alpha corresponds to the lambda parameter in glmnet.\nMore specifically, the optimization objective is::\n\n    1 / (2 * n_samples) * ||y - Xw||^2_2\n    + alpha * l1_ratio * ||w||_1\n    + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\nIf you are interested in controlling the L1 and L2 penalty\nseparately, keep in mind that this is equivalent to::\n\n    a * L1 + b * L2\n\nfor::\n\n    alpha = a + b and l1_ratio = a / (a + b).\n\nSee Also\n--------\nenet_path\nElasticNet"
        },
        {
          "name": "Lasso",
          "decorators": [],
          "superclasses": [
            "ElasticNet"
          ],
          "methods": [],
          "fullDocstring": "Linear Model trained with L1 prior as regularizer (aka the Lasso)\n\nThe optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nTechnically the Lasso model is optimizing the same objective function as\nthe Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n\nRead more in the :ref:`User Guide <lasso>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the L1 term. Defaults to 1.0.\n    ``alpha = 0`` is equivalent to an ordinary least square, solved\n    by the :class:`LinearRegression` object. For numerical\n    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n    Given this, you should use the :class:`LinearRegression` object.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : 'auto', bool or array-like of shape (n_features, n_features),                 default=False\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument. For sparse input\n    this option is always ``True`` to preserve sparsity.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the cost function formula).\n\ndual_gap_ : float or ndarray of shape (n_targets,)\n    Given param alpha, the dual gaps at the end of the optimization,\n    same shape as each observation of y.\n\nsparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n    Readonly property derived from ``coef_``.\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : int or list of int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.Lasso(alpha=0.1)\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\nLasso(alpha=0.1)\n>>> print(clf.coef_)\n[0.85 0.  ]\n>>> print(clf.intercept_)\n0.15...\n\nSee Also\n--------\nlars_path\nlasso_path\nLassoLars\nLassoCV\nLassoLarsCV\nsklearn.decomposition.sparse_encode\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nTo avoid unnecessary memory duplication the X argument of the fit method\nshould be directly passed as a Fortran-contiguous numpy array."
        },
        {
          "name": "LassoCV",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "LinearModelCV"
          ],
          "methods": [],
          "fullDocstring": "Lasso linear model with iterative fitting along a regularization path.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThe best model is selected by cross-validation.\n\nThe optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nRead more in the :ref:`User Guide <lasso>`.\n\nParameters\n----------\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``.\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path.\n\nalphas : ndarray, default=None\n    List of alphas where to compute the models.\n    If ``None`` alphas are set automatically.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - int, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nverbose : bool or int, default=False\n    Amount of verbosity.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npositive : bool, default=False\n    If positive, restrict regression coefficients to be positive.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nalpha_ : float\n    The amount of penalization chosen by cross validation.\n\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the cost function formula).\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\nmse_path_ : ndarray of shape (n_alphas, n_folds)\n    Mean square error for the test set on each fold, varying alpha.\n\nalphas_ : ndarray of shape (n_alphas,)\n    The grid of alphas used for fitting.\n\ndual_gap_ : float or ndarray of shape (n_targets,)\n    The dual gap at the end of the optimization for the optimal alpha\n    (``alpha_``).\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance for the optimal alpha.\n\nExamples\n--------\n>>> from sklearn.linear_model import LassoCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(noise=4, random_state=0)\n>>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9993...\n>>> reg.predict(X[:1,])\narray([-78.4951...])\n\nNotes\n-----\nFor an example, see\n:ref:`examples/linear_model/plot_lasso_model_selection.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\nTo avoid unnecessary memory duplication the X argument of the fit method\nshould be directly passed as a Fortran-contiguous numpy array.\n\nSee Also\n--------\nlars_path\nlasso_path\nLassoLars\nLasso\nLassoLarsCV"
        },
        {
          "name": "LinearModelCV",
          "decorators": [],
          "superclasses": [
            "LinearModel",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data. Pass directly as Fortran-contiguous data\nto avoid unnecessary memory duplication. If y is mono-output,\nX can be sparse."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit linear model with coordinate descent.\n\nFit is on grid of alphas and best alpha estimated by cross-validation.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data. Pass directly as Fortran-contiguous data\n    to avoid unnecessary memory duplication. If y is mono-output,\n    X can be sparse.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values."
            }
          ],
          "fullDocstring": "Base class for iterative model fitting along a regularization path."
        },
        {
          "name": "MultiTaskElasticNet",
          "decorators": [],
          "superclasses": [
            "Lasso"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target. Will be cast to X's dtype if necessary."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit MultiTaskElasticNet model with coordinate descent\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data.\ny : ndarray of shape (n_samples, n_tasks)\n    Target. Will be cast to X's dtype if necessary.\n\nNotes\n-----\n\nCoordinate descent is an algorithm that considers each column of\ndata at a time hence it will automatically convert the X input\nas a Fortran-contiguous numpy array if necessary.\n\nTo avoid memory re-allocation it is advised to allocate the\ninitial data in memory directly using that format."
            }
          ],
          "fullDocstring": "Multi-task ElasticNet model trained with L1/L2 mixed-norm as\nregularizer.\n\nThe optimization objective for MultiTaskElasticNet is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n    + alpha * l1_ratio * ||W||_21\n    + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\nWhere::\n\n    ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\n\ni.e. the sum of norms of each row.\n\nRead more in the :ref:`User Guide <multi_task_elastic_net>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the L1/L2 term. Defaults to 1.0.\n\nl1_ratio : float, default=0.5\n    The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n    For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n    is an L2 penalty.\n    For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nintercept_ : ndarray of shape (n_tasks,)\n    Independent term in decision function.\n\ncoef_ : ndarray of shape (n_tasks, n_features)\n    Parameter vector (W in the cost function formula). If a 1D y is\n    passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\n    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance.\n\ndual_gap_ : float\n    The dual gaps at the end of the optimization.\n\neps_ : float\n    The tolerance scaled scaled by the variance of the target `y`.\n\nsparse_coef_ : sparse matrix of shape (n_features,) or             (n_tasks, n_features)\n    Sparse representation of the `coef_`.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\nMultiTaskElasticNet(alpha=0.1)\n>>> print(clf.coef_)\n[[0.45663524 0.45612256]\n [0.45663524 0.45612256]]\n>>> print(clf.intercept_)\n[0.0872422 0.0872422]\n\nSee Also\n--------\nMultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n    cross-validation.\nElasticNet\nMultiTaskLasso\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nTo avoid unnecessary memory duplication the X and y arguments of the fit\nmethod should be directly passed as Fortran-contiguous numpy arrays."
        },
        {
          "name": "MultiTaskElasticNetCV",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "LinearModelCV"
          ],
          "methods": [],
          "fullDocstring": "Multi-task L1/L2 ElasticNet with built-in cross-validation.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThe optimization objective for MultiTaskElasticNet is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n    + alpha * l1_ratio * ||W||_21\n    + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\nWhere::\n\n    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_elastic_net>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\nl1_ratio : float or list of float, default=0.5\n    The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n    For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n    is an L2 penalty.\n    For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n    This parameter can be a list, in which case the different\n    values are tested by cross-validation and the one giving the best\n    prediction score is used. Note that a good choice of list of\n    values for l1_ratio is often to put more values close to 1\n    (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n    .9, .95, .99, 1]``\n\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``.\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path.\n\nalphas : array-like, default=None\n    List of alphas where to compute the models.\n    If not provided, set automatically.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - int, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nverbose : bool or int, default=0\n    Amount of verbosity.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation. Note that this is\n    used only if multiple values for l1_ratio are given.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nintercept_ : ndarray of shape (n_tasks,)\n    Independent term in decision function.\n\ncoef_ : ndarray of shape (n_tasks, n_features)\n    Parameter vector (W in the cost function formula).\n    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\nalpha_ : float\n    The amount of penalization chosen by cross validation.\n\nmse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n    Mean square error for the test set on each fold, varying alpha.\n\nalphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n    The grid of alphas used for fitting, for each l1_ratio.\n\nl1_ratio_ : float\n    Best l1_ratio obtained by cross-validation.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance for the optimal alpha.\n\ndual_gap_ : float\n    The dual gap at the end of the optimization for the optimal alpha.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n>>> clf.fit([[0,0], [1, 1], [2, 2]],\n...         [[0, 0], [1, 1], [2, 2]])\nMultiTaskElasticNetCV(cv=3)\n>>> print(clf.coef_)\n[[0.52875032 0.46958558]\n [0.52875032 0.46958558]]\n>>> print(clf.intercept_)\n[0.00166409 0.00166409]\n\nSee Also\n--------\nMultiTaskElasticNet\nElasticNetCV\nMultiTaskLassoCV\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nTo avoid unnecessary memory duplication the X and y arguments of the fit\nmethod should be directly passed as Fortran-contiguous numpy arrays."
        },
        {
          "name": "MultiTaskLasso",
          "decorators": [],
          "superclasses": [
            "MultiTaskElasticNet"
          ],
          "methods": [],
          "fullDocstring": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\nThe optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\nWhere::\n\n    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_lasso>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the L1/L2 term. Defaults to 1.0.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_tasks, n_features)\n    Parameter vector (W in the cost function formula).\n    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\nintercept_ : ndarray of shape (n_tasks,)\n    Independent term in decision function.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance.\n\ndual_gap_ : ndarray of shape (n_alphas,)\n    The dual gaps at the end of the optimization for each alpha.\n\neps_ : float\n    The tolerance scaled scaled by the variance of the target `y`.\n\nsparse_coef_ : sparse matrix of shape (n_features,) or             (n_tasks, n_features)\n    Sparse representation of the `coef_`.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n>>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\nMultiTaskLasso(alpha=0.1)\n>>> print(clf.coef_)\n[[0.         0.60809415]\n[0.         0.94592424]]\n>>> print(clf.intercept_)\n[-0.41888636 -0.87382323]\n\nSee Also\n--------\nMultiTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation.\nLasso\nMultiTaskElasticNet\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nTo avoid unnecessary memory duplication the X and y arguments of the fit\nmethod should be directly passed as Fortran-contiguous numpy arrays."
        },
        {
          "name": "MultiTaskLassoCV",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "LinearModelCV"
          ],
          "methods": [],
          "fullDocstring": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThe optimization objective for MultiTaskLasso is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n\nWhere::\n\n    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_lasso>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``.\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path.\n\nalphas : array-like, default=None\n    List of alphas where to compute the models.\n    If not provided, set automatically.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - int, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nverbose : bool or int, default=False\n    Amount of verbosity.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation. Note that this is\n    used only if multiple values for l1_ratio are given.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nintercept_ : ndarray of shape (n_tasks,)\n    Independent term in decision function.\n\ncoef_ : ndarray of shape (n_tasks, n_features)\n    Parameter vector (W in the cost function formula).\n    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\nalpha_ : float\n    The amount of penalization chosen by cross validation.\n\nmse_path_ : ndarray of shape (n_alphas, n_folds)\n    Mean square error for the test set on each fold, varying alpha.\n\nalphas_ : ndarray of shape (n_alphas,)\n    The grid of alphas used for fitting.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance for the optimal alpha.\n\ndual_gap_ : float\n    The dual gap at the end of the optimization for the optimal alpha.\n\nExamples\n--------\n>>> from sklearn.linear_model import MultiTaskLassoCV\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.metrics import r2_score\n>>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n>>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n>>> r2_score(y, reg.predict(X))\n0.9994...\n>>> reg.alpha_\n0.5713...\n>>> reg.predict(X[:1,])\narray([[153.7971...,  94.9015...]])\n\nSee Also\n--------\nMultiTaskElasticNet\nElasticNetCV\nMultiTaskElasticNetCV\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nTo avoid unnecessary memory duplication the X and y arguments of the fit\nmethod should be directly passed as Fortran-contiguous numpy arrays."
        }
      ],
      "functions": [
        {
          "name": "enet_path",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training data. Pass directly as Fortran-contiguous data to avoid\nunnecessary memory duplication. If ``y`` is mono-output then ``X``\ncan be sparse."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values."
            }
          ],
          "results": [
            {
              "name": "alphas",
              "type": null,
              "description": "The alphas along the path where models are computed."
            },
            {
              "name": "coefs",
              "type": null,
              "description": "Coefficients along the path."
            },
            {
              "name": "dual_gaps",
              "type": null,
              "description": "The dual gaps at the end of the optimization for each alpha."
            },
            {
              "name": "n_iters",
              "type": null,
              "description": "The number of iterations taken by the coordinate descent optimizer to\nreach the specified tolerance for each alpha.\n(Is returned when ``return_n_iter`` is set to True)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute elastic net path with coordinate descent.\n\nThe elastic net optimization function varies for mono and multi-outputs.\n\nFor mono-output tasks it is::\n\n    1 / (2 * n_samples) * ||y - Xw||^2_2\n    + alpha * l1_ratio * ||w||_1\n    + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\nFor multi-output tasks it is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n    + alpha * l1_ratio * ||W||_21\n    + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\nWhere::\n\n    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <elastic_net>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data. Pass directly as Fortran-contiguous data to avoid\n    unnecessary memory duplication. If ``y`` is mono-output then ``X``\n    can be sparse.\n\ny : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n    Target values.\n\nl1_ratio : float, default=0.5\n    Number between 0 and 1 passed to elastic net (scaling between\n    l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``.\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path.\n\nalphas : ndarray, default=None\n    List of alphas where to compute the models.\n    If None alphas are set automatically.\n\nprecompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nXy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n    Xy = np.dot(X.T, y) that can be precomputed. It is useful\n    only when the Gram matrix is precomputed.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\ncoef_init : ndarray of shape (n_features, ), default=None\n    The initial values of the coefficients.\n\nverbose : bool or int, default=False\n    Amount of verbosity.\n\nreturn_n_iter : bool, default=False\n    Whether to return the number of iterations or not.\n\npositive : bool, default=False\n    If set to True, forces coefficients to be positive.\n    (Only allowed when ``y.ndim == 1``).\n\ncheck_input : bool, default=True\n    If set to False, the input validation checks are skipped (including the\n    Gram matrix when provided). It is assumed that they are handled\n    by the caller.\n\n**params : kwargs\n    Keyword arguments passed to the coordinate descent solver.\n\nReturns\n-------\nalphas : ndarray of shape (n_alphas,)\n    The alphas along the path where models are computed.\n\ncoefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n    Coefficients along the path.\n\ndual_gaps : ndarray of shape (n_alphas,)\n    The dual gaps at the end of the optimization for each alpha.\n\nn_iters : list of int\n    The number of iterations taken by the coordinate descent optimizer to\n    reach the specified tolerance for each alpha.\n    (Is returned when ``return_n_iter`` is set to True).\n\nSee Also\n--------\nMultiTaskElasticNet\nMultiTaskElasticNetCV\nElasticNet\nElasticNetCV\n\nNotes\n-----\nFor an example, see\n:ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`."
        },
        {
          "name": "lasso_path",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training data. Pass directly as Fortran-contiguous data to avoid\nunnecessary memory duplication. If ``y`` is mono-output then ``X``\ncan be sparse."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values"
            }
          ],
          "results": [
            {
              "name": "alphas",
              "type": null,
              "description": "The alphas along the path where models are computed."
            },
            {
              "name": "coefs",
              "type": null,
              "description": "Coefficients along the path."
            },
            {
              "name": "dual_gaps",
              "type": null,
              "description": "The dual gaps at the end of the optimization for each alpha."
            },
            {
              "name": "n_iters",
              "type": null,
              "description": "The number of iterations taken by the coordinate descent optimizer to\nreach the specified tolerance for each alpha."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Lasso path with coordinate descent\n\nThe Lasso optimization function varies for mono and multi-outputs.\n\nFor mono-output tasks it is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nFor multi-output tasks it is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\nWhere::\n\n    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <lasso>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data. Pass directly as Fortran-contiguous data to avoid\n    unnecessary memory duplication. If ``y`` is mono-output then ``X``\n    can be sparse.\n\ny : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n    Target values\n\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path\n\nalphas : ndarray, default=None\n    List of alphas where to compute the models.\n    If ``None`` alphas are set automatically\n\nprecompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nXy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n    Xy = np.dot(X.T, y) that can be precomputed. It is useful\n    only when the Gram matrix is precomputed.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\ncoef_init : ndarray of shape (n_features, ), default=None\n    The initial values of the coefficients.\n\nverbose : bool or int, default=False\n    Amount of verbosity.\n\nreturn_n_iter : bool, default=False\n    whether to return the number of iterations or not.\n\npositive : bool, default=False\n    If set to True, forces coefficients to be positive.\n    (Only allowed when ``y.ndim == 1``).\n\n**params : kwargs\n    keyword arguments passed to the coordinate descent solver.\n\nReturns\n-------\nalphas : ndarray of shape (n_alphas,)\n    The alphas along the path where models are computed.\n\ncoefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n    Coefficients along the path.\n\ndual_gaps : ndarray of shape (n_alphas,)\n    The dual gaps at the end of the optimization for each alpha.\n\nn_iters : list of int\n    The number of iterations taken by the coordinate descent optimizer to\n    reach the specified tolerance for each alpha.\n\nNotes\n-----\nFor an example, see\n:ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n\nTo avoid unnecessary memory duplication the X argument of the fit method\nshould be directly passed as a Fortran-contiguous numpy array.\n\nNote that in certain cases, the Lars solver may be significantly\nfaster to implement this functionality. In particular, linear\ninterpolation can be used to retrieve model coefficients between the\nvalues output by lars_path\n\nExamples\n--------\n\nComparing lasso_path and lars_path with interpolation:\n\n>>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n>>> y = np.array([1, 2, 3.1])\n>>> # Use lasso_path to compute a coefficient path\n>>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n>>> print(coef_path)\n[[0.         0.         0.46874778]\n [0.2159048  0.4425765  0.23689075]]\n\n>>> # Now use lars_path and 1D linear interpolation to compute the\n>>> # same path\n>>> from sklearn.linear_model import lars_path\n>>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n>>> from scipy import interpolate\n>>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n...                                             coef_path_lars[:, ::-1])\n>>> print(coef_path_continuous([5., 1., .5]))\n[[0.         0.         0.46915237]\n [0.2159048  0.4425765  0.23668876]]\n\nSee Also\n--------\nlars_path\nLasso\nLassoLars\nLassoCV\nLassoLarsCV\nsklearn.decomposition.sparse_encode"
        }
      ]
    },
    {
      "name": "sklearn.linear_model._glm",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.linear_model.glm",
          "declaration": "GammaRegressor",
          "alias": null
        },
        {
          "module": "sklearn.linear_model.glm",
          "declaration": "GeneralizedLinearRegressor",
          "alias": null
        },
        {
          "module": "sklearn.linear_model.glm",
          "declaration": "PoissonRegressor",
          "alias": null
        },
        {
          "module": "sklearn.linear_model.glm",
          "declaration": "TweedieRegressor",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._glm.glm",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.optimize",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn._loss.glm_distribution",
          "declaration": "EDM_DISTRIBUTIONS",
          "alias": null
        },
        {
          "module": "sklearn._loss.glm_distribution",
          "declaration": "ExponentialDispersionModel",
          "alias": null
        },
        {
          "module": "sklearn._loss.glm_distribution",
          "declaration": "TweedieDistribution",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._glm.link",
          "declaration": "BaseLink",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._glm.link",
          "declaration": "IdentityLink",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._glm.link",
          "declaration": "LogLink",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.optimize",
          "declaration": "_check_optimize_result",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "GammaRegressor",
          "decorators": [],
          "superclasses": [
            "GeneralizedLinearRegressor"
          ],
          "methods": [
            {
              "name": "family",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "value",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Generalized Linear Model with a Gamma distribution.\n\nRead more in the :ref:`User Guide <Generalized_linear_regression>`.\n\n.. versionadded:: 0.23\n\nParameters\n----------\nalpha : float, default=1\n    Constant that multiplies the penalty term and thus determines the\n    regularization strength. ``alpha = 0`` is equivalent to unpenalized\n    GLMs. In this case, the design matrix `X` must have full column rank\n    (no collinearities).\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the linear predictor (X @ coef + intercept).\n\nmax_iter : int, default=100\n    The maximal number of iterations for the solver.\n\ntol : float, default=1e-4\n    Stopping criterion. For the lbfgs solver,\n    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n    where ``g_j`` is the j-th component of the gradient (derivative) of\n    the objective function.\n\nwarm_start : bool, default=False\n    If set to ``True``, reuse the solution of the previous call to ``fit``\n    as initialization for ``coef_`` and ``intercept_`` .\n\nverbose : int, default=0\n    For the lbfgs solver set verbose to any positive number for verbosity.\n\nAttributes\n----------\ncoef_ : array of shape (n_features,)\n    Estimated coefficients for the linear predictor (`X * coef_ +\n    intercept_`) in the GLM.\n\nintercept_ : float\n    Intercept (a.k.a. bias) added to linear predictor.\n\nn_iter_ : int\n    Actual number of iterations used in the solver.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.GammaRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [19, 26, 33, 30]\n>>> clf.fit(X, y)\nGammaRegressor()\n>>> clf.score(X, y)\n0.773...\n>>> clf.coef_\narray([0.072..., 0.066...])\n>>> clf.intercept_\n2.896...\n>>> clf.predict([[1, 0], [2, 8]])\narray([19.483..., 35.795...])"
        },
        {
          "name": "GeneralizedLinearRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit a Generalized Linear Model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Samples."
                }
              ],
              "results": [
                {
                  "name": "y_pred",
                  "type": null,
                  "description": "Returns predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using GLM with feature matrix X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Samples.\n\nReturns\n-------\ny_pred : array of shape (n_samples,)\n    Returns predicted values."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True values of target."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "D^2 of self.predict(X) w.r.t. y."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute D^2, the percentage of deviance explained.\n\nD^2 is a generalization of the coefficient of determination R^2.\nR^2 uses squared error and D^2 deviance. Note that those two are equal\nfor ``family='normal'``.\n\nD^2 is defined as\n:math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n:math:`D_{null}` is the null deviance, i.e. the deviance of a model\nwith intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\nThe mean :math:`\\bar{y}` is averaged by sample_weight.\nBest possible score is 1.0 and it can be negative (because the model\ncan be arbitrarily worse).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Test samples.\n\ny : array-like of shape (n_samples,)\n    True values of target.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    D^2 of self.predict(X) w.r.t. y."
            }
          ],
          "fullDocstring": "Regression via a penalized Generalized Linear Model (GLM).\n\nGLMs based on a reproductive Exponential Dispersion Model (EDM) aim at\nfitting and predicting the mean of the target y as y_pred=h(X*w).\nTherefore, the fit minimizes the following objective function with L2\npriors as regularizer::\n\n        1/(2*sum(s)) * deviance(y, h(X*w); s)\n        + 1/2 * alpha * |w|_2\n\nwith inverse link function h and s=sample_weight.\nThe parameter ``alpha`` corresponds to the lambda parameter in glmnet.\n\nRead more in the :ref:`User Guide <Generalized_linear_regression>`.\n\n.. versionadded:: 0.23\n\nParameters\n----------\nalpha : float, default=1\n    Constant that multiplies the penalty term and thus determines the\n    regularization strength. ``alpha = 0`` is equivalent to unpenalized\n    GLMs. In this case, the design matrix `X` must have full column rank\n    (no collinearities).\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the linear predictor (X @ coef + intercept).\n\nfamily : {'normal', 'poisson', 'gamma', 'inverse-gaussian'}             or an ExponentialDispersionModel instance, default='normal'\n    The distributional assumption of the GLM, i.e. which distribution from\n    the EDM, specifies the loss function to be minimized.\n\nlink : {'auto', 'identity', 'log'} or an instance of class BaseLink,             default='auto'\n    The link function of the GLM, i.e. mapping from linear predictor\n    `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\n    the link depending on the chosen family as follows:\n\n    - 'identity' for Normal distribution\n    - 'log' for Poisson,  Gamma and Inverse Gaussian distributions\n\nsolver : 'lbfgs', default='lbfgs'\n    Algorithm to use in the optimization problem:\n\n    'lbfgs'\n        Calls scipy's L-BFGS-B optimizer.\n\nmax_iter : int, default=100\n    The maximal number of iterations for the solver.\n\ntol : float, default=1e-4\n    Stopping criterion. For the lbfgs solver,\n    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n    where ``g_j`` is the j-th component of the gradient (derivative) of\n    the objective function.\n\nwarm_start : bool, default=False\n    If set to ``True``, reuse the solution of the previous call to ``fit``\n    as initialization for ``coef_`` and ``intercept_``.\n\nverbose : int, default=0\n    For the lbfgs solver set verbose to any positive number for verbosity.\n\nAttributes\n----------\ncoef_ : array of shape (n_features,)\n    Estimated coefficients for the linear predictor (`X @ coef_ +\n    intercept_`) in the GLM.\n\nintercept_ : float\n    Intercept (a.k.a. bias) added to linear predictor.\n\nn_iter_ : int\n    Actual number of iterations used in the solver."
        },
        {
          "name": "PoissonRegressor",
          "decorators": [],
          "superclasses": [
            "GeneralizedLinearRegressor"
          ],
          "methods": [
            {
              "name": "family",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "value",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Generalized Linear Model with a Poisson distribution.\n\nRead more in the :ref:`User Guide <Generalized_linear_regression>`.\n\n.. versionadded:: 0.23\n\nParameters\n----------\nalpha : float, default=1\n    Constant that multiplies the penalty term and thus determines the\n    regularization strength. ``alpha = 0`` is equivalent to unpenalized\n    GLMs. In this case, the design matrix `X` must have full column rank\n    (no collinearities).\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the linear predictor (X @ coef + intercept).\n\nmax_iter : int, default=100\n    The maximal number of iterations for the solver.\n\ntol : float, default=1e-4\n    Stopping criterion. For the lbfgs solver,\n    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n    where ``g_j`` is the j-th component of the gradient (derivative) of\n    the objective function.\n\nwarm_start : bool, default=False\n    If set to ``True``, reuse the solution of the previous call to ``fit``\n    as initialization for ``coef_`` and ``intercept_`` .\n\nverbose : int, default=0\n    For the lbfgs solver set verbose to any positive number for verbosity.\n\nAttributes\n----------\ncoef_ : array of shape (n_features,)\n    Estimated coefficients for the linear predictor (`X @ coef_ +\n    intercept_`) in the GLM.\n\nintercept_ : float\n    Intercept (a.k.a. bias) added to linear predictor.\n\nn_iter_ : int\n    Actual number of iterations used in the solver.\n\nExamples\n----------\n>>> from sklearn import linear_model\n>>> clf = linear_model.PoissonRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [12, 17, 22, 21]\n>>> clf.fit(X, y)\nPoissonRegressor()\n>>> clf.score(X, y)\n0.990...\n>>> clf.coef_\narray([0.121..., 0.158...])\n>>> clf.intercept_\n2.088...\n>>> clf.predict([[1, 1], [3, 4]])\narray([10.676..., 21.875...])"
        },
        {
          "name": "TweedieRegressor",
          "decorators": [],
          "superclasses": [
            "GeneralizedLinearRegressor"
          ],
          "methods": [
            {
              "name": "family",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": {
                    "valid_values": []
                  },
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "value",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Generalized Linear Model with a Tweedie distribution.\n\nThis estimator can be used to model different GLMs depending on the\n``power`` parameter, which determines the underlying distribution.\n\nRead more in the :ref:`User Guide <Generalized_linear_regression>`.\n\n.. versionadded:: 0.23\n\nParameters\n----------\npower : float, default=0\n        The power determines the underlying target distribution according\n        to the following table:\n\n        +-------+------------------------+\n        | Power | Distribution           |\n        +=======+========================+\n        | 0     | Normal                 |\n        +-------+------------------------+\n        | 1     | Poisson                |\n        +-------+------------------------+\n        | (1,2) | Compound Poisson Gamma |\n        +-------+------------------------+\n        | 2     | Gamma                  |\n        +-------+------------------------+\n        | 3     | Inverse Gaussian       |\n        +-------+------------------------+\n\n        For ``0 < power < 1``, no distribution exists.\n\nalpha : float, default=1\n    Constant that multiplies the penalty term and thus determines the\n    regularization strength. ``alpha = 0`` is equivalent to unpenalized\n    GLMs. In this case, the design matrix `X` must have full column rank\n    (no collinearities).\n\nlink : {'auto', 'identity', 'log'}, default='auto'\n    The link function of the GLM, i.e. mapping from linear predictor\n    `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\n    the link depending on the chosen family as follows:\n\n    - 'identity' for Normal distribution\n    - 'log' for Poisson,  Gamma and Inverse Gaussian distributions\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the linear predictor (X @ coef + intercept).\n\nmax_iter : int, default=100\n    The maximal number of iterations for the solver.\n\ntol : float, default=1e-4\n    Stopping criterion. For the lbfgs solver,\n    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n    where ``g_j`` is the j-th component of the gradient (derivative) of\n    the objective function.\n\nwarm_start : bool, default=False\n    If set to ``True``, reuse the solution of the previous call to ``fit``\n    as initialization for ``coef_`` and ``intercept_`` .\n\nverbose : int, default=0\n    For the lbfgs solver set verbose to any positive number for verbosity.\n\nAttributes\n----------\ncoef_ : array of shape (n_features,)\n    Estimated coefficients for the linear predictor (`X @ coef_ +\n    intercept_`) in the GLM.\n\nintercept_ : float\n    Intercept (a.k.a. bias) added to linear predictor.\n\nn_iter_ : int\n    Actual number of iterations used in the solver.\n\nExamples\n----------\n>>> from sklearn import linear_model\n>>> clf = linear_model.TweedieRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [2, 3.5, 5, 5.5]\n>>> clf.fit(X, y)\nTweedieRegressor()\n>>> clf.score(X, y)\n0.839...\n>>> clf.coef_\narray([0.599..., 0.299...])\n>>> clf.intercept_\n1.600...\n>>> clf.predict([[1, 1], [3, 4]])\narray([2.500..., 4.599...])"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._glm.link",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "logit",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseLink",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "derivative",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Usually the (predicted) mean."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the derivative of the link g'(y_pred).\n\nParameters\n----------\ny_pred : array of shape (n_samples,)\n    Usually the (predicted) mean."
            },
            {
              "name": "inverse",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "lin_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Usually the (fitted) linear predictor."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the inverse link function h(lin_pred).\n\nGives the inverse relationship between linear predictor and the mean\ny_pred=E[Y], i.e. h(linear predictor) = y_pred.\n\nParameters\n----------\nlin_pred : array of shape (n_samples,)\n    Usually the (fitted) linear predictor."
            },
            {
              "name": "inverse_derivative",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "lin_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Usually the (fitted) linear predictor."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the derivative of the inverse link function h'(lin_pred).\n\nParameters\n----------\nlin_pred : array of shape (n_samples,)\n    Usually the (fitted) linear predictor."
            }
          ],
          "fullDocstring": "Abstract base class for Link functions."
        },
        {
          "name": "IdentityLink",
          "decorators": [],
          "superclasses": [
            "BaseLink"
          ],
          "methods": [
            {
              "name": "derivative",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "inverse",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "lin_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "inverse_derivative",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "lin_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "The identity link function g(x)=x."
        },
        {
          "name": "LogLink",
          "decorators": [],
          "superclasses": [
            "BaseLink"
          ],
          "methods": [
            {
              "name": "derivative",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "inverse",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "lin_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "inverse_derivative",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "lin_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "The log link function g(x)=log(x)."
        },
        {
          "name": "LogitLink",
          "decorators": [],
          "superclasses": [
            "BaseLink"
          ],
          "methods": [
            {
              "name": "derivative",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "inverse",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "lin_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "inverse_derivative",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "lin_pred",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "The logit link function g(x)=logit(x)."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._huber",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "optimize",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearModel",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "axis0_safe_slice",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.optimize",
          "declaration": "_check_optimize_result",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "HuberRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseEstimator",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target vector relative to X."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weight given to each sample."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model according to the given training data.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples and\n    n_features is the number of features.\n\ny : array-like, shape (n_samples,)\n    Target vector relative to X.\n\nsample_weight : array-like, shape (n_samples,)\n    Weight given to each sample.\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "Linear regression model that is robust to outliers.\n\nThe Huber Regressor optimizes the squared loss for the samples where\n``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\nwhere ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\nto be optimized. The parameter sigma makes sure that if y is scaled up\nor down by a certain factor, one does not need to rescale epsilon to\nachieve the same robustness. Note that this does not take into account\nthe fact that the different features of X may be of different scales.\n\nThis makes sure that the loss function is not heavily influenced by the\noutliers while not completely ignoring their effect.\n\nRead more in the :ref:`User Guide <huber_regression>`\n\n.. versionadded:: 0.18\n\nParameters\n----------\nepsilon : float, greater than 1.0, default=1.35\n    The parameter epsilon controls the number of samples that should be\n    classified as outliers. The smaller the epsilon, the more robust it is\n    to outliers.\n\nmax_iter : int, default=100\n    Maximum number of iterations that\n    ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.\n\nalpha : float, default=0.0001\n    Regularization parameter.\n\nwarm_start : bool, default=False\n    This is useful if the stored attributes of a previously used model\n    has to be reused. If set to False, then the coefficients will\n    be rewritten for every call to fit.\n    See :term:`the Glossary <warm_start>`.\n\nfit_intercept : bool, default=True\n    Whether or not to fit the intercept. This can be set to False\n    if the data is already centered around the origin.\n\ntol : float, default=1e-05\n    The iteration will stop when\n    ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n    where pg_i is the i-th component of the projected gradient.\n\nAttributes\n----------\ncoef_ : array, shape (n_features,)\n    Features got by optimizing the Huber loss.\n\nintercept_ : float\n    Bias.\n\nscale_ : float\n    The value by which ``|y - X'w - c|`` is scaled down.\n\nn_iter_ : int\n    Number of iterations that\n    ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.\n\n    .. versionchanged:: 0.20\n\n        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\noutliers_ : array, shape (n_samples,)\n    A boolean mask which is set to True where the samples are identified\n    as outliers.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import HuberRegressor, LinearRegression\n>>> from sklearn.datasets import make_regression\n>>> rng = np.random.RandomState(0)\n>>> X, y, coef = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n>>> X[:4] = rng.uniform(10, 20, (4, 2))\n>>> y[:4] = rng.uniform(10, 20, 4)\n>>> huber = HuberRegressor().fit(X, y)\n>>> huber.score(X, y)\n-7.284...\n>>> huber.predict(X[:1,])\narray([806.7200...])\n>>> linear = LinearRegression().fit(X, y)\n>>> print(\"True coefficients:\", coef)\nTrue coefficients: [20.4923...  34.1698...]\n>>> print(\"Huber coefficients:\", huber.coef_)\nHuber coefficients: [17.7906... 31.0106...]\n>>> print(\"Linear Regression coefficients:\", linear.coef_)\nLinear Regression coefficients: [-1.9221...  7.0226...]\n\nReferences\n----------\n.. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n       Concomitant scale estimates, pg 172\n.. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n       https://statweb.stanford.edu/~owen/reports/hhu.pdf"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._least_angle",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "sys",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "log",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "interpolate",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy.linalg.lapack",
          "declaration": "get_lapack_funcs",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearModel",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "arrayfuncs",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "as_float_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "Lars",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "MultiOutputMixin",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "Xy",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Xy = np.dot(X.T, y) that can be precomputed. It is useful\nonly when the Gram matrix is precomputed."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model using X, y as training data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values.\n\nXy : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n    Xy = np.dot(X.T, y) that can be precomputed. It is useful\n    only when the Gram matrix is precomputed.\n\nReturns\n-------\nself : object\n    returns an instance of self."
            }
          ],
          "fullDocstring": "Least Angle Regression model a.k.a. LAR\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nnormalize : bool, default=True\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : bool, 'auto' or array-like , default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nn_nonzero_coefs : int, default=500\n    Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nfit_path : bool, default=True\n    If True the full path is stored in the ``coef_path_`` attribute.\n    If you compute the solution for a large problem or many targets,\n    setting ``fit_path`` to ``False`` will lead to a speedup, especially\n    with a small alpha.\n\njitter : float, default=None\n    Upper bound on a uniform noise parameter to be added to the\n    `y` values, to satisfy the model's assumption of\n    one-at-a-time computations. Might help with stability.\n\n    .. versionadded:: 0.23\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for jittering. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nalphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n    Maximum of covariances (in absolute value) at each iteration.\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\n    is smaller. If this is a list of array-like, the length of the outer\n    list is `n_targets`.\n\nactive_ : list of shape (n_alphas,) or list of such lists\n    Indices of active variables at the end of the path.\n    If this is a list of list, the length of the outer list is `n_targets`.\n\ncoef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n    The varying values of the coefficients along the path. It is not\n    present if the ``fit_path`` parameter is ``False``. If this is a list\n    of array-like, the length of the outer list is `n_targets`.\n\ncoef_ : array-like of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the formulation formula).\n\nintercept_ : float or array-like of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : array-like or int\n    The number of iterations taken by lars_path to find the\n    grid of alphas for each target.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> reg = linear_model.Lars(n_nonzero_coefs=1)\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\nLars(n_nonzero_coefs=1)\n>>> print(reg.coef_)\n[ 0. -1.11...]\n\nSee Also\n--------\nlars_path, LarsCV\nsklearn.decomposition.sparse_encode"
        },
        {
          "name": "LarsCV",
          "decorators": [],
          "superclasses": [
            "Lars"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model using X, y as training data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nReturns\n-------\nself : object\n    returns an instance of self."
            }
          ],
          "fullDocstring": "Cross-validated Least Angle Regression model.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform.\n\nnormalize : bool, default=True\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : bool, 'auto' or array-like , default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram matrix\n    cannot be passed as argument since we will use only subsets of X.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nmax_n_alphas : int, default=1000\n    The maximum number of points on the path used to compute the\n    residuals in the cross-validation\n\nn_jobs : int or None, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nAttributes\n----------\nactive_ : list of length n_alphas or list of such lists\n    Indices of active variables at the end of the path.\n    If this is a list of lists, the outer list length is `n_targets`.\n\ncoef_ : array-like of shape (n_features,)\n    parameter vector (w in the formulation formula)\n\nintercept_ : float\n    independent term in decision function\n\ncoef_path_ : array-like of shape (n_features, n_alphas)\n    the varying values of the coefficients along the path\n\nalpha_ : float\n    the estimated regularization parameter alpha\n\nalphas_ : array-like of shape (n_alphas,)\n    the different values of alpha along the path\n\ncv_alphas_ : array-like of shape (n_cv_alphas,)\n    all the values of alpha along the path for the different folds\n\nmse_path_ : array-like of shape (n_folds, n_cv_alphas)\n    the mean square error on left-out for each fold along the path\n    (alpha values given by ``cv_alphas``)\n\nn_iter_ : array-like or int\n    the number of iterations run by Lars with the optimal alpha.\n\nExamples\n--------\n>>> from sklearn.linear_model import LarsCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n>>> reg = LarsCV(cv=5).fit(X, y)\n>>> reg.score(X, y)\n0.9996...\n>>> reg.alpha_\n0.0254...\n>>> reg.predict(X[:1,])\narray([154.0842...])\n\nSee Also\n--------\nlars_path, LassoLars, LassoLarsCV"
        },
        {
          "name": "LassoLars",
          "decorators": [],
          "superclasses": [
            "Lars"
          ],
          "methods": [],
          "fullDocstring": "Lasso model fit with Least Angle Regression a.k.a. Lars\n\nIt is a Linear Model trained with an L1 prior as regularizer.\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the penalty term. Defaults to 1.0.\n    ``alpha = 0`` is equivalent to an ordinary least square, solved\n    by :class:`LinearRegression`. For numerical reasons, using\n    ``alpha = 0`` with the LassoLars object is not advised and you\n    should prefer the LinearRegression object.\n\nfit_intercept : bool, default=True\n    whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nnormalize : bool, default=True\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : bool, 'auto' or array-like, default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nfit_path : bool, default=True\n    If ``True`` the full path is stored in the ``coef_path_`` attribute.\n    If you compute the solution for a large problem or many targets,\n    setting ``fit_path`` to ``False`` will lead to a speedup, especially\n    with a small alpha.\n\npositive : bool, default=False\n    Restrict coefficients to be >= 0. Be aware that you might want to\n    remove fit_intercept which is set True by default.\n    Under the positive restriction the model coefficients will not converge\n    to the ordinary-least-squares solution for small values of alpha.\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n    algorithm are typically in congruence with the solution of the\n    coordinate descent Lasso estimator.\n\njitter : float, default=None\n    Upper bound on a uniform noise parameter to be added to the\n    `y` values, to satisfy the model's assumption of\n    one-at-a-time computations. Might help with stability.\n\n    .. versionadded:: 0.23\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for jittering. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nalphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n    Maximum of covariances (in absolute value) at each iteration.\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\n    is smaller. If this is a list of array-like, the length of the outer\n    list is `n_targets`.\n\nactive_ : list of length n_alphas or list of such lists\n    Indices of active variables at the end of the path.\n    If this is a list of list, the length of the outer list is `n_targets`.\n\ncoef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n    If a list is passed it's expected to be one of n_targets such arrays.\n    The varying values of the coefficients along the path. It is not\n    present if the ``fit_path`` parameter is ``False``. If this is a list\n    of array-like, the length of the outer list is `n_targets`.\n\ncoef_ : array-like of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the formulation formula).\n\nintercept_ : float or array-like of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : array-like or int\n    The number of iterations taken by lars_path to find the\n    grid of alphas for each target.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> reg = linear_model.LassoLars(alpha=0.01)\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\nLassoLars(alpha=0.01)\n>>> print(reg.coef_)\n[ 0.         -0.963257...]\n\nSee Also\n--------\nlars_path\nlasso_path\nLasso\nLassoCV\nLassoLarsCV\nLassoLarsIC\nsklearn.decomposition.sparse_encode"
        },
        {
          "name": "LassoLarsCV",
          "decorators": [],
          "superclasses": [
            "LarsCV"
          ],
          "methods": [],
          "fullDocstring": "Cross-validated Lasso, using the LARS algorithm.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform.\n\nnormalize : bool, default=True\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : bool or 'auto' , default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram matrix\n    cannot be passed as argument since we will use only subsets of X.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nmax_n_alphas : int, default=1000\n    The maximum number of points on the path used to compute the\n    residuals in the cross-validation\n\nn_jobs : int or None, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\npositive : bool, default=False\n    Restrict coefficients to be >= 0. Be aware that you might want to\n    remove fit_intercept which is set True by default.\n    Under the positive restriction the model coefficients do not converge\n    to the ordinary-least-squares solution for small values of alpha.\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n    algorithm are typically in congruence with the solution of the\n    coordinate descent Lasso estimator.\n    As a consequence using LassoLarsCV only makes sense for problems where\n    a sparse solution is expected and/or reached.\n\nAttributes\n----------\ncoef_ : array-like of shape (n_features,)\n    parameter vector (w in the formulation formula)\n\nintercept_ : float\n    independent term in decision function.\n\ncoef_path_ : array-like of shape (n_features, n_alphas)\n    the varying values of the coefficients along the path\n\nalpha_ : float\n    the estimated regularization parameter alpha\n\nalphas_ : array-like of shape (n_alphas,)\n    the different values of alpha along the path\n\ncv_alphas_ : array-like of shape (n_cv_alphas,)\n    all the values of alpha along the path for the different folds\n\nmse_path_ : array-like of shape (n_folds, n_cv_alphas)\n    the mean square error on left-out for each fold along the path\n    (alpha values given by ``cv_alphas``)\n\nn_iter_ : array-like or int\n    the number of iterations run by Lars with the optimal alpha.\n\nactive_ : list of int\n    Indices of active variables at the end of the path.\n\nExamples\n--------\n>>> from sklearn.linear_model import LassoLarsCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(noise=4.0, random_state=0)\n>>> reg = LassoLarsCV(cv=5).fit(X, y)\n>>> reg.score(X, y)\n0.9992...\n>>> reg.alpha_\n0.0484...\n>>> reg.predict(X[:1,])\narray([-77.8723...])\n\nNotes\n-----\n\nThe object solves the same problem as the LassoCV object. However,\nunlike the LassoCV, it find the relevant alphas values by itself.\nIn general, because of this property, it will be more stable.\nHowever, it is more fragile to heavily multicollinear datasets.\n\nIt is more efficient than the LassoCV if only a small number of\nfeatures are selected compared to the total number, for instance if\nthere are very few samples compared to the number of features.\n\nSee Also\n--------\nlars_path, LassoLars, LarsCV, LassoCV"
        },
        {
          "name": "LassoLarsIC",
          "decorators": [],
          "superclasses": [
            "LassoLars"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "target values. Will be cast to X's dtype if necessary"
                },
                {
                  "name": "copy_X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "If provided, this parameter will override the choice\nof copy_X made at instance creation.\nIf ``True``, X will be copied; else, it may be overwritten."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model using X, y as training data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    training data.\n\ny : array-like of shape (n_samples,)\n    target values. Will be cast to X's dtype if necessary\n\ncopy_X : bool, default=None\n    If provided, this parameter will override the choice\n    of copy_X made at instance creation.\n    If ``True``, X will be copied; else, it may be overwritten.\n\nReturns\n-------\nself : object\n    returns an instance of self."
            }
          ],
          "fullDocstring": "Lasso model fit with Lars using BIC or AIC for model selection\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nAIC is the Akaike information criterion and BIC is the Bayes\nInformation criterion. Such criteria are useful to select the value\nof the regularization parameter by making a trade-off between the\ngoodness of fit and the complexity of the model. A good model should\nexplain well the data while being simple.\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\ncriterion : {'bic' , 'aic'}, default='aic'\n    The type of criterion to use.\n\nfit_intercept : bool, default=True\n    whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nnormalize : bool, default=True\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : bool, 'auto' or array-like, default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform. Can be used for\n    early stopping.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\npositive : bool, default=False\n    Restrict coefficients to be >= 0. Be aware that you might want to\n    remove fit_intercept which is set True by default.\n    Under the positive restriction the model coefficients do not converge\n    to the ordinary-least-squares solution for small values of alpha.\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n    algorithm are typically in congruence with the solution of the\n    coordinate descent Lasso estimator.\n    As a consequence using LassoLarsIC only makes sense for problems where\n    a sparse solution is expected and/or reached.\n\nAttributes\n----------\ncoef_ : array-like of shape (n_features,)\n    parameter vector (w in the formulation formula)\n\nintercept_ : float\n    independent term in decision function.\n\nalpha_ : float\n    the alpha parameter chosen by the information criterion\n\nalphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n    Maximum of covariances (in absolute value) at each iteration.\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\n    is smaller. If a list, it will be of length `n_targets`.\n\nn_iter_ : int\n    number of iterations run by lars_path to find the grid of\n    alphas.\n\ncriterion_ : array-like of shape (n_alphas,)\n    The value of the information criteria ('aic', 'bic') across all\n    alphas. The alpha which has the smallest information criterion is\n    chosen. This value is larger by a factor of ``n_samples`` compared to\n    Eqns. 2.15 and 2.16 in (Zou et al, 2007).\n\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> reg = linear_model.LassoLarsIC(criterion='bic')\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\nLassoLarsIC(criterion='bic')\n>>> print(reg.coef_)\n[ 0.  -1.11...]\n\nNotes\n-----\nThe estimation of the number of degrees of freedom is given by:\n\n\"On the degrees of freedom of the lasso\"\nHui Zou, Trevor Hastie, and Robert Tibshirani\nAnn. Statist. Volume 35, Number 5 (2007), 2173-2192.\n\nhttps://en.wikipedia.org/wiki/Akaike_information_criterion\nhttps://en.wikipedia.org/wiki/Bayesian_information_criterion\n\nSee Also\n--------\nlars_path, LassoLars, LassoLarsCV"
        }
      ],
      "functions": [
        {
          "name": "lars_path",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data. Note that if X is None then the Gram matrix must be\nspecified, i.e., cannot be None or False."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input targets."
            },
            {
              "name": "Xy",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Xy = np.dot(X.T, y) that can be precomputed. It is useful\nonly when the Gram matrix is precomputed."
            }
          ],
          "results": [
            {
              "name": "alphas",
              "type": null,
              "description": "Maximum of covariances (in absolute value) at each iteration.\n``n_alphas`` is either ``max_iter``, ``n_features`` or the\nnumber of nodes in the path with ``alpha >= alpha_min``, whichever\nis smaller."
            },
            {
              "name": "active",
              "type": null,
              "description": "Indices of active variables at the end of the path."
            },
            {
              "name": "coefs",
              "type": null,
              "description": "Coefficients along the path"
            },
            {
              "name": "n_iter",
              "type": "int",
              "description": "Number of iterations run. Returned only if return_n_iter is set\nto True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n\nThe optimization objective for the case method='lasso' is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nin the case of method='lars', the objective function is only known in\nthe form of an implicit equation (see discussion in [1])\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nX : None or array-like of shape (n_samples, n_features)\n    Input data. Note that if X is None then the Gram matrix must be\n    specified, i.e., cannot be None or False.\n\ny : None or array-like of shape (n_samples,)\n    Input targets.\n\nXy : array-like of shape (n_samples,) or (n_samples, n_targets),             default=None\n    Xy = np.dot(X.T, y) that can be precomputed. It is useful\n    only when the Gram matrix is precomputed.\n\nGram : None, 'auto', array-like of shape (n_features, n_features),             default=None\n    Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n    matrix is precomputed from the given X, if there are more samples\n    than features.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform, set to infinity for no limit.\n\nalpha_min : float, default=0\n    Minimum correlation along the path. It corresponds to the\n    regularization parameter alpha parameter in the Lasso.\n\nmethod : {'lar', 'lasso'}, default='lar'\n    Specifies the returned model. Select ``'lar'`` for Least Angle\n    Regression, ``'lasso'`` for the Lasso.\n\ncopy_X : bool, default=True\n    If ``False``, ``X`` is overwritten.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_Gram : bool, default=True\n    If ``False``, ``Gram`` is overwritten.\n\nverbose : int, default=0\n    Controls output verbosity.\n\nreturn_path : bool, default=True\n    If ``return_path==True`` returns the entire path, else returns only the\n    last point of the path.\n\nreturn_n_iter : bool, default=False\n    Whether to return the number of iterations.\n\npositive : bool, default=False\n    Restrict coefficients to be >= 0.\n    This option is only allowed with method 'lasso'. Note that the model\n    coefficients will not converge to the ordinary-least-squares solution\n    for small values of alpha. Only coefficients up to the smallest alpha\n    value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n    the stepwise Lars-Lasso algorithm are typically in congruence with the\n    solution of the coordinate descent lasso_path function.\n\nReturns\n-------\nalphas : array-like of shape (n_alphas + 1,)\n    Maximum of covariances (in absolute value) at each iteration.\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\n    is smaller.\n\nactive : array-like of shape (n_alphas,)\n    Indices of active variables at the end of the path.\n\ncoefs : array-like of shape (n_features, n_alphas + 1)\n    Coefficients along the path\n\nn_iter : int\n    Number of iterations run. Returned only if return_n_iter is set\n    to True.\n\nSee Also\n--------\nlars_path_gram\nlasso_path\nlasso_path_gram\nLassoLars\nLars\nLassoLarsCV\nLarsCV\nsklearn.decomposition.sparse_encode\n\nReferences\n----------\n.. [1] \"Least Angle Regression\", Efron et al.\n       http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n.. [2] `Wikipedia entry on the Least-angle regression\n       <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n.. [3] `Wikipedia entry on the Lasso\n       <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_"
        },
        {
          "name": "lars_path_gram",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "Xy",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Xy = np.dot(X.T, y)."
            },
            {
              "name": "Gram",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Gram = np.dot(X.T * X)."
            }
          ],
          "results": [
            {
              "name": "alphas",
              "type": null,
              "description": "Maximum of covariances (in absolute value) at each iteration.\n``n_alphas`` is either ``max_iter``, ``n_features`` or the\nnumber of nodes in the path with ``alpha >= alpha_min``, whichever\nis smaller."
            },
            {
              "name": "active",
              "type": null,
              "description": "Indices of active variables at the end of the path."
            },
            {
              "name": "coefs",
              "type": null,
              "description": "Coefficients along the path"
            },
            {
              "name": "n_iter",
              "type": "int",
              "description": "Number of iterations run. Returned only if return_n_iter is set\nto True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "lars_path in the sufficient stats mode [1]\n\nThe optimization objective for the case method='lasso' is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nin the case of method='lars', the objective function is only known in\nthe form of an implicit equation (see discussion in [1])\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nXy : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Xy = np.dot(X.T, y).\n\nGram : array-like of shape (n_features, n_features)\n    Gram = np.dot(X.T * X).\n\nn_samples : int or float\n    Equivalent size of sample.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform, set to infinity for no limit.\n\nalpha_min : float, default=0\n    Minimum correlation along the path. It corresponds to the\n    regularization parameter alpha parameter in the Lasso.\n\nmethod : {'lar', 'lasso'}, default='lar'\n    Specifies the returned model. Select ``'lar'`` for Least Angle\n    Regression, ``'lasso'`` for the Lasso.\n\ncopy_X : bool, default=True\n    If ``False``, ``X`` is overwritten.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_Gram : bool, default=True\n    If ``False``, ``Gram`` is overwritten.\n\nverbose : int, default=0\n    Controls output verbosity.\n\nreturn_path : bool, default=True\n    If ``return_path==True`` returns the entire path, else returns only the\n    last point of the path.\n\nreturn_n_iter : bool, default=False\n    Whether to return the number of iterations.\n\npositive : bool, default=False\n    Restrict coefficients to be >= 0.\n    This option is only allowed with method 'lasso'. Note that the model\n    coefficients will not converge to the ordinary-least-squares solution\n    for small values of alpha. Only coefficients up to the smallest alpha\n    value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n    the stepwise Lars-Lasso algorithm are typically in congruence with the\n    solution of the coordinate descent lasso_path function.\n\nReturns\n-------\nalphas : array-like of shape (n_alphas + 1,)\n    Maximum of covariances (in absolute value) at each iteration.\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\n    is smaller.\n\nactive : array-like of shape (n_alphas,)\n    Indices of active variables at the end of the path.\n\ncoefs : array-like of shape (n_features, n_alphas + 1)\n    Coefficients along the path\n\nn_iter : int\n    Number of iterations run. Returned only if return_n_iter is set\n    to True.\n\nSee Also\n--------\nlars_path\nlasso_path\nlasso_path_gram\nLassoLars\nLars\nLassoLarsCV\nLarsCV\nsklearn.decomposition.sparse_encode\n\nReferences\n----------\n.. [1] \"Least Angle Regression\", Efron et al.\n       http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n.. [2] `Wikipedia entry on the Least-angle regression\n       <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n.. [3] `Wikipedia entry on the Lasso\n       <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_"
        }
      ]
    },
    {
      "name": "sklearn.linear_model._logistic",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "optimize",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "logsumexp",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "SparseCoefMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sag",
          "declaration": "sag_solver",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "get_scorer",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.svm._base",
          "declaration": "_fit_liblinear",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "compute_class_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "log_logistic",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "softmax",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "squared_norm",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_joblib_parallel_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.optimize",
          "declaration": "_check_optimize_result",
          "alias": null
        },
        {
          "module": "sklearn.utils.optimize",
          "declaration": "_newton_cg",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "LogisticRegression",
          "decorators": [],
          "superclasses": [
            "LinearClassifierMixin",
            "SparseCoefMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target vector relative to X."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of weights that are assigned to individual samples.\nIf not provided, then each sample is given unit weight.\n\n.. versionadded:: 0.17\n   *sample_weight* support to LogisticRegression."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model according to the given training data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target vector relative to X.\n\nsample_weight : array-like of shape (n_samples,) default=None\n    Array of weights that are assigned to individual samples.\n    If not provided, then each sample is given unit weight.\n\n    .. versionadded:: 0.17\n       *sample_weight* support to LogisticRegression.\n\nReturns\n-------\nself\n    Fitted estimator.\n\nNotes\n-----\nThe SAGA solver supports both float64 and float32 bit arrays."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Vector to be scored, where `n_samples` is the number of samples and\n`n_features` is the number of features."
                }
              ],
              "results": [
                {
                  "name": "T",
                  "type": null,
                  "description": "Returns the probability of the sample for each class in the model,\nwhere classes are ordered as they are in ``self.classes_``."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Probability estimates.\n\nThe returned estimates for all classes are ordered by the\nlabel of classes.\n\nFor a multi_class problem, if multi_class is set to be \"multinomial\"\nthe softmax function is used to find the predicted probability of\neach class.\nElse use a one-vs-rest approach, i.e calculate the probability\nof each class assuming it to be positive using the logistic function.\nand normalize these values across all the classes.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Vector to be scored, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n\nReturns\n-------\nT : array-like of shape (n_samples, n_classes)\n    Returns the probability of the sample for each class in the model,\n    where classes are ordered as they are in ``self.classes_``."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Vector to be scored, where `n_samples` is the number of samples and\n`n_features` is the number of features."
                }
              ],
              "results": [
                {
                  "name": "T",
                  "type": null,
                  "description": "Returns the log-probability of the sample for each class in the\nmodel, where classes are ordered as they are in ``self.classes_``."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict logarithm of probability estimates.\n\nThe returned estimates for all classes are ordered by the\nlabel of classes.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Vector to be scored, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n\nReturns\n-------\nT : array-like of shape (n_samples, n_classes)\n    Returns the log-probability of the sample for each class in the\n    model, where classes are ordered as they are in ``self.classes_``."
            }
          ],
          "fullDocstring": "Logistic Regression (aka logit, MaxEnt) classifier.\n\nIn the multiclass case, the training algorithm uses the one-vs-rest (OvR)\nscheme if the 'multi_class' option is set to 'ovr', and uses the\ncross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n(Currently the 'multinomial' option is supported only by the 'lbfgs',\n'sag', 'saga' and 'newton-cg' solvers.)\n\nThis class implements regularized logistic regression using the\n'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\nthat regularization is applied by default**. It can handle both dense\nand sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\nfloats for optimal performance; any other input format will be converted\n(and copied).\n\nThe 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\nwith primal formulation, or no regularization. The 'liblinear' solver\nsupports both L1 and L2 regularization, with a dual formulation only for\nthe L2 penalty. The Elastic-Net regularization is only supported by the\n'saga' solver.\n\nRead more in the :ref:`User Guide <logistic_regression>`.\n\nParameters\n----------\npenalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n\ndual : bool, default=False\n    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n\ntol : float, default=1e-4\n    Tolerance for stopping criteria.\n\nC : float, default=1.0\n    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n\nintercept_scaling : float, default=1\n    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n\nclass_weight : dict or 'balanced', default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n\nrandom_state : int, RandomState instance, default=None\n    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n\nsolver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n\n    Algorithm to use in the optimization problem.\n\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n\nmax_iter : int, default=100\n    Maximum number of iterations taken for the solvers to converge.\n\nmulti_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n\nverbose : int, default=0\n    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n\nn_jobs : int, default=None\n    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n\nl1_ratio : float, default=None\n    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n\nAttributes\n----------\n\nclasses_ : ndarray of shape (n_classes, )\n    A list of class labels known to the classifier.\n\ncoef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n    Coefficient of the features in the decision function.\n\n    `coef_` is of shape (1, n_features) when the given problem is binary.\n    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n\nintercept_ : ndarray of shape (1,) or (n_classes,)\n    Intercept (a.k.a. bias) added to the decision function.\n\n    If `fit_intercept` is set to False, the intercept is set to zero.\n    `intercept_` is of shape (1,) when the given problem is binary.\n    In particular, when `multi_class='multinomial'`, `intercept_`\n    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n    outcome 0 (False).\n\nn_iter_ : ndarray of shape (n_classes,) or (1, )\n    Actual number of iterations for all classes. If binary or multinomial,\n    it returns only 1 element. For liblinear solver, only the maximum\n    number of iteration across all classes is given.\n\n    .. versionchanged:: 0.20\n\n        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\nSee Also\n--------\nSGDClassifier : Incrementally trained logistic regression (when given\n    the parameter ``loss=\"log\"``).\nLogisticRegressionCV : Logistic regression with built-in cross validation.\n\nNotes\n-----\nThe underlying C implementation uses a random number generator to\nselect features when fitting the model. It is thus not uncommon,\nto have slightly different results for the same input data. If\nthat happens, try with a smaller tol parameter.\n\nPredict output may not match that of standalone liblinear in certain\ncases. See :ref:`differences from liblinear <liblinear_differences>`\nin the narrative documentation.\n\nReferences\n----------\n\nL-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n\nLIBLINEAR -- A Library for Large Linear Classification\n    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\nSAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n    Minimizing Finite Sums with the Stochastic Average Gradient\n    https://hal.inria.fr/hal-00860051/document\n\nSAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n    SAGA: A Fast Incremental Gradient Method With Support\n    for Non-Strongly Convex Composite Objectives\n    https://arxiv.org/abs/1407.0202\n\nHsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n    methods for logistic regression and maximum entropy models.\n    Machine Learning 85(1-2):41-75.\n    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegression\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = LogisticRegression(random_state=0).fit(X, y)\n>>> clf.predict(X[:2, :])\narray([0, 0])\n>>> clf.predict_proba(X[:2, :])\narray([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n       [9.7...e-01, 2.8...e-02, ...e-08]])\n>>> clf.score(X, y)\n0.97..."
        },
        {
          "name": "LogisticRegressionCV",
          "decorators": [],
          "superclasses": [
            "LinearClassifierMixin",
            "LogisticRegression",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target vector relative to X."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of weights that are assigned to individual samples.\nIf not provided, then each sample is given unit weight."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model according to the given training data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target vector relative to X.\n\nsample_weight : array-like of shape (n_samples,) default=None\n    Array of weights that are assigned to individual samples.\n    If not provided, then each sample is given unit weight.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True labels for X."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "Score of self.predict(X) wrt. y."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the score using the `scoring` option on the given\ntest data and labels.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples.\n\ny : array-like of shape (n_samples,)\n    True labels for X.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    Score of self.predict(X) wrt. y."
            }
          ],
          "fullDocstring": "Logistic Regression CV (aka logit, MaxEnt) classifier.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThis class implements logistic regression using liblinear, newton-cg, sag\nof lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\nregularization with primal formulation. The liblinear solver supports both\nL1 and L2 regularization, with a dual formulation only for the L2 penalty.\nElastic-Net penalty is only supported by the saga solver.\n\nFor the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\nis selected by the cross-validator\n:class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\nusing the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\nsolvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n\nRead more in the :ref:`User Guide <logistic_regression>`.\n\nParameters\n----------\nCs : int or list of floats, default=10\n    Each of the values in Cs describes the inverse of regularization\n    strength. If Cs is as an int, then a grid of Cs values are chosen\n    in a logarithmic scale between 1e-4 and 1e4.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n\ncv : int or cross-validation generator, default=None\n    The default cross-validation generator used is Stratified K-Folds.\n    If an integer is provided, then it is the number of folds used.\n    See the module :mod:`sklearn.model_selection` module for the\n    list of possible cross-validation objects.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\ndual : bool, default=False\n    Dual or primal formulation. Dual formulation is only implemented for\n    l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n\npenalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver.\n\nscoring : str or callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``. For a list of scoring functions\n    that can be used, look at :mod:`sklearn.metrics`. The\n    default scoring option used is 'accuracy'.\n\nsolver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n\n    Algorithm to use in the optimization problem.\n\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n      'liblinear' and 'saga' handle L1 penalty.\n    - 'liblinear' might be slower in LogisticRegressionCV because it does\n      not handle warm-starting.\n\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can preprocess the data\n    with a scaler from sklearn.preprocessing.\n\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n\ntol : float, default=1e-4\n    Tolerance for stopping criteria.\n\nmax_iter : int, default=100\n    Maximum number of iterations of the optimization algorithm.\n\nclass_weight : dict or 'balanced', default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\n    .. versionadded:: 0.17\n       class_weight == 'balanced'\n\nn_jobs : int, default=None\n    Number of CPU cores used during the cross-validation loop.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int, default=0\n    For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n    positive number for verbosity.\n\nrefit : bool, default=True\n    If set to True, the scores are averaged across all folds, and the\n    coefs and the C that corresponds to the best score is taken, and a\n    final refit is done using these parameters.\n    Otherwise the coefs, intercepts and C that correspond to the\n    best scores across folds are averaged.\n\nintercept_scaling : float, default=1\n    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n\nmulti_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n\nrandom_state : int, RandomState instance, default=None\n    Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n    Note that this only applies to the solver and not the cross-validation\n    generator. See :term:`Glossary <random_state>` for details.\n\nl1_ratios : list of float, default=None\n    The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n    Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n    using ``penalty='l2'``, while 1 is equivalent to using\n    ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n    of L1 and L2.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes, )\n    A list of class labels known to the classifier.\n\ncoef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n    Coefficient of the features in the decision function.\n\n    `coef_` is of shape (1, n_features) when the given problem\n    is binary.\n\nintercept_ : ndarray of shape (1,) or (n_classes,)\n    Intercept (a.k.a. bias) added to the decision function.\n\n    If `fit_intercept` is set to False, the intercept is set to zero.\n    `intercept_` is of shape(1,) when the problem is binary.\n\nCs_ : ndarray of shape (n_cs)\n    Array of C i.e. inverse of regularization parameter values used\n    for cross-validation.\n\nl1_ratios_ : ndarray of shape (n_l1_ratios)\n    Array of l1_ratios used for cross-validation. If no l1_ratio is used\n    (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n\ncoefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n    dict with classes as the keys, and the path of coefficients obtained\n    during cross-validating across each fold and then across each Cs\n    after doing an OvR for the corresponding class as values.\n    If the 'multi_class' option is set to 'multinomial', then\n    the coefs_paths are the coefficients corresponding to each class.\n    Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n    ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n    intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n    ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n    ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n\nscores_ : dict\n    dict with classes as the keys, and the values as the\n    grid of scores obtained during cross-validating each fold, after doing\n    an OvR for the corresponding class. If the 'multi_class' option\n    given is 'multinomial' then the same scores are repeated across\n    all classes, since this is the multinomial class. Each dict value\n    has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n    ``penalty='elasticnet'``.\n\nC_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n    Array of C that maps to the best scores across every class. If refit is\n    set to False, then for each class, the best C is the average of the\n    C's that correspond to the best scores for each fold.\n    `C_` is of shape(n_classes,) when the problem is binary.\n\nl1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n    Array of l1_ratio that maps to the best scores across every class. If\n    refit is set to False, then for each class, the best l1_ratio is the\n    average of the l1_ratio's that correspond to the best scores for each\n    fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n\nn_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n    Actual number of iterations for all classes, folds and Cs.\n    In the binary or multinomial cases, the first dimension is equal to 1.\n    If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n    n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegressionCV\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n>>> clf.predict(X[:2, :])\narray([0, 0])\n>>> clf.predict_proba(X[:2, :]).shape\n(2, 3)\n>>> clf.score(X, y)\n0.98...\n\nSee Also\n--------\nLogisticRegression"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._omp",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "sqrt",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy.linalg.lapack",
          "declaration": "get_lapack_funcs",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearModel",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "_pre_fit",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "as_float_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "OrthogonalMatchingPursuit",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "MultiOutputMixin",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values. Will be cast to X's dtype if necessary"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model using X, y as training data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values. Will be cast to X's dtype if necessary\n\n\nReturns\n-------\nself : object\n    returns an instance of self."
            }
          ],
          "fullDocstring": "Orthogonal Matching Pursuit model (OMP).\n\nRead more in the :ref:`User Guide <omp>`.\n\nParameters\n----------\nn_nonzero_coefs : int, default=None\n    Desired number of non-zero entries in the solution. If None (by\n    default) this value is set to 10% of n_features.\n\ntol : float, default=None\n    Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n\nfit_intercept : bool, default=True\n    whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=True\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nprecompute : 'auto' or bool, default='auto'\n    Whether to use a precomputed Gram and Xy matrix to speed up\n    calculations. Improves performance when :term:`n_targets` or\n    :term:`n_samples` is very large. Note that if you already have such\n    matrices, you can pass them directly to the fit method.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the formula).\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : int or array-like\n    Number of active features across every target.\n\nn_nonzero_coefs_ : int\n    The number of non-zero coefficients in the solution. If\n    `n_nonzero_coefs` is None and `tol` is None this value is either set\n    to 10% of `n_features` or 1, whichever is greater.\n\nExamples\n--------\n>>> from sklearn.linear_model import OrthogonalMatchingPursuit\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(noise=4, random_state=0)\n>>> reg = OrthogonalMatchingPursuit().fit(X, y)\n>>> reg.score(X, y)\n0.9991...\n>>> reg.predict(X[:1,])\narray([-78.3854...])\n\nNotes\n-----\nOrthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\nMatching pursuits with time-frequency dictionaries, IEEE Transactions on\nSignal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n(http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n\nThis implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\nM., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\nMatching Pursuit Technical Report - CS Technion, April 2008.\nhttps://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\nSee Also\n--------\northogonal_mp\northogonal_mp_gram\nlars_path\nLars\nLassoLars\nsklearn.decomposition.sparse_encode\nOrthogonalMatchingPursuitCV"
        },
        {
          "name": "OrthogonalMatchingPursuitCV",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values. Will be cast to X's dtype if necessary."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model using X, y as training data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,)\n    Target values. Will be cast to X's dtype if necessary.\n\nReturns\n-------\nself : object\n    returns an instance of self."
            }
          ],
          "fullDocstring": "Cross-validated Orthogonal Matching Pursuit model (OMP).\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <omp>`.\n\nParameters\n----------\ncopy : bool, default=True\n    Whether the design matrix X must be copied by the algorithm. A false\n    value is only helpful if X is already Fortran-ordered, otherwise a\n    copy is made anyway.\n\nfit_intercept : bool, default=True\n    whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=True\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nmax_iter : int, default=None\n    Maximum numbers of iterations to perform, therefore maximum features\n    to include. 10% of ``n_features`` but at least 5 if available.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nAttributes\n----------\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the problem formulation).\n\nn_nonzero_coefs_ : int\n    Estimated number of non-zero coefficients giving the best mean squared\n    error over the cross-validation folds.\n\nn_iter_ : int or array-like\n    Number of active features across every target for the model refit with\n    the best hyperparameters got by cross-validating across all folds.\n\nExamples\n--------\n>>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=100, n_informative=10,\n...                        noise=4, random_state=0)\n>>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\n>>> reg.score(X, y)\n0.9991...\n>>> reg.n_nonzero_coefs_\n10\n>>> reg.predict(X[:1,])\narray([-78.3854...])\n\nSee Also\n--------\northogonal_mp\northogonal_mp_gram\nlars_path\nLars\nLassoLars\nOrthogonalMatchingPursuit\nLarsCV\nLassoLarsCV\nsklearn.decomposition.sparse_encode"
        }
      ],
      "functions": [
        {
          "name": "orthogonal_mp",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data. Columns are assumed to have unit norm."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input targets."
            }
          ],
          "results": [
            {
              "name": "coef",
              "type": null,
              "description": "Coefficients of the OMP solution. If `return_path=True`, this contains\nthe whole coefficient path. In this case its shape is\n(n_features, n_features) or (n_features, n_targets, n_features) and\niterating over the last axis yields coefficients in increasing order\nof active features."
            },
            {
              "name": "n_iters",
              "type": null,
              "description": "Number of active features across every target. Returned only if\n`return_n_iter` is set to True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Orthogonal Matching Pursuit (OMP).\n\nSolves n_targets Orthogonal Matching Pursuit problems.\nAn instance of the problem has the form:\n\nWhen parametrized by the number of non-zero coefficients using\n`n_nonzero_coefs`:\nargmin ||y - X\\gamma||^2 subject to ||\\gamma||_0 <= n_{nonzero coefs}\n\nWhen parametrized by error using the parameter `tol`:\nargmin ||\\gamma||_0 subject to ||y - X\\gamma||^2 <= tol\n\nRead more in the :ref:`User Guide <omp>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Input data. Columns are assumed to have unit norm.\n\ny : ndarray of shape (n_samples,) or (n_samples, n_targets)\n    Input targets.\n\nn_nonzero_coefs : int, default=None\n    Desired number of non-zero entries in the solution. If None (by\n    default) this value is set to 10% of n_features.\n\ntol : float, default=None\n    Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n\nprecompute : 'auto' or bool, default=False\n    Whether to perform precomputations. Improves performance when n_targets\n    or n_samples is very large.\n\ncopy_X : bool, default=True\n    Whether the design matrix X must be copied by the algorithm. A false\n    value is only helpful if X is already Fortran-ordered, otherwise a\n    copy is made anyway.\n\nreturn_path : bool, default=False\n    Whether to return every value of the nonzero coefficients along the\n    forward path. Useful for cross-validation.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\ncoef : ndarray of shape (n_features,) or (n_features, n_targets)\n    Coefficients of the OMP solution. If `return_path=True`, this contains\n    the whole coefficient path. In this case its shape is\n    (n_features, n_features) or (n_features, n_targets, n_features) and\n    iterating over the last axis yields coefficients in increasing order\n    of active features.\n\nn_iters : array-like or int\n    Number of active features across every target. Returned only if\n    `return_n_iter` is set to True.\n\nSee Also\n--------\nOrthogonalMatchingPursuit\northogonal_mp_gram\nlars_path\nsklearn.decomposition.sparse_encode\n\nNotes\n-----\nOrthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\nMatching pursuits with time-frequency dictionaries, IEEE Transactions on\nSignal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n(http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n\nThis implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\nM., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\nMatching Pursuit Technical Report - CS Technion, April 2008.\nhttps://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf"
        },
        {
          "name": "orthogonal_mp_gram",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "Gram",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Gram matrix of the input data: X.T * X."
            },
            {
              "name": "Xy",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input targets multiplied by X: X.T * y."
            }
          ],
          "results": [
            {
              "name": "coef",
              "type": null,
              "description": "Coefficients of the OMP solution. If `return_path=True`, this contains\nthe whole coefficient path. In this case its shape is\n(n_features, n_features) or (n_features, n_targets, n_features) and\niterating over the last axis yields coefficients in increasing order\nof active features."
            },
            {
              "name": "n_iters",
              "type": null,
              "description": "Number of active features across every target. Returned only if\n`return_n_iter` is set to True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Gram Orthogonal Matching Pursuit (OMP).\n\nSolves n_targets Orthogonal Matching Pursuit problems using only\nthe Gram matrix X.T * X and the product X.T * y.\n\nRead more in the :ref:`User Guide <omp>`.\n\nParameters\n----------\nGram : ndarray of shape (n_features, n_features)\n    Gram matrix of the input data: X.T * X.\n\nXy : ndarray of shape (n_features,) or (n_features, n_targets)\n    Input targets multiplied by X: X.T * y.\n\nn_nonzero_coefs : int, default=None\n    Desired number of non-zero entries in the solution. If None (by\n    default) this value is set to 10% of n_features.\n\ntol : float, default=None\n    Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n\nnorms_squared : array-like of shape (n_targets,), default=None\n    Squared L2 norms of the lines of y. Required if tol is not None.\n\ncopy_Gram : bool, default=True\n    Whether the gram matrix must be copied by the algorithm. A false\n    value is only helpful if it is already Fortran-ordered, otherwise a\n    copy is made anyway.\n\ncopy_Xy : bool, default=True\n    Whether the covariance vector Xy must be copied by the algorithm.\n    If False, it may be overwritten.\n\nreturn_path : bool, default=False\n    Whether to return every value of the nonzero coefficients along the\n    forward path. Useful for cross-validation.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\ncoef : ndarray of shape (n_features,) or (n_features, n_targets)\n    Coefficients of the OMP solution. If `return_path=True`, this contains\n    the whole coefficient path. In this case its shape is\n    (n_features, n_features) or (n_features, n_targets, n_features) and\n    iterating over the last axis yields coefficients in increasing order\n    of active features.\n\nn_iters : array-like or int\n    Number of active features across every target. Returned only if\n    `return_n_iter` is set to True.\n\nSee Also\n--------\nOrthogonalMatchingPursuit\northogonal_mp\nlars_path\nsklearn.decomposition.sparse_encode\n\nNotes\n-----\nOrthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\nMatching pursuits with time-frequency dictionaries, IEEE Transactions on\nSignal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n(http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n\nThis implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\nM., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\nMatching Pursuit Technical Report - CS Technion, April 2008.\nhttps://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf"
        }
      ]
    },
    {
      "name": "sklearn.linear_model._passive_aggressive",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.linear_model._stochastic_gradient",
          "declaration": "BaseSGDClassifier",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._stochastic_gradient",
          "declaration": "BaseSGDRegressor",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._stochastic_gradient",
          "declaration": "DEFAULT_EPSILON",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "PassiveAggressiveClassifier",
          "decorators": [],
          "superclasses": [
            "BaseSGDClassifier"
          ],
          "methods": [
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Subset of the training data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Subset of the target values"
                },
                {
                  "name": "classes",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Classes across all calls to partial_fit.\nCan be obtained by via `np.unique(y_all)`, where y_all is the\ntarget vector of the entire dataset.\nThis argument is required for the first call to partial_fit\nand can be omitted in the subsequent calls.\nNote that y doesn't need to contain all labels in `classes`."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit linear model with Passive Aggressive algorithm.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Subset of the training data\n\ny : numpy array of shape [n_samples]\n    Subset of the target values\n\nclasses : array, shape = [n_classes]\n    Classes across all calls to partial_fit.\n    Can be obtained by via `np.unique(y_all)`, where y_all is the\n    target vector of the entire dataset.\n    This argument is required for the first call to partial_fit\n    and can be omitted in the subsequent calls.\n    Note that y doesn't need to contain all labels in `classes`.\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values"
                },
                {
                  "name": "coef_init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The initial coefficients to warm-start the optimization."
                },
                {
                  "name": "intercept_init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The initial intercept to warm-start the optimization."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit linear model with Passive Aggressive algorithm.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data\n\ny : numpy array of shape [n_samples]\n    Target values\n\ncoef_init : array, shape = [n_classes,n_features]\n    The initial coefficients to warm-start the optimization.\n\nintercept_init : array, shape = [n_classes]\n    The initial intercept to warm-start the optimization.\n\nReturns\n-------\nself : returns an instance of self."
            }
          ],
          "fullDocstring": "Passive Aggressive Classifier\n\nRead more in the :ref:`User Guide <passive_aggressive>`.\n\nParameters\n----------\n\nC : float, default=1.0\n    Maximum step size (regularization). Defaults to 1.0.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n\n    .. versionadded:: 0.19\n\ntol : float or None, default=1e-3\n    The stopping criterion. If it is not None, the iterations will stop\n    when (loss > previous_loss - tol).\n\n    .. versionadded:: 0.19\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation.\n    score is not improving. If set to True, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score is not improving by at least tol for\n    n_iter_no_change consecutive epochs.\n\n    .. versionadded:: 0.20\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before early stopping.\n\n    .. versionadded:: 0.20\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : integer, default=0\n    The verbosity level\n\nloss : string, default=\"hinge\"\n    The loss function to be used:\n    hinge: equivalent to PA-I in the reference paper.\n    squared_hinge: equivalent to PA-II in the reference paper.\n\nn_jobs : int or None, default=None\n    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    Used to shuffle the training data, when ``shuffle`` is set to\n    ``True``. Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n\nclass_weight : dict, {class_label: weight} or \"balanced\" or None,             default=None\n    Preset for the class_weight fit parameter.\n\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    .. versionadded:: 0.17\n       parameter *class_weight* to automatically weight samples.\n\naverage : bool or int, default=False\n    When set to True, computes the averaged SGD weights and stores the\n    result in the ``coef_`` attribute. If set to an int greater than 1,\n    averaging will begin once the total number of samples seen reaches\n    average. So average=10 will begin averaging after seeing 10 samples.\n\n    .. versionadded:: 0.19\n       parameter *average* to use weights averaging in SGD\n\nAttributes\n----------\ncoef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n    Weights assigned to the features.\n\nintercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n    Constants in decision function.\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n    For multiclass fits, it is the maximum over every binary fit.\n\nclasses_ : array of shape (n_classes,)\n    The unique classes labels.\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples)``.\n\nloss_function_ : callable\n    Loss function used by the algorithm.\n\nExamples\n--------\n>>> from sklearn.linear_model import PassiveAggressiveClassifier\n>>> from sklearn.datasets import make_classification\n\n>>> X, y = make_classification(n_features=4, random_state=0)\n>>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n... tol=1e-3)\n>>> clf.fit(X, y)\nPassiveAggressiveClassifier(random_state=0)\n>>> print(clf.coef_)\n[[0.26642044 0.45070924 0.67251877 0.64185414]]\n>>> print(clf.intercept_)\n[1.84127814]\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]\n\nSee Also\n--------\nSGDClassifier\nPerceptron\n\nReferences\n----------\nOnline Passive-Aggressive Algorithms\n<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\nK. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)"
        },
        {
          "name": "PassiveAggressiveRegressor",
          "decorators": [],
          "superclasses": [
            "BaseSGDRegressor"
          ],
          "methods": [
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Subset of training data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Subset of target values"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit linear model with Passive Aggressive algorithm.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Subset of training data\n\ny : numpy array of shape [n_samples]\n    Subset of target values\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values"
                },
                {
                  "name": "coef_init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The initial coefficients to warm-start the optimization."
                },
                {
                  "name": "intercept_init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The initial intercept to warm-start the optimization."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit linear model with Passive Aggressive algorithm.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data\n\ny : numpy array of shape [n_samples]\n    Target values\n\ncoef_init : array, shape = [n_features]\n    The initial coefficients to warm-start the optimization.\n\nintercept_init : array, shape = [1]\n    The initial intercept to warm-start the optimization.\n\nReturns\n-------\nself : returns an instance of self."
            }
          ],
          "fullDocstring": "Passive Aggressive Regressor\n\nRead more in the :ref:`User Guide <passive_aggressive>`.\n\nParameters\n----------\n\nC : float, default=1.0\n    Maximum step size (regularization). Defaults to 1.0.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered. Defaults to True.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n\n    .. versionadded:: 0.19\n\ntol : float or None, default=1e-3\n    The stopping criterion. If it is not None, the iterations will stop\n    when (loss > previous_loss - tol).\n\n    .. versionadded:: 0.19\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation.\n    score is not improving. If set to True, it will automatically set aside\n    a fraction of training data as validation and terminate\n    training when validation score is not improving by at least tol for\n    n_iter_no_change consecutive epochs.\n\n    .. versionadded:: 0.20\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before early stopping.\n\n    .. versionadded:: 0.20\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : integer, default=0\n    The verbosity level\n\nloss : string, default=\"epsilon_insensitive\"\n    The loss function to be used:\n    epsilon_insensitive: equivalent to PA-I in the reference paper.\n    squared_epsilon_insensitive: equivalent to PA-II in the reference\n    paper.\n\nepsilon : float, default=DEFAULT_EPSILON\n    If the difference between the current prediction and the correct label\n    is below this threshold, the model is not updated.\n\nrandom_state : int, RandomState instance, default=None\n    Used to shuffle the training data, when ``shuffle`` is set to\n    ``True``. Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n\naverage : bool or int, default=False\n    When set to True, computes the averaged SGD weights and stores the\n    result in the ``coef_`` attribute. If set to an int greater than 1,\n    averaging will begin once the total number of samples seen reaches\n    average. So average=10 will begin averaging after seeing 10 samples.\n\n    .. versionadded:: 0.19\n       parameter *average* to use weights averaging in SGD\n\nAttributes\n----------\ncoef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n    Weights assigned to the features.\n\nintercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n    Constants in decision function.\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples)``.\n\nExamples\n--------\n>>> from sklearn.linear_model import PassiveAggressiveRegressor\n>>> from sklearn.datasets import make_regression\n\n>>> X, y = make_regression(n_features=4, random_state=0)\n>>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n... tol=1e-3)\n>>> regr.fit(X, y)\nPassiveAggressiveRegressor(max_iter=100, random_state=0)\n>>> print(regr.coef_)\n[20.48736655 34.18818427 67.59122734 87.94731329]\n>>> print(regr.intercept_)\n[-0.02306214]\n>>> print(regr.predict([[0, 0, 0, 0]]))\n[-0.02306214]\n\nSee Also\n--------\nSGDRegressor\n\nReferences\n----------\nOnline Passive-Aggressive Algorithms\n<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\nK. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._perceptron",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.linear_model._stochastic_gradient",
          "declaration": "BaseSGDClassifier",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "Perceptron",
          "decorators": [],
          "superclasses": [
            "BaseSGDClassifier"
          ],
          "methods": [],
          "fullDocstring": "Perceptron\n\nRead more in the :ref:`User Guide <perceptron>`.\n\nParameters\n----------\n\npenalty : {'l2','l1','elasticnet'}, default=None\n    The penalty (aka regularization term) to be used.\n\nalpha : float, default=0.0001\n    Constant that multiplies the regularization term if regularization is\n    used.\n\nl1_ratio : float, default=0.15\n    The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.\n    `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.\n    Only used if `penalty='elasticnet'`.\n\n    .. versionadded:: 0.24\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-3\n    The stopping criterion. If it is not None, the iterations will stop\n    when (loss > previous_loss - tol).\n\n    .. versionadded:: 0.19\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : int, default=0\n    The verbosity level\n\neta0 : double, default=1\n    Constant by which the updates are multiplied.\n\nn_jobs : int, default=None\n    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    Used to shuffle the training data, when ``shuffle`` is set to\n    ``True``. Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation.\n    score is not improving. If set to True, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score is not improving by at least tol for\n    n_iter_no_change consecutive epochs.\n\n    .. versionadded:: 0.20\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before early stopping.\n\n    .. versionadded:: 0.20\n\nclass_weight : dict, {class_label: weight} or \"balanced\", default=None\n    Preset for the class_weight fit parameter.\n\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution. See\n    :term:`the Glossary <warm_start>`.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    The unique classes labels.\n\ncoef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n    Weights assigned to the features.\n\nintercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n    Constants in decision function.\n\nloss_function_ : concrete\u00a0LossFunction\n    The function that determines the loss, or difference between the\n    output of the algorithm and the target values.\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n    For multiclass fits, it is the maximum over every binary fit.\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples)``.\n\nNotes\n-----\n\n``Perceptron`` is a classification algorithm which shares the same\nunderlying implementation with ``SGDClassifier``. In fact,\n``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\neta0=1, learning_rate=\"constant\", penalty=None)`.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.linear_model import Perceptron\n>>> X, y = load_digits(return_X_y=True)\n>>> clf = Perceptron(tol=1e-3, random_state=0)\n>>> clf.fit(X, y)\nPerceptron()\n>>> clf.score(X, y)\n0.939...\n\nSee Also\n--------\nSGDClassifier\n\nReferences\n----------\n\nhttps://en.wikipedia.org/wiki/Perceptron and references therein."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._ransac",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearRegression",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.random",
          "declaration": "sample_without_replacement",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "has_fit_parameter",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "RANSACRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "MetaEstimatorMixin",
            "BaseEstimator",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Individual weights for each sample\nraises error if sample_weight is passed and base_estimator\nfit method does not support it.\n\n.. versionadded:: 0.18"
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit estimator using RANSAC algorithm.\n\nParameters\n----------\nX : array-like or sparse matrix, shape [n_samples, n_features]\n    Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample\n    raises error if sample_weight is passed and base_estimator\n    fit method does not support it.\n\n    .. versionadded:: 0.18\n\nRaises\n------\nValueError\n    If no valid consensus set could be found. This occurs if\n    `is_data_valid` and `is_model_valid` return False for all\n    `max_trials` randomly chosen sub-samples."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Returns predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the estimated model.\n\nThis is a wrapper for `estimator_.predict(X)`.\n\nParameters\n----------\nX : numpy array of shape [n_samples, n_features]\n\nReturns\n-------\ny : array, shape = [n_samples] or [n_samples, n_targets]\n    Returns predicted values."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "z",
                  "type": "float",
                  "description": "Score of the prediction."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the score of the prediction.\n\nThis is a wrapper for `estimator_.score(X, y)`.\n\nParameters\n----------\nX : numpy array or sparse matrix of shape [n_samples, n_features]\n    Training data.\n\ny : array, shape = [n_samples] or [n_samples, n_targets]\n    Target values.\n\nReturns\n-------\nz : float\n    Score of the prediction."
            }
          ],
          "fullDocstring": "RANSAC (RANdom SAmple Consensus) algorithm.\n\nRANSAC is an iterative algorithm for the robust estimation of parameters\nfrom a subset of inliers from the complete data set.\n\nRead more in the :ref:`User Guide <ransac_regression>`.\n\nParameters\n----------\nbase_estimator : object, default=None\n    Base estimator object which implements the following methods:\n\n     * `fit(X, y)`: Fit model to given training data and target values.\n     * `score(X, y)`: Returns the mean accuracy on the given test data,\n       which is used for the stop criterion defined by `stop_score`.\n       Additionally, the score is used to decide which of two equally\n       large consensus sets is chosen as the better one.\n     * `predict(X)`: Returns predicted values using the linear model,\n       which is used to compute residual error using loss function.\n\n    If `base_estimator` is None, then\n    :class:`~sklearn.linear_model.LinearRegression` is used for\n    target values of dtype float.\n\n    Note that the current implementation only supports regression\n    estimators.\n\nmin_samples : int (>= 1) or float ([0, 1]), default=None\n    Minimum number of samples chosen randomly from original data. Treated\n    as an absolute number of samples for `min_samples >= 1`, treated as a\n    relative number `ceil(min_samples * X.shape[0]`) for\n    `min_samples < 1`. This is typically chosen as the minimal number of\n    samples necessary to estimate the given `base_estimator`. By default a\n    ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\n    `min_samples` is chosen as ``X.shape[1] + 1``.\n\nresidual_threshold : float, default=None\n    Maximum residual for a data sample to be classified as an inlier.\n    By default the threshold is chosen as the MAD (median absolute\n    deviation) of the target values `y`.\n\nis_data_valid : callable, default=None\n    This function is called with the randomly selected data before the\n    model is fitted to it: `is_data_valid(X, y)`. If its return value is\n    False the current randomly chosen sub-sample is skipped.\n\nis_model_valid : callable, default=None\n    This function is called with the estimated model and the randomly\n    selected data: `is_model_valid(model, X, y)`. If its return value is\n    False the current randomly chosen sub-sample is skipped.\n    Rejecting samples with this function is computationally costlier than\n    with `is_data_valid`. `is_model_valid` should therefore only be used if\n    the estimated model is needed for making the rejection decision.\n\nmax_trials : int, default=100\n    Maximum number of iterations for random sample selection.\n\nmax_skips : int, default=np.inf\n    Maximum number of iterations that can be skipped due to finding zero\n    inliers or invalid data defined by ``is_data_valid`` or invalid models\n    defined by ``is_model_valid``.\n\n    .. versionadded:: 0.19\n\nstop_n_inliers : int, default=np.inf\n    Stop iteration if at least this number of inliers are found.\n\nstop_score : float, default=np.inf\n    Stop iteration if score is greater equal than this threshold.\n\nstop_probability : float in range [0, 1], default=0.99\n    RANSAC iteration stops if at least one outlier-free set of the training\n    data is sampled in RANSAC. This requires to generate at least N\n    samples (iterations)::\n\n        N >= log(1 - probability) / log(1 - e**m)\n\n    where the probability (confidence) is typically set to high value such\n    as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n    the total number of samples.\n\nloss : string, callable, default='absolute_loss'\n    String inputs, \"absolute_loss\" and \"squared_loss\" are supported which\n    find the absolute loss and squared loss per sample\n    respectively.\n\n    If ``loss`` is a callable, then it should be a function that takes\n    two arrays as inputs, the true and predicted value and returns a 1-D\n    array with the i-th value of the array corresponding to the loss\n    on ``X[i]``.\n\n    If the loss on a sample is greater than the ``residual_threshold``,\n    then this sample is classified as an outlier.\n\n    .. versionadded:: 0.18\n\nrandom_state : int, RandomState instance, default=None\n    The generator used to initialize the centers.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nestimator_ : object\n    Best fitted model (copy of the `base_estimator` object).\n\nn_trials_ : int\n    Number of random selection trials until one of the stop criteria is\n    met. It is always ``<= max_trials``.\n\ninlier_mask_ : bool array of shape [n_samples]\n    Boolean mask of inliers classified as ``True``.\n\nn_skips_no_inliers_ : int\n    Number of iterations skipped due to finding zero inliers.\n\n    .. versionadded:: 0.19\n\nn_skips_invalid_data_ : int\n    Number of iterations skipped due to invalid data defined by\n    ``is_data_valid``.\n\n    .. versionadded:: 0.19\n\nn_skips_invalid_model_ : int\n    Number of iterations skipped due to an invalid model defined by\n    ``is_model_valid``.\n\n    .. versionadded:: 0.19\n\nExamples\n--------\n>>> from sklearn.linear_model import RANSACRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = RANSACRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9885...\n>>> reg.predict(X[:1,])\narray([-31.9417...])\n\nReferences\n----------\n.. [1] https://en.wikipedia.org/wiki/RANSAC\n.. [2] https://www.sri.com/sites/default/files/publications/ransac-publication.pdf\n.. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model._ridge",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "linalg",
          "alias": "sp_linalg"
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearModel",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "_rescale_data",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sag",
          "declaration": "sag_solver",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "check_scoring",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "GridSearchCV",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "compute_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "mean_variance_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "Ridge",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "_BaseRidge",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values"
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Individual weights for each sample. If given a float, every sample\nwill have the same weight."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit Ridge regression model.\n\nParameters\n----------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Training data\n\ny : ndarray of shape (n_samples,) or (n_samples, n_targets)\n    Target values\n\nsample_weight : float or ndarray of shape (n_samples,), default=None\n    Individual weights for each sample. If given a float, every sample\n    will have the same weight.\n\nReturns\n-------\nself : returns an instance of self."
            }
          ],
          "fullDocstring": "Linear least squares with l2 regularization.\n\nMinimizes the objective function::\n\n||y - Xw||^2_2 + alpha * ||w||^2_2\n\nThis model solves a regression model where the loss function is\nthe linear least squares function and regularization is given by\nthe l2-norm. Also known as Ridge Regression or Tikhonov regularization.\nThis estimator has built-in support for multi-variate regression\n(i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nalpha : {float, ndarray of shape (n_targets,)}, default=1.0\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n\nfit_intercept : bool, default=True\n    Whether to fit the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. ``X`` and ``y`` are expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=None\n    Maximum number of iterations for conjugate gradient solver.\n    For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n    by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n\ntol : float, default=1e-3\n    Precision of the solution.\n\nsolver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n    Solver to use in the computational routines:\n\n    - 'auto' chooses the solver automatically based on the type of data.\n\n    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n      coefficients. More stable for singular matrices than 'cholesky'.\n\n    - 'cholesky' uses the standard scipy.linalg.solve function to\n      obtain a closed-form solution.\n\n    - 'sparse_cg' uses the conjugate gradient solver as found in\n      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n      more appropriate than 'cholesky' for large-scale data\n      (possibility to set `tol` and `max_iter`).\n\n    - 'lsqr' uses the dedicated regularized least-squares routine\n      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n      procedure.\n\n    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n      its improved, unbiased version named SAGA. Both methods also use an\n      iterative procedure, and are often faster than other solvers when\n      both n_samples and n_features are large. Note that 'sag' and\n      'saga' fast convergence is only guaranteed on features with\n      approximately the same scale. You can preprocess the data with a\n      scaler from sklearn.preprocessing.\n\n    All last five solvers support both dense and sparse data. However, only\n    'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n    True.\n\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n\nrandom_state : int, RandomState instance, default=None\n    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n    See :term:`Glossary <random_state>` for details.\n\n    .. versionadded:: 0.17\n       `random_state` to support Stochastic Average Gradient.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Weight vector(s).\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nn_iter_ : None or ndarray of shape (n_targets,)\n    Actual number of iterations for each target. Available only for\n    sag and lsqr solvers. Other solvers will return None.\n\n    .. versionadded:: 0.17\n\nSee Also\n--------\nRidgeClassifier : Ridge classifier.\nRidgeCV : Ridge regression with built-in cross validation.\n:class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n    combines ridge regression with the kernel trick.\n\nExamples\n--------\n>>> from sklearn.linear_model import Ridge\n>>> import numpy as np\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> clf = Ridge(alpha=1.0)\n>>> clf.fit(X, y)\nRidge()"
        },
        {
          "name": "RidgeCV",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "_BaseRidgeCV",
            "MultiOutputMixin"
          ],
          "methods": [],
          "fullDocstring": "Ridge regression with built-in cross-validation.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nBy default, it performs efficient Leave-One-Out Cross-Validation.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nalphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n    Array of alpha values to try.\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.\n    If using Leave-One-Out cross-validation, alphas must be positive.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nscoring : string, callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n    If None, the negative mean squared error if cv is 'auto' or None\n    (i.e. when using leave-one-out cross-validation), and r2 score\n    otherwise.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the efficient Leave-One-Out cross-validation\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n    :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\ngcv_mode : {'auto', 'svd', eigen'}, default='auto'\n    Flag indicating which strategy to use when performing\n    Leave-One-Out Cross-Validation. Options are::\n\n        'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n        'svd' : force use of singular value decomposition of X when X is\n            dense, eigenvalue decomposition of X^T.X when X is sparse.\n        'eigen' : force computation via eigendecomposition of X.X^T\n\n    The 'auto' mode is the default and is intended to pick the cheaper\n    option of the two depending on the shape of the training data.\n\nstore_cv_values : bool, default=False\n    Flag indicating if the cross-validation values corresponding to\n    each alpha should be stored in the ``cv_values_`` attribute (see\n    below). This flag is only compatible with ``cv=None`` (i.e. using\n    Leave-One-Out Cross-Validation).\n\nalpha_per_target : bool, default=False\n    Flag indicating whether to optimize the alpha value (picked from the\n    `alphas` parameter list) for each target separately (for multi-output\n    settings: multiple prediction targets). When set to `True`, after\n    fitting, the `alpha_` attribute will contain a value for each target.\n    When set to `False`, a single alpha is used for all targets.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncv_values_ : ndarray of shape (n_samples, n_alphas) or         shape (n_samples, n_targets, n_alphas), optional\n    Cross-validation values for each alpha (only available if\n    ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been\n    called, this attribute will contain the mean squared errors\n    (by default) or the values of the ``{loss,score}_func`` function\n    (if provided in the constructor).\n\ncoef_ : ndarray of shape (n_features) or (n_targets, n_features)\n    Weight vector(s).\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nalpha_ : float or ndarray of shape (n_targets,)\n    Estimated regularization parameter, or, if ``alpha_per_target=True``,\n    the estimated regularization parameter for each target.\n\nbest_score_ : float or ndarray of shape (n_targets,)\n    Score of base estimator with best alpha, or, if\n    ``alpha_per_target=True``, a score for each target.\n\n    .. versionadded:: 0.23\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.linear_model import RidgeCV\n>>> X, y = load_diabetes(return_X_y=True)\n>>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n>>> clf.score(X, y)\n0.5166...\n\nSee Also\n--------\nRidge : Ridge regression.\nRidgeClassifier : Ridge classifier.\nRidgeClassifierCV : Ridge classifier with built-in cross validation."
        },
        {
          "name": "RidgeClassifier",
          "decorators": [],
          "superclasses": [
            "LinearClassifierMixin",
            "_BaseRidge"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Individual weights for each sample. If given a float, every sample\nwill have the same weight.\n\n.. versionadded:: 0.17\n   *sample_weight* support to Classifier."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Instance of the estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit Ridge classifier model.\n\nParameters\n----------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : ndarray of shape (n_samples,)\n    Target values.\n\nsample_weight : float or ndarray of shape (n_samples,), default=None\n    Individual weights for each sample. If given a float, every sample\n    will have the same weight.\n\n    .. versionadded:: 0.17\n       *sample_weight* support to Classifier.\n\nReturns\n-------\nself : object\n    Instance of the estimator."
            },
            {
              "name": "classes_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Classifier using Ridge regression.\n\nThis classifier first converts the target values into ``{-1, 1}`` and\nthen treats the problem as a regression task (multi-output regression in\nthe multiclass case).\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set to false, no\n    intercept will be used in calculations (e.g. data is expected to be\n    already centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=None\n    Maximum number of iterations for conjugate gradient solver.\n    The default value is determined by scipy.sparse.linalg.\n\ntol : float, default=1e-3\n    Precision of the solution.\n\nclass_weight : dict or 'balanced', default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\nsolver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n    Solver to use in the computational routines:\n\n    - 'auto' chooses the solver automatically based on the type of data.\n\n    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n      coefficients. More stable for singular matrices than 'cholesky'.\n\n    - 'cholesky' uses the standard scipy.linalg.solve function to\n      obtain a closed-form solution.\n\n    - 'sparse_cg' uses the conjugate gradient solver as found in\n      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n      more appropriate than 'cholesky' for large-scale data\n      (possibility to set `tol` and `max_iter`).\n\n    - 'lsqr' uses the dedicated regularized least-squares routine\n      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n      procedure.\n\n    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n      its unbiased and more flexible version named SAGA. Both methods\n      use an iterative procedure, and are often faster than other solvers\n      when both n_samples and n_features are large. Note that 'sag' and\n      'saga' fast convergence is only guaranteed on features with\n      approximately the same scale. You can preprocess the data with a\n      scaler from sklearn.preprocessing.\n\n      .. versionadded:: 0.17\n         Stochastic Average Gradient descent solver.\n      .. versionadded:: 0.19\n       SAGA solver.\n\nrandom_state : int, RandomState instance, default=None\n    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n    See :term:`Glossary <random_state>` for details.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n    Coefficient of the features in the decision function.\n\n    ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nn_iter_ : None or ndarray of shape (n_targets,)\n    Actual number of iterations for each target. Available only for\n    sag and lsqr solvers. Other solvers will return None.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nSee Also\n--------\nRidge : Ridge regression.\nRidgeClassifierCV :  Ridge classifier with built-in cross validation.\n\nNotes\n-----\nFor multi-class classification, n_class classifiers are trained in\na one-versus-all approach. Concretely, this is implemented by taking\nadvantage of the multi-variate response support in Ridge.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.linear_model import RidgeClassifier\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = RidgeClassifier().fit(X, y)\n>>> clf.score(X, y)\n0.9595..."
        },
        {
          "name": "RidgeClassifierCV",
          "decorators": [],
          "superclasses": [
            "LinearClassifierMixin",
            "_BaseRidgeCV"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples\nand n_features is the number of features. When using GCV,\nwill be cast to float64 if necessary."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values. Will be cast to X's dtype if necessary."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Individual weights for each sample. If given a float, every sample\nwill have the same weight."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit Ridge classifier with cv.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples\n    and n_features is the number of features. When using GCV,\n    will be cast to float64 if necessary.\n\ny : ndarray of shape (n_samples,)\n    Target values. Will be cast to X's dtype if necessary.\n\nsample_weight : float or ndarray of shape (n_samples,), default=None\n    Individual weights for each sample. If given a float, every sample\n    will have the same weight.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "classes_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Ridge classifier with built-in cross-validation.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nBy default, it performs Leave-One-Out Cross-Validation. Currently,\nonly the n_features > n_samples case is handled efficiently.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nalphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n    Array of alpha values to try.\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nscoring : string, callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the efficient Leave-One-Out cross-validation\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\nclass_weight : dict or 'balanced', default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\nstore_cv_values : bool, default=False\n    Flag indicating if the cross-validation values corresponding to\n    each alpha should be stored in the ``cv_values_`` attribute (see\n    below). This flag is only compatible with ``cv=None`` (i.e. using\n    Leave-One-Out Cross-Validation).\n\nAttributes\n----------\ncv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\n    Cross-validation values for each alpha (if ``store_cv_values=True`` and\n    ``cv=None``). After ``fit()`` has been called, this attribute will\n    contain the mean squared errors (by default) or the values of the\n    ``{loss,score}_func`` function (if provided in the constructor). This\n    attribute exists only when ``store_cv_values`` is True.\n\ncoef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\n    Coefficient of the features in the decision function.\n\n    ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nalpha_ : float\n    Estimated regularization parameter.\n\nbest_score_ : float\n    Score of base estimator with best alpha.\n\n    .. versionadded:: 0.23\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.linear_model import RidgeClassifierCV\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n>>> clf.score(X, y)\n0.9630...\n\nSee Also\n--------\nRidge : Ridge regression.\nRidgeClassifier : Ridge classifier.\nRidgeCV : Ridge regression with built-in cross validation.\n\nNotes\n-----\nFor multi-class classification, n_class classifiers are trained in\na one-versus-all approach. Concretely, this is implemented by taking\nadvantage of the multi-variate response support in Ridge."
        }
      ],
      "functions": [
        {
          "name": "ridge_regression",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training data"
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values"
            },
            {
              "name": "alpha",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Regularization strength; must be a positive float. Regularization\nimproves the conditioning of the problem and reduces the variance of\nthe estimates. Larger values specify stronger regularization.\nAlpha corresponds to ``1 / (2C)`` in other linear models such as\n:class:`~sklearn.linear_model.LogisticRegression` or\n:class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\nassumed to be specific to the targets. Hence they must correspond in\nnumber."
            }
          ],
          "results": [
            {
              "name": "coef",
              "type": null,
              "description": "Weight vector(s)."
            },
            {
              "name": "n_iter",
              "type": null,
              "description": "The actual number of iteration performed by the solver.\nOnly returned if `return_n_iter` is True."
            },
            {
              "name": "intercept",
              "type": null,
              "description": "The intercept of the model. Only returned if `return_intercept`\nis True and if X is a scipy sparse array."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Solve the ridge equation by the method of normal equations.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nX : {ndarray, sparse matrix, LinearOperator} of shape         (n_samples, n_features)\n    Training data\n\ny : ndarray of shape (n_samples,) or (n_samples, n_targets)\n    Target values\n\nalpha : float or array-like of shape (n_targets,)\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n\nsample_weight : float or array-like of shape (n_samples,), default=None\n    Individual weights for each sample. If given a float, every sample\n    will have the same weight. If sample_weight is not None and\n    solver='auto', the solver will be set to 'cholesky'.\n\n    .. versionadded:: 0.17\n\nsolver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n    Solver to use in the computational routines:\n\n    - 'auto' chooses the solver automatically based on the type of data.\n\n    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n      coefficients. More stable for singular matrices than 'cholesky'.\n\n    - 'cholesky' uses the standard scipy.linalg.solve function to\n      obtain a closed-form solution via a Cholesky decomposition of\n      dot(X.T, X)\n\n    - 'sparse_cg' uses the conjugate gradient solver as found in\n      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n      more appropriate than 'cholesky' for large-scale data\n      (possibility to set `tol` and `max_iter`).\n\n    - 'lsqr' uses the dedicated regularized least-squares routine\n      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n      procedure.\n\n    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n      its improved, unbiased version named SAGA. Both methods also use an\n      iterative procedure, and are often faster than other solvers when\n      both n_samples and n_features are large. Note that 'sag' and\n      'saga' fast convergence is only guaranteed on features with\n      approximately the same scale. You can preprocess the data with a\n      scaler from sklearn.preprocessing.\n\n\n    All last five solvers support both dense and sparse data. However, only\n    'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n    True.\n\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n\nmax_iter : int, default=None\n    Maximum number of iterations for conjugate gradient solver.\n    For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n    by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n    1000.\n\ntol : float, default=1e-3\n    Precision of the solution.\n\nverbose : int, default=0\n    Verbosity level. Setting verbose > 0 will display additional\n    information depending on the solver used.\n\nrandom_state : int, RandomState instance, default=None\n    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n    See :term:`Glossary <random_state>` for details.\n\nreturn_n_iter : bool, default=False\n    If True, the method also returns `n_iter`, the actual number of\n    iteration performed by the solver.\n\n    .. versionadded:: 0.17\n\nreturn_intercept : bool, default=False\n    If True and if X is sparse, the method also returns the intercept,\n    and the solver is automatically changed to 'sag'. This is only a\n    temporary fix for fitting the intercept with sparse data. For dense\n    data, use sklearn.linear_model._preprocess_data before your regression.\n\n    .. versionadded:: 0.17\n\ncheck_input : bool, default=True\n    If False, the input arrays X and y will not be checked.\n\n    .. versionadded:: 0.21\n\nReturns\n-------\ncoef : ndarray of shape (n_features,) or (n_targets, n_features)\n    Weight vector(s).\n\nn_iter : int, optional\n    The actual number of iteration performed by the solver.\n    Only returned if `return_n_iter` is True.\n\nintercept : float or ndarray of shape (n_targets,)\n    The intercept of the model. Only returned if `return_intercept`\n    is True and if X is a scipy sparse array.\n\nNotes\n-----\nThis function won't compute the intercept."
        }
      ]
    },
    {
      "name": "sklearn.linear_model._sag",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "make_dataset",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sag_fast",
          "declaration": "sag32",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sag_fast",
          "declaration": "sag64",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "get_auto_step_size",
          "decorators": [],
          "parameters": [
            {
              "name": "max_squared_sum",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Maximum squared sum of X over samples."
            },
            {
              "name": "alpha_scaled",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Constant that multiplies the regularization term, scaled by\n1. / n_samples, the number of samples."
            },
            {
              "name": "loss",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The loss function used in SAG solver."
            },
            {
              "name": "fit_intercept",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Specifies if a constant (a.k.a. bias or intercept) will be\nadded to the decision function."
            },
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of rows in X. Useful if is_saga=True."
            },
            {
              "name": "is_saga",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "Whether to return step size for the SAGA algorithm or the SAG\nalgorithm."
            }
          ],
          "results": [
            {
              "name": "step_size",
              "type": "float",
              "description": "Step size used in SAG solver."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute automatic step size for SAG solver.\n\nThe step size is set to 1 / (alpha_scaled + L + fit_intercept) where L is\nthe max sum of squares for over all samples.\n\nParameters\n----------\nmax_squared_sum : float\n    Maximum squared sum of X over samples.\n\nalpha_scaled : float\n    Constant that multiplies the regularization term, scaled by\n    1. / n_samples, the number of samples.\n\nloss : {'log', 'squared', 'multinomial'}\n    The loss function used in SAG solver.\n\nfit_intercept : bool\n    Specifies if a constant (a.k.a. bias or intercept) will be\n    added to the decision function.\n\nn_samples : int, default=None\n    Number of rows in X. Useful if is_saga=True.\n\nis_saga : bool, default=False\n    Whether to return step size for the SAGA algorithm or the SAG\n    algorithm.\n\nReturns\n-------\nstep_size : float\n    Step size used in SAG solver.\n\nReferences\n----------\nSchmidt, M., Roux, N. L., & Bach, F. (2013).\nMinimizing finite sums with the stochastic average gradient\nhttps://hal.inria.fr/hal-00860051/document\n\nDefazio, A., Bach F. & Lacoste-Julien S. (2014).\nSAGA: A Fast Incremental Gradient Method With Support\nfor Non-Strongly Convex Composite Objectives\nhttps://arxiv.org/abs/1407.0202"
        },
        {
          "name": "sag_solver",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training data."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values. With loss='multinomial', y must be label encoded\n(see preprocessing.LabelEncoder)."
            },
            {
              "name": "sample_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Weights applied to individual samples (1. for unweighted)."
            },
            {
              "name": "loss",
              "type": "Any",
              "hasDefault": true,
              "default": "log",
              "limitation": null,
              "ignored": false,
              "description": "Loss function that will be optimized:\n-'log' is the binary logistic loss, as used in LogisticRegression.\n-'squared' is the squared loss, as used in Ridge.\n-'multinomial' is the multinomial logistic loss, as used in\n LogisticRegression.\n\n.. versionadded:: 0.18\n   *loss='multinomial'*"
            },
            {
              "name": "alpha",
              "type": "Any",
              "hasDefault": true,
              "default": "1.0",
              "limitation": null,
              "ignored": false,
              "description": "L2 regularization term in the objective function\n``(0.5 * alpha * || W ||_F^2)``."
            },
            {
              "name": "beta",
              "type": "Any",
              "hasDefault": false,
              "default": "0.0",
              "limitation": null,
              "ignored": false,
              "description": "L1 regularization term in the objective function\n``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True."
            },
            {
              "name": "max_iter",
              "type": "Any",
              "hasDefault": true,
              "default": "1000",
              "limitation": null,
              "ignored": false,
              "description": "The max number of passes over the training data if the stopping\ncriteria is not reached."
            },
            {
              "name": "tol",
              "type": "Any",
              "hasDefault": true,
              "default": "0.001",
              "limitation": null,
              "ignored": false,
              "description": "The stopping criteria for the weights. The iterations will stop when\nmax(change in weights) / max(weights) < tol."
            },
            {
              "name": "verbose",
              "type": "Any",
              "hasDefault": false,
              "default": "0",
              "limitation": null,
              "ignored": false,
              "description": "The verbosity level."
            },
            {
              "name": "random_state",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Used when shuffling the data. Pass an int for reproducible output\nacross multiple function calls.\nSee :term:`Glossary <random_state>`."
            },
            {
              "name": "check_input",
              "type": "Any",
              "hasDefault": true,
              "default": "True",
              "limitation": null,
              "ignored": false,
              "description": "If False, the input arrays X and y will not be checked."
            },
            {
              "name": "max_squared_sum",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Maximum squared sum of X over samples. If None, it will be computed,\ngoing through all the samples. The value should be precomputed\nto speed up cross validation."
            },
            {
              "name": "warm_start_mem",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The initialization parameters used for warm starting. Warm starting is\ncurrently used in LogisticRegression but not in Ridge.\nIt contains:\n    - 'coef': the weight vector, with the intercept in last line\n        if the intercept is fitted.\n    - 'gradient_memory': the scalar gradient for all seen samples.\n    - 'sum_gradient': the sum of gradient over all seen samples,\n        for each feature.\n    - 'intercept_sum_gradient': the sum of gradient over all seen\n        samples, for the intercept.\n    - 'seen': array of boolean describing the seen samples.\n    - 'num_seen': the number of seen samples."
            },
            {
              "name": "is_saga",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves\nbetter in the first epochs, and allow for l1 regularisation."
            }
          ],
          "results": [
            {
              "name": "coef_",
              "type": null,
              "description": "Weight vector."
            },
            {
              "name": "n_iter_",
              "type": "int",
              "description": "The number of full pass on all samples."
            },
            {
              "name": "warm_start_mem",
              "type": "Dict",
              "description": "Contains a 'coef' key with the fitted result, and possibly the\nfitted intercept at the end of the array. Contains also other keys\nused for warm starting."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "SAG solver for Ridge and LogisticRegression.\n\nSAG stands for Stochastic Average Gradient: the gradient of the loss is\nestimated each sample at a time and the model is updated along the way with\na constant learning rate.\n\nIMPORTANT NOTE: 'sag' solver converges faster on columns that are on the\nsame scale. You can normalize the data by using\nsklearn.preprocessing.StandardScaler on your data before passing it to the\nfit method.\n\nThis implementation works with data represented as dense numpy arrays or\nsparse scipy arrays of floating point values for the features. It will\nfit the data according to squared loss or log loss.\n\nThe regularizer is a penalty added to the loss function that shrinks model\nparameters towards the zero vector using the squared euclidean norm L2.\n\n.. versionadded:: 0.17\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : ndarray of shape (n_samples,)\n    Target values. With loss='multinomial', y must be label encoded\n    (see preprocessing.LabelEncoder).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weights applied to individual samples (1. for unweighted).\n\nloss : {'log', 'squared', 'multinomial'}, default='log'\n    Loss function that will be optimized:\n    -'log' is the binary logistic loss, as used in LogisticRegression.\n    -'squared' is the squared loss, as used in Ridge.\n    -'multinomial' is the multinomial logistic loss, as used in\n     LogisticRegression.\n\n    .. versionadded:: 0.18\n       *loss='multinomial'*\n\nalpha : float, default=1.\n    L2 regularization term in the objective function\n    ``(0.5 * alpha * || W ||_F^2)``.\n\nbeta : float, default=0.\n    L1 regularization term in the objective function\n    ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True.\n\nmax_iter : int, default=1000\n    The max number of passes over the training data if the stopping\n    criteria is not reached.\n\ntol : double, default=0.001\n    The stopping criteria for the weights. The iterations will stop when\n    max(change in weights) / max(weights) < tol.\n\nverbose : int, default=0\n    The verbosity level.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when shuffling the data. Pass an int for reproducible output\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ncheck_input : bool, default=True\n    If False, the input arrays X and y will not be checked.\n\nmax_squared_sum : float, default=None\n    Maximum squared sum of X over samples. If None, it will be computed,\n    going through all the samples. The value should be precomputed\n    to speed up cross validation.\n\nwarm_start_mem : dict, default=None\n    The initialization parameters used for warm starting. Warm starting is\n    currently used in LogisticRegression but not in Ridge.\n    It contains:\n        - 'coef': the weight vector, with the intercept in last line\n            if the intercept is fitted.\n        - 'gradient_memory': the scalar gradient for all seen samples.\n        - 'sum_gradient': the sum of gradient over all seen samples,\n            for each feature.\n        - 'intercept_sum_gradient': the sum of gradient over all seen\n            samples, for the intercept.\n        - 'seen': array of boolean describing the seen samples.\n        - 'num_seen': the number of seen samples.\n\nis_saga : bool, default=False\n    Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves\n    better in the first epochs, and allow for l1 regularisation.\n\nReturns\n-------\ncoef_ : ndarray of shape (n_features,)\n    Weight vector.\n\nn_iter_ : int\n    The number of full pass on all samples.\n\nwarm_start_mem : dict\n    Contains a 'coef' key with the fitted result, and possibly the\n    fitted intercept at the end of the array. Contains also other keys\n    used for warm starting.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import linear_model\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> X = rng.randn(n_samples, n_features)\n>>> y = rng.randn(n_samples)\n>>> clf = linear_model.Ridge(solver='sag')\n>>> clf.fit(X, y)\nRidge(solver='sag')\n\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> y = np.array([1, 1, 2, 2])\n>>> clf = linear_model.LogisticRegression(\n...     solver='sag', multi_class='multinomial')\n>>> clf.fit(X, y)\nLogisticRegression(multi_class='multinomial', solver='sag')\n\nReferences\n----------\nSchmidt, M., Roux, N. L., & Bach, F. (2013).\nMinimizing finite sums with the stochastic average gradient\nhttps://hal.inria.fr/hal-00860051/document\n\nDefazio, A., Bach F. & Lacoste-Julien S. (2014).\nSAGA: A Fast Incremental Gradient Method With Support\nfor Non-Strongly Convex Composite Objectives\nhttps://arxiv.org/abs/1407.0202\n\nSee Also\n--------\nRidge, SGDRegressor, ElasticNet, Lasso, SVR,\nLogisticRegression, SGDClassifier, LinearSVC, Perceptron"
        }
      ]
    },
    {
      "name": "sklearn.linear_model._stochastic_gradient",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "SparseCoefMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "make_dataset",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sgd_fast",
          "declaration": "EpsilonInsensitive",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sgd_fast",
          "declaration": "Hinge",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sgd_fast",
          "declaration": "Huber",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sgd_fast",
          "declaration": "Log",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sgd_fast",
          "declaration": "ModifiedHuber",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sgd_fast",
          "declaration": "SquaredEpsilonInsensitive",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sgd_fast",
          "declaration": "SquaredHinge",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sgd_fast",
          "declaration": "SquaredLoss",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._sgd_fast",
          "declaration": "_plain_sgd",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "ShuffleSplit",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "StratifiedShuffleSplit",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "compute_class_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_joblib_parallel_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "_check_partial_fit_first_call",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseSGD",
          "decorators": [],
          "superclasses": [
            "SparseCoefMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "set_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Estimator instance."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Set and validate the parameters of estimator.\n\nParameters\n----------\n**kwargs : dict\n    Estimator parameters.\n\nReturns\n-------\nself : object\n    Estimator instance."
            },
            {
              "name": "fit",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit model."
            },
            {
              "name": "standard_coef_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "standard_intercept_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "average_coef_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "average_intercept_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Base class for SGD classification and regression."
        },
        {
          "name": "BaseSGDClassifier",
          "decorators": [],
          "superclasses": [
            "LinearClassifierMixin",
            "BaseSGD"
          ],
          "methods": [
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Subset of the training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Subset of the target values."
                },
                {
                  "name": "classes",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Classes across all calls to partial_fit.\nCan be obtained by via `np.unique(y_all)`, where y_all is the\ntarget vector of the entire dataset.\nThis argument is required for the first call to partial_fit\nand can be omitted in the subsequent calls.\nNote that y doesn't need to contain all labels in `classes`."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights applied to individual samples.\nIf not provided, uniform weights are assumed."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform one epoch of stochastic gradient descent on given samples.\n\nInternally, this method uses ``max_iter = 1``. Therefore, it is not\nguaranteed that a minimum of the cost function is reached after calling\nit once. Matters such as objective convergence and early stopping\nshould be handled by the user.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Subset of the training data.\n\ny : ndarray of shape (n_samples,)\n    Subset of the target values.\n\nclasses : ndarray of shape (n_classes,), default=None\n    Classes across all calls to partial_fit.\n    Can be obtained by via `np.unique(y_all)`, where y_all is the\n    target vector of the entire dataset.\n    This argument is required for the first call to partial_fit\n    and can be omitted in the subsequent calls.\n    Note that y doesn't need to contain all labels in `classes`.\n\nsample_weight : array-like, shape (n_samples,), default=None\n    Weights applied to individual samples.\n    If not provided, uniform weights are assumed.\n\nReturns\n-------\nself :\n    Returns an instance of self."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "coef_init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The initial coefficients to warm-start the optimization."
                },
                {
                  "name": "intercept_init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The initial intercept to warm-start the optimization."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights applied to individual samples.\nIf not provided, uniform weights are assumed. These weights will\nbe multiplied with class_weight (passed through the\nconstructor) if class_weight is specified."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit linear model with Stochastic Gradient Descent.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Training data.\n\ny : ndarray of shape (n_samples,)\n    Target values.\n\ncoef_init : ndarray of shape (n_classes, n_features), default=None\n    The initial coefficients to warm-start the optimization.\n\nintercept_init : ndarray of shape (n_classes,), default=None\n    The initial intercept to warm-start the optimization.\n\nsample_weight : array-like, shape (n_samples,), default=None\n    Weights applied to individual samples.\n    If not provided, uniform weights are assumed. These weights will\n    be multiplied with class_weight (passed through the\n    constructor) if class_weight is specified.\n\nReturns\n-------\nself :\n    Returns an instance of self."
            }
          ],
          "fullDocstring": null
        },
        {
          "name": "BaseSGDRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseSGD"
          ],
          "methods": [
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Subset of training data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Subset of target values"
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights applied to individual samples.\nIf not provided, uniform weights are assumed."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform one epoch of stochastic gradient descent on given samples.\n\nInternally, this method uses ``max_iter = 1``. Therefore, it is not\nguaranteed that a minimum of the cost function is reached after calling\nit once. Matters such as objective convergence and early stopping\nshould be handled by the user.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Subset of training data\n\ny : numpy array of shape (n_samples,)\n    Subset of target values\n\nsample_weight : array-like, shape (n_samples,), default=None\n    Weights applied to individual samples.\n    If not provided, uniform weights are assumed.\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values"
                },
                {
                  "name": "coef_init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The initial coefficients to warm-start the optimization."
                },
                {
                  "name": "intercept_init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The initial intercept to warm-start the optimization."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights applied to individual samples (1. for unweighted)."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit linear model with Stochastic Gradient Descent.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Training data\n\ny : ndarray of shape (n_samples,)\n    Target values\n\ncoef_init : ndarray of shape (n_features,), default=None\n    The initial coefficients to warm-start the optimization.\n\nintercept_init : ndarray of shape (1,), default=None\n    The initial intercept to warm-start the optimization.\n\nsample_weight : array-like, shape (n_samples,), default=None\n    Weights applied to individual samples (1. for unweighted).\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Predicted target values per element in X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the linear model\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n\nReturns\n-------\nndarray of shape (n_samples,)\n   Predicted target values per element in X."
            }
          ],
          "fullDocstring": null
        },
        {
          "name": "SGDClassifier",
          "decorators": [],
          "superclasses": [
            "BaseSGDClassifier"
          ],
          "methods": [
            {
              "name": "predict_proba",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "Returns the probability of the sample for each class in the model,\nwhere classes are ordered as they are in `self.classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Probability estimates.\n\nThis method is only available for log loss and modified Huber loss.\n\nMulticlass probability estimates are derived from binary (one-vs.-rest)\nestimates by simple normalization, as recommended by Zadrozny and\nElkan.\n\nBinary probability estimates for loss=\"modified_huber\" are given by\n(clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\nit is necessary to perform proper probability calibration by wrapping\nthe classifier with\n:class:`~sklearn.calibration.CalibratedClassifierCV` instead.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Input data for prediction.\n\nReturns\n-------\nndarray of shape (n_samples, n_classes)\n    Returns the probability of the sample for each class in the model,\n    where classes are ordered as they are in `self.classes_`.\n\nReferences\n----------\nZadrozny and Elkan, \"Transforming classifier scores into multiclass\nprobability estimates\", SIGKDD'02,\nhttp://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf\n\nThe justification for the formula in the loss=\"modified_huber\"\ncase is in the appendix B in:\nhttp://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf"
            },
            {
              "name": "predict_log_proba",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "T",
                  "type": null,
                  "description": "Returns the log-probability of the sample for each class in the\nmodel, where classes are ordered as they are in\n`self.classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Log of probability estimates.\n\nThis method is only available for log loss and modified Huber loss.\n\nWhen loss=\"modified_huber\", probability estimates may be hard zeros\nand ones, so taking the logarithm is not possible.\n\nSee ``predict_proba`` for details.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data for prediction.\n\nReturns\n-------\nT : array-like, shape (n_samples, n_classes)\n    Returns the log-probability of the sample for each class in the\n    model, where classes are ordered as they are in\n    `self.classes_`."
            }
          ],
          "fullDocstring": "Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n\nThis estimator implements regularized linear models with stochastic\ngradient descent (SGD) learning: the gradient of the loss is estimated\neach sample at a time and the model is updated along the way with a\ndecreasing strength schedule (aka learning rate). SGD allows minibatch\n(online/out-of-core) learning via the `partial_fit` method.\nFor best results using the default learning rate schedule, the data should\nhave zero mean and unit variance.\n\nThis implementation works with data represented as dense or sparse arrays\nof floating point values for the features. The model it fits can be\ncontrolled with the loss parameter; by default, it fits a linear support\nvector machine (SVM).\n\nThe regularizer is a penalty added to the loss function that shrinks model\nparameters towards the zero vector using either the squared euclidean norm\nL2 or the absolute norm L1 or a combination of both (Elastic Net). If the\nparameter update crosses the 0.0 value because of the regularizer, the\nupdate is truncated to 0.0 to allow for learning sparse models and achieve\nonline feature selection.\n\nRead more in the :ref:`User Guide <sgd>`.\n\nParameters\n----------\nloss : str, default='hinge'\n    The loss function to be used. Defaults to 'hinge', which gives a\n    linear SVM.\n\n    The possible options are 'hinge', 'log', 'modified_huber',\n    'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n\n    The 'log' loss gives logistic regression, a probabilistic classifier.\n    'modified_huber' is another smooth loss that brings tolerance to\n    outliers as well as probability estimates.\n    'squared_hinge' is like hinge but is quadratically penalized.\n    'perceptron' is the linear loss used by the perceptron algorithm.\n    The other losses are designed for regression but can be useful in\n    classification as well; see\n    :class:`~sklearn.linear_model.SGDRegressor` for a description.\n\n    More details about the losses formulas can be found in the\n    :ref:`User Guide <sgd_mathematical_formulation>`.\n\npenalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n    The penalty (aka regularization term) to be used. Defaults to 'l2'\n    which is the standard regularizer for linear SVM models. 'l1' and\n    'elasticnet' might bring sparsity to the model (feature selection)\n    not achievable with 'l2'.\n\nalpha : float, default=0.0001\n    Constant that multiplies the regularization term. The higher the\n    value, the stronger the regularization.\n    Also used to compute the learning rate when set to `learning_rate` is\n    set to 'optimal'.\n\nl1_ratio : float, default=0.15\n    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n    Only used if `penalty` is 'elasticnet'.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-3\n    The stopping criterion. If it is not None, training will stop\n    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n    epochs.\n\n    .. versionadded:: 0.19\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : int, default=0\n    The verbosity level.\n\nepsilon : float, default=0.1\n    Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n    For 'huber', determines the threshold at which it becomes less\n    important to get the prediction exactly right.\n    For epsilon-insensitive, any differences between the current prediction\n    and the correct label are ignored if they are less than this threshold.\n\nn_jobs : int, default=None\n    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    Used for shuffling the data, when ``shuffle`` is set to ``True``.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nlearning_rate : str, default='optimal'\n    The learning rate schedule:\n\n    - 'constant': `eta = eta0`\n    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n      where t0 is chosen by a heuristic proposed by Leon Bottou.\n    - 'invscaling': `eta = eta0 / pow(t, power_t)`\n    - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n      Each time n_iter_no_change consecutive epochs fail to decrease the\n      training loss by tol or fail to increase validation score by tol if\n      early_stopping is True, the current learning rate is divided by 5.\n\n        .. versionadded:: 0.20\n            Added 'adaptive' option\n\neta0 : double, default=0.0\n    The initial learning rate for the 'constant', 'invscaling' or\n    'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n    the default schedule 'optimal'.\n\npower_t : double, default=0.5\n    The exponent for inverse scaling learning rate [default 0.5].\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to True, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score returned by the `score` method is not\n    improving by at least tol for n_iter_no_change consecutive epochs.\n\n    .. versionadded:: 0.20\n        Added 'early_stopping' option\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if `early_stopping` is True.\n\n    .. versionadded:: 0.20\n        Added 'validation_fraction' option\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before early stopping.\n\n    .. versionadded:: 0.20\n        Added 'n_iter_no_change' option\n\nclass_weight : dict, {class_label: weight} or \"balanced\", default=None\n    Preset for the class_weight fit parameter.\n\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n    If a dynamic learning rate is used, the learning rate is adapted\n    depending on the number of samples already seen. Calling ``fit`` resets\n    this counter, while ``partial_fit`` will result in increasing the\n    existing counter.\n\naverage : bool or int, default=False\n    When set to True, computes the averaged SGD weights accross all\n    updates and stores the result in the ``coef_`` attribute. If set to\n    an int greater than 1, averaging will begin once the total number of\n    samples seen reaches `average`. So ``average=10`` will begin\n    averaging after seeing 10 samples.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n    Weights assigned to the features.\n\nintercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n    Constants in decision function.\n\nn_iter_ : int\n    The actual number of iterations before reaching the stopping criterion.\n    For multiclass fits, it is the maximum over every binary fit.\n\nloss_function_ : concrete ``LossFunction``\n\nclasses_ : array of shape (n_classes,)\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples)``.\n\nSee Also\n--------\nsklearn.svm.LinearSVC : Linear support vector classification.\nLogisticRegression : Logistic regression.\nPerceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n    ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n    penalty=None)``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import SGDClassifier\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.pipeline import make_pipeline\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> Y = np.array([1, 1, 2, 2])\n>>> # Always scale the input. The most convenient way is to use a pipeline.\n>>> clf = make_pipeline(StandardScaler(),\n...                     SGDClassifier(max_iter=1000, tol=1e-3))\n>>> clf.fit(X, Y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('sgdclassifier', SGDClassifier())])\n>>> print(clf.predict([[-0.8, -1]]))\n[1]"
        },
        {
          "name": "SGDRegressor",
          "decorators": [],
          "superclasses": [
            "BaseSGDRegressor"
          ],
          "methods": [],
          "fullDocstring": "Linear model fitted by minimizing a regularized empirical loss with SGD\n\nSGD stands for Stochastic Gradient Descent: the gradient of the loss is\nestimated each sample at a time and the model is updated along the way with\na decreasing strength schedule (aka learning rate).\n\nThe regularizer is a penalty added to the loss function that shrinks model\nparameters towards the zero vector using either the squared euclidean norm\nL2 or the absolute norm L1 or a combination of both (Elastic Net). If the\nparameter update crosses the 0.0 value because of the regularizer, the\nupdate is truncated to 0.0 to allow for learning sparse models and achieve\nonline feature selection.\n\nThis implementation works with data represented as dense numpy arrays of\nfloating point values for the features.\n\nRead more in the :ref:`User Guide <sgd>`.\n\nParameters\n----------\nloss : str, default='squared_loss'\n    The loss function to be used. The possible values are 'squared_loss',\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n\n    The 'squared_loss' refers to the ordinary least squares fit.\n    'huber' modifies 'squared_loss' to focus less on getting outliers\n    correct by switching from squared to linear loss past a distance of\n    epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n    linear past that; this is the loss function used in SVR.\n    'squared_epsilon_insensitive' is the same but becomes squared loss past\n    a tolerance of epsilon.\n\n    More details about the losses formulas can be found in the\n    :ref:`User Guide <sgd_mathematical_formulation>`.\n\npenalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n    The penalty (aka regularization term) to be used. Defaults to 'l2'\n    which is the standard regularizer for linear SVM models. 'l1' and\n    'elasticnet' might bring sparsity to the model (feature selection)\n    not achievable with 'l2'.\n\nalpha : float, default=0.0001\n    Constant that multiplies the regularization term. The higher the\n    value, the stronger the regularization.\n    Also used to compute the learning rate when set to `learning_rate` is\n    set to 'optimal'.\n\nl1_ratio : float, default=0.15\n    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n    Only used if `penalty` is 'elasticnet'.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-3\n    The stopping criterion. If it is not None, training will stop\n    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n    epochs.\n\n    .. versionadded:: 0.19\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : int, default=0\n    The verbosity level.\n\nepsilon : float, default=0.1\n    Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n    For 'huber', determines the threshold at which it becomes less\n    important to get the prediction exactly right.\n    For epsilon-insensitive, any differences between the current prediction\n    and the correct label are ignored if they are less than this threshold.\n\nrandom_state : int, RandomState instance, default=None\n    Used for shuffling the data, when ``shuffle`` is set to ``True``.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nlearning_rate : string, default='invscaling'\n    The learning rate schedule:\n\n    - 'constant': `eta = eta0`\n    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n      where t0 is chosen by a heuristic proposed by Leon Bottou.\n    - 'invscaling': `eta = eta0 / pow(t, power_t)`\n    - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n      Each time n_iter_no_change consecutive epochs fail to decrease the\n      training loss by tol or fail to increase validation score by tol if\n      early_stopping is True, the current learning rate is divided by 5.\n\n        .. versionadded:: 0.20\n            Added 'adaptive' option\n\neta0 : double, default=0.01\n    The initial learning rate for the 'constant', 'invscaling' or\n    'adaptive' schedules. The default value is 0.01.\n\npower_t : double, default=0.25\n    The exponent for inverse scaling learning rate.\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to True, it will automatically set aside\n    a fraction of training data as validation and terminate\n    training when validation score returned by the `score` method is not\n    improving by at least `tol` for `n_iter_no_change` consecutive\n    epochs.\n\n    .. versionadded:: 0.20\n        Added 'early_stopping' option\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if `early_stopping` is True.\n\n    .. versionadded:: 0.20\n        Added 'validation_fraction' option\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before early stopping.\n\n    .. versionadded:: 0.20\n        Added 'n_iter_no_change' option\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n    If a dynamic learning rate is used, the learning rate is adapted\n    depending on the number of samples already seen. Calling ``fit`` resets\n    this counter, while ``partial_fit``  will result in increasing the\n    existing counter.\n\naverage : bool or int, default=False\n    When set to True, computes the averaged SGD weights accross all\n    updates and stores the result in the ``coef_`` attribute. If set to\n    an int greater than 1, averaging will begin once the total number of\n    samples seen reaches `average`. So ``average=10`` will begin\n    averaging after seeing 10 samples.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,)\n    Weights assigned to the features.\n\nintercept_ : ndarray of shape (1,)\n    The intercept term.\n\naverage_coef_ : ndarray of shape (n_features,)\n    Averaged weights assigned to the features. Only available\n    if ``average=True``.\n\n    .. deprecated:: 0.23\n        Attribute ``average_coef_`` was deprecated\n        in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n\naverage_intercept_ : ndarray of shape (1,)\n    The averaged intercept term. Only available if ``average=True``.\n\n    .. deprecated:: 0.23\n        Attribute ``average_intercept_`` was deprecated\n        in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n\nn_iter_ : int\n    The actual number of iterations before reaching the stopping criterion.\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples)``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import SGDRegressor\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> # Always scale the input. The most convenient way is to use a pipeline.\n>>> reg = make_pipeline(StandardScaler(),\n...                     SGDRegressor(max_iter=1000, tol=1e-3))\n>>> reg.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('sgdregressor', SGDRegressor())])\n\nSee Also\n--------\nRidge, ElasticNet, Lasso, sklearn.svm.SVR"
        }
      ],
      "functions": [
        {
          "name": "fit_binary",
          "decorators": [],
          "parameters": [
            {
              "name": "est",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The estimator to fit"
            },
            {
              "name": "i",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Index of the positive class"
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training data"
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values"
            },
            {
              "name": "alpha",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The regularization parameter"
            },
            {
              "name": "C",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Maximum step size for passive aggressive"
            },
            {
              "name": "learning_rate",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The learning rate. Accepted values are 'constant', 'optimal',\n'invscaling', 'pa1' and 'pa2'."
            },
            {
              "name": "max_iter",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The maximum number of iterations (epochs)"
            },
            {
              "name": "pos_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The weight of the positive class"
            },
            {
              "name": "neg_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The weight of the negative class"
            },
            {
              "name": "sample_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The weight of each sample"
            },
            {
              "name": "validation_mask",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Precomputed validation mask in case _fit_binary is called in the\ncontext of a one-vs-rest reduction."
            },
            {
              "name": "random_state",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby `np.random`."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Fit a single binary classifier.\n\nThe i'th class is considered the \"positive\" class.\n\nParameters\n----------\nest : Estimator object\n    The estimator to fit\n\ni : int\n    Index of the positive class\n\nX : numpy array or sparse matrix of shape [n_samples,n_features]\n    Training data\n\ny : numpy array of shape [n_samples, ]\n    Target values\n\nalpha : float\n    The regularization parameter\n\nC : float\n    Maximum step size for passive aggressive\n\nlearning_rate : string\n    The learning rate. Accepted values are 'constant', 'optimal',\n    'invscaling', 'pa1' and 'pa2'.\n\nmax_iter : int\n    The maximum number of iterations (epochs)\n\npos_weight : float\n    The weight of the positive class\n\nneg_weight : float\n    The weight of the negative class\n\nsample_weight : numpy array of shape [n_samples, ]\n    The weight of each sample\n\nvalidation_mask : numpy array of shape [n_samples, ], default=None\n    Precomputed validation mask in case _fit_binary is called in the\n    context of a one-vs-rest reduction.\n\nrandom_state : int, RandomState instance, default=None\n    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`."
        }
      ]
    },
    {
      "name": "sklearn.linear_model._theil_sen",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "itertools",
          "declaration": "combinations",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy.linalg.lapack",
          "declaration": "get_lapack_funcs",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "binom",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearModel",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "TheilSenRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit linear model.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Training data.\ny : ndarray of shape (n_samples,)\n    Target values.\n\nReturns\n-------\nself : returns an instance of self."
            }
          ],
          "fullDocstring": "Theil-Sen Estimator: robust multivariate regression model.\n\nThe algorithm calculates least square solutions on subsets with size\nn_subsamples of the samples in X. Any value of n_subsamples between the\nnumber of features and samples leads to an estimator with a compromise\nbetween robustness and efficiency. Since the number of least square\nsolutions is \"n_samples choose n_subsamples\", it can be extremely large\nand can therefore be limited with max_subpopulation. If this limit is\nreached, the subsets are chosen randomly. In a final step, the spatial\nmedian (or L1 median) is calculated of all least square solutions.\n\nRead more in the :ref:`User Guide <theil_sen_regression>`.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nmax_subpopulation : int, default=1e4\n    Instead of computing with a set of cardinality 'n choose k', where n is\n    the number of samples and k is the number of subsamples (at least\n    number of features), consider only a stochastic subpopulation of a\n    given maximal size if 'n choose k' is larger than max_subpopulation.\n    For other than small problem sizes this parameter will determine\n    memory usage and runtime if n_subsamples is not changed.\n\nn_subsamples : int, default=None\n    Number of samples to calculate the parameters. This is at least the\n    number of features (plus 1 if fit_intercept=True) and the number of\n    samples as a maximum. A lower number leads to a higher breakdown\n    point and a low efficiency while a high number leads to a low\n    breakdown point and a high efficiency. If None, take the\n    minimum number of subsamples leading to maximal robustness.\n    If n_subsamples is set to n_samples, Theil-Sen is identical to least\n    squares.\n\nmax_iter : int, default=300\n    Maximum number of iterations for the calculation of spatial median.\n\ntol : float, default=1.e-3\n    Tolerance when calculating spatial median.\n\nrandom_state : int, RandomState instance or None, default=None\n    A random number generator instance to define the state of the random\n    permutations generator. Pass an int for reproducible output across\n    multiple function calls.\n    See :term:`Glossary <random_state>`\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    Verbose mode when fitting the model.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,)\n    Coefficients of the regression model (median of distribution).\n\nintercept_ : float\n    Estimated intercept of regression model.\n\nbreakdown_ : float\n    Approximated breakdown point.\n\nn_iter_ : int\n    Number of iterations needed for the spatial median.\n\nn_subpopulation_ : int\n    Number of combinations taken into account from 'n choose k', where n is\n    the number of samples and k is the number of subsamples.\n\nExamples\n--------\n>>> from sklearn.linear_model import TheilSenRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9884...\n>>> reg.predict(X[:1,])\narray([-31.5871...])\n\nReferences\n----------\n- Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n  Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n  http://home.olemiss.edu/~xdang/papers/MTSE.pdf"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.linear_model.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "sklearn._build_utils",
          "declaration": "gen_from_templates",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.manifold",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._isomap",
          "declaration": "Isomap",
          "alias": null
        },
        {
          "module": "sklearn._locally_linear",
          "declaration": "LocallyLinearEmbedding",
          "alias": null
        },
        {
          "module": "sklearn._locally_linear",
          "declaration": "locally_linear_embedding",
          "alias": null
        },
        {
          "module": "sklearn._mds",
          "declaration": "MDS",
          "alias": null
        },
        {
          "module": "sklearn._mds",
          "declaration": "smacof",
          "alias": null
        },
        {
          "module": "sklearn._spectral_embedding",
          "declaration": "SpectralEmbedding",
          "alias": null
        },
        {
          "module": "sklearn._spectral_embedding",
          "declaration": "spectral_embedding",
          "alias": null
        },
        {
          "module": "sklearn._t_sne",
          "declaration": "TSNE",
          "alias": null
        },
        {
          "module": "sklearn._t_sne",
          "declaration": "trustworthiness",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.manifold._isomap",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.decomposition",
          "declaration": "KernelPCA",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "kneighbors_graph",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "KernelCenterer",
          "alias": null
        },
        {
          "module": "sklearn.utils.graph",
          "declaration": "graph_shortest_path",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "Isomap",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "reconstruction_error",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "reconstruction_error",
                  "type": "float",
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the reconstruction error for the embedding.\n\nReturns\n-------\nreconstruction_error : float\n\nNotes\n-----\nThe cost function of an isomap embedding is\n\n``E = frobenius_norm[K(D) - K(D_fit)] / n_samples``\n\nWhere D is the matrix of distances for the input data X,\nD_fit is the matrix of distances for the output embedding X_fit,\nand K is the isomap kernel:\n\n``K(D) = -0.5 * (I - 1/n_samples) * D^2 * (I - 1/n_samples)``"
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample data, shape = (n_samples, n_features), in the form of a\nnumpy array, sparse graph, precomputed tree, or NearestNeighbors\nobject."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the embedding vectors for data X\n\nParameters\n----------\nX : {array-like, sparse graph, BallTree, KDTree, NearestNeighbors}\n    Sample data, shape = (n_samples, n_features), in the form of a\n    numpy array, sparse graph, precomputed tree, or NearestNeighbors\n    object.\n\ny : Ignored\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model from data in X and transform X.\n\nParameters\n----------\nX : {array-like, sparse graph, BallTree, KDTree}\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nX_new : array-like, shape (n_samples, n_components)"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "If neighbors_algorithm='precomputed', X is assumed to be a\ndistance matrix or a sparse graph of shape\n(n_queries, n_samples_fit)."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X.\n\nThis is implemented by linking the points X into the graph of geodesic\ndistances of the training data. First the `n_neighbors` nearest\nneighbors of X are found in the training data, and from these the\nshortest geodesic distances from each point in X to each point in\nthe training data are computed in order to construct the kernel.\nThe embedding of X is the projection of this kernel onto the\nembedding vectors of the training set.\n\nParameters\n----------\nX : array-like, shape (n_queries, n_features)\n    If neighbors_algorithm='precomputed', X is assumed to be a\n    distance matrix or a sparse graph of shape\n    (n_queries, n_samples_fit).\n\nReturns\n-------\nX_new : array-like, shape (n_queries, n_components)"
            }
          ],
          "fullDocstring": "Isomap Embedding\n\nNon-linear dimensionality reduction through Isometric Mapping\n\nRead more in the :ref:`User Guide <isomap>`.\n\nParameters\n----------\nn_neighbors : int, default=5\n    number of neighbors to consider for each point.\n\nn_components : int, default=2\n    number of coordinates for the manifold\n\neigen_solver : {'auto', 'arpack', 'dense'}, default='auto'\n    'auto' : Attempt to choose the most efficient solver\n    for the given problem.\n\n    'arpack' : Use Arnoldi decomposition to find the eigenvalues\n    and eigenvectors.\n\n    'dense' : Use a direct solver (i.e. LAPACK)\n    for the eigenvalue decomposition.\n\ntol : float, default=0\n    Convergence tolerance passed to arpack or lobpcg.\n    not used if eigen_solver == 'dense'.\n\nmax_iter : int, default=None\n    Maximum number of iterations for the arpack solver.\n    not used if eigen_solver == 'dense'.\n\npath_method : {'auto', 'FW', 'D'}, default='auto'\n    Method to use in finding shortest path.\n\n    'auto' : attempt to choose the best algorithm automatically.\n\n    'FW' : Floyd-Warshall algorithm.\n\n    'D' : Dijkstra's algorithm.\n\nneighbors_algorithm : {'auto', 'brute', 'kd_tree', 'ball_tree'},                           default='auto'\n    Algorithm to use for nearest neighbors search,\n    passed to neighbors.NearestNeighbors instance.\n\nn_jobs : int or None, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmetric : string, or callable, default=\"minkowski\"\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string or callable, it must be one of\n    the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n    its metric parameter.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square. X may be a :term:`Glossary <sparse graph>`.\n\n    .. versionadded:: 0.22\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    .. versionadded:: 0.22\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nembedding_ : array-like, shape (n_samples, n_components)\n    Stores the embedding vectors.\n\nkernel_pca_ : object\n    :class:`~sklearn.decomposition.KernelPCA` object used to implement the\n    embedding.\n\nnbrs_ : sklearn.neighbors.NearestNeighbors instance\n    Stores nearest neighbors instance, including BallTree or KDtree\n    if applicable.\n\ndist_matrix_ : array-like, shape (n_samples, n_samples)\n    Stores the geodesic distance matrix of training data.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.manifold import Isomap\n>>> X, _ = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> embedding = Isomap(n_components=2)\n>>> X_transformed = embedding.fit_transform(X[:100])\n>>> X_transformed.shape\n(100, 2)\n\nReferences\n----------\n\n.. [1] Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. A global geometric\n       framework for nonlinear dimensionality reduction. Science 290 (5500)"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.manifold._locally_linear",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy.linalg",
          "declaration": "eigh",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "qr",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "solve",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "svd",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csr_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "eye",
          "alias": null
        },
        {
          "module": "scipy.sparse.linalg",
          "declaration": "eigsh",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "_UnstableArchMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils._arpack",
          "declaration": "_init_arpack_v0",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "stable_cumsum",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "FLOAT_DTYPES",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "LocallyLinearEmbedding",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "_UnstableArchMixin",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "training set."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the embedding vectors for data X\n\nParameters\n----------\nX : array-like of shape [n_samples, n_features]\n    training set.\n\ny : Ignored\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "training set."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the embedding vectors for data X and transform X.\n\nParameters\n----------\nX : array-like of shape [n_samples, n_features]\n    training set.\n\ny : Ignored\n\nReturns\n-------\nX_new : array-like, shape (n_samples, n_components)"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform new points into embedding space.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nReturns\n-------\nX_new : array, shape = [n_samples, n_components]\n\nNotes\n-----\nBecause of scaling performed by this method, it is discouraged to use\nit together with methods that are not scale-invariant (like SVMs)"
            }
          ],
          "fullDocstring": "Locally Linear Embedding\n\nRead more in the :ref:`User Guide <locally_linear_embedding>`.\n\nParameters\n----------\nn_neighbors : int, default=5\n    number of neighbors to consider for each point.\n\nn_components : int, default=2\n    number of coordinates for the manifold\n\nreg : float, default=1e-3\n    regularization constant, multiplies the trace of the local covariance\n    matrix of the distances.\n\neigen_solver : {'auto', 'arpack', 'dense'}, default='auto'\n    auto : algorithm will attempt to choose the best method for input data\n\n    arpack : use arnoldi iteration in shift-invert mode.\n                For this method, M may be a dense matrix, sparse matrix,\n                or general linear operator.\n                Warning: ARPACK can be unstable for some problems.  It is\n                best to try several random seeds in order to check results.\n\n    dense  : use standard dense matrix operations for the eigenvalue\n                decomposition.  For this method, M must be an array\n                or matrix type.  This method should be avoided for\n                large problems.\n\ntol : float, default=1e-6\n    Tolerance for 'arpack' method\n    Not used if eigen_solver=='dense'.\n\nmax_iter : int, default=100\n    maximum number of iterations for the arpack solver.\n    Not used if eigen_solver=='dense'.\n\nmethod : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'\n    standard : use the standard locally linear embedding algorithm.  see\n               reference [1]\n    hessian  : use the Hessian eigenmap method. This method requires\n               ``n_neighbors > n_components * (1 + (n_components + 1) / 2``\n               see reference [2]\n    modified : use the modified locally linear embedding algorithm.\n               see reference [3]\n    ltsa     : use local tangent space alignment algorithm\n               see reference [4]\n\nhessian_tol : float, default=1e-4\n    Tolerance for Hessian eigenmapping method.\n    Only used if ``method == 'hessian'``\n\nmodified_tol : float, default=1e-12\n    Tolerance for modified LLE method.\n    Only used if ``method == 'modified'``\n\nneighbors_algorithm : {'auto', 'brute', 'kd_tree', 'ball_tree'},                           default='auto'\n    algorithm to use for nearest neighbors search,\n    passed to neighbors.NearestNeighbors instance\n\nrandom_state : int, RandomState instance, default=None\n    Determines the random number generator when\n    ``eigen_solver`` == 'arpack'. Pass an int for reproducible results\n    across multiple function calls. See :term: `Glossary <random_state>`.\n\nn_jobs : int or None, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nembedding_ : array-like, shape [n_samples, n_components]\n    Stores the embedding vectors\n\nreconstruction_error_ : float\n    Reconstruction error associated with `embedding_`\n\nnbrs_ : NearestNeighbors object\n    Stores nearest neighbors instance, including BallTree or KDtree\n    if applicable.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.manifold import LocallyLinearEmbedding\n>>> X, _ = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> embedding = LocallyLinearEmbedding(n_components=2)\n>>> X_transformed = embedding.fit_transform(X[:100])\n>>> X_transformed.shape\n(100, 2)\n\nReferences\n----------\n\n.. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n    by locally linear embedding.  Science 290:2323 (2000).\n.. [2] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally\n    linear embedding techniques for high-dimensional data.\n    Proc Natl Acad Sci U S A.  100:5591 (2003).\n.. [3] Zhang, Z. & Wang, J. MLLE: Modified Locally Linear\n    Embedding Using Multiple Weights.\n    http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382\n.. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n    dimensionality reduction via tangent space alignment.\n    Journal of Shanghai Univ.  8:406 (2004)"
        }
      ],
      "functions": [
        {
          "name": "barycenter_kneighbors_graph",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Sample data, shape = (n_samples, n_features), in the form of a\nnumpy array or a NearestNeighbors object."
            },
            {
              "name": "n_neighbors",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of neighbors for each sample."
            },
            {
              "name": "reg",
              "type": "Any",
              "hasDefault": true,
              "default": "0.001",
              "limitation": null,
              "ignored": false,
              "description": "Amount of regularization when solving the least-squares\nproblem. Only relevant if mode='barycenter'. If None, use the\ndefault."
            },
            {
              "name": "n_jobs",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The number of parallel jobs to run for neighbors search.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
            }
          ],
          "results": [
            {
              "name": "A",
              "type": null,
              "description": "A[i, j] is assigned the weight of edge that connects i to j."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the barycenter weighted graph of k-Neighbors for points in X\n\nParameters\n----------\nX : {array-like, NearestNeighbors}\n    Sample data, shape = (n_samples, n_features), in the form of a\n    numpy array or a NearestNeighbors object.\n\nn_neighbors : int\n    Number of neighbors for each sample.\n\nreg : float, default=1e-3\n    Amount of regularization when solving the least-squares\n    problem. Only relevant if mode='barycenter'. If None, use the\n    default.\n\nn_jobs : int or None, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nA : sparse matrix in CSR format, shape = [n_samples, n_samples]\n    A[i, j] is assigned the weight of edge that connects i to j.\n\nSee Also\n--------\nsklearn.neighbors.kneighbors_graph\nsklearn.neighbors.radius_neighbors_graph"
        },
        {
          "name": "barycenter_weights",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "indices",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Indices of the points in Y used to compute the barycenter"
            },
            {
              "name": "reg",
              "type": "Any",
              "hasDefault": true,
              "default": "0.001",
              "limitation": null,
              "ignored": false,
              "description": "amount of regularization to add for the problem to be\nwell-posed in the case of n_neighbors > n_dim"
            }
          ],
          "results": [
            {
              "name": "B",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute barycenter weights of X from Y along the first axis\n\nWe estimate the weights to assign to each point in Y[indices] to recover\nthe point X[i]. The barycenter weights sum to 1.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_dim)\n\nY : array-like, shape (n_samples, n_dim)\n\nindices : array-like, shape (n_samples, n_dim)\n        Indices of the points in Y used to compute the barycenter\n\nreg : float, default=1e-3\n    amount of regularization to add for the problem to be\n    well-posed in the case of n_neighbors > n_dim\n\nReturns\n-------\nB : array-like, shape (n_samples, n_neighbors)\n\nNotes\n-----\nSee developers note for more information."
        },
        {
          "name": "locally_linear_embedding",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Sample data, shape = (n_samples, n_features), in the form of a\nnumpy array or a NearestNeighbors object."
            }
          ],
          "results": [
            {
              "name": "Y",
              "type": null,
              "description": "Embedding vectors."
            },
            {
              "name": "squared_error",
              "type": "float",
              "description": "Reconstruction error for the embedding vectors. Equivalent to\n``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Perform a Locally Linear Embedding analysis on the data.\n\nRead more in the :ref:`User Guide <locally_linear_embedding>`.\n\nParameters\n----------\nX : {array-like, NearestNeighbors}\n    Sample data, shape = (n_samples, n_features), in the form of a\n    numpy array or a NearestNeighbors object.\n\nn_neighbors : int\n    number of neighbors to consider for each point.\n\nn_components : int\n    number of coordinates for the manifold.\n\nreg : float, default=1e-3\n    regularization constant, multiplies the trace of the local covariance\n    matrix of the distances.\n\neigen_solver : {'auto', 'arpack', 'dense'}, default='auto'\n    auto : algorithm will attempt to choose the best method for input data\n\n    arpack : use arnoldi iteration in shift-invert mode.\n                For this method, M may be a dense matrix, sparse matrix,\n                or general linear operator.\n                Warning: ARPACK can be unstable for some problems.  It is\n                best to try several random seeds in order to check results.\n\n    dense  : use standard dense matrix operations for the eigenvalue\n                decomposition.  For this method, M must be an array\n                or matrix type.  This method should be avoided for\n                large problems.\n\ntol : float, default=1e-6\n    Tolerance for 'arpack' method\n    Not used if eigen_solver=='dense'.\n\nmax_iter : int, default=100\n    maximum number of iterations for the arpack solver.\n\nmethod : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'\n    standard : use the standard locally linear embedding algorithm.\n               see reference [1]_\n    hessian  : use the Hessian eigenmap method.  This method requires\n               n_neighbors > n_components * (1 + (n_components + 1) / 2.\n               see reference [2]_\n    modified : use the modified locally linear embedding algorithm.\n               see reference [3]_\n    ltsa     : use local tangent space alignment algorithm\n               see reference [4]_\n\nhessian_tol : float, default=1e-4\n    Tolerance for Hessian eigenmapping method.\n    Only used if method == 'hessian'\n\nmodified_tol : float, default=1e-12\n    Tolerance for modified LLE method.\n    Only used if method == 'modified'\n\nrandom_state : int, RandomState instance, default=None\n    Determines the random number generator when ``solver`` == 'arpack'.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nn_jobs : int or None, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nY : array-like, shape [n_samples, n_components]\n    Embedding vectors.\n\nsquared_error : float\n    Reconstruction error for the embedding vectors. Equivalent to\n    ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.\n\nReferences\n----------\n\n.. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n    by locally linear embedding.  Science 290:2323 (2000).\n.. [2] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally\n    linear embedding techniques for high-dimensional data.\n    Proc Natl Acad Sci U S A.  100:5591 (2003).\n.. [3] Zhang, Z. & Wang, J. MLLE: Modified Locally Linear\n    Embedding Using Multiple Weights.\n    http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382\n.. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n    dimensionality reduction via tangent space alignment.\n    Journal of Shanghai Univ.  8:406 (2004)"
        },
        {
          "name": "null_space",
          "decorators": [],
          "parameters": [
            {
              "name": "M",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input covariance matrix: should be symmetric positive semi-definite"
            },
            {
              "name": "k",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of eigenvalues/vectors to return"
            },
            {
              "name": "k_skip",
              "type": "Any",
              "hasDefault": true,
              "default": "1",
              "limitation": null,
              "ignored": false,
              "description": "Number of low eigenvalues to skip."
            },
            {
              "name": "eigen_solver",
              "type": "Any",
              "hasDefault": true,
              "default": "arpack",
              "limitation": null,
              "ignored": false,
              "description": "auto : algorithm will attempt to choose the best method for input data\narpack : use arnoldi iteration in shift-invert mode.\n            For this method, M may be a dense matrix, sparse matrix,\n            or general linear operator.\n            Warning: ARPACK can be unstable for some problems.  It is\n            best to try several random seeds in order to check results.\ndense  : use standard dense matrix operations for the eigenvalue\n            decomposition.  For this method, M must be an array\n            or matrix type.  This method should be avoided for\n            large problems."
            },
            {
              "name": "tol",
              "type": "Any",
              "hasDefault": true,
              "default": "1e-06",
              "limitation": null,
              "ignored": false,
              "description": "Tolerance for 'arpack' method.\nNot used if eigen_solver=='dense'."
            },
            {
              "name": "max_iter",
              "type": "Any",
              "hasDefault": true,
              "default": "100",
              "limitation": null,
              "ignored": false,
              "description": "Maximum number of iterations for 'arpack' method.\nNot used if eigen_solver=='dense'"
            },
            {
              "name": "random_state",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Determines the random number generator when ``solver`` == 'arpack'.\nPass an int for reproducible results across multiple function calls.\nSee :term: `Glossary <random_state>`."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Find the null space of a matrix M.\n\nParameters\n----------\nM : {array, matrix, sparse matrix, LinearOperator}\n    Input covariance matrix: should be symmetric positive semi-definite\n\nk : int\n    Number of eigenvalues/vectors to return\n\nk_skip : int, default=1\n    Number of low eigenvalues to skip.\n\neigen_solver : {'auto', 'arpack', 'dense'}, default='arpack'\n    auto : algorithm will attempt to choose the best method for input data\n    arpack : use arnoldi iteration in shift-invert mode.\n                For this method, M may be a dense matrix, sparse matrix,\n                or general linear operator.\n                Warning: ARPACK can be unstable for some problems.  It is\n                best to try several random seeds in order to check results.\n    dense  : use standard dense matrix operations for the eigenvalue\n                decomposition.  For this method, M must be an array\n                or matrix type.  This method should be avoided for\n                large problems.\n\ntol : float, default=1e-6\n    Tolerance for 'arpack' method.\n    Not used if eigen_solver=='dense'.\n\nmax_iter : int, default=100\n    Maximum number of iterations for 'arpack' method.\n    Not used if eigen_solver=='dense'\n\nrandom_state : int, RandomState instance, default=None\n    Determines the random number generator when ``solver`` == 'arpack'.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`."
        }
      ]
    },
    {
      "name": "sklearn.manifold._mds",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.isotonic",
          "declaration": "IsotonicRegression",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "euclidean_distances",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_symmetric",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "MDS",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data. If ``dissimilarity=='precomputed'``, the input should\nbe the dissimilarity matrix."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Starting configuration of the embedding to initialize the SMACOF\nalgorithm. By default, the algorithm is initialized with a randomly\nchosen array."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Computes the position of the points in the embedding space.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n    Input data. If ``dissimilarity=='precomputed'``, the input should\n    be the dissimilarity matrix.\n\ny : Ignored\n\ninit : ndarray of shape (n_samples,), default=None\n    Starting configuration of the embedding to initialize the SMACOF\n    algorithm. By default, the algorithm is initialized with a randomly\n    chosen array."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data. If ``dissimilarity=='precomputed'``, the input should\nbe the dissimilarity matrix."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "init",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Starting configuration of the embedding to initialize the SMACOF\nalgorithm. By default, the algorithm is initialized with a randomly\nchosen array."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the data from X, and returns the embedded coordinates.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n    Input data. If ``dissimilarity=='precomputed'``, the input should\n    be the dissimilarity matrix.\n\ny : Ignored\n\ninit : ndarray of shape (n_samples,), default=None\n    Starting configuration of the embedding to initialize the SMACOF\n    algorithm. By default, the algorithm is initialized with a randomly\n    chosen array."
            }
          ],
          "fullDocstring": "Multidimensional scaling.\n\nRead more in the :ref:`User Guide <multidimensional_scaling>`.\n\nParameters\n----------\nn_components : int, default=2\n    Number of dimensions in which to immerse the dissimilarities.\n\nmetric : bool, default=True\n    If ``True``, perform metric MDS; otherwise, perform nonmetric MDS.\n\nn_init : int, default=4\n    Number of times the SMACOF algorithm will be run with different\n    initializations. The final results will be the best output of the runs,\n    determined by the run with the smallest final stress.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the SMACOF algorithm for a single run.\n\nverbose : int, default=0\n    Level of verbosity.\n\neps : float, default=1e-3\n    Relative tolerance with respect to stress at which to declare\n    convergence.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. If multiple\n    initializations are used (``n_init``), each run of the algorithm is\n    computed in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the random number generator used to initialize the centers.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\ndissimilarity : {'euclidean', 'precomputed'}, default='euclidean'\n    Dissimilarity measure to use:\n\n    - 'euclidean':\n        Pairwise Euclidean distances between points in the dataset.\n\n    - 'precomputed':\n        Pre-computed dissimilarities are passed directly to ``fit`` and\n        ``fit_transform``.\n\nAttributes\n----------\nembedding_ : ndarray of shape (n_samples, n_components)\n    Stores the position of the dataset in the embedding space.\n\nstress_ : float\n    The final value of the stress (sum of squared distance of the\n    disparities and the distances for all constrained points).\n\ndissimilarity_matrix_ : ndarray of shape (n_samples, n_samples)\n    Pairwise dissimilarities between the points. Symmetric matrix that:\n\n    - either uses a custom dissimilarity matrix by setting `dissimilarity`\n      to 'precomputed';\n    - or constructs a dissimilarity matrix from data using\n      Euclidean distances.\n\nn_iter_ : int\n    The number of iterations corresponding to the best stress.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.manifold import MDS\n>>> X, _ = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> embedding = MDS(n_components=2)\n>>> X_transformed = embedding.fit_transform(X[:100])\n>>> X_transformed.shape\n(100, 2)\n\nReferences\n----------\n\"Modern Multidimensional Scaling - Theory and Applications\" Borg, I.;\nGroenen P. Springer Series in Statistics (1997)\n\n\"Nonmetric multidimensional scaling: a numerical method\" Kruskal, J.\nPsychometrika, 29 (1964)\n\n\"Multidimensional scaling by optimizing goodness of fit to a nonmetric\nhypothesis\" Kruskal, J. Psychometrika, 29, (1964)"
        }
      ],
      "functions": [
        {
          "name": "smacof",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "dissimilarities",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Pairwise dissimilarities between the points. Must be symmetric."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "Coordinates of the points in a ``n_components``-space."
            },
            {
              "name": "stress",
              "type": "float",
              "description": "The final value of the stress (sum of squared distance of the\ndisparities and the distances for all constrained points)."
            },
            {
              "name": "n_iter",
              "type": "int",
              "description": "The number of iterations corresponding to the best stress. Returned\nonly if ``return_n_iter`` is set to ``True``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes multidimensional scaling using the SMACOF algorithm.\n\nThe SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a\nmultidimensional scaling algorithm which minimizes an objective function\n(the *stress*) using a majorization technique. Stress majorization, also\nknown as the Guttman Transform, guarantees a monotone convergence of\nstress, and is more powerful than traditional techniques such as gradient\ndescent.\n\nThe SMACOF algorithm for metric MDS can summarized by the following steps:\n\n1. Set an initial start configuration, randomly or not.\n2. Compute the stress\n3. Compute the Guttman Transform\n4. Iterate 2 and 3 until convergence.\n\nThe nonmetric algorithm adds a monotonic regression step before computing\nthe stress.\n\nParameters\n----------\ndissimilarities : ndarray of shape (n_samples, n_samples)\n    Pairwise dissimilarities between the points. Must be symmetric.\n\nmetric : bool, default=True\n    Compute metric or nonmetric SMACOF algorithm.\n\nn_components : int, default=2\n    Number of dimensions in which to immerse the dissimilarities. If an\n    ``init`` array is provided, this option is overridden and the shape of\n    ``init`` is used to determine the dimensionality of the embedding\n    space.\n\ninit : ndarray of shape (n_samples, n_components), default=None\n    Starting configuration of the embedding to initialize the algorithm. By\n    default, the algorithm is initialized with a randomly chosen array.\n\nn_init : int, default=8\n    Number of times the SMACOF algorithm will be run with different\n    initializations. The final results will be the best output of the runs,\n    determined by the run with the smallest final stress. If ``init`` is\n    provided, this option is overridden and a single run is performed.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. If multiple\n    initializations are used (``n_init``), each run of the algorithm is\n    computed in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the SMACOF algorithm for a single run.\n\nverbose : int, default=0\n    Level of verbosity.\n\neps : float, default=1e-3\n    Relative tolerance with respect to stress at which to declare\n    convergence.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the random number generator used to initialize the centers.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_components)\n    Coordinates of the points in a ``n_components``-space.\n\nstress : float\n    The final value of the stress (sum of squared distance of the\n    disparities and the distances for all constrained points).\n\nn_iter : int\n    The number of iterations corresponding to the best stress. Returned\n    only if ``return_n_iter`` is set to ``True``.\n\nNotes\n-----\n\"Modern Multidimensional Scaling - Theory and Applications\" Borg, I.;\nGroenen P. Springer Series in Statistics (1997)\n\n\"Nonmetric multidimensional scaling: a numerical method\" Kruskal, J.\nPsychometrika, 29 (1964)\n\n\"Multidimensional scaling by optimizing goodness of fit to a nonmetric\nhypothesis\" Kruskal, J. Psychometrika, 29, (1964)"
        }
      ]
    },
    {
      "name": "sklearn.manifold._spectral_embedding",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy.linalg",
          "declaration": "eigh",
          "alias": null
        },
        {
          "module": "scipy.sparse.csgraph",
          "declaration": "connected_components",
          "alias": null
        },
        {
          "module": "scipy.sparse.csgraph",
          "declaration": "laplacian",
          "alias": "csgraph_laplacian"
        },
        {
          "module": "scipy.sparse.linalg",
          "declaration": "eigsh",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "rbf_kernel",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "kneighbors_graph",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_symmetric",
          "alias": null
        },
        {
          "module": "sklearn.utils._arpack",
          "declaration": "_init_arpack_v0",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "_deterministic_vector_sign_flip",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "lobpcg",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "SpectralEmbedding",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples\nand n_features is the number of features.\n\nIf affinity is \"precomputed\"\nX : {array-like, sparse matrix}, shape (n_samples, n_samples),\nInterpret X as precomputed adjacency graph computed from\nsamples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns the instance itself."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model from data in X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples\n    and n_features is the number of features.\n\n    If affinity is \"precomputed\"\n    X : {array-like, sparse matrix}, shape (n_samples, n_samples),\n    Interpret X as precomputed adjacency graph computed from\n    samples.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples\nand n_features is the number of features.\n\nIf affinity is \"precomputed\"\nX : {array-like, sparse matrix} of shape (n_samples, n_samples),\nInterpret X as precomputed adjacency graph computed from\nsamples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model from data in X and transform X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples\n    and n_features is the number of features.\n\n    If affinity is \"precomputed\"\n    X : {array-like, sparse matrix} of shape (n_samples, n_samples),\n    Interpret X as precomputed adjacency graph computed from\n    samples.\n\ny : Ignored\n\nReturns\n-------\nX_new : array-like of shape (n_samples, n_components)"
            }
          ],
          "fullDocstring": "Spectral embedding for non-linear dimensionality reduction.\n\nForms an affinity matrix given by the specified function and\napplies spectral decomposition to the corresponding graph laplacian.\nThe resulting transformation is given by the value of the\neigenvectors for each data point.\n\nNote : Laplacian Eigenmaps is the actual algorithm implemented here.\n\nRead more in the :ref:`User Guide <spectral_embedding>`.\n\nParameters\n----------\nn_components : int, default=2\n    The dimension of the projected subspace.\n\naffinity : {'nearest_neighbors', 'rbf', 'precomputed',                 'precomputed_nearest_neighbors'} or callable,                 default='nearest_neighbors'\n    How to construct the affinity matrix.\n     - 'nearest_neighbors' : construct the affinity matrix by computing a\n       graph of nearest neighbors.\n     - 'rbf' : construct the affinity matrix by computing a radial basis\n       function (RBF) kernel.\n     - 'precomputed' : interpret ``X`` as a precomputed affinity matrix.\n     - 'precomputed_nearest_neighbors' : interpret ``X`` as a sparse graph\n       of precomputed nearest neighbors, and constructs the affinity matrix\n       by selecting the ``n_neighbors`` nearest neighbors.\n     - callable : use passed in function as affinity\n       the function takes in data matrix (n_samples, n_features)\n       and return affinity matrix (n_samples, n_samples).\n\ngamma : float, default=None\n    Kernel coefficient for rbf kernel. If None, gamma will be set to\n    1/n_features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the random number generator used for the initialization of\n    the lobpcg eigenvectors when ``solver`` == 'amg'.  Pass an int for\n    reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\neigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n    The eigenvalue decomposition strategy to use. AMG requires pyamg\n    to be installed. It can be faster on very large, sparse problems.\n    If None, then ``'arpack'`` is used.\n\nn_neighbors : int, default=None\n    Number of nearest neighbors for nearest_neighbors graph building.\n    If None, n_neighbors will be set to max(n_samples/10, 1).\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nembedding_ : ndarray of shape (n_samples, n_components)\n    Spectral embedding of the training matrix.\n\naffinity_matrix_ : ndarray of shape (n_samples, n_samples)\n    Affinity_matrix constructed from samples or precomputed.\n\nn_neighbors_ : int\n    Number of nearest neighbors effectively used.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.manifold import SpectralEmbedding\n>>> X, _ = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> embedding = SpectralEmbedding(n_components=2)\n>>> X_transformed = embedding.fit_transform(X[:100])\n>>> X_transformed.shape\n(100, 2)\n\nReferences\n----------\n\n- A Tutorial on Spectral Clustering, 2007\n  Ulrike von Luxburg\n  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n- On Spectral Clustering: Analysis and an algorithm, 2001\n  Andrew Y. Ng, Michael I. Jordan, Yair Weiss\n  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100\n\n- Normalized cuts and image segmentation, 2000\n  Jianbo Shi, Jitendra Malik\n  http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324"
        }
      ],
      "functions": [
        {
          "name": "spectral_embedding",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "adjacency",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The adjacency matrix of the graph to embed."
            }
          ],
          "results": [
            {
              "name": "embedding",
              "type": null,
              "description": "The reduced samples."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Project the sample on the first eigenvectors of the graph Laplacian.\n\nThe adjacency matrix is used to compute a normalized graph Laplacian\nwhose spectrum (especially the eigenvectors associated to the\nsmallest eigenvalues) has an interpretation in terms of minimal\nnumber of cuts necessary to split the graph into comparably sized\ncomponents.\n\nThis embedding can also 'work' even if the ``adjacency`` variable is\nnot strictly the adjacency matrix of a graph but more generally\nan affinity or similarity matrix between samples (for instance the\nheat kernel of a euclidean distance matrix or a k-NN matrix).\n\nHowever care must taken to always make the affinity matrix symmetric\nso that the eigenvector decomposition works as expected.\n\nNote : Laplacian Eigenmaps is the actual algorithm implemented here.\n\nRead more in the :ref:`User Guide <spectral_embedding>`.\n\nParameters\n----------\nadjacency : {array-like, sparse graph} of shape (n_samples, n_samples)\n    The adjacency matrix of the graph to embed.\n\nn_components : int, default=8\n    The dimension of the projection subspace.\n\neigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n    The eigenvalue decomposition strategy to use. AMG requires pyamg\n    to be installed. It can be faster on very large, sparse problems,\n    but may also lead to instabilities. If None, then ``'arpack'`` is\n    used.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the random number generator used for the initialization of\n    the lobpcg eigenvectors decomposition when ``solver`` == 'amg'. Pass\n    an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\neigen_tol : float, default=0.0\n    Stopping criterion for eigendecomposition of the Laplacian matrix\n    when using arpack eigen_solver.\n\nnorm_laplacian : bool, default=True\n    If True, then compute normalized Laplacian.\n\ndrop_first : bool, default=True\n    Whether to drop the first eigenvector. For spectral embedding, this\n    should be True as the first eigenvector should be constant vector for\n    connected graph, but for spectral clustering, this should be kept as\n    False to retain the first eigenvector.\n\nReturns\n-------\nembedding : ndarray of shape (n_samples, n_components)\n    The reduced samples.\n\nNotes\n-----\nSpectral Embedding (Laplacian Eigenmaps) is most useful when the graph\nhas one connected component. If there graph has many components, the first\nfew eigenvectors will simply uncover the connected components of the graph.\n\nReferences\n----------\n* https://en.wikipedia.org/wiki/LOBPCG\n\n* Toward the Optimal Preconditioned Eigensolver: Locally Optimal\n  Block Preconditioned Conjugate Gradient Method\n  Andrew V. Knyazev\n  https://doi.org/10.1137%2FS1064827500366124"
        }
      ]
    },
    {
      "name": "sklearn.manifold._t_sne",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csr_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "scipy.spatial.distance",
          "declaration": "pdist",
          "alias": null
        },
        {
          "module": "scipy.spatial.distance",
          "declaration": "squareform",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.decomposition",
          "declaration": "PCA",
          "alias": null
        },
        {
          "module": "sklearn.manifold",
          "declaration": "_barnes_hut_tsne",
          "alias": null
        },
        {
          "module": "sklearn.manifold",
          "declaration": "_utils",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_distances",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils._openmp_helpers",
          "declaration": "_openmp_effective_n_threads",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_non_negative",
          "alias": null
        },
        {
          "module": "time",
          "declaration": "time",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "TSNE",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "If the metric is 'precomputed' X must be a square distance\nmatrix. Otherwise it contains a sample per row. If the method\nis 'exact', X may be a sparse matrix of type 'csr', 'csc'\nor 'coo'. If the method is 'barnes_hut' and the metric is\n'precomputed', X may be a precomputed sparse graph."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Embedding of the training data in low-dimensional space."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit X into an embedded space and return that transformed\noutput.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)\n    If the metric is 'precomputed' X must be a square distance\n    matrix. Otherwise it contains a sample per row. If the method\n    is 'exact', X may be a sparse matrix of type 'csr', 'csc'\n    or 'coo'. If the method is 'barnes_hut' and the metric is\n    'precomputed', X may be a precomputed sparse graph.\n\ny : Ignored\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Embedding of the training data in low-dimensional space."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "If the metric is 'precomputed' X must be a square distance\nmatrix. Otherwise it contains a sample per row. If the method\nis 'exact', X may be a sparse matrix of type 'csr', 'csc'\nor 'coo'. If the method is 'barnes_hut' and the metric is\n'precomputed', X may be a precomputed sparse graph."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit X into an embedded space.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)\n    If the metric is 'precomputed' X must be a square distance\n    matrix. Otherwise it contains a sample per row. If the method\n    is 'exact', X may be a sparse matrix of type 'csr', 'csc'\n    or 'coo'. If the method is 'barnes_hut' and the metric is\n    'precomputed', X may be a precomputed sparse graph.\n\ny : Ignored"
            }
          ],
          "fullDocstring": "t-distributed Stochastic Neighbor Embedding.\n\nt-SNE [1] is a tool to visualize high-dimensional data. It converts\nsimilarities between data points to joint probabilities and tries\nto minimize the Kullback-Leibler divergence between the joint\nprobabilities of the low-dimensional embedding and the\nhigh-dimensional data. t-SNE has a cost function that is not convex,\ni.e. with different initializations we can get different results.\n\nIt is highly recommended to use another dimensionality reduction\nmethod (e.g. PCA for dense data or TruncatedSVD for sparse data)\nto reduce the number of dimensions to a reasonable amount (e.g. 50)\nif the number of features is very high. This will suppress some\nnoise and speed up the computation of pairwise distances between\nsamples. For more tips see Laurens van der Maaten's FAQ [2].\n\nRead more in the :ref:`User Guide <t_sne>`.\n\nParameters\n----------\nn_components : int, default=2\n    Dimension of the embedded space.\n\nperplexity : float, default=30.0\n    The perplexity is related to the number of nearest neighbors that\n    is used in other manifold learning algorithms. Larger datasets\n    usually require a larger perplexity. Consider selecting a value\n    between 5 and 50. Different values can result in significantly\n    different results.\n\nearly_exaggeration : float, default=12.0\n    Controls how tight natural clusters in the original space are in\n    the embedded space and how much space will be between them. For\n    larger values, the space between natural clusters will be larger\n    in the embedded space. Again, the choice of this parameter is not\n    very critical. If the cost function increases during initial\n    optimization, the early exaggeration factor or the learning rate\n    might be too high.\n\nlearning_rate : float, default=200.0\n    The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If\n    the learning rate is too high, the data may look like a 'ball' with any\n    point approximately equidistant from its nearest neighbours. If the\n    learning rate is too low, most points may look compressed in a dense\n    cloud with few outliers. If the cost function gets stuck in a bad local\n    minimum increasing the learning rate may help.\n\nn_iter : int, default=1000\n    Maximum number of iterations for the optimization. Should be at\n    least 250.\n\nn_iter_without_progress : int, default=300\n    Maximum number of iterations without progress before we abort the\n    optimization, used after 250 initial iterations with early\n    exaggeration. Note that progress is only checked every 50 iterations so\n    this value is rounded to the next multiple of 50.\n\n    .. versionadded:: 0.17\n       parameter *n_iter_without_progress* to control stopping criteria.\n\nmin_grad_norm : float, default=1e-7\n    If the gradient norm is below this threshold, the optimization will\n    be stopped.\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by scipy.spatial.distance.pdist for its metric parameter, or\n    a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n    If metric is \"precomputed\", X is assumed to be a distance matrix.\n    Alternatively, if metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays from X as input and return a value indicating\n    the distance between them. The default is \"euclidean\" which is\n    interpreted as squared euclidean distance.\n\ninit : {'random', 'pca'} or ndarray of shape (n_samples, n_components),             default='random'\n    Initialization of embedding. Possible options are 'random', 'pca',\n    and a numpy array of shape (n_samples, n_components).\n    PCA initialization cannot be used with precomputed distances and is\n    usually more globally stable than random initialization.\n\nverbose : int, default=0\n    Verbosity level.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the random number generator. Pass an int for reproducible\n    results across multiple function calls. Note that different\n    initializations might result in different local minima of the cost\n    function. See :term: `Glossary <random_state>`.\n\nmethod : str, default='barnes_hut'\n    By default the gradient calculation algorithm uses Barnes-Hut\n    approximation running in O(NlogN) time. method='exact'\n    will run on the slower, but exact, algorithm in O(N^2) time. The\n    exact algorithm should be used when nearest-neighbor errors need\n    to be better than 3%. However, the exact method cannot scale to\n    millions of examples.\n\n    .. versionadded:: 0.17\n       Approximate optimization *method* via the Barnes-Hut.\n\nangle : float, default=0.5\n    Only used if method='barnes_hut'\n    This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.\n    'angle' is the angular size (referred to as theta in [3]) of a distant\n    node as measured from a point. If this size is below 'angle' then it is\n    used as a summary node of all points contained within it.\n    This method is not very sensitive to changes in this parameter\n    in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing\n    computation time and angle greater 0.8 has quickly increasing error.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search. This parameter\n    has no impact when ``metric=\"precomputed\"`` or\n    (``metric=\"euclidean\"`` and ``method=\"exact\"``).\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.22\n\nsquare_distances : True or 'legacy', default='legacy'\n    Whether TSNE should square the distance values. ``'legacy'`` means\n    that distance values are squared only when ``metric=\"euclidean\"``.\n    ``True`` means that distance values are squared for all metrics.\n\n    .. versionadded:: 0.24\n       Added to provide backward compatibility during deprecation of\n       legacy squaring behavior.\n    .. deprecated:: 0.24\n       Legacy squaring behavior was deprecated in 0.24. The ``'legacy'``\n       value will be removed in 1.1 (renaming of 0.26), at which point the\n       default value will change to ``True``.\n\nAttributes\n----------\nembedding_ : array-like of shape (n_samples, n_components)\n    Stores the embedding vectors.\n\nkl_divergence_ : float\n    Kullback-Leibler divergence after optimization.\n\nn_iter_ : int\n    Number of iterations run.\n\nExamples\n--------\n\n>>> import numpy as np\n>>> from sklearn.manifold import TSNE\n>>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n>>> X_embedded = TSNE(n_components=2).fit_transform(X)\n>>> X_embedded.shape\n(4, 2)\n\nReferences\n----------\n\n[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data\n    Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.\n\n[2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding\n    https://lvdmaaten.github.io/tsne/\n\n[3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.\n    Journal of Machine Learning Research 15(Oct):3221-3245, 2014.\n    https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf"
        }
      ],
      "functions": [
        {
          "name": "trustworthiness",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If the metric is 'precomputed' X must be a square distance\nmatrix. Otherwise it contains a sample per row."
            },
            {
              "name": "X_embedded",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Embedding of the training data in low-dimensional space."
            }
          ],
          "results": [
            {
              "name": "trustworthiness",
              "type": "float",
              "description": "Trustworthiness of the low-dimensional embedding."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Expresses to what extent the local structure is retained.\n\nThe trustworthiness is within [0, 1]. It is defined as\n\n.. math::\n\n    T(k) = 1 - \\frac{2}{nk (2n - 3k - 1)} \\sum^n_{i=1}\n        \\sum_{j \\in \\mathcal{N}_{i}^{k}} \\max(0, (r(i, j) - k))\n\nwhere for each sample i, :math:`\\mathcal{N}_{i}^{k}` are its k nearest\nneighbors in the output space, and every sample j is its :math:`r(i, j)`-th\nnearest neighbor in the input space. In other words, any unexpected nearest\nneighbors in the output space are penalised in proportion to their rank in\nthe input space.\n\n* \"Neighborhood Preservation in Nonlinear Projection Methods: An\n  Experimental Study\"\n  J. Venna, S. Kaski\n* \"Learning a Parametric Embedding by Preserving Local Structure\"\n  L.J.P. van der Maaten\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)\n    If the metric is 'precomputed' X must be a square distance\n    matrix. Otherwise it contains a sample per row.\n\nX_embedded : ndarray of shape (n_samples, n_components)\n    Embedding of the training data in low-dimensional space.\n\nn_neighbors : int, default=5\n    Number of neighbors k that will be considered.\n\nmetric : str or callable, default='euclidean'\n    Which metric to use for computing pairwise distances between samples\n    from the original input space. If metric is 'precomputed', X must be a\n    matrix of pairwise distances or squared distances. Otherwise, see the\n    documentation of argument metric in sklearn.pairwise.pairwise_distances\n    for a list of available metrics.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ntrustworthiness : float\n    Trustworthiness of the low-dimensional embedding."
        }
      ]
    },
    {
      "name": "sklearn.manifold.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.metrics",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn",
          "declaration": "cluster",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "balanced_accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "brier_score_loss",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "classification_report",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "cohen_kappa_score",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "confusion_matrix",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "f1_score",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "fbeta_score",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "hamming_loss",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "hinge_loss",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "jaccard_score",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "log_loss",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "matthews_corrcoef",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "multilabel_confusion_matrix",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "precision_recall_fscore_support",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "precision_score",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "recall_score",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "zero_one_loss",
          "alias": null
        },
        {
          "module": "sklearn._plot.confusion_matrix",
          "declaration": "ConfusionMatrixDisplay",
          "alias": null
        },
        {
          "module": "sklearn._plot.confusion_matrix",
          "declaration": "plot_confusion_matrix",
          "alias": null
        },
        {
          "module": "sklearn._plot.det_curve",
          "declaration": "DetCurveDisplay",
          "alias": null
        },
        {
          "module": "sklearn._plot.det_curve",
          "declaration": "plot_det_curve",
          "alias": null
        },
        {
          "module": "sklearn._plot.precision_recall_curve",
          "declaration": "PrecisionRecallDisplay",
          "alias": null
        },
        {
          "module": "sklearn._plot.precision_recall_curve",
          "declaration": "plot_precision_recall_curve",
          "alias": null
        },
        {
          "module": "sklearn._plot.roc_curve",
          "declaration": "RocCurveDisplay",
          "alias": null
        },
        {
          "module": "sklearn._plot.roc_curve",
          "declaration": "plot_roc_curve",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "auc",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "average_precision_score",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "coverage_error",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "dcg_score",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "det_curve",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "label_ranking_average_precision_score",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "label_ranking_loss",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "ndcg_score",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "precision_recall_curve",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "roc_auc_score",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "roc_curve",
          "alias": null
        },
        {
          "module": "sklearn._ranking",
          "declaration": "top_k_accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "explained_variance_score",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "max_error",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "mean_absolute_error",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "mean_absolute_percentage_error",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "mean_gamma_deviance",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "mean_poisson_deviance",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "mean_squared_error",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "mean_squared_log_error",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "mean_tweedie_deviance",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "median_absolute_error",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "r2_score",
          "alias": null
        },
        {
          "module": "sklearn._scorer",
          "declaration": "SCORERS",
          "alias": null
        },
        {
          "module": "sklearn._scorer",
          "declaration": "check_scoring",
          "alias": null
        },
        {
          "module": "sklearn._scorer",
          "declaration": "get_scorer",
          "alias": null
        },
        {
          "module": "sklearn._scorer",
          "declaration": "make_scorer",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "adjusted_mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "adjusted_rand_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "calinski_harabasz_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "completeness_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "consensus_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "davies_bouldin_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "fowlkes_mallows_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "homogeneity_completeness_v_measure",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "homogeneity_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "normalized_mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "pair_confusion_matrix",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "rand_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "silhouette_samples",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "silhouette_score",
          "alias": null
        },
        {
          "module": "sklearn.cluster",
          "declaration": "v_measure_score",
          "alias": null
        },
        {
          "module": "sklearn.pairwise",
          "declaration": "euclidean_distances",
          "alias": null
        },
        {
          "module": "sklearn.pairwise",
          "declaration": "nan_euclidean_distances",
          "alias": null
        },
        {
          "module": "sklearn.pairwise",
          "declaration": "pairwise_distances",
          "alias": null
        },
        {
          "module": "sklearn.pairwise",
          "declaration": "pairwise_distances_argmin",
          "alias": null
        },
        {
          "module": "sklearn.pairwise",
          "declaration": "pairwise_distances_argmin_min",
          "alias": null
        },
        {
          "module": "sklearn.pairwise",
          "declaration": "pairwise_distances_chunked",
          "alias": null
        },
        {
          "module": "sklearn.pairwise",
          "declaration": "pairwise_kernels",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.metrics._base",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "itertools",
          "declaration": "combinations",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "type_of_target",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.metrics._classification",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy.sparse",
          "declaration": "coo_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csr_matrix",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "UndefinedMetricWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics._base",
          "declaration": "_check_pos_label_consistency",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "assert_all_finite",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "type_of_target",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "unique_labels",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "count_nonzero",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "accuracy_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) labels."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted labels, as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": "float",
              "description": "If ``normalize == True``, return the fraction of correctly\nclassified samples (float), else returns the number of correctly\nclassified samples (int).\n\nThe best performance is 1 with ``normalize == True`` and the number\nof samples with ``normalize == False``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Accuracy classification score.\n\nIn multilabel classification, this function computes subset accuracy:\nthe set of labels predicted for a sample must *exactly* match the\ncorresponding set of labels in y_true.\n\nRead more in the :ref:`User Guide <accuracy_score>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) labels.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Predicted labels, as returned by a classifier.\n\nnormalize : bool, default=True\n    If ``False``, return the number of correctly classified samples.\n    Otherwise, return the fraction of correctly classified samples.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    If ``normalize == True``, return the fraction of correctly\n    classified samples (float), else returns the number of correctly\n    classified samples (int).\n\n    The best performance is 1 with ``normalize == True`` and the number\n    of samples with ``normalize == False``.\n\nSee Also\n--------\njaccard_score, hamming_loss, zero_one_loss\n\nNotes\n-----\nIn binary and multiclass classification, this function is equal\nto the ``jaccard_score`` function.\n\nExamples\n--------\n>>> from sklearn.metrics import accuracy_score\n>>> y_pred = [0, 2, 1, 3]\n>>> y_true = [0, 1, 2, 3]\n>>> accuracy_score(y_true, y_pred)\n0.5\n>>> accuracy_score(y_true, y_pred, normalize=False)\n2\n\nIn the multilabel case with binary label indicators:\n\n>>> import numpy as np\n>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n0.5"
        },
        {
          "name": "balanced_accuracy_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "balanced_accuracy",
              "type": "float",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the balanced accuracy.\n\nThe balanced accuracy in binary and multiclass classification problems to\ndeal with imbalanced datasets. It is defined as the average of recall\nobtained on each class.\n\nThe best value is 1 and the worst value is 0 when ``adjusted=False``.\n\nRead more in the :ref:`User Guide <balanced_accuracy_score>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\ny_true : 1d array-like\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like\n    Estimated targets as returned by a classifier.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nadjusted : bool, default=False\n    When true, the result is adjusted for chance, so that random\n    performance would score 0, and perfect performance scores 1.\n\nReturns\n-------\nbalanced_accuracy : float\n\nSee Also\n--------\nrecall_score, roc_auc_score\n\nNotes\n-----\nSome literature promotes alternative definitions of balanced accuracy. Our\ndefinition is equivalent to :func:`accuracy_score` with class-balanced\nsample weights, and shares desirable properties with the binary case.\nSee the :ref:`User Guide <balanced_accuracy_score>`.\n\nReferences\n----------\n.. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n       The balanced accuracy and its posterior distribution.\n       Proceedings of the 20th International Conference on Pattern\n       Recognition, 3121-24.\n.. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n       `Fundamentals of Machine Learning for Predictive Data Analytics:\n       Algorithms, Worked Examples, and Case Studies\n       <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n\nExamples\n--------\n>>> from sklearn.metrics import balanced_accuracy_score\n>>> y_true = [0, 1, 0, 0, 1, 0]\n>>> y_pred = [0, 1, 0, 0, 0, 1]\n>>> balanced_accuracy_score(y_true, y_pred)\n0.625"
        },
        {
          "name": "brier_score_loss",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True targets."
            },
            {
              "name": "y_prob",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Probabilities of the positive class."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": "float",
              "description": "Brier score loss."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the Brier score loss.\n\nThe smaller the Brier score loss, the better, hence the naming with \"loss\".\nThe Brier score measures the mean squared difference between the predicted\nprobability and the actual outcome. The Brier score always\ntakes on a value between zero and one, since this is the largest\npossible difference between a predicted probability (which must be\nbetween zero and one) and the actual outcome (which can take on values\nof only 0 and 1). It can be decomposed is the sum of refinement loss and\ncalibration loss.\n\nThe Brier score is appropriate for binary and categorical outcomes that\ncan be structured as true or false, but is inappropriate for ordinal\nvariables which can take on three or more values (this is because the\nBrier score assumes that all possible outcomes are equivalently\n\"distant\" from one another). Which label is considered to be the positive\nlabel is controlled via the parameter `pos_label`, which defaults to\nthe greater label unless `y_true` is all 0 or all -1, in which case\n`pos_label` defaults to 1.\n\nRead more in the :ref:`User Guide <brier_score_loss>`.\n\nParameters\n----------\ny_true : array of shape (n_samples,)\n    True targets.\n\ny_prob : array of shape (n_samples,)\n    Probabilities of the positive class.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\npos_label : int or str, default=None\n    Label of the positive class. `pos_label` will be infered in the\n    following manner:\n\n    * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n    * else if `y_true` contains string, an error will be raised and\n      `pos_label` should be explicitely specified;\n    * otherwise, `pos_label` defaults to the greater label,\n      i.e. `np.unique(y_true)[-1]`.\n\nReturns\n-------\nscore : float\n    Brier score loss.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import brier_score_loss\n>>> y_true = np.array([0, 1, 1, 0])\n>>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n>>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n>>> brier_score_loss(y_true, y_prob)\n0.037...\n>>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n0.037...\n>>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n0.037...\n>>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n0.0\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Brier score\n        <https://en.wikipedia.org/wiki/Brier_score>`_."
        },
        {
          "name": "classification_report",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "report",
              "type": null,
              "description": "Text summary of the precision, recall, F1 score for each class.\nDictionary returned if output_dict is True. Dictionary has the\nfollowing structure::\n\n    {'label 1': {'precision':0.5,\n                 'recall':1.0,\n                 'f1-score':0.67,\n                 'support':1},\n     'label 2': { ... },\n      ...\n    }\n\nThe reported averages include macro average (averaging the unweighted\nmean per label), weighted average (averaging the support-weighted mean\nper label), and sample average (only for multilabel classification).\nMicro average (averaging the total true positives, false negatives and\nfalse positives) is only shown for multi-label or multi-class\nwith a subset of classes, because it corresponds to accuracy\notherwise and would be the same for all metrics.\nSee also :func:`precision_recall_fscore_support` for more details\non averages.\n\nNote that in binary classification, recall of the positive class\nis also known as \"sensitivity\"; recall of the negative class is\n\"specificity\"."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Build a text report showing the main classification metrics.\n\nRead more in the :ref:`User Guide <classification_report>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nlabels : array-like of shape (n_labels,), default=None\n    Optional list of label indices to include in the report.\n\ntarget_names : list of str of shape (n_labels,), default=None\n    Optional display names matching the labels (same order).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\ndigits : int, default=2\n    Number of digits for formatting output floating point values.\n    When ``output_dict`` is ``True``, this will be ignored and the\n    returned values will not be rounded.\n\noutput_dict : bool, default=False\n    If True, return output as dict.\n\n    .. versionadded:: 0.20\n\nzero_division : \"warn\", 0 or 1, default=\"warn\"\n    Sets the value to return when there is a zero division. If set to\n    \"warn\", this acts as 0, but warnings are also raised.\n\nReturns\n-------\nreport : string / dict\n    Text summary of the precision, recall, F1 score for each class.\n    Dictionary returned if output_dict is True. Dictionary has the\n    following structure::\n\n        {'label 1': {'precision':0.5,\n                     'recall':1.0,\n                     'f1-score':0.67,\n                     'support':1},\n         'label 2': { ... },\n          ...\n        }\n\n    The reported averages include macro average (averaging the unweighted\n    mean per label), weighted average (averaging the support-weighted mean\n    per label), and sample average (only for multilabel classification).\n    Micro average (averaging the total true positives, false negatives and\n    false positives) is only shown for multi-label or multi-class\n    with a subset of classes, because it corresponds to accuracy\n    otherwise and would be the same for all metrics.\n    See also :func:`precision_recall_fscore_support` for more details\n    on averages.\n\n    Note that in binary classification, recall of the positive class\n    is also known as \"sensitivity\"; recall of the negative class is\n    \"specificity\".\n\nSee Also\n--------\nprecision_recall_fscore_support, confusion_matrix,\nmultilabel_confusion_matrix\n\nExamples\n--------\n>>> from sklearn.metrics import classification_report\n>>> y_true = [0, 1, 2, 2, 2]\n>>> y_pred = [0, 0, 2, 2, 1]\n>>> target_names = ['class 0', 'class 1', 'class 2']\n>>> print(classification_report(y_true, y_pred, target_names=target_names))\n              precision    recall  f1-score   support\n<BLANKLINE>\n     class 0       0.50      1.00      0.67         1\n     class 1       0.00      0.00      0.00         1\n     class 2       1.00      0.67      0.80         3\n<BLANKLINE>\n    accuracy                           0.60         5\n   macro avg       0.50      0.56      0.49         5\nweighted avg       0.70      0.60      0.61         5\n<BLANKLINE>\n>>> y_pred = [1, 1, 0]\n>>> y_true = [1, 1, 1]\n>>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n              precision    recall  f1-score   support\n<BLANKLINE>\n           1       1.00      0.67      0.80         3\n           2       0.00      0.00      0.00         0\n           3       0.00      0.00      0.00         0\n<BLANKLINE>\n   micro avg       1.00      0.67      0.80         3\n   macro avg       0.33      0.22      0.27         3\nweighted avg       1.00      0.67      0.80         3\n<BLANKLINE>"
        },
        {
          "name": "cohen_kappa_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y1",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Labels assigned by the first annotator."
            },
            {
              "name": "y2",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Labels assigned by the second annotator. The kappa statistic is\nsymmetric, so swapping ``y1`` and ``y2`` doesn't change the value."
            }
          ],
          "results": [
            {
              "name": "kappa",
              "type": "float",
              "description": "The kappa statistic, which is a number between -1 and 1. The maximum\nvalue means complete agreement; zero or lower means chance agreement."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Cohen's kappa: a statistic that measures inter-annotator agreement.\n\nThis function computes Cohen's kappa [1]_, a score that expresses the level\nof agreement between two annotators on a classification problem. It is\ndefined as\n\n.. math::\n    \\kappa = (p_o - p_e) / (1 - p_e)\n\nwhere :math:`p_o` is the empirical probability of agreement on the label\nassigned to any sample (the observed agreement ratio), and :math:`p_e` is\nthe expected agreement when both annotators assign labels randomly.\n:math:`p_e` is estimated using a per-annotator empirical prior over the\nclass labels [2]_.\n\nRead more in the :ref:`User Guide <cohen_kappa>`.\n\nParameters\n----------\ny1 : array of shape (n_samples,)\n    Labels assigned by the first annotator.\n\ny2 : array of shape (n_samples,)\n    Labels assigned by the second annotator. The kappa statistic is\n    symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n\nlabels : array-like of shape (n_classes,), default=None\n    List of labels to index the matrix. This may be used to select a\n    subset of labels. If None, all labels that appear at least once in\n    ``y1`` or ``y2`` are used.\n\nweights : {'linear', 'quadratic'}, default=None\n    Weighting type to calculate the score. None means no weighted;\n    \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nkappa : float\n    The kappa statistic, which is a number between -1 and 1. The maximum\n    value means complete agreement; zero or lower means chance agreement.\n\nReferences\n----------\n.. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n       Educational and Psychological Measurement 20(1):37-46.\n       doi:10.1177/001316446002000104.\n.. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n       computational linguistics\". Computational Linguistics 34(4):555-596\n       <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.\n.. [3] `Wikipedia entry for the Cohen's kappa\n        <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_."
        },
        {
          "name": "confusion_matrix",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "C",
              "type": null,
              "description": "Confusion matrix whose i-th row and j-th\ncolumn entry indicates the number of\nsamples with true label being i-th class\nand predicted label being j-th class."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute confusion matrix to evaluate the accuracy of a classification.\n\nBy definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\nis equal to the number of observations known to be in group :math:`i` and\npredicted to be in group :math:`j`.\n\nThus in binary classification, the count of true negatives is\n:math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n:math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n\nRead more in the :ref:`User Guide <confusion_matrix>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated targets as returned by a classifier.\n\nlabels : array-like of shape (n_classes), default=None\n    List of labels to index the matrix. This may be used to reorder\n    or select a subset of labels.\n    If ``None`` is given, those that appear at least once\n    in ``y_true`` or ``y_pred`` are used in sorted order.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.18\n\nnormalize : {'true', 'pred', 'all'}, default=None\n    Normalizes confusion matrix over the true (rows), predicted (columns)\n    conditions or all the population. If None, confusion matrix will not be\n    normalized.\n\nReturns\n-------\nC : ndarray of shape (n_classes, n_classes)\n    Confusion matrix whose i-th row and j-th\n    column entry indicates the number of\n    samples with true label being i-th class\n    and predicted label being j-th class.\n\nSee Also\n--------\nplot_confusion_matrix : Plot Confusion Matrix.\nConfusionMatrixDisplay : Confusion Matrix visualization.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Confusion matrix\n       <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n       (Wikipedia and other references may use a different\n       convention for axes).\n\nExamples\n--------\n>>> from sklearn.metrics import confusion_matrix\n>>> y_true = [2, 0, 2, 2, 0, 1]\n>>> y_pred = [0, 0, 2, 2, 0, 2]\n>>> confusion_matrix(y_true, y_pred)\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])\n\n>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n>>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])\n\nIn the binary case, we can extract true positives, etc as follows:\n\n>>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n>>> (tn, fp, fn, tp)\n(0, 2, 1, 1)"
        },
        {
          "name": "f1_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "f1_score",
              "type": null,
              "description": "F1 score of the positive class in binary classification or weighted\naverage of the F1 scores of each class for the multiclass task."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the F1 score, also known as balanced F-score or F-measure.\n\nThe F1 score can be interpreted as a weighted average of the precision and\nrecall, where an F1 score reaches its best value at 1 and worst score at 0.\nThe relative contribution of precision and recall to the F1 score are\nequal. The formula for the F1 score is::\n\n    F1 = 2 * (precision * recall) / (precision + recall)\n\nIn the multi-class and multi-label case, this is the average of\nthe F1 score of each class with weighting depending on the ``average``\nparameter.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nlabels : array-like, default=None\n    The set of labels to include when ``average != 'binary'``, and their\n    order if ``average is None``. Labels present in the data can be\n    excluded, for example to calculate a multiclass average ignoring a\n    majority negative class, while labels not present in the data will\n    result in 0 components in a macro average. For multilabel targets,\n    labels are column indices. By default, all labels in ``y_true`` and\n    ``y_pred`` are used in sorted order.\n\n    .. versionchanged:: 0.17\n       Parameter `labels` improved for multiclass problem.\n\npos_label : str or int, default=1\n    The class to report if ``average='binary'`` and the data is binary.\n    If the data are multiclass or multilabel, this will be ignored;\n    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n    scores for that label only.\n\naverage : {'micro', 'macro', 'samples','weighted', 'binary'} or None,             default='binary'\n    This parameter is required for multiclass/multilabel targets.\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : \"warn\", 0 or 1, default=\"warn\"\n    Sets the value to return when there is a zero division, i.e. when all\n    predictions and labels are negative. If set to \"warn\", this acts as 0,\n    but warnings are also raised.\n\nReturns\n-------\nf1_score : float or array of float, shape = [n_unique_labels]\n    F1 score of the positive class in binary classification or weighted\n    average of the F1 scores of each class for the multiclass task.\n\nSee Also\n--------\nfbeta_score, precision_recall_fscore_support, jaccard_score,\nmultilabel_confusion_matrix\n\nReferences\n----------\n.. [1] `Wikipedia entry for the F1-score\n       <https://en.wikipedia.org/wiki/F1_score>`_.\n\nExamples\n--------\n>>> from sklearn.metrics import f1_score\n>>> y_true = [0, 1, 2, 0, 1, 2]\n>>> y_pred = [0, 2, 1, 0, 0, 1]\n>>> f1_score(y_true, y_pred, average='macro')\n0.26...\n>>> f1_score(y_true, y_pred, average='micro')\n0.33...\n>>> f1_score(y_true, y_pred, average='weighted')\n0.26...\n>>> f1_score(y_true, y_pred, average=None)\narray([0.8, 0. , 0. ])\n>>> y_true = [0, 0, 0, 0, 0, 0]\n>>> y_pred = [0, 0, 0, 0, 0, 0]\n>>> f1_score(y_true, y_pred, zero_division=1)\n1.0...\n\nNotes\n-----\nWhen ``true positive + false positive == 0``, precision is undefined.\nWhen ``true positive + false negative == 0``, recall is undefined.\nIn such cases, by default the metric will be set to 0, as will f-score,\nand ``UndefinedMetricWarning`` will be raised. This behavior can be\nmodified with ``zero_division``."
        },
        {
          "name": "fbeta_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "fbeta_score",
              "type": null,
              "description": "F-beta score of the positive class in binary classification or weighted\naverage of the F-beta score of each class for the multiclass task."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the F-beta score.\n\nThe F-beta score is the weighted harmonic mean of precision and recall,\nreaching its optimal value at 1 and its worst value at 0.\n\nThe `beta` parameter determines the weight of recall in the combined\nscore. ``beta < 1`` lends more weight to precision, while ``beta > 1``\nfavors recall (``beta -> 0`` considers only precision, ``beta -> +inf``\nonly recall).\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nbeta : float\n    Determines the weight of recall in the combined score.\n\nlabels : array-like, default=None\n    The set of labels to include when ``average != 'binary'``, and their\n    order if ``average is None``. Labels present in the data can be\n    excluded, for example to calculate a multiclass average ignoring a\n    majority negative class, while labels not present in the data will\n    result in 0 components in a macro average. For multilabel targets,\n    labels are column indices. By default, all labels in ``y_true`` and\n    ``y_pred`` are used in sorted order.\n\n    .. versionchanged:: 0.17\n       Parameter `labels` improved for multiclass problem.\n\npos_label : str or int, default=1\n    The class to report if ``average='binary'`` and the data is binary.\n    If the data are multiclass or multilabel, this will be ignored;\n    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n    scores for that label only.\n\naverage : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None             default='binary'\n    This parameter is required for multiclass/multilabel targets.\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : \"warn\", 0 or 1, default=\"warn\"\n    Sets the value to return when there is a zero division, i.e. when all\n    predictions and labels are negative. If set to \"warn\", this acts as 0,\n    but warnings are also raised.\n\nReturns\n-------\nfbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n    F-beta score of the positive class in binary classification or weighted\n    average of the F-beta score of each class for the multiclass task.\n\nSee Also\n--------\nprecision_recall_fscore_support, multilabel_confusion_matrix\n\nNotes\n-----\nWhen ``true positive + false positive == 0`` or\n``true positive + false negative == 0``, f-score returns 0 and raises\n``UndefinedMetricWarning``. This behavior can be\nmodified with ``zero_division``.\n\nReferences\n----------\n.. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n       Modern Information Retrieval. Addison Wesley, pp. 327-328.\n\n.. [2] `Wikipedia entry for the F1-score\n       <https://en.wikipedia.org/wiki/F1_score>`_.\n\nExamples\n--------\n>>> from sklearn.metrics import fbeta_score\n>>> y_true = [0, 1, 2, 0, 1, 2]\n>>> y_pred = [0, 2, 1, 0, 0, 1]\n>>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n0.23...\n>>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n0.33...\n>>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n0.23...\n>>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\narray([0.71..., 0.        , 0.        ])"
        },
        {
          "name": "hamming_loss",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) labels."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted labels, as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": null,
              "description": "Return the average Hamming loss between element of ``y_true`` and\n``y_pred``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the average Hamming loss.\n\nThe Hamming loss is the fraction of labels that are incorrectly predicted.\n\nRead more in the :ref:`User Guide <hamming_loss>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) labels.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Predicted labels, as returned by a classifier.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nloss : float or int\n    Return the average Hamming loss between element of ``y_true`` and\n    ``y_pred``.\n\nSee Also\n--------\naccuracy_score, jaccard_score, zero_one_loss\n\nNotes\n-----\nIn multiclass classification, the Hamming loss corresponds to the Hamming\ndistance between ``y_true`` and ``y_pred`` which is equivalent to the\nsubset ``zero_one_loss`` function, when `normalize` parameter is set to\nTrue.\n\nIn multilabel classification, the Hamming loss is different from the\nsubset zero-one loss. The zero-one loss considers the entire set of labels\nfor a given sample incorrect if it does not entirely match the true set of\nlabels. Hamming loss is more forgiving in that it penalizes only the\nindividual labels.\n\nThe Hamming loss is upperbounded by the subset zero-one loss, when\n`normalize` parameter is set to True. It is always between 0 and 1,\nlower being better.\n\nReferences\n----------\n.. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n       An Overview. International Journal of Data Warehousing & Mining,\n       3(3), 1-13, July-September 2007.\n\n.. [2] `Wikipedia entry on the Hamming distance\n       <https://en.wikipedia.org/wiki/Hamming_distance>`_.\n\nExamples\n--------\n>>> from sklearn.metrics import hamming_loss\n>>> y_pred = [1, 2, 3, 4]\n>>> y_true = [2, 2, 3, 4]\n>>> hamming_loss(y_true, y_pred)\n0.25\n\nIn the multilabel case with binary label indicators:\n\n>>> import numpy as np\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n0.75"
        },
        {
          "name": "hinge_loss",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True target, consisting of integers of two values. The positive label\nmust be greater than the negative label."
            },
            {
              "name": "pred_decision",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted decisions, as output by decision_function (floats)."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": "float",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Average hinge loss (non-regularized).\n\nIn binary class case, assuming labels in y_true are encoded with +1 and -1,\nwhen a prediction mistake is made, ``margin = y_true * pred_decision`` is\nalways negative (since the signs disagree), implying ``1 - margin`` is\nalways greater than 1.  The cumulated hinge loss is therefore an upper\nbound of the number of mistakes made by the classifier.\n\nIn multiclass case, the function expects that either all the labels are\nincluded in y_true or an optional labels argument is provided which\ncontains all the labels. The multilabel margin is calculated according\nto Crammer-Singer's method. As in the binary case, the cumulated hinge loss\nis an upper bound of the number of mistakes made by the classifier.\n\nRead more in the :ref:`User Guide <hinge_loss>`.\n\nParameters\n----------\ny_true : array of shape (n_samples,)\n    True target, consisting of integers of two values. The positive label\n    must be greater than the negative label.\n\npred_decision : array of shape (n_samples,) or (n_samples, n_classes)\n    Predicted decisions, as output by decision_function (floats).\n\nlabels : array-like, default=None\n    Contains all the labels for the problem. Used in multiclass hinge loss.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float\n\nReferences\n----------\n.. [1] `Wikipedia entry on the Hinge loss\n       <https://en.wikipedia.org/wiki/Hinge_loss>`_.\n\n.. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n       Implementation of Multiclass Kernel-based Vector\n       Machines. Journal of Machine Learning Research 2,\n       (2001), 265-292.\n\n.. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n       by Robert C. Moore, John DeNero\n       <http://www.ttic.edu/sigml/symposium2011/papers/\n       Moore+DeNero_Regularization.pdf>`_.\n\nExamples\n--------\n>>> from sklearn import svm\n>>> from sklearn.metrics import hinge_loss\n>>> X = [[0], [1]]\n>>> y = [-1, 1]\n>>> est = svm.LinearSVC(random_state=0)\n>>> est.fit(X, y)\nLinearSVC(random_state=0)\n>>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n>>> pred_decision\narray([-2.18...,  2.36...,  0.09...])\n>>> hinge_loss([-1, 1, 1], pred_decision)\n0.30...\n\nIn the multiclass case:\n\n>>> import numpy as np\n>>> X = np.array([[0], [1], [2], [3]])\n>>> Y = np.array([0, 1, 2, 3])\n>>> labels = np.array([0, 1, 2, 3])\n>>> est = svm.LinearSVC()\n>>> est.fit(X, Y)\nLinearSVC()\n>>> pred_decision = est.decision_function([[-1], [2], [3]])\n>>> y_true = [0, 2, 3]\n>>> hinge_loss(y_true, pred_decision, labels=labels)\n0.56..."
        },
        {
          "name": "jaccard_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) labels."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted labels, as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Jaccard similarity coefficient score.\n\nThe Jaccard index [1], or Jaccard similarity coefficient, defined as\nthe size of the intersection divided by the size of the union of two label\nsets, is used to compare set of predicted labels for a sample to the\ncorresponding set of labels in ``y_true``.\n\nRead more in the :ref:`User Guide <jaccard_similarity_score>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) labels.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Predicted labels, as returned by a classifier.\n\nlabels : array-like of shape (n_classes,), default=None\n    The set of labels to include when ``average != 'binary'``, and their\n    order if ``average is None``. Labels present in the data can be\n    excluded, for example to calculate a multiclass average ignoring a\n    majority negative class, while labels not present in the data will\n    result in 0 components in a macro average. For multilabel targets,\n    labels are column indices. By default, all labels in ``y_true`` and\n    ``y_pred`` are used in sorted order.\n\npos_label : str or int, default=1\n    The class to report if ``average='binary'`` and the data is binary.\n    If the data are multiclass or multilabel, this will be ignored;\n    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n    scores for that label only.\n\naverage : {None, 'micro', 'macro', 'samples', 'weighted',             'binary'}, default='binary'\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average, weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : \"warn\", {0.0, 1.0}, default=\"warn\"\n    Sets the value to return when there is a zero division, i.e. when there\n    there are no negative values in predictions and labels. If set to\n    \"warn\", this acts like 0, but a warning is also raised.\n\nReturns\n-------\nscore : float (if average is not None) or array of floats, shape =            [n_unique_labels]\n\nSee Also\n--------\naccuracy_score, f_score, multilabel_confusion_matrix\n\nNotes\n-----\n:func:`jaccard_score` may be a poor metric if there are no\npositives for some samples or classes. Jaccard is undefined if there are\nno true or predicted labels, and our implementation will return a score\nof 0 with a warning.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Jaccard index\n       <https://en.wikipedia.org/wiki/Jaccard_index>`_.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import jaccard_score\n>>> y_true = np.array([[0, 1, 1],\n...                    [1, 1, 0]])\n>>> y_pred = np.array([[1, 1, 1],\n...                    [1, 0, 0]])\n\nIn the binary case:\n\n>>> jaccard_score(y_true[0], y_pred[0])\n0.6666...\n\nIn the multilabel case:\n\n>>> jaccard_score(y_true, y_pred, average='samples')\n0.5833...\n>>> jaccard_score(y_true, y_pred, average='macro')\n0.6666...\n>>> jaccard_score(y_true, y_pred, average=None)\narray([0.5, 0.5, 1. ])\n\nIn the multiclass case:\n\n>>> y_pred = [0, 2, 1, 2]\n>>> y_true = [0, 1, 2, 2]\n>>> jaccard_score(y_true, y_pred, average=None)\narray([1. , 0. , 0.33...])"
        },
        {
          "name": "log_loss",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) labels for n_samples samples."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted probabilities, as returned by a classifier's\npredict_proba method. If ``y_pred.shape = (n_samples,)``\nthe probabilities provided are assumed to be that of the\npositive class. The labels in ``y_pred`` are assumed to be\nordered alphabetically, as done by\n:class:`preprocessing.LabelBinarizer`."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": "float",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Log loss, aka logistic loss or cross-entropy loss.\n\nThis is the loss function used in (multinomial) logistic regression\nand extensions of it such as neural networks, defined as the negative\nlog-likelihood of a logistic model that returns ``y_pred`` probabilities\nfor its training data ``y_true``.\nThe log loss is only defined for two or more labels.\nFor a single sample with true label :math:`y \\in \\{0,1\\}` and\nand a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log\nloss is:\n\n.. math::\n    L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n\nRead more in the :ref:`User Guide <log_loss>`.\n\nParameters\n----------\ny_true : array-like or label indicator matrix\n    Ground truth (correct) labels for n_samples samples.\n\ny_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n    Predicted probabilities, as returned by a classifier's\n    predict_proba method. If ``y_pred.shape = (n_samples,)``\n    the probabilities provided are assumed to be that of the\n    positive class. The labels in ``y_pred`` are assumed to be\n    ordered alphabetically, as done by\n    :class:`preprocessing.LabelBinarizer`.\n\neps : float, default=1e-15\n    Log loss is undefined for p=0 or p=1, so probabilities are\n    clipped to max(eps, min(1 - eps, p)).\n\nnormalize : bool, default=True\n    If true, return the mean loss per sample.\n    Otherwise, return the sum of the per-sample losses.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nlabels : array-like, default=None\n    If not provided, labels will be inferred from y_true. If ``labels``\n    is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n    assumed to be binary and are inferred from ``y_true``.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nloss : float\n\nNotes\n-----\nThe logarithm used is the natural logarithm (base-e).\n\nExamples\n--------\n>>> from sklearn.metrics import log_loss\n>>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n0.21616...\n\nReferences\n----------\nC.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\np. 209."
        },
        {
          "name": "matthews_corrcoef",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "mcc",
              "type": "float",
              "description": "The Matthews correlation coefficient (+1 represents a perfect\nprediction, 0 an average random prediction and -1 and inverse\nprediction)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the Matthews correlation coefficient (MCC).\n\nThe Matthews correlation coefficient is used in machine learning as a\nmeasure of the quality of binary and multiclass classifications. It takes\ninto account true and false positives and negatives and is generally\nregarded as a balanced measure which can be used even if the classes are of\nvery different sizes. The MCC is in essence a correlation coefficient value\nbetween -1 and +1. A coefficient of +1 represents a perfect prediction, 0\nan average random prediction and -1 an inverse prediction.  The statistic\nis also known as the phi coefficient. [source: Wikipedia]\n\nBinary and multiclass labels are supported.  Only in the binary case does\nthis relate to information about true and false positives and negatives.\nSee references below.\n\nRead more in the :ref:`User Guide <matthews_corrcoef>`.\n\nParameters\n----------\ny_true : array, shape = [n_samples]\n    Ground truth (correct) target values.\n\ny_pred : array, shape = [n_samples]\n    Estimated targets as returned by a classifier.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nmcc : float\n    The Matthews correlation coefficient (+1 represents a perfect\n    prediction, 0 an average random prediction and -1 and inverse\n    prediction).\n\nReferences\n----------\n.. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n   accuracy of prediction algorithms for classification: an overview\n   <https://doi.org/10.1093/bioinformatics/16.5.412>`_.\n\n.. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n   <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_.\n\n.. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n    K-category correlation coefficient\n    <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.\n\n.. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n    Error Measures in MultiClass Prediction\n    <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.\n\nExamples\n--------\n>>> from sklearn.metrics import matthews_corrcoef\n>>> y_true = [+1, +1, +1, -1]\n>>> y_pred = [+1, -1, +1, +1]\n>>> matthews_corrcoef(y_true, y_pred)\n-0.33..."
        },
        {
          "name": "multilabel_confusion_matrix",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "multi_confusion",
              "type": null,
              "description": "A 2x2 confusion matrix corresponding to each output in the input.\nWhen calculating class-wise multi_confusion (default), then\nn_outputs = n_labels; when calculating sample-wise multi_confusion\n(samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\nthe results will be returned in the order specified in ``labels``,\notherwise the results will be returned in sorted order by default."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute a confusion matrix for each class or sample.\n\n.. versionadded:: 0.21\n\nCompute class-wise (default) or sample-wise (samplewise=True) multilabel\nconfusion matrix to evaluate the accuracy of a classification, and output\nconfusion matrices for each class or sample.\n\nIn multilabel confusion matrix :math:`MCM`, the count of true negatives\nis :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\ntrue positives is :math:`MCM_{:,1,1}` and false positives is\n:math:`MCM_{:,0,1}`.\n\nMulticlass data will be treated as if binarized under a one-vs-rest\ntransformation. Returned confusion matrices will be in the order of\nsorted unique labels in the union of (y_true, y_pred).\n\nRead more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n\nParameters\n----------\ny_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n    Estimated targets as returned by a classifier.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nlabels : array-like of shape (n_classes,), default=None\n    A list of classes or column indices to select some (or to force\n    inclusion of classes absent from the data).\n\nsamplewise : bool, default=False\n    In the multilabel case, this calculates a confusion matrix per sample.\n\nReturns\n-------\nmulti_confusion : ndarray of shape (n_outputs, 2, 2)\n    A 2x2 confusion matrix corresponding to each output in the input.\n    When calculating class-wise multi_confusion (default), then\n    n_outputs = n_labels; when calculating sample-wise multi_confusion\n    (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n    the results will be returned in the order specified in ``labels``,\n    otherwise the results will be returned in sorted order by default.\n\nSee Also\n--------\nconfusion_matrix\n\nNotes\n-----\nThe multilabel_confusion_matrix calculates class-wise or sample-wise\nmultilabel confusion matrices, and in multiclass tasks, labels are\nbinarized under a one-vs-rest way; while confusion_matrix calculates\none confusion matrix for confusion between every two classes.\n\nExamples\n--------\nMultilabel-indicator case:\n\n>>> import numpy as np\n>>> from sklearn.metrics import multilabel_confusion_matrix\n>>> y_true = np.array([[1, 0, 1],\n...                    [0, 1, 0]])\n>>> y_pred = np.array([[1, 0, 0],\n...                    [0, 1, 1]])\n>>> multilabel_confusion_matrix(y_true, y_pred)\narray([[[1, 0],\n        [0, 1]],\n<BLANKLINE>\n       [[1, 0],\n        [0, 1]],\n<BLANKLINE>\n       [[0, 1],\n        [1, 0]]])\n\nMulticlass case:\n\n>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n>>> multilabel_confusion_matrix(y_true, y_pred,\n...                             labels=[\"ant\", \"bird\", \"cat\"])\narray([[[3, 1],\n        [0, 2]],\n<BLANKLINE>\n       [[5, 0],\n        [1, 0]],\n<BLANKLINE>\n       [[2, 1],\n        [1, 2]]])"
        },
        {
          "name": "precision_recall_fscore_support",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "precision",
              "type": null,
              "description": ""
            },
            {
              "name": "recall",
              "type": null,
              "description": ""
            },
            {
              "name": "fbeta_score",
              "type": null,
              "description": ""
            },
            {
              "name": "support",
              "type": null,
              "description": "The number of occurrences of each label in ``y_true``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute precision, recall, F-measure and support for each class.\n\nThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\ntrue positives and ``fp`` the number of false positives. The precision is\nintuitively the ability of the classifier not to label as positive a sample\nthat is negative.\n\nThe recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\ntrue positives and ``fn`` the number of false negatives. The recall is\nintuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of\nthe precision and recall, where an F-beta score reaches its best\nvalue at 1 and worst score at 0.\n\nThe F-beta score weights recall more than precision by a factor of\n``beta``. ``beta == 1.0`` means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in ``y_true``.\n\nIf ``pos_label is None`` and in binary classification, this function\nreturns the average precision, recall and F-measure if ``average``\nis one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nbeta : float, default=1.0\n    The strength of recall versus precision in the F-score.\n\nlabels : array-like, default=None\n    The set of labels to include when ``average != 'binary'``, and their\n    order if ``average is None``. Labels present in the data can be\n    excluded, for example to calculate a multiclass average ignoring a\n    majority negative class, while labels not present in the data will\n    result in 0 components in a macro average. For multilabel targets,\n    labels are column indices. By default, all labels in ``y_true`` and\n    ``y_pred`` are used in sorted order.\n\npos_label : str or int, default=1\n    The class to report if ``average='binary'`` and the data is binary.\n    If the data are multiclass or multilabel, this will be ignored;\n    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n    scores for that label only.\n\naverage : {'binary', 'micro', 'macro', 'samples','weighted'},             default=None\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nwarn_for : tuple or set, for internal use\n    This determines which warnings will be made in the case that this\n    function is being used to return only one of its metrics.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : \"warn\", 0 or 1, default=\"warn\"\n    Sets the value to return when there is a zero division:\n       - recall: when there are no positive labels\n       - precision: when there are no positive predictions\n       - f-score: both\n\n    If set to \"warn\", this acts as 0, but warnings are also raised.\n\nReturns\n-------\nprecision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n\nrecall : float (if average is not None) or array of float, , shape =        [n_unique_labels]\n\nfbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n\nsupport : None (if average is not None) or array of int, shape =        [n_unique_labels]\n    The number of occurrences of each label in ``y_true``.\n\nNotes\n-----\nWhen ``true positive + false positive == 0``, precision is undefined.\nWhen ``true positive + false negative == 0``, recall is undefined.\nIn such cases, by default the metric will be set to 0, as will f-score,\nand ``UndefinedMetricWarning`` will be raised. This behavior can be\nmodified with ``zero_division``.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Precision and recall\n       <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\n\n.. [2] `Wikipedia entry for the F1-score\n       <https://en.wikipedia.org/wiki/F1_score>`_.\n\n.. [3] `Discriminative Methods for Multi-labeled Classification Advances\n       in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n       Godbole, Sunita Sarawagi\n       <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import precision_recall_fscore_support\n>>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n>>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n>>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n(0.22..., 0.33..., 0.26..., None)\n>>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n(0.33..., 0.33..., 0.33..., None)\n>>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n(0.22..., 0.33..., 0.26..., None)\n\nIt is possible to compute per-label precisions, recalls, F1-scores and\nsupports instead of averaging:\n\n>>> precision_recall_fscore_support(y_true, y_pred, average=None,\n... labels=['pig', 'dog', 'cat'])\n(array([0.        , 0.        , 0.66...]),\n array([0., 0., 1.]), array([0. , 0. , 0.8]),\n array([2, 2, 2]))"
        },
        {
          "name": "precision_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "precision",
              "type": null,
              "description": "(n_unique_labels,)\nPrecision of the positive class in binary classification or weighted\naverage of the precision of each class for the multiclass task."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the precision.\n\nThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\ntrue positives and ``fp`` the number of false positives. The precision is\nintuitively the ability of the classifier not to label as positive a sample\nthat is negative.\n\nThe best value is 1 and the worst value is 0.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nlabels : array-like, default=None\n    The set of labels to include when ``average != 'binary'``, and their\n    order if ``average is None``. Labels present in the data can be\n    excluded, for example to calculate a multiclass average ignoring a\n    majority negative class, while labels not present in the data will\n    result in 0 components in a macro average. For multilabel targets,\n    labels are column indices. By default, all labels in ``y_true`` and\n    ``y_pred`` are used in sorted order.\n\n    .. versionchanged:: 0.17\n       Parameter `labels` improved for multiclass problem.\n\npos_label : str or int, default=1\n    The class to report if ``average='binary'`` and the data is binary.\n    If the data are multiclass or multilabel, this will be ignored;\n    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n    scores for that label only.\n\naverage : {'micro', 'macro', 'samples', 'weighted', 'binary'}             default='binary'\n    This parameter is required for multiclass/multilabel targets.\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : \"warn\", 0 or 1, default=\"warn\"\n    Sets the value to return when there is a zero division. If set to\n    \"warn\", this acts as 0, but warnings are also raised.\n\nReturns\n-------\nprecision : float (if average is not None) or array of float of shape\n    (n_unique_labels,)\n    Precision of the positive class in binary classification or weighted\n    average of the precision of each class for the multiclass task.\n\nSee Also\n--------\nprecision_recall_fscore_support, multilabel_confusion_matrix\n\nNotes\n-----\nWhen ``true positive + false positive == 0``, precision returns 0 and\nraises ``UndefinedMetricWarning``. This behavior can be\nmodified with ``zero_division``.\n\nExamples\n--------\n>>> from sklearn.metrics import precision_score\n>>> y_true = [0, 1, 2, 0, 1, 2]\n>>> y_pred = [0, 2, 1, 0, 0, 1]\n>>> precision_score(y_true, y_pred, average='macro')\n0.22...\n>>> precision_score(y_true, y_pred, average='micro')\n0.33...\n>>> precision_score(y_true, y_pred, average='weighted')\n0.22...\n>>> precision_score(y_true, y_pred, average=None)\narray([0.66..., 0.        , 0.        ])\n>>> y_pred = [0, 0, 0, 0, 0, 0]\n>>> precision_score(y_true, y_pred, average=None)\narray([0.33..., 0.        , 0.        ])\n>>> precision_score(y_true, y_pred, average=None, zero_division=1)\narray([0.33..., 1.        , 1.        ])"
        },
        {
          "name": "recall_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated targets as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "recall",
              "type": null,
              "description": "(n_unique_labels,)\nRecall of the positive class in binary classification or weighted\naverage of the recall of each class for the multiclass task."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the recall.\n\nThe recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\ntrue positives and ``fn`` the number of false negatives. The recall is\nintuitively the ability of the classifier to find all the positive samples.\n\nThe best value is 1 and the worst value is 0.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nlabels : array-like, default=None\n    The set of labels to include when ``average != 'binary'``, and their\n    order if ``average is None``. Labels present in the data can be\n    excluded, for example to calculate a multiclass average ignoring a\n    majority negative class, while labels not present in the data will\n    result in 0 components in a macro average. For multilabel targets,\n    labels are column indices. By default, all labels in ``y_true`` and\n    ``y_pred`` are used in sorted order.\n\n    .. versionchanged:: 0.17\n       Parameter `labels` improved for multiclass problem.\n\npos_label : str or int, default=1\n    The class to report if ``average='binary'`` and the data is binary.\n    If the data are multiclass or multilabel, this will be ignored;\n    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n    scores for that label only.\n\naverage : {'micro', 'macro', 'samples', 'weighted', 'binary'}             default='binary'\n    This parameter is required for multiclass/multilabel targets.\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : \"warn\", 0 or 1, default=\"warn\"\n    Sets the value to return when there is a zero division. If set to\n    \"warn\", this acts as 0, but warnings are also raised.\n\nReturns\n-------\nrecall : float (if average is not None) or array of float of shape\n    (n_unique_labels,)\n    Recall of the positive class in binary classification or weighted\n    average of the recall of each class for the multiclass task.\n\nSee Also\n--------\nprecision_recall_fscore_support, balanced_accuracy_score,\nmultilabel_confusion_matrix\n\nNotes\n-----\nWhen ``true positive + false negative == 0``, recall returns 0 and raises\n``UndefinedMetricWarning``. This behavior can be modified with\n``zero_division``.\n\nExamples\n--------\n>>> from sklearn.metrics import recall_score\n>>> y_true = [0, 1, 2, 0, 1, 2]\n>>> y_pred = [0, 2, 1, 0, 0, 1]\n>>> recall_score(y_true, y_pred, average='macro')\n0.33...\n>>> recall_score(y_true, y_pred, average='micro')\n0.33...\n>>> recall_score(y_true, y_pred, average='weighted')\n0.33...\n>>> recall_score(y_true, y_pred, average=None)\narray([1., 0., 0.])\n>>> y_true = [0, 0, 0, 0, 0, 0]\n>>> recall_score(y_true, y_pred, average=None)\narray([0.5, 0. , 0. ])\n>>> recall_score(y_true, y_pred, average=None, zero_division=1)\narray([0.5, 1. , 1. ])"
        },
        {
          "name": "zero_one_loss",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) labels."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted labels, as returned by a classifier."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": null,
              "description": "If ``normalize == True``, return the fraction of misclassifications\n(float), else it returns the number of misclassifications (int)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Zero-one classification loss.\n\nIf normalize is ``True``, return the fraction of misclassifications\n(float), else it returns the number of misclassifications (int). The best\nperformance is 0.\n\nRead more in the :ref:`User Guide <zero_one_loss>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) labels.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Predicted labels, as returned by a classifier.\n\nnormalize : bool, default=True\n    If ``False``, return the number of misclassifications.\n    Otherwise, return the fraction of misclassifications.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float or int,\n    If ``normalize == True``, return the fraction of misclassifications\n    (float), else it returns the number of misclassifications (int).\n\nNotes\n-----\nIn multilabel classification, the zero_one_loss function corresponds to\nthe subset zero-one loss: for each sample, the entire set of labels must be\ncorrectly predicted, otherwise the loss for that sample is equal to one.\n\nSee Also\n--------\naccuracy_score, hamming_loss, jaccard_score\n\nExamples\n--------\n>>> from sklearn.metrics import zero_one_loss\n>>> y_pred = [1, 2, 3, 4]\n>>> y_true = [2, 2, 3, 4]\n>>> zero_one_loss(y_true, y_pred)\n0.25\n>>> zero_one_loss(y_true, y_pred, normalize=False)\n1\n\nIn the multilabel case with binary label indicators:\n\n>>> import numpy as np\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n0.5"
        }
      ]
    },
    {
      "name": "sklearn.metrics._plot",
      "imports": [],
      "fromImports": [],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.metrics._plot.base",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.metrics._plot.confusion_matrix",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "itertools",
          "declaration": "product",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "confusion_matrix",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_matplotlib_support",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "unique_labels",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "ConfusionMatrixDisplay",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "plot",
              "decorators": [
                "_deprecate_positional_args"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "display",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Plot visualization.\n\nParameters\n----------\ninclude_values : bool, default=True\n    Includes values in confusion matrix.\n\ncmap : str or matplotlib Colormap, default='viridis'\n    Colormap recognized by matplotlib.\n\nxticks_rotation : {'vertical', 'horizontal'} or float,                          default='horizontal'\n    Rotation of xtick labels.\n\nvalues_format : str, default=None\n    Format specification for values in confusion matrix. If `None`,\n    the format specification is 'd' or '.2g' whichever is shorter.\n\nax : matplotlib axes, default=None\n    Axes object to plot on. If `None`, a new figure and axes is\n    created.\n\ncolorbar : bool, default=True\n    Whether or not to add a colorbar to the plot.\n\nReturns\n-------\ndisplay : :class:`~sklearn.metrics.ConfusionMatrixDisplay`"
            }
          ],
          "fullDocstring": "Confusion Matrix visualization.\n\nIt is recommend to use :func:`~sklearn.metrics.plot_confusion_matrix` to\ncreate a :class:`ConfusionMatrixDisplay`. All parameters are stored as\nattributes.\n\nRead more in the :ref:`User Guide <visualizations>`.\n\nParameters\n----------\nconfusion_matrix : ndarray of shape (n_classes, n_classes)\n    Confusion matrix.\n\ndisplay_labels : ndarray of shape (n_classes,), default=None\n    Display labels for plot. If None, display labels are set from 0 to\n    `n_classes - 1`.\n\nAttributes\n----------\nim_ : matplotlib AxesImage\n    Image representing the confusion matrix.\n\ntext_ : ndarray of shape (n_classes, n_classes), dtype=matplotlib Text,             or None\n    Array of matplotlib axes. `None` if `include_values` is false.\n\nax_ : matplotlib Axes\n    Axes with confusion matrix.\n\nfigure_ : matplotlib Figure\n    Figure containing the confusion matrix.\n\nSee Also\n--------\nconfusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n    classification.\nplot_confusion_matrix : Plot Confusion Matrix.\n\nExamples\n--------\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.svm import SVC\n>>> X, y = make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n...                                                     random_state=0)\n>>> clf = SVC(random_state=0)\n>>> clf.fit(X_train, y_train)\nSVC(random_state=0)\n>>> predictions = clf.predict(X_test)\n>>> cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n>>> disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n...                               display_labels=clf.classes_)\n>>> disp.plot() # doctest: +SKIP"
        }
      ],
      "functions": [
        {
          "name": "plot_confusion_matrix",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\nin which the last estimator is a classifier."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input values."
            },
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values."
            }
          ],
          "results": [
            {
              "name": "display",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Plot Confusion Matrix.\n\nRead more in the :ref:`User Guide <confusion_matrix>`.\n\nParameters\n----------\nestimator : estimator instance\n    Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n    in which the last estimator is a classifier.\n\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input values.\n\ny_true : array-like of shape (n_samples,)\n    Target values.\n\nlabels : array-like of shape (n_classes,), default=None\n    List of labels to index the matrix. This may be used to reorder or\n    select a subset of labels. If `None` is given, those that appear at\n    least once in `y_true` or `y_pred` are used in sorted order.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nnormalize : {'true', 'pred', 'all'}, default=None\n    Normalizes confusion matrix over the true (rows), predicted (columns)\n    conditions or all the population. If None, confusion matrix will not be\n    normalized.\n\ndisplay_labels : array-like of shape (n_classes,), default=None\n    Target names used for plotting. By default, `labels` will be used if\n    it is defined, otherwise the unique labels of `y_true` and `y_pred`\n    will be used.\n\ninclude_values : bool, default=True\n    Includes values in confusion matrix.\n\nxticks_rotation : {'vertical', 'horizontal'} or float,                         default='horizontal'\n    Rotation of xtick labels.\n\nvalues_format : str, default=None\n    Format specification for values in confusion matrix. If `None`,\n    the format specification is 'd' or '.2g' whichever is shorter.\n\ncmap : str or matplotlib Colormap, default='viridis'\n    Colormap recognized by matplotlib.\n\nax : matplotlib Axes, default=None\n    Axes object to plot on. If `None`, a new figure and axes is\n    created.\n\ncolorbar : bool, default=True\n    Whether or not to add a colorbar to the plot.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ndisplay : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n\nSee Also\n--------\nconfusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n    classification.\nConfusionMatrixDisplay : Confusion Matrix visualization.\n\nExamples\n--------\n>>> import matplotlib.pyplot as plt  # doctest: +SKIP\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.metrics import plot_confusion_matrix\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.svm import SVC\n>>> X, y = make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...         X, y, random_state=0)\n>>> clf = SVC(random_state=0)\n>>> clf.fit(X_train, y_train)\nSVC(random_state=0)\n>>> plot_confusion_matrix(clf, X_test, y_test)  # doctest: +SKIP\n>>> plt.show()  # doctest: +SKIP"
        }
      ]
    },
    {
      "name": "sklearn.metrics._plot.det_curve",
      "imports": [
        {
          "module": "scipy",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.metrics",
          "declaration": "det_curve",
          "alias": null
        },
        {
          "module": "sklearn.metrics._plot.base",
          "declaration": "_get_response",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_matplotlib_support",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "DetCurveDisplay",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "plot",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "ax",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Axes object to plot on. If `None`, a new figure and axes is\ncreated."
                }
              ],
              "results": [
                {
                  "name": "display",
                  "type": null,
                  "description": "Object that stores computed values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Plot visualization.\n\nParameters\n----------\nax : matplotlib axes, default=None\n    Axes object to plot on. If `None`, a new figure and axes is\n    created.\n\nname : str, default=None\n    Name of DET curve for labeling. If `None`, use the name of the\n    estimator.\n\nReturns\n-------\ndisplay : :class:`~sklearn.metrics.plot.DetCurveDisplay`\n    Object that stores computed values."
            }
          ],
          "fullDocstring": "DET curve visualization.\n\nIt is recommend to use :func:`~sklearn.metrics.plot_det_curve` to create a\nvisualizer. All parameters are stored as attributes.\n\nRead more in the :ref:`User Guide <visualizations>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nfpr : ndarray\n    False positive rate.\n\ntpr : ndarray\n    True positive rate.\n\nestimator_name : str, default=None\n    Name of estimator. If None, the estimator name is not shown.\n\npos_label : str or int, default=None\n    The label of the positive class.\n\nAttributes\n----------\nline_ : matplotlib Artist\n    DET Curve.\n\nax_ : matplotlib Axes\n    Axes with DET Curve.\n\nfigure_ : matplotlib Figure\n    Figure containing the curve.\n\nSee Also\n--------\ndet_curve : Compute error rates for different probability thresholds.\nplot_det_curve : Plot detection error tradeoff (DET) curve.\n\nExamples\n--------\n>>> import matplotlib.pyplot as plt  # doctest: +SKIP\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([0, 0, 1, 1])\n>>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, fnr, thresholds = metrics.det_curve(y, pred)\n>>> display = metrics.DetCurveDisplay(\n...     fpr=fpr, fnr=fnr, estimator_name='example estimator'\n... )\n>>> display.plot()  # doctest: +SKIP\n>>> plt.show()      # doctest: +SKIP"
        }
      ],
      "functions": [
        {
          "name": "plot_det_curve",
          "decorators": [],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\nin which the last estimator is a classifier."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input values."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values."
            }
          ],
          "results": [
            {
              "name": "display",
              "type": null,
              "description": "Object that stores computed values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Plot detection error tradeoff (DET) curve.\n\nExtra keyword arguments will be passed to matplotlib's `plot`.\n\nRead more in the :ref:`User Guide <visualizations>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nestimator : estimator instance\n    Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n    in which the last estimator is a classifier.\n\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input values.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nresponse_method : {'predict_proba', 'decision_function', 'auto'}             default='auto'\n    Specifies whether to use :term:`predict_proba` or\n    :term:`decision_function` as the predicted target response. If set to\n    'auto', :term:`predict_proba` is tried first and if it does not exist\n    :term:`decision_function` is tried next.\n\nname : str, default=None\n    Name of DET curve for labeling. If `None`, use the name of the\n    estimator.\n\nax : matplotlib axes, default=None\n    Axes object to plot on. If `None`, a new figure and axes is created.\n\npos_label : str or int, default=None\n    The label of the positive class.\n    When `pos_label=None`, if `y_true` is in {-1, 1} or {0, 1},\n    `pos_label` is set to 1, otherwise an error will be raised.\n\nReturns\n-------\ndisplay : :class:`~sklearn.metrics.DetCurveDisplay`\n    Object that stores computed values.\n\nSee Also\n--------\ndet_curve : Compute error rates for different probability thresholds.\nDetCurveDisplay : DET curve visualization.\nplot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n\nExamples\n--------\n>>> import matplotlib.pyplot as plt  # doctest: +SKIP\n>>> from sklearn import datasets, metrics, model_selection, svm\n>>> X, y = datasets.make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = model_selection.train_test_split(\n...     X, y, random_state=0)\n>>> clf = svm.SVC(random_state=0)\n>>> clf.fit(X_train, y_train)\nSVC(random_state=0)\n>>> metrics.plot_det_curve(clf, X_test, y_test)  # doctest: +SKIP\n>>> plt.show()                                   # doctest: +SKIP"
        }
      ]
    },
    {
      "name": "sklearn.metrics._plot.precision_recall_curve",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.metrics",
          "declaration": "average_precision_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "precision_recall_curve",
          "alias": null
        },
        {
          "module": "sklearn.metrics._plot.base",
          "declaration": "_get_response",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_matplotlib_support",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "PrecisionRecallDisplay",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "plot",
              "decorators": [
                "_deprecate_positional_args"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "ax",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Axes object to plot on. If `None`, a new figure and axes is\ncreated."
                }
              ],
              "results": [
                {
                  "name": "display",
                  "type": null,
                  "description": "Object that stores computed values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Plot visualization.\n\nExtra keyword arguments will be passed to matplotlib's `plot`.\n\nParameters\n----------\nax : Matplotlib Axes, default=None\n    Axes object to plot on. If `None`, a new figure and axes is\n    created.\n\nname : str, default=None\n    Name of precision recall curve for labeling. If `None`, use the\n    name of the estimator.\n\n**kwargs : dict\n    Keyword arguments to be passed to matplotlib's `plot`.\n\nReturns\n-------\ndisplay : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n    Object that stores computed values."
            }
          ],
          "fullDocstring": "Precision Recall visualization.\n\nIt is recommend to use :func:`~sklearn.metrics.plot_precision_recall_curve`\nto create a visualizer. All parameters are stored as attributes.\n\nRead more in the :ref:`User Guide <visualizations>`.\n\nParameters\n-----------\nprecision : ndarray\n    Precision values.\n\nrecall : ndarray\n    Recall values.\n\naverage_precision : float, default=None\n    Average precision. If None, the average precision is not shown.\n\nestimator_name : str, default=None\n    Name of estimator. If None, then the estimator name is not shown.\n\npos_label : str or int, default=None\n    The class considered as the positive class. If None, the class will not\n    be shown in the legend.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nline_ : matplotlib Artist\n    Precision recall curve.\n\nax_ : matplotlib Axes\n    Axes with precision recall curve.\n\nfigure_ : matplotlib Figure\n    Figure containing the curve.\n\nSee Also\n--------\nprecision_recall_curve : Compute precision-recall pairs for different\n    probability thresholds.\nplot_precision_recall_curve : Plot Precision Recall Curve for binary\n    classifiers.\n\nExamples\n--------\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.metrics import (precision_recall_curve,\n...                              PrecisionRecallDisplay)\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.svm import SVC\n>>> X, y = make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n...                                                     random_state=0)\n>>> clf = SVC(random_state=0)\n>>> clf.fit(X_train, y_train)\nSVC(random_state=0)\n>>> predictions = clf.predict(X_test)\n>>> precision, recall, _ = precision_recall_curve(y_test, predictions)\n>>> disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n>>> disp.plot() # doctest: +SKIP"
        }
      ],
      "functions": [
        {
          "name": "plot_precision_recall_curve",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\nin which the last estimator is a classifier."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input values."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Binary target values."
            }
          ],
          "results": [
            {
              "name": "display",
              "type": null,
              "description": "Object that stores computed values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Plot Precision Recall Curve for binary classifiers.\n\nExtra keyword arguments will be passed to matplotlib's `plot`.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\nestimator : estimator instance\n    Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n    in which the last estimator is a classifier.\n\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input values.\n\ny : array-like of shape (n_samples,)\n    Binary target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nresponse_method : {'predict_proba', 'decision_function', 'auto'},                       default='auto'\n    Specifies whether to use :term:`predict_proba` or\n    :term:`decision_function` as the target response. If set to 'auto',\n    :term:`predict_proba` is tried first and if it does not exist\n    :term:`decision_function` is tried next.\n\nname : str, default=None\n    Name for labeling curve. If `None`, the name of the\n    estimator is used.\n\nax : matplotlib axes, default=None\n    Axes object to plot on. If `None`, a new figure and axes is created.\n\npos_label : str or int, default=None\n    The class considered as the positive class when computing the precision\n    and recall metrics. By default, `estimators.classes_[1]` is considered\n    as the positive class.\n\n    .. versionadded:: 0.24\n\n**kwargs : dict\n    Keyword arguments to be passed to matplotlib's `plot`.\n\nReturns\n-------\ndisplay : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n    Object that stores computed values.\n\nSee Also\n--------\nprecision_recall_curve : Compute precision-recall pairs for different\n    probability thresholds.\nPrecisionRecallDisplay : Precision Recall visualization."
        }
      ]
    },
    {
      "name": "sklearn.metrics._plot.roc_curve",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.metrics",
          "declaration": "auc",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "roc_curve",
          "alias": null
        },
        {
          "module": "sklearn.metrics._plot.base",
          "declaration": "_get_response",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_matplotlib_support",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "RocCurveDisplay",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "plot",
              "decorators": [
                "_deprecate_positional_args"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "ax",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Axes object to plot on. If `None`, a new figure and axes is\ncreated."
                }
              ],
              "results": [
                {
                  "name": "display",
                  "type": null,
                  "description": "Object that stores computed values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Plot visualization\n\nExtra keyword arguments will be passed to matplotlib's ``plot``.\n\nParameters\n----------\nax : matplotlib axes, default=None\n    Axes object to plot on. If `None`, a new figure and axes is\n    created.\n\nname : str, default=None\n    Name of ROC Curve for labeling. If `None`, use the name of the\n    estimator.\n\nReturns\n-------\ndisplay : :class:`~sklearn.metrics.plot.RocCurveDisplay`\n    Object that stores computed values."
            }
          ],
          "fullDocstring": "ROC Curve visualization.\n\nIt is recommend to use :func:`~sklearn.metrics.plot_roc_curve` to create a\nvisualizer. All parameters are stored as attributes.\n\nRead more in the :ref:`User Guide <visualizations>`.\n\nParameters\n----------\nfpr : ndarray\n    False positive rate.\n\ntpr : ndarray\n    True positive rate.\n\nroc_auc : float, default=None\n    Area under ROC curve. If None, the roc_auc score is not shown.\n\nestimator_name : str, default=None\n    Name of estimator. If None, the estimator name is not shown.\n\npos_label : str or int, default=None\n    The class considered as the positive class when computing the roc auc\n    metrics. By default, `estimators.classes_[1]` is considered\n    as the positive class.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nline_ : matplotlib Artist\n    ROC Curve.\n\nax_ : matplotlib Axes\n    Axes with ROC Curve.\n\nfigure_ : matplotlib Figure\n    Figure containing the curve.\n\nSee Also\n--------\nroc_curve : Compute Receiver operating characteristic (ROC) curve.\nplot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\nroc_auc_score : Compute the area under the ROC curve.\n\nExamples\n--------\n>>> import matplotlib.pyplot as plt  # doctest: +SKIP\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([0, 0, 1, 1])\n>>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n>>> roc_auc = metrics.auc(fpr, tpr)\n>>> display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,                                          estimator_name='example estimator')\n>>> display.plot()  # doctest: +SKIP\n>>> plt.show()      # doctest: +SKIP"
        }
      ],
      "functions": [
        {
          "name": "plot_roc_curve",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\nin which the last estimator is a classifier."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input values."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values."
            }
          ],
          "results": [
            {
              "name": "display",
              "type": null,
              "description": "Object that stores computed values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Plot Receiver operating characteristic (ROC) curve.\n\nExtra keyword arguments will be passed to matplotlib's `plot`.\n\nRead more in the :ref:`User Guide <visualizations>`.\n\nParameters\n----------\nestimator : estimator instance\n    Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n    in which the last estimator is a classifier.\n\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input values.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\ndrop_intermediate : boolean, default=True\n    Whether to drop some suboptimal thresholds which would not appear\n    on a plotted ROC curve. This is useful in order to create lighter\n    ROC curves.\n\nresponse_method : {'predict_proba', 'decision_function', 'auto'}     default='auto'\n    Specifies whether to use :term:`predict_proba` or\n    :term:`decision_function` as the target response. If set to 'auto',\n    :term:`predict_proba` is tried first and if it does not exist\n    :term:`decision_function` is tried next.\n\nname : str, default=None\n    Name of ROC Curve for labeling. If `None`, use the name of the\n    estimator.\n\nax : matplotlib axes, default=None\n    Axes object to plot on. If `None`, a new figure and axes is created.\n\npos_label : str or int, default=None\n    The class considered as the positive class when computing the roc auc\n    metrics. By default, `estimators.classes_[1]` is considered\n    as the positive class.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ndisplay : :class:`~sklearn.metrics.RocCurveDisplay`\n    Object that stores computed values.\n\nSee Also\n--------\nroc_curve : Compute Receiver operating characteristic (ROC) curve.\nRocCurveDisplay : ROC Curve visualization.\nroc_auc_score : Compute the area under the ROC curve.\n\nExamples\n--------\n>>> import matplotlib.pyplot as plt  # doctest: +SKIP\n>>> from sklearn import datasets, metrics, model_selection, svm\n>>> X, y = datasets.make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = model_selection.train_test_split(\n...     X, y, random_state=0)\n>>> clf = svm.SVC(random_state=0)\n>>> clf.fit(X_train, y_train)\nSVC(random_state=0)\n>>> metrics.plot_roc_curve(clf, X_test, y_test)  # doctest: +SKIP\n>>> plt.show()                                   # doctest: +SKIP"
        }
      ]
    },
    {
      "name": "sklearn.metrics._ranking",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csr_matrix",
          "alias": null
        },
        {
          "module": "scipy.stats",
          "declaration": "rankdata",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "UndefinedMetricWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics._base",
          "declaration": "_average_binary_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._base",
          "declaration": "_average_multiclass_ovo_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._base",
          "declaration": "_check_pos_label_consistency",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "label_binarize",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "assert_all_finite",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.utils._encode",
          "declaration": "_encode",
          "alias": null
        },
        {
          "module": "sklearn.utils._encode",
          "declaration": "_unique",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "stable_cumsum",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "type_of_target",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "count_nonzero",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "auc",
          "decorators": [],
          "parameters": [
            {
              "name": "x",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "x coordinates. These must be either monotonic increasing or monotonic\ndecreasing."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "y coordinates."
            }
          ],
          "results": [
            {
              "name": "auc",
              "type": "float",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Area Under the Curve (AUC) using the trapezoidal rule.\n\nThis is a general function, given points on a curve.  For computing the\narea under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\nway to summarize a precision-recall curve, see\n:func:`average_precision_score`.\n\nParameters\n----------\nx : ndarray of shape (n,)\n    x coordinates. These must be either monotonic increasing or monotonic\n    decreasing.\ny : ndarray of shape, (n,)\n    y coordinates.\n\nReturns\n-------\nauc : float\n\nSee Also\n--------\nroc_auc_score : Compute the area under the ROC curve.\naverage_precision_score : Compute average precision from prediction scores.\nprecision_recall_curve : Compute precision-recall pairs for different\n    probability thresholds.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([1, 1, 2, 2])\n>>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n>>> metrics.auc(fpr, tpr)\n0.75"
        },
        {
          "name": "average_precision_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True binary labels or binary label indicators."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores, can either be probability estimates of the positive\nclass, confidence values, or non-thresholded measure of decisions\n(as returned by :term:`decision_function` on some classifiers)."
            }
          ],
          "results": [
            {
              "name": "average_precision",
              "type": "float",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute average precision (AP) from prediction scores.\n\nAP summarizes a precision-recall curve as the weighted mean of precisions\nachieved at each threshold, with the increase in recall from the previous\nthreshold used as the weight:\n\n.. math::\n    \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n\nwhere :math:`P_n` and :math:`R_n` are the precision and recall at the nth\nthreshold [1]_. This implementation is not interpolated and is different\nfrom computing the area under the precision-recall curve with the\ntrapezoidal rule, which uses linear interpolation and can be too\noptimistic.\n\nNote: this implementation is restricted to the binary classification task\nor multilabel classification task.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : ndarray of shape (n_samples,) or (n_samples, n_classes)\n    True binary labels or binary label indicators.\n\ny_score : ndarray of shape (n_samples,) or (n_samples, n_classes)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by :term:`decision_function` on some classifiers).\n\naverage : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'\n    If ``None``, the scores for each class are returned. Otherwise,\n    this determines the type of averaging performed on the data:\n\n    ``'micro'``:\n        Calculate metrics globally by considering each element of the label\n        indicator matrix as a label.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average, weighted\n        by support (the number of true instances for each label).\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average.\n\n    Will be ignored when ``y_true`` is binary.\n\npos_label : int or str, default=1\n    The label of the positive class. Only applied to binary ``y_true``.\n    For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\naverage_precision : float\n\nSee Also\n--------\nroc_auc_score : Compute the area under the ROC curve.\nprecision_recall_curve : Compute precision-recall pairs for different\n    probability thresholds.\n\nNotes\n-----\n.. versionchanged:: 0.19\n  Instead of linearly interpolating between operating points, precisions\n  are weighted by the change in recall since the last operating point.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Average precision\n       <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n       oldid=793358396#Average_precision>`_\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import average_precision_score\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> average_precision_score(y_true, y_scores)\n0.83..."
        },
        {
          "name": "coverage_error",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True binary labels in binary indicator format."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores, can either be probability estimates of the positive\nclass, confidence values, or non-thresholded measure of decisions\n(as returned by \"decision_function\" on some classifiers)."
            }
          ],
          "results": [
            {
              "name": "coverage_error",
              "type": "float",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Coverage error measure.\n\nCompute how far we need to go through the ranked scores to cover all\ntrue labels. The best value is equal to the average number\nof labels in ``y_true`` per sample.\n\nTies in ``y_scores`` are broken by giving maximal rank that would have\nbeen assigned to all tied values.\n\nNote: Our implementation's score is 1 greater than the one given in\nTsoumakas et al., 2010. This extends it to handle the degenerate case\nin which an instance has 0 true labels.\n\nRead more in the :ref:`User Guide <coverage_error>`.\n\nParameters\n----------\ny_true : ndarray of shape (n_samples, n_labels)\n    True binary labels in binary indicator format.\n\ny_score : ndarray of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\ncoverage_error : float\n\nReferences\n----------\n.. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n       Mining multi-label data. In Data mining and knowledge discovery\n       handbook (pp. 667-685). Springer US."
        },
        {
          "name": "dcg_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True targets of multilabel classification, or true scores of entities\nto be ranked."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores, can either be probability estimates, confidence values,\nor non-thresholded measure of decisions (as returned by\n\"decision_function\" on some classifiers)."
            }
          ],
          "results": [
            {
              "name": "discounted_cumulative_gain",
              "type": "float",
              "description": "The averaged sample DCG scores."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Discounted Cumulative Gain.\n\nSum the true scores ranked in the order induced by the predicted scores,\nafter applying a logarithmic discount.\n\nThis ranking metric yields a high value if true labels are ranked high by\n``y_score``.\n\nUsually the Normalized Discounted Cumulative Gain (NDCG, computed by\nndcg_score) is preferred.\n\nParameters\n----------\ny_true : ndarray of shape (n_samples, n_labels)\n    True targets of multilabel classification, or true scores of entities\n    to be ranked.\n\ny_score : ndarray of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates, confidence values,\n    or non-thresholded measure of decisions (as returned by\n    \"decision_function\" on some classifiers).\n\nk : int, default=None\n    Only consider the highest k scores in the ranking. If None, use all\n    outputs.\n\nlog_base : float, default=2\n    Base of the logarithm used for the discount. A low value means a\n    sharper discount (top results are more important).\n\nsample_weight : ndarray of shape (n_samples,), default=None\n    Sample weights. If None, all samples are given the same weight.\n\nignore_ties : bool, default=False\n    Assume that there are no ties in y_score (which is likely to be the\n    case if y_score is continuous) for efficiency gains.\n\nReturns\n-------\ndiscounted_cumulative_gain : float\n    The averaged sample DCG scores.\n\nSee Also\n--------\nndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n    Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n    have a score between 0 and 1.\n\nReferences\n----------\n`Wikipedia entry for Discounted Cumulative Gain\n<https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.\n\nJarvelin, K., & Kekalainen, J. (2002).\nCumulated gain-based evaluation of IR techniques. ACM Transactions on\nInformation Systems (TOIS), 20(4), 422-446.\n\nWang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\nA theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\nAnnual Conference on Learning Theory (COLT 2013).\n\nMcSherry, F., & Najork, M. (2008, March). Computing information retrieval\nperformance measures efficiently in the presence of tied scores. In\nEuropean conference on information retrieval (pp. 414-421). Springer,\nBerlin, Heidelberg.\n\nExamples\n--------\n>>> from sklearn.metrics import dcg_score\n>>> # we have groud-truth relevance of some answers to a query:\n>>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n>>> # we predict scores for the answers\n>>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n>>> dcg_score(true_relevance, scores)\n9.49...\n>>> # we can set k to truncate the sum; only top k answers contribute\n>>> dcg_score(true_relevance, scores, k=2)\n5.63...\n>>> # now we have some ties in our prediction\n>>> scores = np.asarray([[1, 0, 0, 0, 1]])\n>>> # by default ties are averaged, so here we get the average true\n>>> # relevance of our top predictions: (10 + 5) / 2 = 7.5\n>>> dcg_score(true_relevance, scores, k=1)\n7.5\n>>> # we can choose to ignore ties for faster results, but only\n>>> # if we know there aren't ties in our scores, otherwise we get\n>>> # wrong results:\n>>> dcg_score(true_relevance,\n...           scores, k=1, ignore_ties=True)\n5.0"
        },
        {
          "name": "det_curve",
          "decorators": [],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True binary labels. If labels are not either {-1, 1} or {0, 1}, then\npos_label should be explicitly given."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores, can either be probability estimates of the positive\nclass, confidence values, or non-thresholded measure of decisions\n(as returned by \"decision_function\" on some classifiers)."
            },
            {
              "name": "pos_label",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The label of the positive class.\nWhen ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n``pos_label`` is set to 1, otherwise an error will be raised."
            },
            {
              "name": "sample_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Sample weights."
            }
          ],
          "results": [
            {
              "name": "fpr",
              "type": null,
              "description": "False positive rate (FPR) such that element i is the false positive\nrate of predictions with score >= thresholds[i]. This is occasionally\nreferred to as false acceptance propability or fall-out."
            },
            {
              "name": "fnr",
              "type": null,
              "description": "False negative rate (FNR) such that element i is the false negative\nrate of predictions with score >= thresholds[i]. This is occasionally\nreferred to as false rejection or miss rate."
            },
            {
              "name": "thresholds",
              "type": null,
              "description": "Decreasing score values."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute error rates for different probability thresholds.\n\n.. note::\n   This metric is used for evaluation of ranking and error tradeoffs of\n   a binary classification task.\n\nRead more in the :ref:`User Guide <det_curve>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\ny_true : ndarray of shape (n_samples,)\n    True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n    pos_label should be explicitly given.\n\ny_score : ndarray of shape of (n_samples,)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\npos_label : int or str, default=None\n    The label of the positive class.\n    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n    ``pos_label`` is set to 1, otherwise an error will be raised.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nfpr : ndarray of shape (n_thresholds,)\n    False positive rate (FPR) such that element i is the false positive\n    rate of predictions with score >= thresholds[i]. This is occasionally\n    referred to as false acceptance propability or fall-out.\n\nfnr : ndarray of shape (n_thresholds,)\n    False negative rate (FNR) such that element i is the false negative\n    rate of predictions with score >= thresholds[i]. This is occasionally\n    referred to as false rejection or miss rate.\n\nthresholds : ndarray of shape (n_thresholds,)\n    Decreasing score values.\n\nSee Also\n--------\nplot_det_curve : Plot detection error tradeoff (DET) curve.\nDetCurveDisplay : DET curve visualization.\nroc_curve : Compute Receiver operating characteristic (ROC) curve.\nprecision_recall_curve : Compute precision-recall curve.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import det_curve\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, fnr, thresholds = det_curve(y_true, y_scores)\n>>> fpr\narray([0.5, 0.5, 0. ])\n>>> fnr\narray([0. , 0.5, 0.5])\n>>> thresholds\narray([0.35, 0.4 , 0.8 ])"
        },
        {
          "name": "label_ranking_average_precision_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True binary labels in binary indicator format."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores, can either be probability estimates of the positive\nclass, confidence values, or non-thresholded measure of decisions\n(as returned by \"decision_function\" on some classifiers)."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": "float",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute ranking-based average precision.\n\nLabel ranking average precision (LRAP) is the average over each ground\ntruth label assigned to each sample, of the ratio of true vs. total\nlabels with lower score.\n\nThis metric is used in multilabel ranking problem, where the goal\nis to give better rank to the labels associated to each sample.\n\nThe obtained score is always strictly greater than 0 and\nthe best value is 1.\n\nRead more in the :ref:`User Guide <label_ranking_average_precision>`.\n\nParameters\n----------\ny_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n    True binary labels in binary indicator format.\n\ny_score : ndarray of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nscore : float\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import label_ranking_average_precision_score\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n>>> label_ranking_average_precision_score(y_true, y_score)\n0.416..."
        },
        {
          "name": "label_ranking_loss",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True binary labels in binary indicator format."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores, can either be probability estimates of the positive\nclass, confidence values, or non-thresholded measure of decisions\n(as returned by \"decision_function\" on some classifiers)."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": "float",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Ranking loss measure.\n\nCompute the average number of label pairs that are incorrectly ordered\ngiven y_score weighted by the size of the label set and the number of\nlabels not in the label set.\n\nThis is similar to the error set size, but weighted by the number of\nrelevant and irrelevant labels. The best performance is achieved with\na ranking loss of zero.\n\nRead more in the :ref:`User Guide <label_ranking_loss>`.\n\n.. versionadded:: 0.17\n   A function *label_ranking_loss*\n\nParameters\n----------\ny_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n    True binary labels in binary indicator format.\n\ny_score : ndarray of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float\n\nReferences\n----------\n.. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n       Mining multi-label data. In Data mining and knowledge discovery\n       handbook (pp. 667-685). Springer US."
        },
        {
          "name": "ndcg_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True targets of multilabel classification, or true scores of entities\nto be ranked."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores, can either be probability estimates, confidence values,\nor non-thresholded measure of decisions (as returned by\n\"decision_function\" on some classifiers)."
            }
          ],
          "results": [
            {
              "name": "normalized_discounted_cumulative_gain",
              "type": null,
              "description": "The averaged NDCG scores for all samples."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Normalized Discounted Cumulative Gain.\n\nSum the true scores ranked in the order induced by the predicted scores,\nafter applying a logarithmic discount. Then divide by the best possible\nscore (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n0 and 1.\n\nThis ranking metric yields a high value if true labels are ranked high by\n``y_score``.\n\nParameters\n----------\ny_true : ndarray of shape (n_samples, n_labels)\n    True targets of multilabel classification, or true scores of entities\n    to be ranked.\n\ny_score : ndarray of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates, confidence values,\n    or non-thresholded measure of decisions (as returned by\n    \"decision_function\" on some classifiers).\n\nk : int, default=None\n    Only consider the highest k scores in the ranking. If None, use all\n    outputs.\n\nsample_weight : ndarray of shape (n_samples,), default=None\n    Sample weights. If None, all samples are given the same weight.\n\nignore_ties : bool, default=False\n    Assume that there are no ties in y_score (which is likely to be the\n    case if y_score is continuous) for efficiency gains.\n\nReturns\n-------\nnormalized_discounted_cumulative_gain : float in [0., 1.]\n    The averaged NDCG scores for all samples.\n\nSee Also\n--------\ndcg_score : Discounted Cumulative Gain (not normalized).\n\nReferences\n----------\n`Wikipedia entry for Discounted Cumulative Gain\n<https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n\nJarvelin, K., & Kekalainen, J. (2002).\nCumulated gain-based evaluation of IR techniques. ACM Transactions on\nInformation Systems (TOIS), 20(4), 422-446.\n\nWang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\nA theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\nAnnual Conference on Learning Theory (COLT 2013)\n\nMcSherry, F., & Najork, M. (2008, March). Computing information retrieval\nperformance measures efficiently in the presence of tied scores. In\nEuropean conference on information retrieval (pp. 414-421). Springer,\nBerlin, Heidelberg.\n\nExamples\n--------\n>>> from sklearn.metrics import ndcg_score\n>>> # we have groud-truth relevance of some answers to a query:\n>>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n>>> # we predict some scores (relevance) for the answers\n>>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n>>> ndcg_score(true_relevance, scores)\n0.69...\n>>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n>>> ndcg_score(true_relevance, scores)\n0.49...\n>>> # we can set k to truncate the sum; only top k answers contribute.\n>>> ndcg_score(true_relevance, scores, k=4)\n0.35...\n>>> # the normalization takes k into account so a perfect answer\n>>> # would still get 1.0\n>>> ndcg_score(true_relevance, true_relevance, k=4)\n1.0\n>>> # now we have some ties in our prediction\n>>> scores = np.asarray([[1, 0, 0, 0, 1]])\n>>> # by default ties are averaged, so here we get the average (normalized)\n>>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n>>> ndcg_score(true_relevance, scores, k=1)\n0.75\n>>> # we can choose to ignore ties for faster results, but only\n>>> # if we know there aren't ties in our scores, otherwise we get\n>>> # wrong results:\n>>> ndcg_score(true_relevance,\n...           scores, k=1, ignore_ties=True)\n0.5"
        },
        {
          "name": "precision_recall_curve",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True binary labels. If labels are not either {-1, 1} or {0, 1}, then\npos_label should be explicitly given."
            },
            {
              "name": "probas_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated probabilities or output of a decision function."
            }
          ],
          "results": [
            {
              "name": "precision",
              "type": null,
              "description": "Precision values such that element i is the precision of\npredictions with score >= thresholds[i] and the last element is 1."
            },
            {
              "name": "recall",
              "type": null,
              "description": "Decreasing recall values such that element i is the recall of\npredictions with score >= thresholds[i] and the last element is 0."
            },
            {
              "name": "thresholds",
              "type": null,
              "description": "Increasing thresholds on the decision function used to compute\nprecision and recall. n_thresholds <= len(np.unique(probas_pred))."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute precision-recall pairs for different probability thresholds.\n\nNote: this implementation is restricted to the binary classification task.\n\nThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\ntrue positives and ``fp`` the number of false positives. The precision is\nintuitively the ability of the classifier not to label as positive a sample\nthat is negative.\n\nThe recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\ntrue positives and ``fn`` the number of false negatives. The recall is\nintuitively the ability of the classifier to find all the positive samples.\n\nThe last precision and recall values are 1. and 0. respectively and do not\nhave a corresponding threshold. This ensures that the graph starts on the\ny axis.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : ndarray of shape (n_samples,)\n    True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n    pos_label should be explicitly given.\n\nprobas_pred : ndarray of shape (n_samples,)\n    Estimated probabilities or output of a decision function.\n\npos_label : int or str, default=None\n    The label of the positive class.\n    When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n    ``pos_label`` is set to 1, otherwise an error will be raised.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nprecision : ndarray of shape (n_thresholds + 1,)\n    Precision values such that element i is the precision of\n    predictions with score >= thresholds[i] and the last element is 1.\n\nrecall : ndarray of shape (n_thresholds + 1,)\n    Decreasing recall values such that element i is the recall of\n    predictions with score >= thresholds[i] and the last element is 0.\n\nthresholds : ndarray of shape (n_thresholds,)\n    Increasing thresholds on the decision function used to compute\n    precision and recall. n_thresholds <= len(np.unique(probas_pred)).\n\nSee Also\n--------\nplot_precision_recall_curve : Plot Precision Recall Curve for binary\n    classifiers.\nPrecisionRecallDisplay : Precision Recall visualization.\naverage_precision_score : Compute average precision from prediction scores.\ndet_curve: Compute error rates for different probability thresholds.\nroc_curve : Compute Receiver operating characteristic (ROC) curve.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import precision_recall_curve\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> precision, recall, thresholds = precision_recall_curve(\n...     y_true, y_scores)\n>>> precision\narray([0.66666667, 0.5       , 1.        , 1.        ])\n>>> recall\narray([1. , 0.5, 0.5, 0. ])\n>>> thresholds\narray([0.35, 0.4 , 0.8 ])"
        },
        {
          "name": "roc_auc_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True labels or binary label indicators. The binary and multiclass cases\nexpect labels with shape (n_samples,) while the multilabel case expects\nbinary label indicators with shape (n_samples, n_classes)."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores.\n\n* In the binary case, it corresponds to an array of shape\n  `(n_samples,)`. Both probability estimates and non-thresholded\n  decision values can be provided. The probability estimates correspond\n  to the **probability of the class with the greater label**,\n  i.e. `estimator.classes_[1]` and thus\n  `estimator.predict_proba(X, y)[:, 1]`. The decision values\n  corresponds to the output of `estimator.decision_function(X, y)`.\n  See more information in the :ref:`User guide <roc_auc_binary>`;\n* In the multiclass case, it corresponds to an array of shape\n  `(n_samples, n_classes)` of probability estimates provided by the\n  `predict_proba` method. The probability estimates **must**\n  sum to 1 across the possible classes. In addition, the order of the\n  class scores must correspond to the order of ``labels``,\n  if provided, or else to the numerical or lexicographical order of\n  the labels in ``y_true``. See more information in the\n  :ref:`User guide <roc_auc_multiclass>`;\n* In the multilabel case, it corresponds to an array of shape\n  `(n_samples, n_classes)`. Probability estimates are provided by the\n  `predict_proba` method and the non-thresholded decision values by\n  the `decision_function` method. The probability estimates correspond\n  to the **probability of the class with the greater label for each\n  output** of the classifier. See more information in the\n  :ref:`User guide <roc_auc_multilabel>`."
            }
          ],
          "results": [
            {
              "name": "auc",
              "type": "float",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\nfrom prediction scores.\n\nNote: this implementation can be used with binary, multiclass and\nmultilabel classification, but some restrictions apply (see Parameters).\n\nRead more in the :ref:`User Guide <roc_metrics>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n    True labels or binary label indicators. The binary and multiclass cases\n    expect labels with shape (n_samples,) while the multilabel case expects\n    binary label indicators with shape (n_samples, n_classes).\n\ny_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n    Target scores.\n\n    * In the binary case, it corresponds to an array of shape\n      `(n_samples,)`. Both probability estimates and non-thresholded\n      decision values can be provided. The probability estimates correspond\n      to the **probability of the class with the greater label**,\n      i.e. `estimator.classes_[1]` and thus\n      `estimator.predict_proba(X, y)[:, 1]`. The decision values\n      corresponds to the output of `estimator.decision_function(X, y)`.\n      See more information in the :ref:`User guide <roc_auc_binary>`;\n    * In the multiclass case, it corresponds to an array of shape\n      `(n_samples, n_classes)` of probability estimates provided by the\n      `predict_proba` method. The probability estimates **must**\n      sum to 1 across the possible classes. In addition, the order of the\n      class scores must correspond to the order of ``labels``,\n      if provided, or else to the numerical or lexicographical order of\n      the labels in ``y_true``. See more information in the\n      :ref:`User guide <roc_auc_multiclass>`;\n    * In the multilabel case, it corresponds to an array of shape\n      `(n_samples, n_classes)`. Probability estimates are provided by the\n      `predict_proba` method and the non-thresholded decision values by\n      the `decision_function` method. The probability estimates correspond\n      to the **probability of the class with the greater label for each\n      output** of the classifier. See more information in the\n      :ref:`User guide <roc_auc_multilabel>`.\n\naverage : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n    If ``None``, the scores for each class are returned. Otherwise,\n    this determines the type of averaging performed on the data:\n    Note: multiclass ROC AUC currently only handles the 'macro' and\n    'weighted' averages.\n\n    ``'micro'``:\n        Calculate metrics globally by considering each element of the label\n        indicator matrix as a label.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average, weighted\n        by support (the number of true instances for each label).\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average.\n\n    Will be ignored when ``y_true`` is binary.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmax_fpr : float > 0 and <= 1, default=None\n    If not ``None``, the standardized partial AUC [2]_ over the range\n    [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n    should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n    computation currently is not supported for multiclass.\n\nmulti_class : {'raise', 'ovr', 'ovo'}, default='raise'\n    Only used for multiclass targets. Determines the type of configuration\n    to use. The default value raises an error, so either\n    ``'ovr'`` or ``'ovo'`` must be passed explicitly.\n\n    ``'ovr'``:\n        Stands for One-vs-rest. Computes the AUC of each class\n        against the rest [3]_ [4]_. This\n        treats the multiclass case in the same way as the multilabel case.\n        Sensitive to class imbalance even when ``average == 'macro'``,\n        because class imbalance affects the composition of each of the\n        'rest' groupings.\n    ``'ovo'``:\n        Stands for One-vs-one. Computes the average AUC of all\n        possible pairwise combinations of classes [5]_.\n        Insensitive to class imbalance when\n        ``average == 'macro'``.\n\nlabels : array-like of shape (n_classes,), default=None\n    Only used for multiclass targets. List of labels that index the\n    classes in ``y_score``. If ``None``, the numerical or lexicographical\n    order of the labels in ``y_true`` is used.\n\nReturns\n-------\nauc : float\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Receiver operating characteristic\n        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n\n.. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n        <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n\n.. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n       probability estimation trees (Section 6.2), CeDER Working Paper\n       #IS-00-04, Stern School of Business, New York University.\n\n.. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n        Recognition Letters, 27(8), 861-874.\n        <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n\n.. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n        Under the ROC Curve for Multiple Class Classification Problems.\n        Machine Learning, 45(2), 171-186.\n        <http://link.springer.com/article/10.1023/A:1010920819831>`_\n\nSee Also\n--------\naverage_precision_score : Area under the precision-recall curve.\nroc_curve : Compute Receiver operating characteristic (ROC) curve.\nplot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n\nExamples\n--------\nBinary case:\n\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.metrics import roc_auc_score\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)\n>>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n0.99...\n>>> roc_auc_score(y, clf.decision_function(X))\n0.99...\n\nMulticlass case:\n\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n>>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n0.99...\n\nMultilabel case:\n\n>>> from sklearn.datasets import make_multilabel_classification\n>>> from sklearn.multioutput import MultiOutputClassifier\n>>> X, y = make_multilabel_classification(random_state=0)\n>>> clf = MultiOutputClassifier(clf).fit(X, y)\n>>> # get a list of n_output containing probability arrays of shape\n>>> # (n_samples, n_classes)\n>>> y_pred = clf.predict_proba(X)\n>>> # extract the positive columns for each output\n>>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])\n>>> roc_auc_score(y, y_pred, average=None)\narray([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\n>>> from sklearn.linear_model import RidgeClassifierCV\n>>> clf = RidgeClassifierCV().fit(X, y)\n>>> roc_auc_score(y, clf.decision_function(X), average=None)\narray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])"
        },
        {
          "name": "roc_curve",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True binary labels. If labels are not either {-1, 1} or {0, 1}, then\npos_label should be explicitly given."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores, can either be probability estimates of the positive\nclass, confidence values, or non-thresholded measure of decisions\n(as returned by \"decision_function\" on some classifiers)."
            }
          ],
          "results": [
            {
              "name": "fpr",
              "type": null,
              "description": "Increasing false positive rates such that element i is the false\npositive rate of predictions with score >= `thresholds[i]`."
            },
            {
              "name": "tpr",
              "type": null,
              "description": "Increasing true positive rates such that element `i` is the true\npositive rate of predictions with score >= `thresholds[i]`."
            },
            {
              "name": "thresholds",
              "type": null,
              "description": "Decreasing thresholds on the decision function used to compute\nfpr and tpr. `thresholds[0]` represents no instances being predicted\nand is arbitrarily set to `max(y_score) + 1`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Receiver operating characteristic (ROC).\n\nNote: this implementation is restricted to the binary classification task.\n\nRead more in the :ref:`User Guide <roc_metrics>`.\n\nParameters\n----------\ny_true : ndarray of shape (n_samples,)\n    True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n    pos_label should be explicitly given.\n\ny_score : ndarray of shape (n_samples,)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\npos_label : int or str, default=None\n    The label of the positive class.\n    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n    ``pos_label`` is set to 1, otherwise an error will be raised.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\ndrop_intermediate : bool, default=True\n    Whether to drop some suboptimal thresholds which would not appear\n    on a plotted ROC curve. This is useful in order to create lighter\n    ROC curves.\n\n    .. versionadded:: 0.17\n       parameter *drop_intermediate*.\n\nReturns\n-------\nfpr : ndarray of shape (>2,)\n    Increasing false positive rates such that element i is the false\n    positive rate of predictions with score >= `thresholds[i]`.\n\ntpr : ndarray of shape (>2,)\n    Increasing true positive rates such that element `i` is the true\n    positive rate of predictions with score >= `thresholds[i]`.\n\nthresholds : ndarray of shape = (n_thresholds,)\n    Decreasing thresholds on the decision function used to compute\n    fpr and tpr. `thresholds[0]` represents no instances being predicted\n    and is arbitrarily set to `max(y_score) + 1`.\n\nSee Also\n--------\nplot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\nRocCurveDisplay : ROC Curve visualization.\ndet_curve: Compute error rates for different probability thresholds.\nroc_auc_score : Compute the area under the ROC curve.\n\nNotes\n-----\nSince the thresholds are sorted from low to high values, they\nare reversed upon returning them to ensure they correspond to both ``fpr``\nand ``tpr``, which are sorted in reversed order during their calculation.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Receiver operating characteristic\n        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n\n.. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n       Letters, 2006, 27(8):861-874.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([1, 1, 2, 2])\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n>>> fpr\narray([0. , 0. , 0.5, 0.5, 1. ])\n>>> tpr\narray([0. , 0.5, 0.5, 1. , 1. ])\n>>> thresholds\narray([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])"
        },
        {
          "name": "top_k_accuracy_score",
          "decorators": [],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "True labels."
            },
            {
              "name": "y_score",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target scores. These can be either probability estimates or\nnon-thresholded decision values (as returned by\n:term:`decision_function` on some classifiers). The binary case expects\nscores with shape (n_samples,) while the multiclass case expects scores\nwith shape (n_samples, n_classes). In the nulticlass case, the order of\nthe class scores must correspond to the order of ``labels``, if\nprovided, or else to the numerical or lexicographical order of the\nlabels in ``y_true``."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": "float",
              "description": "The top-k accuracy score. The best performance is 1 with\n`normalize == True` and the number of samples with\n`normalize == False`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Top-k Accuracy classification score.\n\nThis metric computes the number of times where the correct label is among\nthe top `k` labels predicted (ranked by predicted scores). Note that the\nmultilabel case isn't covered here.\n\nRead more in the :ref:`User Guide <top_k_accuracy_score>`\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    True labels.\n\ny_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n    Target scores. These can be either probability estimates or\n    non-thresholded decision values (as returned by\n    :term:`decision_function` on some classifiers). The binary case expects\n    scores with shape (n_samples,) while the multiclass case expects scores\n    with shape (n_samples, n_classes). In the nulticlass case, the order of\n    the class scores must correspond to the order of ``labels``, if\n    provided, or else to the numerical or lexicographical order of the\n    labels in ``y_true``.\n\nk : int, default=2\n    Number of most likely outcomes considered to find the correct label.\n\nnormalize : bool, default=True\n    If `True`, return the fraction of correctly classified samples.\n    Otherwise, return the number of correctly classified samples.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If `None`, all samples are given the same weight.\n\nlabels : array-like of shape (n_classes,), default=None\n    Multiclass only. List of labels that index the classes in ``y_score``.\n    If ``None``, the numerical or lexicographical order of the labels in\n    ``y_true`` is used.\n\nReturns\n-------\nscore : float\n    The top-k accuracy score. The best performance is 1 with\n    `normalize == True` and the number of samples with\n    `normalize == False`.\n\nSee also\n--------\naccuracy_score\n\nNotes\n-----\nIn cases where two or more labels are assigned equal predicted scores,\nthe labels with the highest indices will be chosen first. This might\nimpact the result if the correct label falls after the threshold because\nof that.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import top_k_accuracy_score\n>>> y_true = np.array([0, 1, 2, 2])\n>>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n...                     [0.3, 0.4, 0.2],  # 1 is in top 2\n...                     [0.2, 0.4, 0.3],  # 2 is in top 2\n...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n>>> top_k_accuracy_score(y_true, y_score, k=2)\n0.75\n>>> # Not normalizing gives the number of \"correctly\" classified samples\n>>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n3"
        }
      ]
    },
    {
      "name": "sklearn.metrics._regression",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn._loss.glm_distribution",
          "declaration": "TweedieDistribution",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "UndefinedMetricWarning",
          "alias": null
        },
        {
          "module": "sklearn.utils.stats",
          "declaration": "_weighted_percentile",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "column_or_1d",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "explained_variance_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": null,
              "description": "The explained variance or ndarray if 'multioutput' is 'raw_values'."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Explained variance regression score function.\n\nBest possible score is 1.0, lower values are worse.\n\nRead more in the :ref:`User Guide <explained_variance_score>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output scores.\n    Array-like value defines weights used to average scores.\n\n    'raw_values' :\n        Returns a full set of scores in case of multioutput input.\n\n    'uniform_average' :\n        Scores of all outputs are averaged with uniform weight.\n\n    'variance_weighted' :\n        Scores of all outputs are averaged, weighted by the variances\n        of each individual output.\n\nReturns\n-------\nscore : float or ndarray of floats\n    The explained variance or ndarray if 'multioutput' is 'raw_values'.\n\nNotes\n-----\nThis is not a symmetric function.\n\nExamples\n--------\n>>> from sklearn.metrics import explained_variance_score\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> explained_variance_score(y_true, y_pred)\n0.957...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n0.983..."
        },
        {
          "name": "max_error",
          "decorators": [],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values."
            }
          ],
          "results": [
            {
              "name": "max_error",
              "type": "float",
              "description": "A positive floating point value (the best value is 0.0)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "max_error metric calculates the maximum residual error.\n\nRead more in the :ref:`User Guide <max_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated target values.\n\nReturns\n-------\nmax_error : float\n    A positive floating point value (the best value is 0.0).\n\nExamples\n--------\n>>> from sklearn.metrics import max_error\n>>> y_true = [3, 2, 7, 1]\n>>> y_pred = [4, 2, 7, 1]\n>>> max_error(y_true, y_pred)\n1"
        },
        {
          "name": "mean_absolute_error",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": null,
              "description": "If multioutput is 'raw_values', then mean absolute error is returned\nfor each output separately.\nIf multioutput is 'uniform_average' or an ndarray of weights, then the\nweighted average of all output errors is returned.\n\nMAE output is non-negative floating point. The best value is 0.0."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Mean absolute error regression loss.\n\nRead more in the :ref:`User Guide <mean_absolute_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\n\nReturns\n-------\nloss : float or ndarray of floats\n    If multioutput is 'raw_values', then mean absolute error is returned\n    for each output separately.\n    If multioutput is 'uniform_average' or an ndarray of weights, then the\n    weighted average of all output errors is returned.\n\n    MAE output is non-negative floating point. The best value is 0.0.\n\nExamples\n--------\n>>> from sklearn.metrics import mean_absolute_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_absolute_error(y_true, y_pred)\n0.5\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> mean_absolute_error(y_true, y_pred)\n0.75\n>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\narray([0.5, 1. ])\n>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.85..."
        },
        {
          "name": "mean_absolute_percentage_error",
          "decorators": [],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values."
            },
            {
              "name": "sample_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Sample weights."
            },
            {
              "name": "multioutput",
              "type": "Any",
              "hasDefault": true,
              "default": "uniform_average",
              "limitation": null,
              "ignored": false,
              "description": "Defines aggregating of multiple output values.\nArray-like value defines weights used to average errors.\nIf input is list then the shape must be (n_outputs,).\n\n'raw_values' :\n    Returns a full set of errors in case of multioutput input.\n\n'uniform_average' :\n    Errors of all outputs are averaged with uniform weight."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": null,
              "description": "If multioutput is 'raw_values', then mean absolute percentage error\nis returned for each output separately.\nIf multioutput is 'uniform_average' or an ndarray of weights, then the\nweighted average of all output errors is returned.\n\nMAPE output is non-negative floating point. The best value is 0.0.\nBut note the fact that bad predictions can lead to arbitarily large\nMAPE values, especially if some y_true values are very close to zero.\nNote that we return a large value instead of `inf` when y_true is zero."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Mean absolute percentage error regression loss.\n\nNote here that we do not represent the output as a percentage in range\n[0, 100]. Instead, we represent it in range [0, 1/eps]. Read more in the\n:ref:`User Guide <mean_absolute_percentage_error>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n    If input is list then the shape must be (n_outputs,).\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nReturns\n-------\nloss : float or ndarray of floats in the range [0, 1/eps]\n    If multioutput is 'raw_values', then mean absolute percentage error\n    is returned for each output separately.\n    If multioutput is 'uniform_average' or an ndarray of weights, then the\n    weighted average of all output errors is returned.\n\n    MAPE output is non-negative floating point. The best value is 0.0.\n    But note the fact that bad predictions can lead to arbitarily large\n    MAPE values, especially if some y_true values are very close to zero.\n    Note that we return a large value instead of `inf` when y_true is zero.\n\nExamples\n--------\n>>> from sklearn.metrics import mean_absolute_percentage_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_absolute_percentage_error(y_true, y_pred)\n0.3273...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> mean_absolute_percentage_error(y_true, y_pred)\n0.5515...\n>>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.6198..."
        },
        {
          "name": "mean_gamma_deviance",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values. Requires y_true > 0."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values. Requires y_pred > 0."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": "float",
              "description": "A non-negative floating point value (the best value is 0.0)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Mean Gamma deviance regression loss.\n\nGamma deviance is equivalent to the Tweedie deviance with\nthe power parameter `power=2`. It is invariant to scaling of\nthe target variable, and measures relative errors.\n\nRead more in the :ref:`User Guide <mean_tweedie_deviance>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values. Requires y_true > 0.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated target values. Requires y_pred > 0.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float\n    A non-negative floating point value (the best value is 0.0).\n\nExamples\n--------\n>>> from sklearn.metrics import mean_gamma_deviance\n>>> y_true = [2, 0.5, 1, 4]\n>>> y_pred = [0.5, 0.5, 2., 2.]\n>>> mean_gamma_deviance(y_true, y_pred)\n1.0568..."
        },
        {
          "name": "mean_poisson_deviance",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values. Requires y_true >= 0."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values. Requires y_pred > 0."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": "float",
              "description": "A non-negative floating point value (the best value is 0.0)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Mean Poisson deviance regression loss.\n\nPoisson deviance is equivalent to the Tweedie deviance with\nthe power parameter `power=1`.\n\nRead more in the :ref:`User Guide <mean_tweedie_deviance>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values. Requires y_true >= 0.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated target values. Requires y_pred > 0.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float\n    A non-negative floating point value (the best value is 0.0).\n\nExamples\n--------\n>>> from sklearn.metrics import mean_poisson_deviance\n>>> y_true = [2, 0, 1, 4]\n>>> y_pred = [0.5, 0.5, 2., 2.]\n>>> mean_poisson_deviance(y_true, y_pred)\n1.4260..."
        },
        {
          "name": "mean_squared_error",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": null,
              "description": "A non-negative floating point value (the best value is 0.0), or an\narray of floating point values, one for each individual target."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Mean squared error regression loss.\n\nRead more in the :ref:`User Guide <mean_squared_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nsquared : bool, default=True\n    If True returns MSE value, if False returns RMSE value.\n\nReturns\n-------\nloss : float or ndarray of floats\n    A non-negative floating point value (the best value is 0.0), or an\n    array of floating point values, one for each individual target.\n\nExamples\n--------\n>>> from sklearn.metrics import mean_squared_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_squared_error(y_true, y_pred)\n0.375\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_squared_error(y_true, y_pred, squared=False)\n0.612...\n>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n>>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n>>> mean_squared_error(y_true, y_pred)\n0.708...\n>>> mean_squared_error(y_true, y_pred, squared=False)\n0.822...\n>>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\narray([0.41666667, 1.        ])\n>>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.825..."
        },
        {
          "name": "mean_squared_log_error",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": null,
              "description": "A non-negative floating point value (the best value is 0.0), or an\narray of floating point values, one for each individual target."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Mean squared logarithmic error regression loss.\n\nRead more in the :ref:`User Guide <mean_squared_log_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors when the input is of multioutput\n        format.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nReturns\n-------\nloss : float or ndarray of floats\n    A non-negative floating point value (the best value is 0.0), or an\n    array of floating point values, one for each individual target.\n\nExamples\n--------\n>>> from sklearn.metrics import mean_squared_log_error\n>>> y_true = [3, 5, 2.5, 7]\n>>> y_pred = [2.5, 5, 4, 8]\n>>> mean_squared_log_error(y_true, y_pred)\n0.039...\n>>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n>>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n>>> mean_squared_log_error(y_true, y_pred)\n0.044...\n>>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\narray([0.00462428, 0.08377444])\n>>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.060..."
        },
        {
          "name": "mean_tweedie_deviance",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": "float",
              "description": "A non-negative floating point value (the best value is 0.0)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Mean Tweedie deviance regression loss.\n\nRead more in the :ref:`User Guide <mean_tweedie_deviance>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\npower : float, default=0\n    Tweedie power parameter. Either power <= 0 or power >= 1.\n\n    The higher `p` the less weight is given to extreme\n    deviations between true and predicted targets.\n\n    - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n    - power = 0 : Normal distribution, output corresponds to\n      mean_squared_error. y_true and y_pred can be any real numbers.\n    - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n      y_pred > 0.\n    - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n      and y_pred > 0.\n    - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n    - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n      and y_pred > 0.\n    - otherwise : Positive stable distribution. Requires: y_true > 0\n      and y_pred > 0.\n\nReturns\n-------\nloss : float\n    A non-negative floating point value (the best value is 0.0).\n\nExamples\n--------\n>>> from sklearn.metrics import mean_tweedie_deviance\n>>> y_true = [2, 0, 1, 4]\n>>> y_pred = [0.5, 0.5, 2., 2.]\n>>> mean_tweedie_deviance(y_true, y_pred, power=1)\n1.4260..."
        },
        {
          "name": "median_absolute_error",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": null,
              "description": "If multioutput is 'raw_values', then mean absolute error is returned\nfor each output separately.\nIf multioutput is 'uniform_average' or an ndarray of weights, then the\nweighted average of all output errors is returned."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Median absolute error regression loss.\n\nMedian absolute error output is non-negative floating point. The best value\nis 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\n\nParameters\n----------\ny_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n    Estimated target values.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values. Array-like value defines\n    weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nloss : float or ndarray of floats\n    If multioutput is 'raw_values', then mean absolute error is returned\n    for each output separately.\n    If multioutput is 'uniform_average' or an ndarray of weights, then the\n    weighted average of all output errors is returned.\n\nExamples\n--------\n>>> from sklearn.metrics import median_absolute_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> median_absolute_error(y_true, y_pred)\n0.5\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> median_absolute_error(y_true, y_pred)\n0.75\n>>> median_absolute_error(y_true, y_pred, multioutput='raw_values')\narray([0.5, 1. ])\n>>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.85"
        },
        {
          "name": "r2_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) target values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimated target values."
            }
          ],
          "results": [
            {
              "name": "z",
              "type": null,
              "description": "The R^2 score or ndarray of scores if 'multioutput' is\n'raw_values'."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "R^2 (coefficient of determination) regression score function.\n\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n\nRead more in the :ref:`User Guide <r2_score>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n\n    Defines aggregating of multiple output scores.\n    Array-like value defines weights used to average scores.\n    Default is \"uniform_average\".\n\n    'raw_values' :\n        Returns a full set of scores in case of multioutput input.\n\n    'uniform_average' :\n        Scores of all outputs are averaged with uniform weight.\n\n    'variance_weighted' :\n        Scores of all outputs are averaged, weighted by the variances\n        of each individual output.\n\n    .. versionchanged:: 0.19\n        Default value of multioutput is 'uniform_average'.\n\nReturns\n-------\nz : float or ndarray of floats\n    The R^2 score or ndarray of scores if 'multioutput' is\n    'raw_values'.\n\nNotes\n-----\nThis is not a symmetric function.\n\nUnlike most other scores, R^2 score may be negative (it need not actually\nbe the square of a quantity R).\n\nThis metric is not well-defined for single samples and will return a NaN\nvalue if n_samples is less than two.\n\nReferences\n----------\n.. [1] `Wikipedia entry on the Coefficient of determination\n        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n\nExamples\n--------\n>>> from sklearn.metrics import r2_score\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> r2_score(y_true, y_pred)\n0.948...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> r2_score(y_true, y_pred,\n...          multioutput='variance_weighted')\n0.938...\n>>> y_true = [1, 2, 3]\n>>> y_pred = [1, 2, 3]\n>>> r2_score(y_true, y_pred)\n1.0\n>>> y_true = [1, 2, 3]\n>>> y_pred = [2, 2, 2]\n>>> r2_score(y_true, y_pred)\n0.0\n>>> y_true = [1, 2, 3]\n>>> y_pred = [3, 2, 1]\n>>> r2_score(y_true, y_pred)\n-3.0"
        }
      ]
    },
    {
      "name": "sklearn.metrics._scorer",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "Counter",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Iterable",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_regressor",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "average_precision_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "balanced_accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "brier_score_loss",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "explained_variance_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "f1_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "jaccard_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "log_loss",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "max_error",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "mean_absolute_error",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "mean_absolute_percentage_error",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "mean_gamma_deviance",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "mean_poisson_deviance",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "mean_squared_error",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "mean_squared_log_error",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "median_absolute_error",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "precision_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "r2_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "recall_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "roc_auc_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "top_k_accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "adjusted_mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "adjusted_rand_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "completeness_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "fowlkes_mallows_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "homogeneity_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "normalized_mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "rand_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.cluster",
          "declaration": "v_measure_score",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "type_of_target",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "check_scoring",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The object to use to fit the data."
            },
            {
              "name": "scoring",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A string (see model evaluation documentation) or\na scorer callable object / function with signature\n``scorer(estimator, X, y)``."
            }
          ],
          "results": [
            {
              "name": "scoring",
              "type": "Callable",
              "description": "A scorer callable object / function with signature\n``scorer(estimator, X, y)``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Determine scorer from user options.\n\nA TypeError will be thrown if the estimator cannot be scored.\n\nParameters\n----------\nestimator : estimator object implementing 'fit'\n    The object to use to fit the data.\n\nscoring : str or callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\nallow_none : bool, default=False\n    If no scoring is specified and the estimator has no score function, we\n    can either return None or raise an exception.\n\nReturns\n-------\nscoring : callable\n    A scorer callable object / function with signature\n    ``scorer(estimator, X, y)``."
        },
        {
          "name": "get_scorer",
          "decorators": [],
          "parameters": [
            {
              "name": "scoring",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Scoring method as string. If callable it is returned as is."
            }
          ],
          "results": [
            {
              "name": "scorer",
              "type": "Callable",
              "description": "The scorer."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Get a scorer from string.\n\nRead more in the :ref:`User Guide <scoring_parameter>`.\n\nParameters\n----------\nscoring : str or callable\n    Scoring method as string. If callable it is returned as is.\n\nReturns\n-------\nscorer : callable\n    The scorer."
        },
        {
          "name": "make_scorer",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "score_func",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Score function (or loss function) with signature\n``score_func(y, y_pred, **kwargs)``."
            }
          ],
          "results": [
            {
              "name": "scorer",
              "type": "Callable",
              "description": "Callable object that returns a scalar score; greater is better."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Make a scorer from a performance metric or loss function.\n\nThis factory function wraps scoring functions for use in\n:class:`~sklearn.model_selection.GridSearchCV` and\n:func:`~sklearn.model_selection.cross_val_score`.\nIt takes a score function, such as :func:`~sklearn.metrics.accuracy_score`,\n:func:`~sklearn.metrics.mean_squared_error`,\n:func:`~sklearn.metrics.adjusted_rand_index` or\n:func:`~sklearn.metrics.average_precision`\nand returns a callable that scores an estimator's output.\nThe signature of the call is `(estimator, X, y)` where `estimator`\nis the model to be evaluated, `X` is the data and `y` is the\nground truth labeling (or `None` in the case of unsupervised models).\n\nRead more in the :ref:`User Guide <scoring>`.\n\nParameters\n----------\nscore_func : callable\n    Score function (or loss function) with signature\n    ``score_func(y, y_pred, **kwargs)``.\n\ngreater_is_better : bool, default=True\n    Whether score_func is a score function (default), meaning high is good,\n    or a loss function, meaning low is good. In the latter case, the\n    scorer object will sign-flip the outcome of the score_func.\n\nneeds_proba : bool, default=False\n    Whether score_func requires predict_proba to get probability estimates\n    out of a classifier.\n\n    If True, for binary `y_true`, the score function is supposed to accept\n    a 1D `y_pred` (i.e., probability of the positive class, shape\n    `(n_samples,)`).\n\nneeds_threshold : bool, default=False\n    Whether score_func takes a continuous decision certainty.\n    This only works for binary classification using estimators that\n    have either a decision_function or predict_proba method.\n\n    If True, for binary `y_true`, the score function is supposed to accept\n    a 1D `y_pred` (i.e., probability of the positive class or the decision\n    function, shape `(n_samples,)`).\n\n    For example ``average_precision`` or the area under the roc curve\n    can not be computed using discrete predictions alone.\n\n**kwargs : additional arguments\n    Additional parameters to be passed to score_func.\n\nReturns\n-------\nscorer : callable\n    Callable object that returns a scalar score; greater is better.\n\nExamples\n--------\n>>> from sklearn.metrics import fbeta_score, make_scorer\n>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n>>> ftwo_scorer\nmake_scorer(fbeta_score, beta=2)\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.svm import LinearSVC\n>>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n...                     scoring=ftwo_scorer)\n\nNotes\n-----\nIf `needs_proba=False` and `needs_threshold=False`, the score\nfunction is supposed to accept the output of :term:`predict`. If\n`needs_proba=True`, the score function is supposed to accept the\noutput of :term:`predict_proba` (For binary `y_true`, the score function is\nsupposed to accept probability of the positive class). If\n`needs_threshold=True`, the score function is supposed to accept the\noutput of :term:`decision_function`."
        }
      ]
    },
    {
      "name": "sklearn.metrics.cluster",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.metrics._bicluster",
          "declaration": "consensus_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "adjusted_mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "adjusted_rand_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "completeness_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "contingency_matrix",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "entropy",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "expected_mutual_information",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "fowlkes_mallows_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "homogeneity_completeness_v_measure",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "homogeneity_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "normalized_mutual_info_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "pair_confusion_matrix",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "rand_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._supervised",
          "declaration": "v_measure_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._unsupervised",
          "declaration": "calinski_harabasz_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._unsupervised",
          "declaration": "davies_bouldin_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics._unsupervised",
          "declaration": "silhouette_samples",
          "alias": null
        },
        {
          "module": "sklearn.metrics._unsupervised",
          "declaration": "silhouette_score",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.metrics.cluster._bicluster",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy.optimize",
          "declaration": "linear_sum_assignment",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_consistent_length",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "consensus_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "a",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Tuple of row and column indicators for a set of biclusters."
            },
            {
              "name": "b",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Another set of biclusters like ``a``."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "The similarity of two sets of biclusters.\n\nSimilarity between individual biclusters is computed. Then the\nbest matching between sets is found using the Hungarian algorithm.\nThe final score is the sum of similarities divided by the size of\nthe larger set.\n\nRead more in the :ref:`User Guide <biclustering>`.\n\nParameters\n----------\na : (rows, columns)\n    Tuple of row and column indicators for a set of biclusters.\n\nb : (rows, columns)\n    Another set of biclusters like ``a``.\n\nsimilarity : 'jaccard' or callable, default='jaccard'\n    May be the string \"jaccard\" to use the Jaccard coefficient, or\n    any function that takes four arguments, each of which is a 1d\n    indicator vector: (a_rows, a_columns, b_rows, b_columns).\n\nReferences\n----------\n\n* Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n  for bicluster acquisition\n  <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__."
        }
      ]
    },
    {
      "name": "sklearn.metrics.cluster._supervised",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "math",
          "declaration": "log",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": "sp"
        },
        {
          "module": "sklearn.metrics.cluster._expected_mutual_info_fast",
          "declaration": "expected_mutual_information",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_astype_copy_false",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "type_of_target",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_consistent_length",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "adjusted_mutual_info_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A clustering of the data into disjoint subsets."
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A clustering of the data into disjoint subsets."
            }
          ],
          "results": [
            {
              "name": "",
              "type": null,
              "description": "The AMI returns a value of 1 when the two partitions are identical\n(ie perfectly matched). Random partitions (independent labellings) have\nan expected AMI around 0 on average hence can be negative."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Adjusted Mutual Information between two clusterings.\n\nAdjusted Mutual Information (AMI) is an adjustment of the Mutual\nInformation (MI) score to account for chance. It accounts for the fact that\nthe MI is generally higher for two clusterings with a larger number of\nclusters, regardless of whether there is actually more information shared.\nFor two clusterings :math:`U` and :math:`V`, the AMI is given as::\n\n    AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\nBe mindful that this function is an order of magnitude slower than other\nmetrics, such as the Adjusted Rand Index.\n\nRead more in the :ref:`User Guide <mutual_info_score>`.\n\nParameters\n----------\nlabels_true : int array, shape = [n_samples]\n    A clustering of the data into disjoint subsets.\n\nlabels_pred : int array-like of shape (n_samples,)\n    A clustering of the data into disjoint subsets.\n\naverage_method : str, default='arithmetic'\n    How to compute the normalizer in the denominator. Possible options\n    are 'min', 'geometric', 'arithmetic', and 'max'.\n\n    .. versionadded:: 0.20\n\n    .. versionchanged:: 0.22\n       The default value of ``average_method`` changed from 'max' to\n       'arithmetic'.\n\nReturns\n-------\nami: float (upperlimited by 1.0)\n   The AMI returns a value of 1 when the two partitions are identical\n   (ie perfectly matched). Random partitions (independent labellings) have\n   an expected AMI around 0 on average hence can be negative.\n\nSee Also\n--------\nadjusted_rand_score : Adjusted Rand Index.\nmutual_info_score : Mutual Information (not adjusted for chance).\n\nExamples\n--------\n\nPerfect labelings are both homogeneous and complete, hence have\nscore 1.0::\n\n  >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n  >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n  ... # doctest: +SKIP\n  1.0\n  >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n  ... # doctest: +SKIP\n  1.0\n\nIf classes members are completely split across different clusters,\nthe assignment is totally in-complete, hence the AMI is null::\n\n  >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n  ... # doctest: +SKIP\n  0.0\n\nReferences\n----------\n.. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n   Clusterings Comparison: Variants, Properties, Normalization and\n   Correction for Chance, JMLR\n   <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n\n.. [2] `Wikipedia entry for the Adjusted Mutual Information\n   <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_"
        },
        {
          "name": "adjusted_rand_score",
          "decorators": [],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth class labels to be used as a reference"
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Cluster labels to evaluate"
            }
          ],
          "results": [
            {
              "name": "ARI",
              "type": "float",
              "description": "Similarity score between -1.0 and 1.0. Random labelings have an ARI\nclose to 0.0. 1.0 stands for perfect match."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Rand index adjusted for chance.\n\nThe Rand Index computes a similarity measure between two clusterings\nby considering all pairs of samples and counting pairs that are\nassigned in the same or different clusters in the predicted and\ntrue clusterings.\n\nThe raw RI score is then \"adjusted for chance\" into the ARI score\nusing the following scheme::\n\n    ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n\nThe adjusted Rand index is thus ensured to have a value close to\n0.0 for random labeling independently of the number of clusters and\nsamples and exactly 1.0 when the clusterings are identical (up to\na permutation).\n\nARI is a symmetric measure::\n\n    adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n\nRead more in the :ref:`User Guide <adjusted_rand_score>`.\n\nParameters\n----------\nlabels_true : int array, shape = [n_samples]\n    Ground truth class labels to be used as a reference\n\nlabels_pred : array-like of shape (n_samples,)\n    Cluster labels to evaluate\n\nReturns\n-------\nARI : float\n   Similarity score between -1.0 and 1.0. Random labelings have an ARI\n   close to 0.0. 1.0 stands for perfect match.\n\nExamples\n--------\nPerfectly matching labelings have a score of 1 even\n\n  >>> from sklearn.metrics.cluster import adjusted_rand_score\n  >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n  1.0\n  >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nLabelings that assign all classes members to the same clusters\nare complete but may not always be pure, hence penalized::\n\n  >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n  0.57...\n\nARI is symmetric, so labelings that have pure clusters with members\ncoming from the same classes but unnecessary splits are penalized::\n\n  >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])\n  0.57...\n\nIf classes members are completely split across different clusters, the\nassignment is totally incomplete, hence the ARI is very low::\n\n  >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n  0.0\n\nReferences\n----------\n.. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n  Journal of Classification 1985\n  https://link.springer.com/article/10.1007%2FBF01908075\n\n.. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie\n  adjusted Rand index, Psychological Methods 2004\n\n.. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n\nSee Also\n--------\nadjusted_mutual_info_score : Adjusted Mutual Information."
        },
        {
          "name": "check_clusterings",
          "decorators": [],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The true labels."
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The predicted labels."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check that the labels arrays are 1D and of same dimension.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,)\n    The true labels.\n\nlabels_pred : array-like of shape (n_samples,)\n    The predicted labels."
        },
        {
          "name": "completeness_score",
          "decorators": [],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "ground truth class labels to be used as a reference"
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "cluster labels to evaluate"
            }
          ],
          "results": [
            {
              "name": "completeness",
              "type": "float",
              "description": "score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Completeness metric of a cluster labeling given a ground truth.\n\nA clustering result satisfies completeness if all the data points\nthat are members of a given class are elements of the same cluster.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is not symmetric: switching ``label_true`` with ``label_pred``\nwill return the :func:`homogeneity_score` which will be different in\ngeneral.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n\nParameters\n----------\nlabels_true : int array, shape = [n_samples]\n    ground truth class labels to be used as a reference\n\nlabels_pred : array-like of shape (n_samples,)\n    cluster labels to evaluate\n\nReturns\n-------\ncompleteness : float\n   score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n\nReferences\n----------\n\n.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n   conditional entropy-based external cluster evaluation measure\n   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n\nSee Also\n--------\nhomogeneity_score\nv_measure_score\n\nExamples\n--------\n\nPerfect labelings are complete::\n\n  >>> from sklearn.metrics.cluster import completeness_score\n  >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nNon-perfect labelings that assign all classes members to the same clusters\nare still complete::\n\n  >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n  1.0\n  >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n  0.999...\n\nIf classes members are split across different clusters, the\nassignment cannot be complete::\n\n  >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n  0.0\n  >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n  0.0"
        },
        {
          "name": "contingency_matrix",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth class labels to be used as a reference."
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Cluster labels to evaluate."
            }
          ],
          "results": [
            {
              "name": "contingency",
              "type": null,
              "description": "Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in\ntrue class :math:`i` and in predicted class :math:`j`. If\n``eps is None``, the dtype of this array will be integer unless set\notherwise with the ``dtype`` argument. If ``eps`` is given, the dtype\nwill be float.\nWill be a ``sklearn.sparse.csr_matrix`` if ``sparse=True``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Build a contingency matrix describing the relationship between labels.\n\nParameters\n----------\nlabels_true : int array, shape = [n_samples]\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,)\n    Cluster labels to evaluate.\n\neps : float, default=None\n    If a float, that value is added to all values in the contingency\n    matrix. This helps to stop NaN propagation.\n    If ``None``, nothing is adjusted.\n\nsparse : bool, default=False\n    If `True`, return a sparse CSR continency matrix. If `eps` is not\n    `None` and `sparse` is `True` will raise ValueError.\n\n    .. versionadded:: 0.18\n\ndtype : numeric type, default=np.int64\n    Output dtype. Ignored if `eps` is not `None`.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ncontingency : {array-like, sparse}, shape=[n_classes_true, n_classes_pred]\n    Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in\n    true class :math:`i` and in predicted class :math:`j`. If\n    ``eps is None``, the dtype of this array will be integer unless set\n    otherwise with the ``dtype`` argument. If ``eps`` is given, the dtype\n    will be float.\n    Will be a ``sklearn.sparse.csr_matrix`` if ``sparse=True``."
        },
        {
          "name": "entropy",
          "decorators": [],
          "parameters": [
            {
              "name": "labels",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The labels"
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Calculates the entropy for a labeling.\n\nParameters\n----------\nlabels : int array, shape = [n_samples]\n    The labels\n\nNotes\n-----\nThe logarithm used is the natural logarithm (base-e)."
        },
        {
          "name": "fowlkes_mallows_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A clustering of the data into disjoint subsets."
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A clustering of the data into disjoint subsets."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": "float",
              "description": "The resulting Fowlkes-Mallows score."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Measure the similarity of two clusterings of a set of points.\n\n.. versionadded:: 0.18\n\nThe Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\nthe precision and recall::\n\n    FMI = TP / sqrt((TP + FP) * (TP + FN))\n\nWhere ``TP`` is the number of **True Positive** (i.e. the number of pair of\npoints that belongs in the same clusters in both ``labels_true`` and\n``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\nnumber of pair of points that belongs in the same clusters in\n``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n**False Negative** (i.e the number of pair of points that belongs in the\nsame clusters in ``labels_pred`` and not in ``labels_True``).\n\nThe score ranges from 0 to 1. A high value indicates a good similarity\nbetween two clusters.\n\nRead more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n\nParameters\n----------\nlabels_true : int array, shape = (``n_samples``,)\n    A clustering of the data into disjoint subsets.\n\nlabels_pred : array, shape = (``n_samples``, )\n    A clustering of the data into disjoint subsets.\n\nsparse : bool, default=False\n    Compute contingency matrix internally with sparse matrix.\n\nReturns\n-------\nscore : float\n   The resulting Fowlkes-Mallows score.\n\nExamples\n--------\n\nPerfect labelings are both homogeneous and complete, hence have\nscore 1.0::\n\n  >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n  >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n  1.0\n  >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nIf classes members are completely split across different clusters,\nthe assignment is totally random, hence the FMI is null::\n\n  >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n  0.0\n\nReferences\n----------\n.. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n   hierarchical clusterings\". Journal of the American Statistical\n   Association\n   <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_\n\n.. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n       <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_"
        },
        {
          "name": "homogeneity_completeness_v_measure",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "ground truth class labels to be used as a reference"
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "cluster labels to evaluate"
            }
          ],
          "results": [
            {
              "name": "homogeneity",
              "type": "float",
              "description": "score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling"
            },
            {
              "name": "completeness",
              "type": "float",
              "description": "score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling"
            },
            {
              "name": "v_measure",
              "type": "float",
              "description": "harmonic mean of the first two"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the homogeneity and completeness and V-Measure scores at once.\n\nThose metrics are based on normalized conditional entropy measures of\nthe clustering labeling to evaluate given the knowledge of a Ground\nTruth class labels of the same samples.\n\nA clustering result satisfies homogeneity if all of its clusters\ncontain only data points which are members of a single class.\n\nA clustering result satisfies completeness if all the data points\nthat are members of a given class are elements of the same cluster.\n\nBoth scores have positive values between 0.0 and 1.0, larger values\nbeing desirable.\n\nThose 3 metrics are independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore values in any way.\n\nV-Measure is furthermore symmetric: swapping ``labels_true`` and\n``label_pred`` will give the same score. This does not hold for\nhomogeneity and completeness. V-Measure is identical to\n:func:`normalized_mutual_info_score` with the arithmetic averaging\nmethod.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n\nParameters\n----------\nlabels_true : int array, shape = [n_samples]\n    ground truth class labels to be used as a reference\n\nlabels_pred : array-like of shape (n_samples,)\n    cluster labels to evaluate\n\nbeta : float, default=1.0\n    Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n    If ``beta`` is greater than 1, ``completeness`` is weighted more\n    strongly in the calculation. If ``beta`` is less than 1,\n    ``homogeneity`` is weighted more strongly.\n\nReturns\n-------\nhomogeneity : float\n   score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n\ncompleteness : float\n   score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n\nv_measure : float\n    harmonic mean of the first two\n\nSee Also\n--------\nhomogeneity_score\ncompleteness_score\nv_measure_score"
        },
        {
          "name": "homogeneity_score",
          "decorators": [],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "ground truth class labels to be used as a reference"
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "cluster labels to evaluate"
            }
          ],
          "results": [
            {
              "name": "homogeneity",
              "type": "float",
              "description": "score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Homogeneity metric of a cluster labeling given a ground truth.\n\nA clustering result satisfies homogeneity if all of its clusters\ncontain only data points which are members of a single class.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is not symmetric: switching ``label_true`` with ``label_pred``\nwill return the :func:`completeness_score` which will be different in\ngeneral.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n\nParameters\n----------\nlabels_true : int array, shape = [n_samples]\n    ground truth class labels to be used as a reference\n\nlabels_pred : array-like of shape (n_samples,)\n    cluster labels to evaluate\n\nReturns\n-------\nhomogeneity : float\n   score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n\nReferences\n----------\n\n.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n   conditional entropy-based external cluster evaluation measure\n   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n\nSee Also\n--------\ncompleteness_score\nv_measure_score\n\nExamples\n--------\n\nPerfect labelings are homogeneous::\n\n  >>> from sklearn.metrics.cluster import homogeneity_score\n  >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nNon-perfect labelings that further split classes into more clusters can be\nperfectly homogeneous::\n\n  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n  1.000000\n  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n  1.000000\n\nClusters that include samples from different classes do not make for an\nhomogeneous labeling::\n\n  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n  0.0...\n  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n  0.0..."
        },
        {
          "name": "mutual_info_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A clustering of the data into disjoint subsets."
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A clustering of the data into disjoint subsets."
            }
          ],
          "results": [
            {
              "name": "mi",
              "type": "float",
              "description": "Mutual information, a non-negative value"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Mutual Information between two clusterings.\n\nThe Mutual Information is a measure of the similarity between two labels of\nthe same data. Where :math:`|U_i|` is the number of the samples\nin cluster :math:`U_i` and :math:`|V_j|` is the number of the\nsamples in cluster :math:`V_j`, the Mutual Information\nbetween clusterings :math:`U` and :math:`V` is given as:\n\n.. math::\n\n    MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n    \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\nRead more in the :ref:`User Guide <mutual_info_score>`.\n\nParameters\n----------\nlabels_true : int array, shape = [n_samples]\n    A clustering of the data into disjoint subsets.\n\nlabels_pred : int array-like of shape (n_samples,)\n    A clustering of the data into disjoint subsets.\n\ncontingency : {ndarray, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None\n    A contingency matrix given by the :func:`contingency_matrix` function.\n    If value is ``None``, it will be computed, otherwise the given value is\n    used, with ``labels_true`` and ``labels_pred`` ignored.\n\nReturns\n-------\nmi : float\n   Mutual information, a non-negative value\n\nNotes\n-----\nThe logarithm used is the natural logarithm (base-e).\n\nSee Also\n--------\nadjusted_mutual_info_score : Adjusted against chance Mutual Information.\nnormalized_mutual_info_score : Normalized Mutual Information."
        },
        {
          "name": "normalized_mutual_info_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A clustering of the data into disjoint subsets."
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A clustering of the data into disjoint subsets."
            }
          ],
          "results": [
            {
              "name": "nmi",
              "type": "float",
              "description": "score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Normalized Mutual Information between two clusterings.\n\nNormalized Mutual Information (NMI) is a normalization of the Mutual\nInformation (MI) score to scale the results between 0 (no mutual\ninformation) and 1 (perfect correlation). In this function, mutual\ninformation is normalized by some generalized mean of ``H(labels_true)``\nand ``H(labels_pred))``, defined by the `average_method`.\n\nThis measure is not adjusted for chance. Therefore\n:func:`adjusted_mutual_info_score` might be preferred.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\nRead more in the :ref:`User Guide <mutual_info_score>`.\n\nParameters\n----------\nlabels_true : int array, shape = [n_samples]\n    A clustering of the data into disjoint subsets.\n\nlabels_pred : int array-like of shape (n_samples,)\n    A clustering of the data into disjoint subsets.\n\naverage_method : str, default='arithmetic'\n    How to compute the normalizer in the denominator. Possible options\n    are 'min', 'geometric', 'arithmetic', and 'max'.\n\n    .. versionadded:: 0.20\n\n    .. versionchanged:: 0.22\n       The default value of ``average_method`` changed from 'geometric' to\n       'arithmetic'.\n\nReturns\n-------\nnmi : float\n   score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n\nSee Also\n--------\nv_measure_score : V-Measure (NMI with arithmetic mean option).\nadjusted_rand_score : Adjusted Rand Index.\nadjusted_mutual_info_score : Adjusted Mutual Information (adjusted\n    against chance).\n\nExamples\n--------\n\nPerfect labelings are both homogeneous and complete, hence have\nscore 1.0::\n\n  >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n  >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n  ... # doctest: +SKIP\n  1.0\n  >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n  ... # doctest: +SKIP\n  1.0\n\nIf classes members are completely split across different clusters,\nthe assignment is totally in-complete, hence the NMI is null::\n\n  >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n  ... # doctest: +SKIP\n  0.0"
        },
        {
          "name": "pair_confusion_matrix",
          "decorators": [],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth class labels to be used as a reference."
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Cluster labels to evaluate."
            }
          ],
          "results": [
            {
              "name": "C",
              "type": null,
              "description": "The contingency matrix."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Pair confusion matrix arising from two clusterings.\n\nThe pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\nbetween two clusterings by considering all pairs of samples and counting\npairs that are assigned into the same or into different clusters under\nthe true and predicted clusterings.\n\nConsidering a pair of samples that is clustered together a positive pair,\nthen as in binary classification the count of true negatives is\n:math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n:math:`C_{11}` and false positives is :math:`C_{01}`.\n\nRead more in the :ref:`User Guide <pair_confusion_matrix>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,), dtype=integral\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,), dtype=integral\n    Cluster labels to evaluate.\n\nReturns\n-------\nC : ndarray of shape (2, 2), dtype=np.int64\n    The contingency matrix.\n\nSee Also\n--------\nrand_score: Rand Score\nadjusted_rand_score: Adjusted Rand Score\nadjusted_mutual_info_score: Adjusted Mutual Information\n\nExamples\n--------\nPerfectly matching labelings have all non-zero entries on the\ndiagonal regardless of actual label values:\n\n  >>> from sklearn.metrics.cluster import pair_confusion_matrix\n  >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n  array([[8, 0],\n         [0, 4]]...\n\nLabelings that assign all classes members to the same clusters\nare complete but may be not always pure, hence penalized, and\nhave some off-diagonal non-zero entries:\n\n  >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n  array([[8, 2],\n         [0, 2]]...\n\nNote that the matrix is not symmetric.\n\nReferences\n----------\n.. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n  Classification 1985\n  https://link.springer.com/article/10.1007%2FBF01908075"
        },
        {
          "name": "rand_score",
          "decorators": [],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth class labels to be used as a reference."
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Cluster labels to evaluate."
            }
          ],
          "results": [
            {
              "name": "RI",
              "type": "float",
              "description": "Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for\nperfect match."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Rand index.\n\nThe Rand Index computes a similarity measure between two clusterings\nby considering all pairs of samples and counting pairs that are\nassigned in the same or different clusters in the predicted and\ntrue clusterings.\n\nThe raw RI score is:\n\n    RI = (number of agreeing pairs) / (number of pairs)\n\nRead more in the :ref:`User Guide <rand_score>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,), dtype=integral\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,), dtype=integral\n    Cluster labels to evaluate.\n\nReturns\n-------\nRI : float\n   Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for\n   perfect match.\n\nSee Also\n--------\nadjusted_rand_score: Adjusted Rand Score\nadjusted_mutual_info_score: Adjusted Mutual Information\n\nExamples\n--------\nPerfectly matching labelings have a score of 1 even\n\n  >>> from sklearn.metrics.cluster import rand_score\n  >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nLabelings that assign all classes members to the same clusters\nare complete but may not always be pure, hence penalized:\n\n  >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n  0.83...\n\nReferences\n----------\n.. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n  Classification 1985\n  https://link.springer.com/article/10.1007%2FBF01908075\n\n.. https://en.wikipedia.org/wiki/Simple_matching_coefficient\n\n.. https://en.wikipedia.org/wiki/Rand_index"
        },
        {
          "name": "v_measure_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "labels_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "ground truth class labels to be used as a reference"
            },
            {
              "name": "labels_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "cluster labels to evaluate"
            }
          ],
          "results": [
            {
              "name": "v_measure",
              "type": "float",
              "description": "score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "V-measure cluster labeling given a ground truth.\n\nThis score is identical to :func:`normalized_mutual_info_score` with\nthe ``'arithmetic'`` option for averaging.\n\nThe V-measure is the harmonic mean between homogeneity and completeness::\n\n    v = (1 + beta) * homogeneity * completeness\n         / (beta * homogeneity + completeness)\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n\nParameters\n----------\nlabels_true : int array, shape = [n_samples]\n    ground truth class labels to be used as a reference\n\nlabels_pred : array-like of shape (n_samples,)\n    cluster labels to evaluate\n\nbeta : float, default=1.0\n    Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n    If ``beta`` is greater than 1, ``completeness`` is weighted more\n    strongly in the calculation. If ``beta`` is less than 1,\n    ``homogeneity`` is weighted more strongly.\n\nReturns\n-------\nv_measure : float\n   score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n\nReferences\n----------\n\n.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n   conditional entropy-based external cluster evaluation measure\n   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n\nSee Also\n--------\nhomogeneity_score\ncompleteness_score\nnormalized_mutual_info_score\n\nExamples\n--------\n\nPerfect labelings are both homogeneous and complete, hence have score 1.0::\n\n  >>> from sklearn.metrics.cluster import v_measure_score\n  >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n  1.0\n  >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nLabelings that assign all classes members to the same clusters\nare complete be not homogeneous, hence penalized::\n\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n  0.8...\n  >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n  0.66...\n\nLabelings that have pure clusters with members coming from the same\nclasses are homogeneous but un-necessary splits harms completeness\nand thus penalize V-measure as well::\n\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n  0.8...\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n  0.66...\n\nIf classes members are completely split across different clusters,\nthe assignment is totally incomplete, hence the V-Measure is null::\n\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n  0.0...\n\nClusters that include samples from totally different classes totally\ndestroy the homogeneity of the labeling, hence::\n\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n  0.0..."
        }
      ]
    },
    {
      "name": "sklearn.metrics.cluster._unsupervised",
      "imports": [
        {
          "module": "functools",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_distances",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_distances_chunked",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "calinski_harabasz_score",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A list of ``n_features``-dimensional data points. Each row corresponds\nto a single data point."
            },
            {
              "name": "labels",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted labels for each sample."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": "float",
              "description": "The resulting Calinski-Harabasz score."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the Calinski and Harabasz score.\n\nIt is also known as the Variance Ratio Criterion.\n\nThe score is defined as ratio between the within-cluster dispersion and\nthe between-cluster dispersion.\n\nRead more in the :ref:`User Guide <calinski_harabasz_index>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    A list of ``n_features``-dimensional data points. Each row corresponds\n    to a single data point.\n\nlabels : array-like of shape (n_samples,)\n    Predicted labels for each sample.\n\nReturns\n-------\nscore : float\n    The resulting Calinski-Harabasz score.\n\nReferences\n----------\n.. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n   analysis\". Communications in Statistics\n   <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_"
        },
        {
          "name": "check_number_of_labels",
          "decorators": [],
          "parameters": [
            {
              "name": "n_labels",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of labels."
            },
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of samples."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check that number of labels are valid.\n\nParameters\n----------\nn_labels : int\n    Number of labels.\n\nn_samples : int\n    Number of samples."
        },
        {
          "name": "davies_bouldin_score",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A list of ``n_features``-dimensional data points. Each row corresponds\nto a single data point."
            },
            {
              "name": "labels",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted labels for each sample."
            }
          ],
          "results": [
            {
              "name": "",
              "type": null,
              "description": "The resulting Davies-Bouldin score."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the Davies-Bouldin score.\n\nThe score is defined as the average similarity measure of each cluster with\nits most similar cluster, where similarity is the ratio of within-cluster\ndistances to between-cluster distances. Thus, clusters which are farther\napart and less dispersed will result in a better score.\n\nThe minimum score is zero, with lower values indicating better clustering.\n\nRead more in the :ref:`User Guide <davies-bouldin_index>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    A list of ``n_features``-dimensional data points. Each row corresponds\n    to a single data point.\n\nlabels : array-like of shape (n_samples,)\n    Predicted labels for each sample.\n\nReturns\n-------\nscore: float\n    The resulting Davies-Bouldin score.\n\nReferences\n----------\n.. [1] Davies, David L.; Bouldin, Donald W. (1979).\n   `\"A Cluster Separation Measure\"\n   <https://ieeexplore.ieee.org/document/4766909>`__.\n   IEEE Transactions on Pattern Analysis and Machine Intelligence.\n   PAMI-1 (2): 224-227"
        },
        {
          "name": "silhouette_samples",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "An array of pairwise distances between samples, or a feature array."
            },
            {
              "name": "labels",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Label values for each sample."
            }
          ],
          "results": [
            {
              "name": "silhouette",
              "type": null,
              "description": "Silhouette Coefficients for each sample."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the Silhouette Coefficient for each sample.\n\nThe Silhouette Coefficient is a measure of how well samples are clustered\nwith samples that are similar to themselves. Clustering models with a high\nSilhouette Coefficient are said to be dense, where samples in the same\ncluster are similar to each other, and well separated, where samples in\ndifferent clusters are not very similar to each other.\n\nThe Silhouette Coefficient is calculated using the mean intra-cluster\ndistance (``a``) and the mean nearest-cluster distance (``b``) for each\nsample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\nb)``.\nNote that Silhouette Coefficient is only defined if number of labels\nis 2 ``<= n_labels <= n_samples - 1``.\n\nThis function returns the Silhouette Coefficient for each sample.\n\nThe best value is 1 and the worst value is -1. Values near 0 indicate\noverlapping clusters.\n\nRead more in the :ref:`User Guide <silhouette_coefficient>`.\n\nParameters\n----------\nX : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n    An array of pairwise distances between samples, or a feature array.\n\nlabels : array-like of shape (n_samples,)\n    Label values for each sample.\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`.\n    If ``X`` is the distance array itself, use \"precomputed\" as the metric.\n    Precomputed distance matrices must have 0 along the diagonal.\n\n`**kwds` : optional keyword parameters\n    Any further parameters are passed directly to the distance function.\n    If using a ``scipy.spatial.distance`` metric, the parameters are still\n    metric dependent. See the scipy docs for usage examples.\n\nReturns\n-------\nsilhouette : array-like of shape (n_samples,)\n    Silhouette Coefficients for each sample.\n\nReferences\n----------\n\n.. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n   Interpretation and Validation of Cluster Analysis\". Computational\n   and Applied Mathematics 20: 53-65.\n   <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n\n.. [2] `Wikipedia entry on the Silhouette Coefficient\n   <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_"
        },
        {
          "name": "silhouette_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "An array of pairwise distances between samples, or a feature array."
            },
            {
              "name": "labels",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted labels for each sample."
            }
          ],
          "results": [
            {
              "name": "silhouette",
              "type": "float",
              "description": "Mean Silhouette Coefficient for all samples."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the mean Silhouette Coefficient of all samples.\n\nThe Silhouette Coefficient is calculated using the mean intra-cluster\ndistance (``a``) and the mean nearest-cluster distance (``b``) for each\nsample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\nb)``.  To clarify, ``b`` is the distance between a sample and the nearest\ncluster that the sample is not a part of.\nNote that Silhouette Coefficient is only defined if number of labels\nis ``2 <= n_labels <= n_samples - 1``.\n\nThis function returns the mean Silhouette Coefficient over all samples.\nTo obtain the values for each sample, use :func:`silhouette_samples`.\n\nThe best value is 1 and the worst value is -1. Values near 0 indicate\noverlapping clusters. Negative values generally indicate that a sample has\nbeen assigned to the wrong cluster, as a different cluster is more similar.\n\nRead more in the :ref:`User Guide <silhouette_coefficient>`.\n\nParameters\n----------\nX : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n    An array of pairwise distances between samples, or a feature array.\n\nlabels : array-like of shape (n_samples,)\n    Predicted labels for each sample.\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by :func:`metrics.pairwise.pairwise_distances\n    <sklearn.metrics.pairwise.pairwise_distances>`. If ``X`` is\n    the distance array itself, use ``metric=\"precomputed\"``.\n\nsample_size : int, default=None\n    The size of the sample to use when computing the Silhouette Coefficient\n    on a random subset of the data.\n    If ``sample_size is None``, no sampling is used.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for selecting a subset of samples.\n    Used when ``sample_size is not None``.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n**kwds : optional keyword parameters\n    Any further parameters are passed directly to the distance function.\n    If using a scipy.spatial.distance metric, the parameters are still\n    metric dependent. See the scipy docs for usage examples.\n\nReturns\n-------\nsilhouette : float\n    Mean Silhouette Coefficient for all samples.\n\nReferences\n----------\n\n.. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n   Interpretation and Validation of Cluster Analysis\". Computational\n   and Applied Mathematics 20: 53-65.\n   <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n\n.. [2] `Wikipedia entry on the Silhouette Coefficient\n       <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_"
        }
      ]
    },
    {
      "name": "sklearn.metrics.cluster.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "numpy.distutils.misc_util",
          "declaration": "Configuration",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.metrics.pairwise",
      "imports": [
        {
          "module": "itertools",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csr_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "scipy.spatial",
          "declaration": "distance",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "DataConversionWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics._pairwise_fast",
          "declaration": "_chi2_kernel_fast",
          "alias": null
        },
        {
          "module": "sklearn.metrics._pairwise_fast",
          "declaration": "_sparse_manhattan",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "normalize",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_batches",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_even_slices",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "get_chunk_n_rows",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "is_scalar_nan",
          "alias": null
        },
        {
          "module": "sklearn.utils._mask",
          "declaration": "_get_mask",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "parse_version",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "sp_version",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_non_negative",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "additive_chi2_kernel",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "kernel_matrix",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the additive chi-squared kernel between observations in X and\nY.\n\nThe chi-squared kernel is computed between each pair of rows in X and Y.  X\nand Y have to be non-negative. This kernel is most commonly applied to\nhistograms.\n\nThe chi-squared kernel is given by::\n\n    k(x, y) = -Sum [(x - y)^2 / (x + y)]\n\nIt can be interpreted as a weighted difference per entry.\n\nRead more in the :ref:`User Guide <chi2_kernel>`.\n\nNotes\n-----\nAs the negative of a distance, this kernel is only conditionally positive\ndefinite.\n\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features)\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n\nReturns\n-------\nkernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)\n\nSee Also\n--------\nchi2_kernel : The exponentiated version of the kernel, which is usually\n    preferable.\nsklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation\n    to this kernel.\n\nReferences\n----------\n* Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.\n  Local features and kernels for classification of texture and object\n  categories: A comprehensive study\n  International Journal of Computer Vision 2007\n  https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf"
        },
        {
          "name": "check_paired_arrays",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "safe_X",
              "type": null,
              "description": "An array equal to X, guaranteed to be a numpy array."
            },
            {
              "name": "safe_Y",
              "type": null,
              "description": "An array equal to Y if Y was not None, guaranteed to be a numpy array.\nIf Y was None, safe_Y will be a pointer to X."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Set X and Y appropriately and checks inputs for paired distances.\n\nAll paired distance metrics should use this function first to assert that\nthe given parameters are correct and safe to use.\n\nSpecifically, this function first ensures that both X and Y are arrays,\nthen checks that they are at least two dimensional while ensuring that\ntheir elements are floats. Finally, the function checks that the size\nof the dimensions of the two arrays are equal.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n\nReturns\n-------\nsafe_X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n    An array equal to X, guaranteed to be a numpy array.\n\nsafe_Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n    An array equal to Y if Y was not None, guaranteed to be a numpy array.\n    If Y was None, safe_Y will be a pointer to X."
        },
        {
          "name": "check_pairwise_arrays",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "safe_X",
              "type": null,
              "description": "An array equal to X, guaranteed to be a numpy array."
            },
            {
              "name": "safe_Y",
              "type": null,
              "description": "An array equal to Y if Y was not None, guaranteed to be a numpy array.\nIf Y was None, safe_Y will be a pointer to X."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Set X and Y appropriately and checks inputs.\n\nIf Y is None, it is set as a pointer to X (i.e. not a copy).\nIf Y is given, this does not happen.\nAll distance metrics should use this function first to assert that the\ngiven parameters are correct and safe to use.\n\nSpecifically, this function first ensures that both X and Y are arrays,\nthen checks that they are at least two dimensional while ensuring that\ntheir elements are floats (or dtype if provided). Finally, the function\nchecks that the size of the second dimension of the two arrays is equal, or\nthe equivalent check for a precomputed distance matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n\nprecomputed : bool, default=False\n    True if X is to be treated as precomputed distances to the samples in\n    Y.\n\ndtype : str, type, list of type, default=None\n    Data type required for X and Y. If None, the dtype will be an\n    appropriate float type selected by _return_float_dtype.\n\n    .. versionadded:: 0.18\n\naccept_sparse : str, bool or list/tuple of str, default='csr'\n    String[s] representing allowed sparse matrix formats, such as 'csc',\n    'csr', etc. If the input is sparse but not in the allowed format,\n    it will be converted to the first listed format. True allows the input\n    to be any format. False means that a sparse matrix input will\n    raise an error.\n\nforce_all_finite : bool or 'allow-nan', default=True\n    Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n    possibilities are:\n\n    - True: Force all values of array to be finite.\n    - False: accepts np.inf, np.nan, pd.NA in array.\n    - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n      cannot be infinite.\n\n    .. versionadded:: 0.22\n       ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n    .. versionchanged:: 0.23\n       Accepts `pd.NA` and converts it into `np.nan`.\n\ncopy : bool, default=False\n    Whether a forced copy will be triggered. If copy=False, a copy might\n    be triggered by a conversion.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\nsafe_X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n    An array equal to X, guaranteed to be a numpy array.\n\nsafe_Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n    An array equal to Y if Y was not None, guaranteed to be a numpy array.\n    If Y was None, safe_Y will be a pointer to X."
        },
        {
          "name": "chi2_kernel",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "gamma",
              "type": "Any",
              "hasDefault": true,
              "default": "1.0",
              "limitation": null,
              "ignored": false,
              "description": "Scaling parameter of the chi2 kernel."
            }
          ],
          "results": [
            {
              "name": "kernel_matrix",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the exponential chi-squared kernel X and Y.\n\nThe chi-squared kernel is computed between each pair of rows in X and Y.  X\nand Y have to be non-negative. This kernel is most commonly applied to\nhistograms.\n\nThe chi-squared kernel is given by::\n\n    k(x, y) = exp(-gamma Sum [(x - y)^2 / (x + y)])\n\nIt can be interpreted as a weighted difference per entry.\n\nRead more in the :ref:`User Guide <chi2_kernel>`.\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features)\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n\ngamma : float, default=1.\n    Scaling parameter of the chi2 kernel.\n\nReturns\n-------\nkernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)\n\nSee Also\n--------\nadditive_chi2_kernel : The additive version of this kernel.\nsklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation\n    to the additive version of this kernel.\n\nReferences\n----------\n* Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.\n  Local features and kernels for classification of texture and object\n  categories: A comprehensive study\n  International Journal of Computer Vision 2007\n  https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf"
        },
        {
          "name": "cosine_distances",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix `X`."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix `Y`."
            }
          ],
          "results": [
            {
              "name": "distance matrix",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute cosine distance between samples in X and Y.\n\nCosine distance is defined as 1.0 minus the cosine similarity.\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n    Matrix `X`.\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n    Matrix `Y`.\n\nReturns\n-------\ndistance matrix : ndarray of shape (n_samples_X, n_samples_Y)\n\nSee Also\n--------\ncosine_similarity\nscipy.spatial.distance.cosine : Dense matrices only."
        },
        {
          "name": "cosine_similarity",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data. If ``None``, the output will be the pairwise\nsimilarities between all samples in ``X``."
            },
            {
              "name": "dense_output",
              "type": "Any",
              "hasDefault": true,
              "default": "True",
              "limitation": null,
              "ignored": false,
              "description": "Whether to return dense output even when the input is sparse. If\n``False``, the output is sparse if both input arrays are sparse.\n\n.. versionadded:: 0.17\n   parameter ``dense_output`` for dense output."
            }
          ],
          "results": [
            {
              "name": "kernel matrix",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute cosine similarity between samples in X and Y.\n\nCosine similarity, or the cosine kernel, computes similarity as the\nnormalized dot product of X and Y:\n\n    K(X, Y) = <X, Y> / (||X||*||Y||)\n\nOn L2-normalized data, this function is equivalent to linear_kernel.\n\nRead more in the :ref:`User Guide <cosine_similarity>`.\n\nParameters\n----------\nX : {ndarray, sparse matrix} of shape (n_samples_X, n_features)\n    Input data.\n\nY : {ndarray, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n    Input data. If ``None``, the output will be the pairwise\n    similarities between all samples in ``X``.\n\ndense_output : bool, default=True\n    Whether to return dense output even when the input is sparse. If\n    ``False``, the output is sparse if both input arrays are sparse.\n\n    .. versionadded:: 0.17\n       parameter ``dense_output`` for dense output.\n\nReturns\n-------\nkernel matrix : ndarray of shape (n_samples_X, n_samples_Y)"
        },
        {
          "name": "distance_metrics",
          "decorators": [],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Valid metrics for pairwise_distances.\n\nThis function simply returns the valid pairwise distance metrics.\nIt exists to allow for a description of the mapping for\neach of the valid strings.\n\nThe valid distance metrics, and the function they map to, are:\n\n=============== ========================================\nmetric          Function\n=============== ========================================\n'cityblock'     metrics.pairwise.manhattan_distances\n'cosine'        metrics.pairwise.cosine_distances\n'euclidean'     metrics.pairwise.euclidean_distances\n'haversine'     metrics.pairwise.haversine_distances\n'l1'            metrics.pairwise.manhattan_distances\n'l2'            metrics.pairwise.euclidean_distances\n'manhattan'     metrics.pairwise.manhattan_distances\n'nan_euclidean' metrics.pairwise.nan_euclidean_distances\n=============== ========================================\n\nRead more in the :ref:`User Guide <metrics>`."
        },
        {
          "name": "euclidean_distances",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "distances",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Considering the rows of X (and Y=X) as vectors, compute the\ndistance matrix between each pair of vectors.\n\nFor efficiency reasons, the euclidean distance between a pair of row\nvector x and y is computed as::\n\n    dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n\nThis formulation has two advantages over other ways of computing distances.\nFirst, it is computationally efficient when dealing with sparse data.\nSecond, if one argument varies but the other remains unchanged, then\n`dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n\nHowever, this is not the most precise way of doing this computation,\nbecause this equation potentially suffers from \"catastrophic cancellation\".\nAlso, the distance matrix returned by this function may not be exactly\nsymmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n\nY_norm_squared : array-like of shape (n_samples_Y,), default=None\n    Pre-computed dot-products of vectors in Y (e.g.,\n    ``(Y**2).sum(axis=1)``)\n    May be ignored in some cases, see the note below.\n\nsquared : bool, default=False\n    Return squared Euclidean distances.\n\nX_norm_squared : array-like of shape (n_samples,), default=None\n    Pre-computed dot-products of vectors in X (e.g.,\n    ``(X**2).sum(axis=1)``)\n    May be ignored in some cases, see the note below.\n\nNotes\n-----\nTo achieve better accuracy, `X_norm_squared`\u00a0and `Y_norm_squared` may be\nunused if they are passed as ``float32``.\n\nReturns\n-------\ndistances : ndarray of shape (n_samples_X, n_samples_Y)\n\nSee Also\n--------\npaired_distances : Distances betweens pairs of elements of X and Y.\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import euclidean_distances\n>>> X = [[0, 1], [1, 1]]\n>>> # distance between rows of X\n>>> euclidean_distances(X, X)\narray([[0., 1.],\n       [1., 0.]])\n>>> # get distance to origin\n>>> euclidean_distances(X, [[0, 0]])\narray([[1.        ],\n       [1.41421356]])"
        },
        {
          "name": "haversine_distances",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "distance",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the Haversine distance between samples in X and Y.\n\nThe Haversine (or great circle) distance is the angular distance between\ntwo points on the surface of a sphere. The first coordinate of each point\nis assumed to be the latitude, the second is the longitude, given\nin radians. The dimension of the data must be 2.\n\n.. math::\n   D(x, y) = 2\\arcsin[\\sqrt{\\sin^2((x1 - y1) / 2)\n                            + \\cos(x1)\\cos(y1)\\sin^2((x2 - y2) / 2)}]\n\nParameters\n----------\nX : array-like of shape (n_samples_X, 2)\n\nY : array-like of shape (n_samples_Y, 2), default=None\n\nReturns\n-------\ndistance : ndarray of shape (n_samples_X, n_samples_Y)\n\nNotes\n-----\nAs the Earth is nearly spherical, the haversine formula provides a good\napproximation of the distance between two points of the Earth surface, with\na less than 1% error on average.\n\nExamples\n--------\nWe want to calculate the distance between the Ezeiza Airport\n(Buenos Aires, Argentina) and the Charles de Gaulle Airport (Paris,\nFrance).\n\n>>> from sklearn.metrics.pairwise import haversine_distances\n>>> from math import radians\n>>> bsas = [-34.83333, -58.5166646]\n>>> paris = [49.0083899664, 2.53844117956]\n>>> bsas_in_radians = [radians(_) for _ in bsas]\n>>> paris_in_radians = [radians(_) for _ in paris]\n>>> result = haversine_distances([bsas_in_radians, paris_in_radians])\n>>> result * 6371000/1000  # multiply by Earth radius to get kilometers\narray([[    0.        , 11099.54035582],\n       [11099.54035582,     0.        ]])"
        },
        {
          "name": "kernel_metrics",
          "decorators": [],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Valid metrics for pairwise_kernels.\n\nThis function simply returns the valid pairwise distance metrics.\nIt exists, however, to allow for a verbose description of the mapping for\neach of the valid strings.\n\nThe valid distance metrics, and the function they map to, are:\n  ===============   ========================================\n  metric            Function\n  ===============   ========================================\n  'additive_chi2'   sklearn.pairwise.additive_chi2_kernel\n  'chi2'            sklearn.pairwise.chi2_kernel\n  'linear'          sklearn.pairwise.linear_kernel\n  'poly'            sklearn.pairwise.polynomial_kernel\n  'polynomial'      sklearn.pairwise.polynomial_kernel\n  'rbf'             sklearn.pairwise.rbf_kernel\n  'laplacian'       sklearn.pairwise.laplacian_kernel\n  'sigmoid'         sklearn.pairwise.sigmoid_kernel\n  'cosine'          sklearn.pairwise.cosine_similarity\n  ===============   ========================================\n\nRead more in the :ref:`User Guide <metrics>`."
        },
        {
          "name": "laplacian_kernel",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "gamma",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If None, defaults to 1.0 / n_features."
            }
          ],
          "results": [
            {
              "name": "kernel_matrix",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the laplacian kernel between X and Y.\n\nThe laplacian kernel is defined as::\n\n    K(x, y) = exp(-gamma ||x-y||_1)\n\nfor each pair of rows x in X and y in Y.\nRead more in the :ref:`User Guide <laplacian_kernel>`.\n\n.. versionadded:: 0.17\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_features)\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n\ngamma : float, default=None\n    If None, defaults to 1.0 / n_features.\n\nReturns\n-------\nkernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)"
        },
        {
          "name": "linear_kernel",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "dense_output",
              "type": "Any",
              "hasDefault": true,
              "default": "True",
              "limitation": null,
              "ignored": false,
              "description": "Whether to return dense output even when the input is sparse. If\n``False``, the output is sparse if both input arrays are sparse.\n\n.. versionadded:: 0.20"
            }
          ],
          "results": [
            {
              "name": "Gram matrix",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the linear kernel between X and Y.\n\nRead more in the :ref:`User Guide <linear_kernel>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_features)\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n\ndense_output : bool, default=True\n    Whether to return dense output even when the input is sparse. If\n    ``False``, the output is sparse if both input arrays are sparse.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nGram matrix : ndarray of shape (n_samples_X, n_samples_Y)"
        },
        {
          "name": "manhattan_distances",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "D",
              "type": null,
              "description": "If sum_over_features is False shape is\n(n_samples_X * n_samples_Y, n_features) and D contains the\ncomponentwise L1 pairwise-distances (ie. absolute difference),\nelse shape is (n_samples_X, n_samples_Y) and D contains\nthe pairwise L1 distances."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the L1 distances between the vectors in X and Y.\n\nWith sum_over_features equal to False it returns the componentwise\ndistances.\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features)\n\nY : array-like of shape (n_samples_Y, n_features), default=None\n\nsum_over_features : bool, default=True\n    If True the function returns the pairwise distance matrix\n    else it returns the componentwise L1 pairwise-distances.\n    Not supported for sparse matrix inputs.\n\nReturns\n-------\nD : ndarray of shape (n_samples_X * n_samples_Y, n_features) or             (n_samples_X, n_samples_Y)\n    If sum_over_features is False shape is\n    (n_samples_X * n_samples_Y, n_features) and D contains the\n    componentwise L1 pairwise-distances (ie. absolute difference),\n    else shape is (n_samples_X, n_samples_Y) and D contains\n    the pairwise L1 distances.\n\nNotes\n--------\nWhen X and/or Y are CSR sparse matrices and they are not already\nin canonical format, this function modifies them in-place to\nmake them canonical.\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import manhattan_distances\n>>> manhattan_distances([[3]], [[3]])\narray([[0.]])\n>>> manhattan_distances([[3]], [[2]])\narray([[1.]])\n>>> manhattan_distances([[2]], [[3]])\narray([[1.]])\n>>> manhattan_distances([[1, 2], [3, 4]],         [[1, 2], [0, 3]])\narray([[0., 2.],\n       [4., 4.]])\n>>> import numpy as np\n>>> X = np.ones((1, 2))\n>>> y = np.full((2, 2), 2.)\n>>> manhattan_distances(X, y, sum_over_features=False)\narray([[1., 1.],\n       [1., 1.]])"
        },
        {
          "name": "nan_euclidean_distances",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "distances",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Calculate the euclidean distances in the presence of missing values.\n\nCompute the euclidean distance between each pair of samples in X and Y,\nwhere Y=X is assumed if Y=None. When calculating the distance between a\npair of samples, this formulation ignores feature coordinates with a\nmissing value in either sample and scales up the weight of the remaining\ncoordinates:\n\n    dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n    where,\n    weight = Total # of coordinates / # of present coordinates\n\nFor example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``\nis:\n\n    .. math::\n        \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n\nIf all the coordinates are missing or if there are no common present\ncoordinates then NaN is returned for that pair.\n\nRead more in the :ref:`User Guide <metrics>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nX : array-like of shape=(n_samples_X, n_features)\n\nY : array-like of shape=(n_samples_Y, n_features), default=None\n\nsquared : bool, default=False\n    Return squared Euclidean distances.\n\nmissing_values : np.nan or int, default=np.nan\n    Representation of missing value.\n\ncopy : bool, default=True\n    Make and use a deep copy of X and Y (if Y exists).\n\nReturns\n-------\ndistances : ndarray of shape (n_samples_X, n_samples_Y)\n\nSee Also\n--------\npaired_distances : Distances between pairs of elements of X and Y.\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import nan_euclidean_distances\n>>> nan = float(\"NaN\")\n>>> X = [[0, 1], [1, nan]]\n>>> nan_euclidean_distances(X, X) # distance between rows of X\narray([[0.        , 1.41421356],\n       [1.41421356, 0.        ]])\n\n>>> # get distance to origin\n>>> nan_euclidean_distances(X, [[0, 0]])\narray([[1.        ],\n       [1.41421356]])\n\nReferences\n----------\n* John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n  IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n  10, pp. 617 - 621, Oct. 1979.\n  http://ieeexplore.ieee.org/abstract/document/4310090/"
        },
        {
          "name": "paired_cosine_distances",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "distances",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the paired cosine distances between X and Y.\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nY : array-like of shape (n_samples, n_features)\n\nReturns\n-------\ndistances : ndarray of shape (n_samples,)\n\nNotes\n-----\nThe cosine distance is equivalent to the half the squared\neuclidean distance if each sample is normalized to unit norm."
        },
        {
          "name": "paired_distances",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array 1 for distance computation."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array 2 for distance computation."
            }
          ],
          "results": [
            {
              "name": "distances",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the paired distances between X and Y.\n\nComputes the distances between (X[0], Y[0]), (X[1], Y[1]), etc...\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Array 1 for distance computation.\n\nY : ndarray of shape (n_samples, n_features)\n    Array 2 for distance computation.\n\nmetric : str or callable, default=\"euclidean\"\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    specified in PAIRED_DISTANCES, including \"euclidean\",\n    \"manhattan\", or \"cosine\".\n    Alternatively, if metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays from X as input and return a value indicating\n    the distance between them.\n\nReturns\n-------\ndistances : ndarray of shape (n_samples,)\n\nSee Also\n--------\npairwise_distances : Computes the distance between every pair of samples.\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import paired_distances\n>>> X = [[0, 1], [1, 1]]\n>>> Y = [[0, 1], [2, 1]]\n>>> paired_distances(X, Y)\narray([0., 1.])"
        },
        {
          "name": "paired_euclidean_distances",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "distances",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the paired euclidean distances between X and Y.\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nY : array-like of shape (n_samples, n_features)\n\nReturns\n-------\ndistances : ndarray of shape (n_samples,)"
        },
        {
          "name": "paired_manhattan_distances",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "distances",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the L1 distances between the vectors in X and Y.\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nY : array-like of shape (n_samples, n_features)\n\nReturns\n-------\ndistances : ndarray of shape (n_samples,)"
        },
        {
          "name": "pairwise_distances",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array of pairwise distances between samples, or a feature array.\nThe shape of the array should be (n_samples_X, n_samples_X) if\nmetric == \"precomputed\" and (n_samples_X, n_features) otherwise."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "An optional second feature array. Only allowed if\nmetric != \"precomputed\"."
            },
            {
              "name": "metric",
              "type": "Any",
              "hasDefault": true,
              "default": "euclidean",
              "limitation": null,
              "ignored": false,
              "description": "The metric to use when calculating distance between instances in a\nfeature array. If metric is a string, it must be one of the options\nallowed by scipy.spatial.distance.pdist for its metric parameter, or\na metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\nIf metric is \"precomputed\", X is assumed to be a distance matrix.\nAlternatively, if metric is a callable function, it is called on each\npair of instances (rows) and the resulting value recorded. The callable\nshould take two arrays from X as input and return a value indicating\nthe distance between them."
            }
          ],
          "results": [
            {
              "name": "D",
              "type": null,
              "description": "A distance matrix D such that D_{i, j} is the distance between the\nith and jth vectors of the given matrix X, if Y is None.\nIf Y is not None, then D_{i, j} is the distance between the ith array\nfrom X and the jth array from Y."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the distance matrix from a vector array X and optional Y.\n\nThis method takes either a vector array or a distance matrix, and returns\na distance matrix. If the input is a vector array, the distances are\ncomputed. If the input is a distances matrix, it is returned instead.\n\nThis method provides a safe way to take a distance matrix as input, while\npreserving compatibility with many other algorithms that take a vector\narray.\n\nIf Y is given (default is None), then the returned matrix is the pairwise\ndistance between the arrays from both X and Y.\n\nValid values for metric are:\n\n- From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n  'manhattan']. These metrics support sparse matrix\n  inputs.\n  ['nan_euclidean'] but it does not yet support sparse matrices.\n\n- From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n  'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n  'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n  'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n  See the documentation for scipy.spatial.distance for details on these\n  metrics. These metrics do not support sparse matrix inputs.\n\nNote that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\nvalid scipy.spatial.distance metrics), the scikit-learn implementation\nwill be used, which is faster and has support for sparse matrices (except\nfor 'cityblock'). For a verbose description of the metrics from\nscikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\nfunction.\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n    Array of pairwise distances between samples, or a feature array.\n    The shape of the array should be (n_samples_X, n_samples_X) if\n    metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n    An optional second feature array. Only allowed if\n    metric != \"precomputed\".\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by scipy.spatial.distance.pdist for its metric parameter, or\n    a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n    If metric is \"precomputed\", X is assumed to be a distance matrix.\n    Alternatively, if metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays from X as input and return a value indicating\n    the distance between them.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nforce_all_finite : bool or 'allow-nan', default=True\n    Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n    for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n    possibilities are:\n\n    - True: Force all values of array to be finite.\n    - False: accepts np.inf, np.nan, pd.NA in array.\n    - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n      cannot be infinite.\n\n    .. versionadded:: 0.22\n       ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n    .. versionchanged:: 0.23\n       Accepts `pd.NA` and converts it into `np.nan`.\n\n**kwds : optional keyword parameters\n    Any further parameters are passed directly to the distance function.\n    If using a scipy.spatial.distance metric, the parameters are still\n    metric dependent. See the scipy docs for usage examples.\n\nReturns\n-------\nD : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n    A distance matrix D such that D_{i, j} is the distance between the\n    ith and jth vectors of the given matrix X, if Y is None.\n    If Y is not None, then D_{i, j} is the distance between the ith array\n    from X and the jth array from Y.\n\nSee Also\n--------\npairwise_distances_chunked : Performs the same calculation as this\n    function, but returns a generator of chunks of the distance matrix, in\n    order to limit memory usage.\npaired_distances : Computes the distances between corresponding elements\n    of two arrays."
        },
        {
          "name": "pairwise_distances_argmin",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array containing points."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Arrays containing points."
            }
          ],
          "results": [
            {
              "name": "argmin",
              "type": null,
              "description": "Y[argmin[i], :] is the row in Y that is closest to X[i, :]."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute minimum distances between one point and a set of points.\n\nThis function computes for each row in X, the index of the row of Y which\nis closest (according to the specified distance).\n\nThis is mostly equivalent to calling:\n\n    pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n\nbut uses much less memory, and is faster for large arrays.\n\nThis function works with dense 2D arrays only.\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features)\n    Array containing points.\n\nY : array-like of shape (n_samples_Y, n_features)\n    Arrays containing points.\n\naxis : int, default=1\n    Axis along which the argmin and distances are to be computed.\n\nmetric : str or callable, default=\"euclidean\"\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\nmetric_kwargs : dict, default=None\n    Keyword arguments to pass to specified metric function.\n\nReturns\n-------\nargmin : numpy.ndarray\n    Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n\nSee Also\n--------\nsklearn.metrics.pairwise_distances\nsklearn.metrics.pairwise_distances_argmin_min"
        },
        {
          "name": "pairwise_distances_argmin_min",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array containing points."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array containing points."
            }
          ],
          "results": [
            {
              "name": "argmin",
              "type": "NDArray",
              "description": "Y[argmin[i], :] is the row in Y that is closest to X[i, :]."
            },
            {
              "name": "distances",
              "type": "NDArray",
              "description": "distances[i] is the distance between the i-th row in X and the\nargmin[i]-th row in Y."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute minimum distances between one point and a set of points.\n\nThis function computes for each row in X, the index of the row of Y which\nis closest (according to the specified distance). The minimal distances are\nalso returned.\n\nThis is mostly equivalent to calling:\n\n    (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n     pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n\nbut uses much less memory, and is faster for large arrays.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n    Array containing points.\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n    Array containing points.\n\naxis : int, default=1\n    Axis along which the argmin and distances are to be computed.\n\nmetric : str or callable, default='euclidean'\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\nmetric_kwargs : dict, default=None\n    Keyword arguments to pass to specified metric function.\n\nReturns\n-------\nargmin : ndarray\n    Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n\ndistances : ndarray\n    distances[i] is the distance between the i-th row in X and the\n    argmin[i]-th row in Y.\n\nSee Also\n--------\nsklearn.metrics.pairwise_distances\nsklearn.metrics.pairwise_distances_argmin"
        },
        {
          "name": "pairwise_distances_chunked",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array of pairwise distances between samples, or a feature array.\nThe shape the array should be (n_samples_X, n_samples_X) if\nmetric='precomputed' and (n_samples_X, n_features) otherwise."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "An optional second feature array. Only allowed if\nmetric != \"precomputed\"."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a distance matrix chunk by chunk with optional reduction.\n\nIn cases where not all of a pairwise distance matrix needs to be stored at\nonce, this is used to calculate pairwise distances in\n``working_memory``-sized chunks.  If ``reduce_func`` is given, it is run\non each chunk and its return values are concatenated into lists, arrays\nor sparse matrices.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n    Array of pairwise distances between samples, or a feature array.\n    The shape the array should be (n_samples_X, n_samples_X) if\n    metric='precomputed' and (n_samples_X, n_features) otherwise.\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n    An optional second feature array. Only allowed if\n    metric != \"precomputed\".\n\nreduce_func : callable, default=None\n    The function which is applied on each chunk of the distance matrix,\n    reducing it to needed values.  ``reduce_func(D_chunk, start)``\n    is called repeatedly, where ``D_chunk`` is a contiguous vertical\n    slice of the pairwise distance matrix, starting at row ``start``.\n    It should return one of: None; an array, a list, or a sparse matrix\n    of length ``D_chunk.shape[0]``; or a tuple of such objects. Returning\n    None is useful for in-place operations, rather than reductions.\n\n    If None, pairwise_distances_chunked returns a generator of vertical\n    chunks of the distance matrix.\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by scipy.spatial.distance.pdist for its metric parameter, or\n    a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n    If metric is \"precomputed\", X is assumed to be a distance matrix.\n    Alternatively, if metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays from X as input and return a value indicating\n    the distance between them.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nworking_memory : int, default=None\n    The sought maximum memory for temporary distance matrix chunks.\n    When None (default), the value of\n    ``sklearn.get_config()['working_memory']`` is used.\n\n`**kwds` : optional keyword parameters\n    Any further parameters are passed directly to the distance function.\n    If using a scipy.spatial.distance metric, the parameters are still\n    metric dependent. See the scipy docs for usage examples.\n\nYields\n------\nD_chunk : {ndarray, sparse matrix}\n    A contiguous slice of distance matrix, optionally processed by\n    ``reduce_func``.\n\nExamples\n--------\nWithout reduce_func:\n\n>>> import numpy as np\n>>> from sklearn.metrics import pairwise_distances_chunked\n>>> X = np.random.RandomState(0).rand(5, 3)\n>>> D_chunk = next(pairwise_distances_chunked(X))\n>>> D_chunk\narray([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n       [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n       [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n       [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n       [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n\nRetrieve all neighbors and average distance within radius r:\n\n>>> r = .2\n>>> def reduce_func(D_chunk, start):\n...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n...     return neigh, avg_dist\n>>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n>>> neigh, avg_dist = next(gen)\n>>> neigh\n[array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n>>> avg_dist\narray([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n\nWhere r is defined per sample, we need to make use of ``start``:\n\n>>> r = [.2, .4, .4, .3, .1]\n>>> def reduce_func(D_chunk, start):\n...     neigh = [np.flatnonzero(d < r[i])\n...              for i, d in enumerate(D_chunk, start)]\n...     return neigh\n>>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n>>> neigh\n[array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n\nForce row-by-row generation by reducing ``working_memory``:\n\n>>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n...                                  working_memory=0)\n>>> next(gen)\n[array([0, 3])]\n>>> next(gen)\n[array([0, 1])]"
        },
        {
          "name": "pairwise_kernels",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array of pairwise kernels between samples, or a feature array.\nThe shape of the array should be (n_samples_X, n_samples_X) if\nmetric == \"precomputed\" and (n_samples_X, n_features) otherwise."
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A second feature array only if X has shape (n_samples_X, n_features)."
            },
            {
              "name": "metric",
              "type": "Any",
              "hasDefault": true,
              "default": "linear",
              "limitation": null,
              "ignored": false,
              "description": "The metric to use when calculating kernel between instances in a\nfeature array. If metric is a string, it must be one of the metrics\nin pairwise.PAIRWISE_KERNEL_FUNCTIONS.\nIf metric is \"precomputed\", X is assumed to be a kernel matrix.\nAlternatively, if metric is a callable function, it is called on each\npair of instances (rows) and the resulting value recorded. The callable\nshould take two rows from X as input and return the corresponding\nkernel value as a single number. This means that callables from\n:mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\nmatrices, not single samples. Use the string identifying the kernel\ninstead."
            }
          ],
          "results": [
            {
              "name": "K",
              "type": null,
              "description": "A kernel matrix K such that K_{i, j} is the kernel between the\nith and jth vectors of the given matrix X, if Y is None.\nIf Y is not None, then K_{i, j} is the kernel between the ith array\nfrom X and the jth array from Y."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the kernel between arrays X and optional array Y.\n\nThis method takes either a vector array or a kernel matrix, and returns\na kernel matrix. If the input is a vector array, the kernels are\ncomputed. If the input is a kernel matrix, it is returned instead.\n\nThis method provides a safe way to take a kernel matrix as input, while\npreserving compatibility with many other algorithms that take a vector\narray.\n\nIf Y is given (default is None), then the returned matrix is the pairwise\nkernel between the arrays from both X and Y.\n\nValid values for metric are:\n    ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n    'laplacian', 'sigmoid', 'cosine']\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n    Array of pairwise kernels between samples, or a feature array.\n    The shape of the array should be (n_samples_X, n_samples_X) if\n    metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n    A second feature array only if X has shape (n_samples_X, n_features).\n\nmetric : str or callable, default=\"linear\"\n    The metric to use when calculating kernel between instances in a\n    feature array. If metric is a string, it must be one of the metrics\n    in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n    If metric is \"precomputed\", X is assumed to be a kernel matrix.\n    Alternatively, if metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two rows from X as input and return the corresponding\n    kernel value as a single number. This means that callables from\n    :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n    matrices, not single samples. Use the string identifying the kernel\n    instead.\n\nfilter_params : bool, default=False\n    Whether to filter invalid parameters or not.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n**kwds : optional keyword parameters\n    Any further parameters are passed directly to the kernel function.\n\nReturns\n-------\nK : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n    A kernel matrix K such that K_{i, j} is the kernel between the\n    ith and jth vectors of the given matrix X, if Y is None.\n    If Y is not None, then K_{i, j} is the kernel between the ith array\n    from X and the jth array from Y.\n\nNotes\n-----\nIf metric is 'precomputed', Y is ignored and X is returned."
        },
        {
          "name": "polynomial_kernel",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "degree",
              "type": "Any",
              "hasDefault": true,
              "default": "3",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "gamma",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If None, defaults to 1.0 / n_features."
            },
            {
              "name": "coef0",
              "type": "Any",
              "hasDefault": true,
              "default": "1",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "Gram matrix",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the polynomial kernel between X and Y::\n\n    K(X, Y) = (gamma <X, Y> + coef0)^degree\n\nRead more in the :ref:`User Guide <polynomial_kernel>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_features)\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n\ndegree : int, default=3\n\ngamma : float, default=None\n    If None, defaults to 1.0 / n_features.\n\ncoef0 : float, default=1\n\nReturns\n-------\nGram matrix : ndarray of shape (n_samples_X, n_samples_Y)"
        },
        {
          "name": "rbf_kernel",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "gamma",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If None, defaults to 1.0 / n_features."
            }
          ],
          "results": [
            {
              "name": "kernel_matrix",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the rbf (gaussian) kernel between X and Y::\n\n    K(x, y) = exp(-gamma ||x-y||^2)\n\nfor each pair of rows x in X and y in Y.\n\nRead more in the :ref:`User Guide <rbf_kernel>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_features)\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n\ngamma : float, default=None\n    If None, defaults to 1.0 / n_features.\n\nReturns\n-------\nkernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)"
        },
        {
          "name": "sigmoid_kernel",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "gamma",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If None, defaults to 1.0 / n_features."
            },
            {
              "name": "coef0",
              "type": "Any",
              "hasDefault": true,
              "default": "1",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "Gram matrix",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the sigmoid kernel between X and Y::\n\n    K(X, Y) = tanh(gamma <X, Y> + coef0)\n\nRead more in the :ref:`User Guide <sigmoid_kernel>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples_X, n_features)\n\nY : ndarray of shape (n_samples_Y, n_features), default=None\n\ngamma : float, default=None\n    If None, defaults to 1.0 / n_features.\n\ncoef0 : float, default=1\n\nReturns\n-------\nGram matrix : ndarray of shape (n_samples_X, n_samples_Y)"
        }
      ]
    },
    {
      "name": "sklearn.metrics.setup",
      "imports": [
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "numpy.distutils.misc_util",
          "declaration": "Configuration",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.mixture",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._bayesian_mixture",
          "declaration": "BayesianGaussianMixture",
          "alias": null
        },
        {
          "module": "sklearn._gaussian_mixture",
          "declaration": "GaussianMixture",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.mixture._base",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "logsumexp",
          "alias": null
        },
        {
          "module": "sklearn",
          "declaration": "cluster",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "DensityMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "time",
          "declaration": "time",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseMixture",
          "decorators": [],
          "superclasses": [
            "DensityMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of n_features-dimensional data points. Each row\ncorresponds to a single data point."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Estimate model parameters with the EM algorithm.\n\nThe method fits the model ``n_init`` times and sets the parameters with\nwhich the model has the largest likelihood or lower bound. Within each\ntrial, the method iterates between E-step and M-step for ``max_iter``\ntimes until the change of likelihood or lower bound is less than\n``tol``, otherwise, a ``ConvergenceWarning`` is raised.\nIf ``warm_start`` is ``True``, then ``n_init`` is ignored and a single\ninitialization is performed upon the first call. Upon consecutive\ncalls, training starts where it left off.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    List of n_features-dimensional data points. Each row\n    corresponds to a single data point.\n\nReturns\n-------\nself"
            },
            {
              "name": "fit_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of n_features-dimensional data points. Each row\ncorresponds to a single data point."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Component labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Estimate model parameters using X and predict the labels for X.\n\nThe method fits the model n_init times and sets the parameters with\nwhich the model has the largest likelihood or lower bound. Within each\ntrial, the method iterates between E-step and M-step for `max_iter`\ntimes until the change of likelihood or lower bound is less than\n`tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is\nraised. After fitting, it predicts the most probable label for the\ninput data points.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    List of n_features-dimensional data points. Each row\n    corresponds to a single data point.\n\nReturns\n-------\nlabels : array, shape (n_samples,)\n    Component labels."
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of n_features-dimensional data points. Each row\ncorresponds to a single data point."
                }
              ],
              "results": [
                {
                  "name": "log_prob",
                  "type": null,
                  "description": "Log probabilities of each data point in X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the weighted log probabilities for each sample.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    List of n_features-dimensional data points. Each row\n    corresponds to a single data point.\n\nReturns\n-------\nlog_prob : array, shape (n_samples,)\n    Log probabilities of each data point in X."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of n_features-dimensional data points. Each row\ncorresponds to a single data point."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "log_likelihood",
                  "type": "float",
                  "description": "Log likelihood of the Gaussian mixture given X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the per-sample average log-likelihood of the given data X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_dimensions)\n    List of n_features-dimensional data points. Each row\n    corresponds to a single data point.\n\nReturns\n-------\nlog_likelihood : float\n    Log likelihood of the Gaussian mixture given X."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of n_features-dimensional data points. Each row\ncorresponds to a single data point."
                }
              ],
              "results": [
                {
                  "name": "labels",
                  "type": null,
                  "description": "Component labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the labels for the data samples in X using trained model.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    List of n_features-dimensional data points. Each row\n    corresponds to a single data point.\n\nReturns\n-------\nlabels : array, shape (n_samples,)\n    Component labels."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of n_features-dimensional data points. Each row\ncorresponds to a single data point."
                }
              ],
              "results": [
                {
                  "name": "resp",
                  "type": null,
                  "description": "Returns the probability each Gaussian (state) in\nthe model given each sample."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict posterior probability of each component given the data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    List of n_features-dimensional data points. Each row\n    corresponds to a single data point.\n\nReturns\n-------\nresp : array, shape (n_samples, n_components)\n    Returns the probability each Gaussian (state) in\n    the model given each sample."
            },
            {
              "name": "sample",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "n_samples",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "1",
                  "limitation": null,
                  "ignored": false,
                  "description": "Number of samples to generate."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Randomly generated sample"
                },
                {
                  "name": "y",
                  "type": null,
                  "description": "Component labels"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate random samples from the fitted Gaussian distribution.\n\nParameters\n----------\nn_samples : int, default=1\n    Number of samples to generate.\n\nReturns\n-------\nX : array, shape (n_samples, n_features)\n    Randomly generated sample\n\ny : array, shape (nsamples,)\n    Component labels"
            }
          ],
          "fullDocstring": "Base class for mixture models.\n\nThis abstract class specifies an interface for all mixture classes and\nprovides basic common methods for mixture models."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.mixture._bayesian_mixture",
      "imports": [
        {
          "module": "math",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy.special",
          "declaration": "betaln",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "digamma",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "gammaln",
          "alias": null
        },
        {
          "module": "sklearn.mixture._base",
          "declaration": "BaseMixture",
          "alias": null
        },
        {
          "module": "sklearn.mixture._base",
          "declaration": "_check_shape",
          "alias": null
        },
        {
          "module": "sklearn.mixture._gaussian_mixture",
          "declaration": "_check_precision_matrix",
          "alias": null
        },
        {
          "module": "sklearn.mixture._gaussian_mixture",
          "declaration": "_check_precision_positivity",
          "alias": null
        },
        {
          "module": "sklearn.mixture._gaussian_mixture",
          "declaration": "_compute_log_det_cholesky",
          "alias": null
        },
        {
          "module": "sklearn.mixture._gaussian_mixture",
          "declaration": "_compute_precision_cholesky",
          "alias": null
        },
        {
          "module": "sklearn.mixture._gaussian_mixture",
          "declaration": "_estimate_gaussian_parameters",
          "alias": null
        },
        {
          "module": "sklearn.mixture._gaussian_mixture",
          "declaration": "_estimate_log_gaussian_prob",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BayesianGaussianMixture",
          "decorators": [],
          "superclasses": [
            "BaseMixture"
          ],
          "methods": [],
          "fullDocstring": "Variational Bayesian estimation of a Gaussian mixture.\n\nThis class allows to infer an approximate posterior distribution over the\nparameters of a Gaussian mixture distribution. The effective number of\ncomponents can be inferred from the data.\n\nThis class implements two types of prior for the weights distribution: a\nfinite mixture model with Dirichlet distribution and an infinite mixture\nmodel with the Dirichlet Process. In practice Dirichlet Process inference\nalgorithm is approximated and uses a truncated distribution with a fixed\nmaximum number of components (called the Stick-breaking representation).\nThe number of components actually used almost always depends on the data.\n\n.. versionadded:: 0.18\n\nRead more in the :ref:`User Guide <bgmm>`.\n\nParameters\n----------\nn_components : int, default=1\n    The number of mixture components. Depending on the data and the value\n    of the `weight_concentration_prior` the model can decide to not use\n    all the components by setting some component `weights_` to values very\n    close to zero. The number of effective components is therefore smaller\n    than n_components.\n\ncovariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'\n    String describing the type of covariance parameters to use.\n    Must be one of::\n\n        'full' (each component has its own general covariance matrix),\n        'tied' (all components share the same general covariance matrix),\n        'diag' (each component has its own diagonal covariance matrix),\n        'spherical' (each component has its own single variance).\n\ntol : float, default=1e-3\n    The convergence threshold. EM iterations will stop when the\n    lower bound average gain on the likelihood (of the training data with\n    respect to the model) is below this threshold.\n\nreg_covar : float, default=1e-6\n    Non-negative regularization added to the diagonal of covariance.\n    Allows to assure that the covariance matrices are all positive.\n\nmax_iter : int, default=100\n    The number of EM iterations to perform.\n\nn_init : int, default=1\n    The number of initializations to perform. The result with the highest\n    lower bound value on the likelihood is kept.\n\ninit_params : {'kmeans', 'random'}, default='kmeans'\n    The method used to initialize the weights, the means and the\n    covariances.\n    Must be one of::\n\n        'kmeans' : responsibilities are initialized using kmeans.\n        'random' : responsibilities are initialized randomly.\n\nweight_concentration_prior_type : str, default='dirichlet_process'\n    String describing the type of the weight concentration prior.\n    Must be one of::\n\n        'dirichlet_process' (using the Stick-breaking representation),\n        'dirichlet_distribution' (can favor more uniform weights).\n\nweight_concentration_prior : float | None, default=None.\n    The dirichlet concentration of each component on the weight\n    distribution (Dirichlet). This is commonly called gamma in the\n    literature. The higher concentration puts more mass in\n    the center and will lead to more components being active, while a lower\n    concentration parameter will lead to more mass at the edge of the\n    mixture weights simplex. The value of the parameter must be greater\n    than 0. If it is None, it's set to ``1. / n_components``.\n\nmean_precision_prior : float | None, default=None.\n    The precision prior on the mean distribution (Gaussian).\n    Controls the extent of where means can be placed. Larger\n    values concentrate the cluster means around `mean_prior`.\n    The value of the parameter must be greater than 0.\n    If it is None, it is set to 1.\n\nmean_prior : array-like, shape (n_features,), default=None.\n    The prior on the mean distribution (Gaussian).\n    If it is None, it is set to the mean of X.\n\ndegrees_of_freedom_prior : float | None, default=None.\n    The prior of the number of degrees of freedom on the covariance\n    distributions (Wishart). If it is None, it's set to `n_features`.\n\ncovariance_prior : float or array-like, default=None.\n    The prior on the covariance distribution (Wishart).\n    If it is None, the emiprical covariance prior is initialized using the\n    covariance of X. The shape depends on `covariance_type`::\n\n            (n_features, n_features) if 'full',\n            (n_features, n_features) if 'tied',\n            (n_features)             if 'diag',\n            float                    if 'spherical'\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given to the method chosen to initialize the\n    parameters (see `init_params`).\n    In addition, it controls the generation of random samples from the\n    fitted distribution (see the method `sample`).\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nwarm_start : bool, default=False\n    If 'warm_start' is True, the solution of the last fitting is used as\n    initialization for the next call of fit(). This can speed up\n    convergence when fit is called several times on similar problems.\n    See :term:`the Glossary <warm_start>`.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints the current\n    initialization and each iteration step. If greater than 1 then\n    it prints also the log probability and the time needed\n    for each step.\n\nverbose_interval : int, default=10\n    Number of iteration done before the next print.\n\nAttributes\n----------\nweights_ : array-like of shape (n_components,)\n    The weights of each mixture components.\n\nmeans_ : array-like of shape (n_components, n_features)\n    The mean of each mixture component.\n\ncovariances_ : array-like\n    The covariance of each mixture component.\n    The shape depends on `covariance_type`::\n\n        (n_components,)                        if 'spherical',\n        (n_features, n_features)               if 'tied',\n        (n_components, n_features)             if 'diag',\n        (n_components, n_features, n_features) if 'full'\n\nprecisions_ : array-like\n    The precision matrices for each component in the mixture. A precision\n    matrix is the inverse of a covariance matrix. A covariance matrix is\n    symmetric positive definite so the mixture of Gaussian can be\n    equivalently parameterized by the precision matrices. Storing the\n    precision matrices instead of the covariance matrices makes it more\n    efficient to compute the log-likelihood of new samples at test time.\n    The shape depends on ``covariance_type``::\n\n        (n_components,)                        if 'spherical',\n        (n_features, n_features)               if 'tied',\n        (n_components, n_features)             if 'diag',\n        (n_components, n_features, n_features) if 'full'\n\nprecisions_cholesky_ : array-like\n    The cholesky decomposition of the precision matrices of each mixture\n    component. A precision matrix is the inverse of a covariance matrix.\n    A covariance matrix is symmetric positive definite so the mixture of\n    Gaussian can be equivalently parameterized by the precision matrices.\n    Storing the precision matrices instead of the covariance matrices makes\n    it more efficient to compute the log-likelihood of new samples at test\n    time. The shape depends on ``covariance_type``::\n\n        (n_components,)                        if 'spherical',\n        (n_features, n_features)               if 'tied',\n        (n_components, n_features)             if 'diag',\n        (n_components, n_features, n_features) if 'full'\n\nconverged_ : bool\n    True when convergence was reached in fit(), False otherwise.\n\nn_iter_ : int\n    Number of step used by the best fit of inference to reach the\n    convergence.\n\nlower_bound_ : float\n    Lower bound value on the likelihood (of the training data with\n    respect to the model) of the best fit of inference.\n\nweight_concentration_prior_ : tuple or float\n    The dirichlet concentration of each component on the weight\n    distribution (Dirichlet). The type depends on\n    ``weight_concentration_prior_type``::\n\n        (float, float) if 'dirichlet_process' (Beta parameters),\n        float          if 'dirichlet_distribution' (Dirichlet parameters).\n\n    The higher concentration puts more mass in\n    the center and will lead to more components being active, while a lower\n    concentration parameter will lead to more mass at the edge of the\n    simplex.\n\nweight_concentration_ : array-like of shape (n_components,)\n    The dirichlet concentration of each component on the weight\n    distribution (Dirichlet).\n\nmean_precision_prior_ : float\n    The precision prior on the mean distribution (Gaussian).\n    Controls the extent of where means can be placed.\n    Larger values concentrate the cluster means around `mean_prior`.\n    If mean_precision_prior is set to None, `mean_precision_prior_` is set\n    to 1.\n\nmean_precision_ : array-like of shape (n_components,)\n    The precision of each components on the mean distribution (Gaussian).\n\nmean_prior_ : array-like of shape (n_features,)\n    The prior on the mean distribution (Gaussian).\n\ndegrees_of_freedom_prior_ : float\n    The prior of the number of degrees of freedom on the covariance\n    distributions (Wishart).\n\ndegrees_of_freedom_ : array-like of shape (n_components,)\n    The number of degrees of freedom of each components in the model.\n\ncovariance_prior_ : float or array-like\n    The prior on the covariance distribution (Wishart).\n    The shape depends on `covariance_type`::\n\n        (n_features, n_features) if 'full',\n        (n_features, n_features) if 'tied',\n        (n_features)             if 'diag',\n        float                    if 'spherical'\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.mixture import BayesianGaussianMixture\n>>> X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [12, 4], [10, 7]])\n>>> bgm = BayesianGaussianMixture(n_components=2, random_state=42).fit(X)\n>>> bgm.means_\narray([[2.49... , 2.29...],\n       [8.45..., 4.52... ]])\n>>> bgm.predict([[0, 0], [9, 3]])\narray([0, 1])\n\nSee Also\n--------\nGaussianMixture : Finite Gaussian mixture fit with EM.\n\nReferences\n----------\n\n.. [1] `Bishop, Christopher M. (2006). \"Pattern recognition and machine\n   learning\". Vol. 4 No. 4. New York: Springer.\n   <https://www.springer.com/kr/book/9780387310732>`_\n\n.. [2] `Hagai Attias. (2000). \"A Variational Bayesian Framework for\n   Graphical Models\". In Advances in Neural Information Processing\n   Systems 12.\n   <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.2841&rep=rep1&type=pdf>`_\n\n.. [3] `Blei, David M. and Michael I. Jordan. (2006). \"Variational\n   inference for Dirichlet process mixtures\". Bayesian analysis 1.1\n   <https://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/BleiJordan2005.pdf>`_"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.mixture._gaussian_mixture",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "sklearn.mixture._base",
          "declaration": "BaseMixture",
          "alias": null
        },
        {
          "module": "sklearn.mixture._base",
          "declaration": "_check_shape",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "GaussianMixture",
          "decorators": [],
          "superclasses": [
            "BaseMixture"
          ],
          "methods": [
            {
              "name": "bic",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "bic",
                  "type": "float",
                  "description": "The lower the better."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Bayesian information criterion for the current model on the input X.\n\nParameters\n----------\nX : array of shape (n_samples, n_dimensions)\n\nReturns\n-------\nbic : float\n    The lower the better."
            },
            {
              "name": "aic",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "aic",
                  "type": "float",
                  "description": "The lower the better."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Akaike information criterion for the current model on the input X.\n\nParameters\n----------\nX : array of shape (n_samples, n_dimensions)\n\nReturns\n-------\naic : float\n    The lower the better."
            }
          ],
          "fullDocstring": "Gaussian Mixture.\n\nRepresentation of a Gaussian mixture model probability distribution.\nThis class allows to estimate the parameters of a Gaussian mixture\ndistribution.\n\nRead more in the :ref:`User Guide <gmm>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nn_components : int, default=1\n    The number of mixture components.\n\ncovariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'\n    String describing the type of covariance parameters to use.\n    Must be one of:\n\n    'full'\n        each component has its own general covariance matrix\n    'tied'\n        all components share the same general covariance matrix\n    'diag'\n        each component has its own diagonal covariance matrix\n    'spherical'\n        each component has its own single variance\n\ntol : float, default=1e-3\n    The convergence threshold. EM iterations will stop when the\n    lower bound average gain is below this threshold.\n\nreg_covar : float, default=1e-6\n    Non-negative regularization added to the diagonal of covariance.\n    Allows to assure that the covariance matrices are all positive.\n\nmax_iter : int, default=100\n    The number of EM iterations to perform.\n\nn_init : int, default=1\n    The number of initializations to perform. The best results are kept.\n\ninit_params : {'kmeans', 'random'}, default='kmeans'\n    The method used to initialize the weights, the means and the\n    precisions.\n    Must be one of::\n\n        'kmeans' : responsibilities are initialized using kmeans.\n        'random' : responsibilities are initialized randomly.\n\nweights_init : array-like of shape (n_components, ), default=None\n    The user-provided initial weights.\n    If it is None, weights are initialized using the `init_params` method.\n\nmeans_init : array-like of shape (n_components, n_features), default=None\n    The user-provided initial means,\n    If it is None, means are initialized using the `init_params` method.\n\nprecisions_init : array-like, default=None\n    The user-provided initial precisions (inverse of the covariance\n    matrices).\n    If it is None, precisions are initialized using the 'init_params'\n    method.\n    The shape depends on 'covariance_type'::\n\n        (n_components,)                        if 'spherical',\n        (n_features, n_features)               if 'tied',\n        (n_components, n_features)             if 'diag',\n        (n_components, n_features, n_features) if 'full'\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given to the method chosen to initialize the\n    parameters (see `init_params`).\n    In addition, it controls the generation of random samples from the\n    fitted distribution (see the method `sample`).\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nwarm_start : bool, default=False\n    If 'warm_start' is True, the solution of the last fitting is used as\n    initialization for the next call of fit(). This can speed up\n    convergence when fit is called several times on similar problems.\n    In that case, 'n_init' is ignored and only a single initialization\n    occurs upon the first call.\n    See :term:`the Glossary <warm_start>`.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints the current\n    initialization and each iteration step. If greater than 1 then\n    it prints also the log probability and the time needed\n    for each step.\n\nverbose_interval : int, default=10\n    Number of iteration done before the next print.\n\nAttributes\n----------\nweights_ : array-like of shape (n_components,)\n    The weights of each mixture components.\n\nmeans_ : array-like of shape (n_components, n_features)\n    The mean of each mixture component.\n\ncovariances_ : array-like\n    The covariance of each mixture component.\n    The shape depends on `covariance_type`::\n\n        (n_components,)                        if 'spherical',\n        (n_features, n_features)               if 'tied',\n        (n_components, n_features)             if 'diag',\n        (n_components, n_features, n_features) if 'full'\n\nprecisions_ : array-like\n    The precision matrices for each component in the mixture. A precision\n    matrix is the inverse of a covariance matrix. A covariance matrix is\n    symmetric positive definite so the mixture of Gaussian can be\n    equivalently parameterized by the precision matrices. Storing the\n    precision matrices instead of the covariance matrices makes it more\n    efficient to compute the log-likelihood of new samples at test time.\n    The shape depends on `covariance_type`::\n\n        (n_components,)                        if 'spherical',\n        (n_features, n_features)               if 'tied',\n        (n_components, n_features)             if 'diag',\n        (n_components, n_features, n_features) if 'full'\n\nprecisions_cholesky_ : array-like\n    The cholesky decomposition of the precision matrices of each mixture\n    component. A precision matrix is the inverse of a covariance matrix.\n    A covariance matrix is symmetric positive definite so the mixture of\n    Gaussian can be equivalently parameterized by the precision matrices.\n    Storing the precision matrices instead of the covariance matrices makes\n    it more efficient to compute the log-likelihood of new samples at test\n    time. The shape depends on `covariance_type`::\n\n        (n_components,)                        if 'spherical',\n        (n_features, n_features)               if 'tied',\n        (n_components, n_features)             if 'diag',\n        (n_components, n_features, n_features) if 'full'\n\nconverged_ : bool\n    True when convergence was reached in fit(), False otherwise.\n\nn_iter_ : int\n    Number of step used by the best fit of EM to reach the convergence.\n\nlower_bound_ : float\n    Lower bound value on the log-likelihood (of the training data with\n    respect to the model) of the best fit of EM.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.mixture import GaussianMixture\n>>> X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n>>> gm = GaussianMixture(n_components=2, random_state=0).fit(X)\n>>> gm.means_\narray([[10.,  2.],\n       [ 1.,  2.]])\n>>> gm.predict([[0, 0], [12, 3]])\narray([1, 0])\n\nSee Also\n--------\nBayesianGaussianMixture : Gaussian mixture model fit with a variational\n    inference."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.model_selection",
      "imports": [
        {
          "module": "typing",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn._search",
          "declaration": "GridSearchCV",
          "alias": null
        },
        {
          "module": "sklearn._search",
          "declaration": "ParameterGrid",
          "alias": null
        },
        {
          "module": "sklearn._search",
          "declaration": "ParameterSampler",
          "alias": null
        },
        {
          "module": "sklearn._search",
          "declaration": "RandomizedSearchCV",
          "alias": null
        },
        {
          "module": "sklearn._search",
          "declaration": "fit_grid_point",
          "alias": null
        },
        {
          "module": "sklearn._search_successive_halving",
          "declaration": "HalvingGridSearchCV",
          "alias": null
        },
        {
          "module": "sklearn._search_successive_halving",
          "declaration": "HalvingRandomSearchCV",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "BaseCrossValidator",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "GroupKFold",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "GroupShuffleSplit",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "KFold",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "LeaveOneGroupOut",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "LeaveOneOut",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "LeavePGroupsOut",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "LeavePOut",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "PredefinedSplit",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "RepeatedKFold",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "RepeatedStratifiedKFold",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "ShuffleSplit",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "StratifiedKFold",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "StratifiedShuffleSplit",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "TimeSeriesSplit",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn._split",
          "declaration": "train_test_split",
          "alias": null
        },
        {
          "module": "sklearn._validation",
          "declaration": "cross_val_predict",
          "alias": null
        },
        {
          "module": "sklearn._validation",
          "declaration": "cross_val_score",
          "alias": null
        },
        {
          "module": "sklearn._validation",
          "declaration": "cross_validate",
          "alias": null
        },
        {
          "module": "sklearn._validation",
          "declaration": "learning_curve",
          "alias": null
        },
        {
          "module": "sklearn._validation",
          "declaration": "permutation_test_score",
          "alias": null
        },
        {
          "module": "sklearn._validation",
          "declaration": "validation_curve",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.model_selection._search",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "operator",
          "alias": null
        },
        {
          "module": "time",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "collections",
          "declaration": "defaultdict",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Iterable",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Mapping",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Sequence",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "reduce",
          "alias": null
        },
        {
          "module": "itertools",
          "declaration": "product",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "numpy.ma",
          "declaration": "MaskedArray",
          "alias": null
        },
        {
          "module": "scipy.stats",
          "declaration": "rankdata",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "check_scoring",
          "alias": null
        },
        {
          "module": "sklearn.metrics._scorer",
          "declaration": "_check_multimetric_scoring",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._split",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._validation",
          "declaration": "_aggregate_score_dicts",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._validation",
          "declaration": "_fit_and_score",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._validation",
          "declaration": "_insert_error_scores",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._validation",
          "declaration": "_normalize_score_results",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_safe_tags",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "if_delegate_has_method",
          "alias": null
        },
        {
          "module": "sklearn.utils.random",
          "declaration": "sample_without_replacement",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_fit_params",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "indexable",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseSearchCV",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target relative to X for classification or regression;\nNone for unsupervised learning."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the score on the given data, if the estimator has been refit.\n\nThis uses the score defined by ``scoring`` where provided, and the\n``best_estimator_.score`` method otherwise.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input data, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n    Target relative to X for classification or regression;\n    None for unsupervised learning.\n\nReturns\n-------\nscore : float"
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to predict on. Must fulfill input requirements\nof the underlying estimator."
                }
              ],
              "results": [
                {
                  "name": "y_score",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Call score_samples on the estimator with the best found parameters.\n\nOnly available if ``refit=True`` and the underlying estimator supports\n``score_samples``.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nX : iterable\n    Data to predict on. Must fulfill input requirements\n    of the underlying estimator.\n\nReturns\n-------\ny_score : ndarray of shape (n_samples,)"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Must fulfill the input assumptions of the\nunderlying estimator."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Call predict on the estimator with the best found parameters.\n\nOnly available if ``refit=True`` and the underlying estimator supports\n``predict``.\n\nParameters\n----------\nX : indexable, length n_samples\n    Must fulfill the input assumptions of the\n    underlying estimator."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Must fulfill the input assumptions of the\nunderlying estimator."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Call predict_proba on the estimator with the best found parameters.\n\nOnly available if ``refit=True`` and the underlying estimator supports\n``predict_proba``.\n\nParameters\n----------\nX : indexable, length n_samples\n    Must fulfill the input assumptions of the\n    underlying estimator."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Must fulfill the input assumptions of the\nunderlying estimator."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Call predict_log_proba on the estimator with the best found parameters.\n\nOnly available if ``refit=True`` and the underlying estimator supports\n``predict_log_proba``.\n\nParameters\n----------\nX : indexable, length n_samples\n    Must fulfill the input assumptions of the\n    underlying estimator."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Must fulfill the input assumptions of the\nunderlying estimator."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Call decision_function on the estimator with the best found parameters.\n\nOnly available if ``refit=True`` and the underlying estimator supports\n``decision_function``.\n\nParameters\n----------\nX : indexable, length n_samples\n    Must fulfill the input assumptions of the\n    underlying estimator."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Must fulfill the input assumptions of the\nunderlying estimator."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Call transform on the estimator with the best found parameters.\n\nOnly available if the underlying estimator supports ``transform`` and\n``refit=True``.\n\nParameters\n----------\nX : indexable, length n_samples\n    Must fulfill the input assumptions of the\n    underlying estimator."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "Xt",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Must fulfill the input assumptions of the\nunderlying estimator."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Call inverse_transform on the estimator with the best found params.\n\nOnly available if the underlying estimator implements\n``inverse_transform`` and ``refit=True``.\n\nParameters\n----------\nXt : indexable, length n_samples\n    Must fulfill the input assumptions of the\n    underlying estimator."
            },
            {
              "name": "n_features_in_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "classes_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "fit",
              "decorators": [
                "_deprecate_positional_args"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target relative to X for classification or regression;\nNone for unsupervised learning."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Run fit with all sets of parameters.\n\nParameters\n----------\n\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n    Target relative to X for classification or regression;\n    None for unsupervised learning.\n\ngroups : array-like of shape (n_samples,), default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n\n**fit_params : dict of str -> object\n    Parameters passed to the ``fit`` method of the estimator"
            }
          ],
          "fullDocstring": "Abstract base class for hyper parameter search with cross-validation.\n    "
        },
        {
          "name": "GridSearchCV",
          "decorators": [],
          "superclasses": [
            "BaseSearchCV"
          ],
          "methods": [],
          "fullDocstring": "Exhaustive search over specified parameter values for an estimator.\n\nImportant members are fit, predict.\n\nGridSearchCV implements a \"fit\" and a \"score\" method.\nIt also implements \"score_samples\", \"predict\", \"predict_proba\",\n\"decision_function\", \"transform\" and \"inverse_transform\" if they are\nimplemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized\nby cross-validated grid-search over a parameter grid.\n\nRead more in the :ref:`User Guide <grid_search>`.\n\nParameters\n----------\nestimator : estimator object.\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparam_grid : dict or list of dictionaries\n    Dictionary with parameters names (`str`) as keys and lists of\n    parameter settings to try as values, or a list of such\n    dictionaries, in which case the grids spanned by each dictionary\n    in the list are explored. This enables searching over any sequence\n    of parameter settings.\n\nscoring : str, callable, list/tuple or dict, default=None\n    A single str (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n    For evaluating multiple metrics, either give a list of (unique) strings\n    or a dict with names as keys and callables as values.\n\n    NOTE that when using custom scorers, each scorer should return a single\n    value. Metric functions returning a list/array of values can be wrapped\n    into multiple scorers that return one value each.\n\n    See :ref:`multimetric_grid_search` for an example.\n\n    If None, the estimator's score method is used.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\npre_dispatch : int, or str, default=n_jobs\n    Controls the number of jobs that get dispatched during parallel\n    execution. Reducing this number can be useful to avoid an\n    explosion of memory consumption when more jobs get dispatched\n    than CPUs can process. This parameter can be:\n\n        - None, in which case all the jobs are immediately\n          created and spawned. Use this for lightweight and\n          fast-running jobs, to avoid delays due to on-demand\n          spawning of the jobs\n\n        - An int, giving the exact number of total jobs that are\n          spawned\n\n        - A str, giving an expression as a function of n_jobs,\n          as in '2*n_jobs'\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nrefit : bool, str, or callable, default=True\n    Refit an estimator using the best found parameters on the whole\n    dataset.\n\n    For multiple metric evaluation, this needs to be a `str` denoting the\n    scorer that would be used to find the best parameters for refitting\n    the estimator at the end.\n\n    Where there are considerations other than maximum score in\n    choosing a best estimator, ``refit`` can be set to a function which\n    returns the selected ``best_index_`` given ``cv_results_``. In that\n    case, the ``best_estimator_`` and ``best_params_`` will be set\n    according to the returned ``best_index_`` while the ``best_score_``\n    attribute will not be available.\n\n    The refitted estimator is made available at the ``best_estimator_``\n    attribute and permits using ``predict`` directly on this\n    ``GridSearchCV`` instance.\n\n    Also for multiple metric evaluation, the attributes ``best_index_``,\n    ``best_score_`` and ``best_params_`` will only be available if\n    ``refit`` is set and all of them will be determined w.r.t this specific\n    scorer.\n\n    See ``scoring`` parameter to know more about multiple metric\n    evaluation.\n\n    .. versionchanged:: 0.20\n        Support for callable added.\n\nverbose : int\n    Controls the verbosity: the higher, the more messages.\n\n    - >1 : the computation time for each fold and parameter candidate is\n      displayed;\n    - >2 : the score is also displayed;\n    - >3 : the fold and candidate parameter indexes are also displayed\n      together with the starting time of the computation.\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error.\n\nreturn_train_score : bool, default=False\n    If ``False``, the ``cv_results_`` attribute will not include training\n    scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\n    .. versionadded:: 0.19\n\n    .. versionchanged:: 0.21\n        Default value was changed from ``True`` to ``False``\n\n\nExamples\n--------\n>>> from sklearn import svm, datasets\n>>> from sklearn.model_selection import GridSearchCV\n>>> iris = datasets.load_iris()\n>>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n>>> svc = svm.SVC()\n>>> clf = GridSearchCV(svc, parameters)\n>>> clf.fit(iris.data, iris.target)\nGridSearchCV(estimator=SVC(),\n             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n>>> sorted(clf.cv_results_.keys())\n['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n 'param_C', 'param_kernel', 'params',...\n 'rank_test_score', 'split0_test_score',...\n 'split2_test_score', ...\n 'std_fit_time', 'std_score_time', 'std_test_score']\n\nAttributes\n----------\ncv_results_ : dict of numpy (masked) ndarrays\n    A dict with keys as column headers and values as columns, that can be\n    imported into a pandas ``DataFrame``.\n\n    For instance the below given table\n\n    +------------+-----------+------------+-----------------+---+---------+\n    |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n    +============+===========+============+=================+===+=========+\n    |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n    +------------+-----------+------------+-----------------+---+---------+\n    |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n    +------------+-----------+------------+-----------------+---+---------+\n    |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n    +------------+-----------+------------+-----------------+---+---------+\n    |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n    +------------+-----------+------------+-----------------+---+---------+\n\n    will be represented by a ``cv_results_`` dict of::\n\n        {\n        'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n                                     mask = [False False False False]...)\n        'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n                                    mask = [ True  True False False]...),\n        'param_degree': masked_array(data = [2.0 3.0 -- --],\n                                     mask = [False False  True  True]...),\n        'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n        'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n        'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n        'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n        'rank_test_score'    : [2, 4, 3, 1],\n        'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n        'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n        'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n        'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n        'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n        'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n        'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n        'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n        'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n        }\n\n    NOTE\n\n    The key ``'params'`` is used to store a list of parameter\n    settings dicts for all the parameter candidates.\n\n    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n    ``std_score_time`` are all in seconds.\n\n    For multi-metric evaluation, the scores for all the scorers are\n    available in the ``cv_results_`` dict at the keys ending with that\n    scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n    above. ('split0_test_precision', 'mean_train_precision' etc.)\n\nbest_estimator_ : estimator\n    Estimator that was chosen by the search, i.e. estimator\n    which gave highest score (or smallest loss if specified)\n    on the left out data. Not available if ``refit=False``.\n\n    See ``refit`` parameter for more information on allowed values.\n\nbest_score_ : float\n    Mean cross-validated score of the best_estimator\n\n    For multi-metric evaluation, this is present only if ``refit`` is\n    specified.\n\n    This attribute is not available if ``refit`` is a function.\n\nbest_params_ : dict\n    Parameter setting that gave the best results on the hold out data.\n\n    For multi-metric evaluation, this is present only if ``refit`` is\n    specified.\n\nbest_index_ : int\n    The index (of the ``cv_results_`` arrays) which corresponds to the best\n    candidate parameter setting.\n\n    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n    the parameter setting for the best model, that gives the highest\n    mean score (``search.best_score_``).\n\n    For multi-metric evaluation, this is present only if ``refit`` is\n    specified.\n\nscorer_ : function or a dict\n    Scorer function used on the held out data to choose the best\n    parameters for the model.\n\n    For multi-metric evaluation, this attribute holds the validated\n    ``scoring`` dict which maps the scorer key to the scorer callable.\n\nn_splits_ : int\n    The number of cross-validation splits (folds/iterations).\n\nrefit_time_ : float\n    Seconds used for refitting the best model on the whole dataset.\n\n    This is present only if ``refit`` is not False.\n\n    .. versionadded:: 0.20\n\nmultimetric_ : bool\n    Whether or not the scorers compute several metrics.\n\nNotes\n-----\nThe parameters selected are those that maximize the score of the left out\ndata, unless an explicit score is passed in which case it is used instead.\n\nIf `n_jobs` was set to a value higher than one, the data is copied for each\npoint in the grid (and not `n_jobs` times). This is done for efficiency\nreasons if individual jobs take very little time, but may raise errors if\nthe dataset is large and not enough memory is available.  A workaround in\nthis case is to set `pre_dispatch`. Then, the memory is copied only\n`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\nn_jobs`.\n\nSee Also\n---------\nParameterGrid : Generates all the combinations of a hyperparameter grid.\ntrain_test_split : Utility function to split the data into a development\n    set usable for fitting a GridSearchCV instance and an evaluation set\n    for its final evaluation.\nsklearn.metrics.make_scorer : Make a scorer from a performance metric or\n    loss function."
        },
        {
          "name": "ParameterGrid",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "Grid of parameters with a discrete number of values for each.\n\nCan be used to iterate over parameter value combinations with the\nPython built-in function iter.\nThe order of the generated parameter combinations is deterministic.\n\nRead more in the :ref:`User Guide <grid_search>`.\n\nParameters\n----------\nparam_grid : dict of str to sequence, or sequence of such\n    The parameter grid to explore, as a dictionary mapping estimator\n    parameters to sequences of allowed values.\n\n    An empty dict signifies default parameters.\n\n    A sequence of dicts signifies a sequence of grids to search, and is\n    useful to avoid exploring parameter combinations that make no sense\n    or have no effect. See the examples below.\n\nExamples\n--------\n>>> from sklearn.model_selection import ParameterGrid\n>>> param_grid = {'a': [1, 2], 'b': [True, False]}\n>>> list(ParameterGrid(param_grid)) == (\n...    [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n...     {'a': 2, 'b': True}, {'a': 2, 'b': False}])\nTrue\n\n>>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n>>> list(ParameterGrid(grid)) == [{'kernel': 'linear'},\n...                               {'kernel': 'rbf', 'gamma': 1},\n...                               {'kernel': 'rbf', 'gamma': 10}]\nTrue\n>>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}\nTrue\n\nSee Also\n--------\nGridSearchCV : Uses :class:`ParameterGrid` to perform a full parallelized\n    parameter search."
        },
        {
          "name": "ParameterSampler",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "Generator on parameters sampled from given distributions.\n\nNon-deterministic iterable over random candidate combinations for hyper-\nparameter search. If all parameters are presented as a list,\nsampling without replacement is performed. If at least one parameter\nis given as a distribution, sampling with replacement is used.\nIt is highly recommended to use continuous distributions for continuous\nparameters.\n\nRead more in the :ref:`User Guide <grid_search>`.\n\nParameters\n----------\nparam_distributions : dict\n    Dictionary with parameters names (`str`) as keys and distributions\n    or lists of parameters to try. Distributions must provide a ``rvs``\n    method for sampling (such as those from scipy.stats.distributions).\n    If a list is given, it is sampled uniformly.\n    If a list of dicts is given, first a dict is sampled uniformly, and\n    then a parameter is sampled using that dict as above.\n\nn_iter : int\n    Number of parameter settings that are produced.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo random number generator state used for random uniform sampling\n    from lists of possible values instead of scipy.stats distributions.\n    Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nparams : dict of str to any\n    **Yields** dictionaries mapping each estimator parameter to\n    as sampled value.\n\nExamples\n--------\n>>> from sklearn.model_selection import ParameterSampler\n>>> from scipy.stats.distributions import expon\n>>> import numpy as np\n>>> rng = np.random.RandomState(0)\n>>> param_grid = {'a':[1, 2], 'b': expon()}\n>>> param_list = list(ParameterSampler(param_grid, n_iter=4,\n...                                    random_state=rng))\n>>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())\n...                 for d in param_list]\n>>> rounded_list == [{'b': 0.89856, 'a': 1},\n...                  {'b': 0.923223, 'a': 1},\n...                  {'b': 1.878964, 'a': 2},\n...                  {'b': 1.038159, 'a': 2}]\nTrue"
        },
        {
          "name": "RandomizedSearchCV",
          "decorators": [],
          "superclasses": [
            "BaseSearchCV"
          ],
          "methods": [],
          "fullDocstring": "Randomized search on hyper parameters.\n\nRandomizedSearchCV implements a \"fit\" and a \"score\" method.\nIt also implements \"score_samples\", \"predict\", \"predict_proba\",\n\"decision_function\", \"transform\" and \"inverse_transform\" if they are\nimplemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized\nby cross-validated search over parameter settings.\n\nIn contrast to GridSearchCV, not all parameter values are tried out, but\nrather a fixed number of parameter settings is sampled from the specified\ndistributions. The number of parameter settings that are tried is\ngiven by n_iter.\n\nIf all parameters are presented as a list,\nsampling without replacement is performed. If at least one parameter\nis given as a distribution, sampling with replacement is used.\nIt is highly recommended to use continuous distributions for continuous\nparameters.\n\nRead more in the :ref:`User Guide <randomized_parameter_search>`.\n\n.. versionadded:: 0.14\n\nParameters\n----------\nestimator : estimator object.\n    A object of that type is instantiated for each grid point.\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparam_distributions : dict or list of dicts\n    Dictionary with parameters names (`str`) as keys and distributions\n    or lists of parameters to try. Distributions must provide a ``rvs``\n    method for sampling (such as those from scipy.stats.distributions).\n    If a list is given, it is sampled uniformly.\n    If a list of dicts is given, first a dict is sampled uniformly, and\n    then a parameter is sampled using that dict as above.\n\nn_iter : int, default=10\n    Number of parameter settings that are sampled. n_iter trades\n    off runtime vs quality of the solution.\n\nscoring : str, callable, list/tuple or dict, default=None\n    A single str (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n    For evaluating multiple metrics, either give a list of (unique) strings\n    or a dict with names as keys and callables as values.\n\n    NOTE that when using custom scorers, each scorer should return a single\n    value. Metric functions returning a list/array of values can be wrapped\n    into multiple scorers that return one value each.\n\n    See :ref:`multimetric_grid_search` for an example.\n\n    If None, the estimator's score method is used.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\npre_dispatch : int, or str, default=None\n    Controls the number of jobs that get dispatched during parallel\n    execution. Reducing this number can be useful to avoid an\n    explosion of memory consumption when more jobs get dispatched\n    than CPUs can process. This parameter can be:\n\n        - None, in which case all the jobs are immediately\n          created and spawned. Use this for lightweight and\n          fast-running jobs, to avoid delays due to on-demand\n          spawning of the jobs\n\n        - An int, giving the exact number of total jobs that are\n          spawned\n\n        - A str, giving an expression as a function of n_jobs,\n          as in '2*n_jobs'\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nrefit : bool, str, or callable, default=True\n    Refit an estimator using the best found parameters on the whole\n    dataset.\n\n    For multiple metric evaluation, this needs to be a `str` denoting the\n    scorer that would be used to find the best parameters for refitting\n    the estimator at the end.\n\n    Where there are considerations other than maximum score in\n    choosing a best estimator, ``refit`` can be set to a function which\n    returns the selected ``best_index_`` given the ``cv_results``. In that\n    case, the ``best_estimator_`` and ``best_params_`` will be set\n    according to the returned ``best_index_`` while the ``best_score_``\n    attribute will not be available.\n\n    The refitted estimator is made available at the ``best_estimator_``\n    attribute and permits using ``predict`` directly on this\n    ``RandomizedSearchCV`` instance.\n\n    Also for multiple metric evaluation, the attributes ``best_index_``,\n    ``best_score_`` and ``best_params_`` will only be available if\n    ``refit`` is set and all of them will be determined w.r.t this specific\n    scorer.\n\n    See ``scoring`` parameter to know more about multiple metric\n    evaluation.\n\n    .. versionchanged:: 0.20\n        Support for callable added.\n\nverbose : int\n    Controls the verbosity: the higher, the more messages.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo random number generator state used for random uniform sampling\n    from lists of possible values instead of scipy.stats distributions.\n    Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error.\n\nreturn_train_score : bool, default=False\n    If ``False``, the ``cv_results_`` attribute will not include training\n    scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\n    .. versionadded:: 0.19\n\n    .. versionchanged:: 0.21\n        Default value was changed from ``True`` to ``False``\n\nAttributes\n----------\ncv_results_ : dict of numpy (masked) ndarrays\n    A dict with keys as column headers and values as columns, that can be\n    imported into a pandas ``DataFrame``.\n\n    For instance the below given table\n\n    +--------------+-------------+-------------------+---+---------------+\n    | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n    +==============+=============+===================+===+===============+\n    |    'rbf'     |     0.1     |       0.80        |...|       1       |\n    +--------------+-------------+-------------------+---+---------------+\n    |    'rbf'     |     0.2     |       0.84        |...|       3       |\n    +--------------+-------------+-------------------+---+---------------+\n    |    'rbf'     |     0.3     |       0.70        |...|       2       |\n    +--------------+-------------+-------------------+---+---------------+\n\n    will be represented by a ``cv_results_`` dict of::\n\n        {\n        'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n                                      mask = False),\n        'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n        'split0_test_score'  : [0.80, 0.84, 0.70],\n        'split1_test_score'  : [0.82, 0.50, 0.70],\n        'mean_test_score'    : [0.81, 0.67, 0.70],\n        'std_test_score'     : [0.01, 0.24, 0.00],\n        'rank_test_score'    : [1, 3, 2],\n        'split0_train_score' : [0.80, 0.92, 0.70],\n        'split1_train_score' : [0.82, 0.55, 0.70],\n        'mean_train_score'   : [0.81, 0.74, 0.70],\n        'std_train_score'    : [0.01, 0.19, 0.00],\n        'mean_fit_time'      : [0.73, 0.63, 0.43],\n        'std_fit_time'       : [0.01, 0.02, 0.01],\n        'mean_score_time'    : [0.01, 0.06, 0.04],\n        'std_score_time'     : [0.00, 0.00, 0.00],\n        'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n        }\n\n    NOTE\n\n    The key ``'params'`` is used to store a list of parameter\n    settings dicts for all the parameter candidates.\n\n    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n    ``std_score_time`` are all in seconds.\n\n    For multi-metric evaluation, the scores for all the scorers are\n    available in the ``cv_results_`` dict at the keys ending with that\n    scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n    above. ('split0_test_precision', 'mean_train_precision' etc.)\n\nbest_estimator_ : estimator\n    Estimator that was chosen by the search, i.e. estimator\n    which gave highest score (or smallest loss if specified)\n    on the left out data. Not available if ``refit=False``.\n\n    For multi-metric evaluation, this attribute is present only if\n    ``refit`` is specified.\n\n    See ``refit`` parameter for more information on allowed values.\n\nbest_score_ : float\n    Mean cross-validated score of the best_estimator.\n\n    For multi-metric evaluation, this is not available if ``refit`` is\n    ``False``. See ``refit`` parameter for more information.\n\n    This attribute is not available if ``refit`` is a function.\n\nbest_params_ : dict\n    Parameter setting that gave the best results on the hold out data.\n\n    For multi-metric evaluation, this is not available if ``refit`` is\n    ``False``. See ``refit`` parameter for more information.\n\nbest_index_ : int\n    The index (of the ``cv_results_`` arrays) which corresponds to the best\n    candidate parameter setting.\n\n    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n    the parameter setting for the best model, that gives the highest\n    mean score (``search.best_score_``).\n\n    For multi-metric evaluation, this is not available if ``refit`` is\n    ``False``. See ``refit`` parameter for more information.\n\nscorer_ : function or a dict\n    Scorer function used on the held out data to choose the best\n    parameters for the model.\n\n    For multi-metric evaluation, this attribute holds the validated\n    ``scoring`` dict which maps the scorer key to the scorer callable.\n\nn_splits_ : int\n    The number of cross-validation splits (folds/iterations).\n\nrefit_time_ : float\n    Seconds used for refitting the best model on the whole dataset.\n\n    This is present only if ``refit`` is not False.\n\n    .. versionadded:: 0.20\n\nmultimetric_ : bool\n    Whether or not the scorers compute several metrics.\n\nNotes\n-----\nThe parameters selected are those that maximize the score of the held-out\ndata, according to the scoring parameter.\n\nIf `n_jobs` was set to a value higher than one, the data is copied for each\nparameter setting(and not `n_jobs` times). This is done for efficiency\nreasons if individual jobs take very little time, but may raise errors if\nthe dataset is large and not enough memory is available.  A workaround in\nthis case is to set `pre_dispatch`. Then, the memory is copied only\n`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\nn_jobs`.\n\nSee Also\n--------\nGridSearchCV : Does exhaustive search over a grid of parameters.\nParameterSampler : A generator over parameter settings, constructed from\n    param_distributions.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.model_selection import RandomizedSearchCV\n>>> from scipy.stats import uniform\n>>> iris = load_iris()\n>>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n...                               random_state=0)\n>>> distributions = dict(C=uniform(loc=0, scale=4),\n...                      penalty=['l2', 'l1'])\n>>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n>>> search = clf.fit(iris.data, iris.target)\n>>> search.best_params_\n{'C': 2..., 'penalty': 'l1'}"
        }
      ],
      "functions": [
        {
          "name": "fit_grid_point",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Targets for input data."
            },
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A object of that type is instantiated for each grid point.\nThis is assumed to implement the scikit-learn estimator interface.\nEither estimator needs to provide a ``score`` function,\nor ``scoring`` must be passed."
            },
            {
              "name": "parameters",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Parameters to be set on estimator for this grid point."
            },
            {
              "name": "train",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Boolean mask or indices for training set."
            },
            {
              "name": "test",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Boolean mask or indices for test set."
            },
            {
              "name": "scorer",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The scorer callable object / function must have its signature as\n``scorer(estimator, X, y)``.\n\nIf ``None`` the estimator's score method is used."
            },
            {
              "name": "verbose",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Verbosity level."
            },
            {
              "name": "error_score",
              "type": "Any",
              "hasDefault": true,
              "default": "<ast.Name object at 0x0000014A7B8443A0>",
              "limitation": null,
              "ignored": false,
              "description": "Value to assign to the score if an error occurs in estimator fitting.\nIf set to 'raise', the error is raised. If a numeric value is given,\nFitFailedWarning is raised. This parameter does not affect the refit\nstep, which will always raise the error."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": "float",
              "description": "Score of this parameter setting on given test split."
            },
            {
              "name": "parameters",
              "type": "Dict",
              "description": "The parameters that have been evaluated."
            },
            {
              "name": "n_samples_test",
              "type": "int",
              "description": "Number of test samples in this split."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Run fit on one set of parameters.\n\nParameters\n----------\nX : array-like, sparse matrix or list\n    Input data.\n\ny : array-like or None\n    Targets for input data.\n\nestimator : estimator object\n    A object of that type is instantiated for each grid point.\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparameters : dict\n    Parameters to be set on estimator for this grid point.\n\ntrain : ndarray, dtype int or bool\n    Boolean mask or indices for training set.\n\ntest : ndarray, dtype int or bool\n    Boolean mask or indices for test set.\n\nscorer : callable or None\n    The scorer callable object / function must have its signature as\n    ``scorer(estimator, X, y)``.\n\n    If ``None`` the estimator's score method is used.\n\nverbose : int\n    Verbosity level.\n\n**fit_params : kwargs\n    Additional parameter passed to the fit function of the estimator.\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error.\n\nReturns\n-------\nscore : float\n     Score of this parameter setting on given test split.\n\nparameters : dict\n    The parameters that have been evaluated.\n\nn_samples_test : int\n    Number of test samples in this split."
        }
      ]
    },
    {
      "name": "sklearn.model_selection._search_successive_halving",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "ceil",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "floor",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "log",
          "alias": null
        },
        {
          "module": "numbers",
          "declaration": "Integral",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "ParameterGrid",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "ParameterSampler",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._search",
          "declaration": "BaseSearchCV",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._search",
          "declaration": "_check_param_grid",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._split",
          "declaration": "_yields_constant_splits",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._split",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "resample",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseSuccessiveHalving",
          "decorators": [],
          "superclasses": [
            "BaseSearchCV"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target relative to X for classification or regression;\nNone for unsupervised learning."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set. Only used in conjunction with a \"Group\" :term:`cv`\ninstance (e.g., :class:`~sklearn.model_selection.GroupKFold`)."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Run fit with all sets of parameters.\n\nParameters\n----------\n\nX : array-like, shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like, shape (n_samples,) or (n_samples, n_output), optional\n    Target relative to X for classification or regression;\n    None for unsupervised learning.\n\ngroups : array-like of shape (n_samples,), default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n\n**fit_params : dict of string -> object\n    Parameters passed to the ``fit`` method of the estimator"
            }
          ],
          "fullDocstring": "Implements successive halving.\n\nRef:\nAlmost optimal exploration in multi-armed bandits, ICML 13\nZohar Karnin, Tomer Koren, Oren Somekh"
        },
        {
          "name": "HalvingGridSearchCV",
          "decorators": [],
          "superclasses": [
            "BaseSuccessiveHalving"
          ],
          "methods": [],
          "fullDocstring": "Search over specified parameter values with successive halving.\n\nThe search strategy starts evaluating all the candidates with a small\namount of resources and iteratively selects the best candidates, using\nmore and more resources.\n\nRead more in the :ref:`User guide <successive_halving_user_guide>`.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_halving_search_cv``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_halving_search_cv # noqa\n    >>> # now you can import normally from model_selection\n    >>> from sklearn.model_selection import HalvingGridSearchCV\n\nParameters\n----------\nestimator : estimator object.\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparam_grid : dict or list of dictionaries\n    Dictionary with parameters names (string) as keys and lists of\n    parameter settings to try as values, or a list of such\n    dictionaries, in which case the grids spanned by each dictionary\n    in the list are explored. This enables searching over any sequence\n    of parameter settings.\n\nfactor : int or float, default=3\n    The 'halving' parameter, which determines the proportion of candidates\n    that are selected for each subsequent iteration. For example,\n    ``factor=3`` means that only one third of the candidates are selected.\n\nresource : ``'n_samples'`` or str, default='n_samples'\n    Defines the resource that increases with each iteration. By default,\n    the resource is the number of samples. It can also be set to any\n    parameter of the base estimator that accepts positive integer\n    values, e.g. 'n_iterations' or 'n_estimators' for a gradient\n    boosting estimator. In this case ``max_resources`` cannot be 'auto'\n    and must be set explicitly.\n\nmax_resources : int, default='auto'\n    The maximum amount of resource that any candidate is allowed to use\n    for a given iteration. By default, this is set to ``n_samples`` when\n    ``resource='n_samples'`` (default), else an error is raised.\n\nmin_resources : {'exhaust', 'smallest'} or int, default='exhaust'\n    The minimum amount of resource that any candidate is allowed to use\n    for a given iteration. Equivalently, this defines the amount of\n    resources `r0` that are allocated for each candidate at the first\n    iteration.\n\n    - 'smallest' is a heuristic that sets `r0` to a small value:\n        - ``n_splits * 2`` when ``resource='n_samples'`` for a regression\n           problem\n        - ``n_classes * n_splits * 2`` when ``resource='n_samples'`` for a\n           classification problem\n        - ``1`` when ``resource != 'n_samples'``\n    - 'exhaust' will set `r0` such that the **last** iteration uses as\n      much resources as possible. Namely, the last iteration will use the\n      highest value smaller than ``max_resources`` that is a multiple of\n      both ``min_resources`` and ``factor``. In general, using 'exhaust'\n      leads to a more accurate estimator, but is slightly more time\n      consuming.\n\n    Note that the amount of resources used at each iteration is always a\n    multiple of ``min_resources``.\n\naggressive_elimination : bool, default=False\n    This is only relevant in cases where there isn't enough resources to\n    reduce the remaining candidates to at most `factor` after the last\n    iteration. If ``True``, then the search process will 'replay' the\n    first iteration for as long as needed until the number of candidates\n    is small enough. This is ``False`` by default, which means that the\n    last iteration may evaluate more than ``factor`` candidates. See\n    :ref:`aggressive_elimination` for more details.\n\ncv : int, cross-validation generator or iterable, default=5\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. note::\n        Due to implementation details, the folds produced by `cv` must be\n        the same across multiple calls to `cv.split()`. For\n        built-in `scikit-learn` iterators, this can be achieved by\n        deactivating shuffling (`shuffle=False`), or by setting the\n        `cv`'s `random_state` parameter to an integer.\n\nscoring : string, callable, or None, default=None\n    A single string (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n    If None, the estimator's score method is used.\n\nrefit : bool, default=True\n    If True, refit an estimator using the best found parameters on the\n    whole dataset.\n\n    The refitted estimator is made available at the ``best_estimator_``\n    attribute and permits using ``predict`` directly on this\n    ``HalvingGridSearchCV`` instance.\n\nerror_score : 'raise' or numeric\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error. Default is ``np.nan``\n\nreturn_train_score : bool, default=False\n    If ``False``, the ``cv_results_`` attribute will not include training\n    scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo random number generator state used for subsampling the dataset\n    when `resources != 'n_samples'`. Ignored otherwise.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int or None, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int\n    Controls the verbosity: the higher, the more messages.\n\nAttributes\n----------\nn_resources_ : list of int\n    The amount of resources used at each iteration.\n\nn_candidates_ : list of int\n    The number of candidate parameters that were evaluated at each\n    iteration.\n\nn_remaining_candidates_ : int\n    The number of candidate parameters that are left after the last\n    iteration. It corresponds to `ceil(n_candidates[-1] / factor)`\n\nmax_resources_ : int\n    The maximum number of resources that any candidate is allowed to use\n    for a given iteration. Note that since the number of resources used\n    at each iteration must be a multiple of ``min_resources_``, the\n    actual number of resources used at the last iteration may be smaller\n    than ``max_resources_``.\n\nmin_resources_ : int\n    The amount of resources that are allocated for each candidate at the\n    first iteration.\n\nn_iterations_ : int\n    The actual number of iterations that were run. This is equal to\n    ``n_required_iterations_`` if ``aggressive_elimination`` is ``True``.\n    Else, this is equal to ``min(n_possible_iterations_,\n    n_required_iterations_)``.\n\nn_possible_iterations_ : int\n    The number of iterations that are possible starting with\n    ``min_resources_`` resources and without exceeding\n    ``max_resources_``.\n\nn_required_iterations_ : int\n    The number of iterations that are required to end up with less than\n    ``factor`` candidates at the last iteration, starting with\n    ``min_resources_`` resources. This will be smaller than\n    ``n_possible_iterations_`` when there isn't enough resources.\n\ncv_results_ : dict of numpy (masked) ndarrays\n    A dict with keys as column headers and values as columns, that can be\n    imported into a pandas ``DataFrame``. It contains many informations for\n    analysing the results of a search.\n    Please refer to the :ref:`User guide<successive_halving_cv_results>`\n    for details.\n\nbest_estimator_ : estimator or dict\n    Estimator that was chosen by the search, i.e. estimator\n    which gave highest score (or smallest loss if specified)\n    on the left out data. Not available if ``refit=False``.\n\nbest_score_ : float\n    Mean cross-validated score of the best_estimator.\n\nbest_params_ : dict\n    Parameter setting that gave the best results on the hold out data.\n\nbest_index_ : int\n    The index (of the ``cv_results_`` arrays) which corresponds to the best\n    candidate parameter setting.\n\n    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n    the parameter setting for the best model, that gives the highest\n    mean score (``search.best_score_``).\n\nscorer_ : function or a dict\n    Scorer function used on the held out data to choose the best\n    parameters for the model.\n\nn_splits_ : int\n    The number of cross-validation splits (folds/iterations).\n\nrefit_time_ : float\n    Seconds used for refitting the best model on the whole dataset.\n\n    This is present only if ``refit`` is not False.\n\nSee Also\n--------\n:class:`HalvingRandomSearchCV`:\n    Random search over a set of parameters using successive halving.\n\nNotes\n-----\nThe parameters selected are those that maximize the score of the held-out\ndata, according to the scoring parameter.\n\nExamples\n--------\n\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\n>>> from sklearn.model_selection import HalvingGridSearchCV\n...\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = RandomForestClassifier(random_state=0)\n...\n>>> param_grid = {\"max_depth\": [3, None],\n...               \"min_samples_split\": [5, 10]}\n>>> search = HalvingGridSearchCV(clf, param_grid, resource='n_estimators',\n...                              max_resources=10,\n...                              random_state=0).fit(X, y)\n>>> search.best_params_  # doctest: +SKIP\n{'max_depth': None, 'min_samples_split': 10, 'n_estimators': 9}"
        },
        {
          "name": "HalvingRandomSearchCV",
          "decorators": [],
          "superclasses": [
            "BaseSuccessiveHalving"
          ],
          "methods": [],
          "fullDocstring": "Randomized search on hyper parameters.\n\nThe search strategy starts evaluating all the candidates with a small\namount of resources and iteratively selects the best candidates, using more\nand more resources.\n\nThe candidates are sampled at random from the parameter space and the\nnumber of sampled candidates is determined by ``n_candidates``.\n\nRead more in the :ref:`User guide<successive_halving_user_guide>`.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_halving_search_cv``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_halving_search_cv # noqa\n    >>> # now you can import normally from model_selection\n    >>> from sklearn.model_selection import HalvingRandomSearchCV\n\nParameters\n----------\nestimator : estimator object.\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparam_distributions : dict\n    Dictionary with parameters names (string) as keys and distributions\n    or lists of parameters to try. Distributions must provide a ``rvs``\n    method for sampling (such as those from scipy.stats.distributions).\n    If a list is given, it is sampled uniformly.\n\nn_candidates : int, default='exhaust'\n    The number of candidate parameters to sample, at the first\n    iteration. Using 'exhaust' will sample enough candidates so that the\n    last iteration uses as many resources as possible, based on\n    `min_resources`, `max_resources` and `factor`. In this case,\n    `min_resources` cannot be 'exhaust'.\n\nfactor : int or float, default=3\n    The 'halving' parameter, which determines the proportion of candidates\n    that are selected for each subsequent iteration. For example,\n    ``factor=3`` means that only one third of the candidates are selected.\n\nresource : ``'n_samples'`` or str, default='n_samples'\n    Defines the resource that increases with each iteration. By default,\n    the resource is the number of samples. It can also be set to any\n    parameter of the base estimator that accepts positive integer\n    values, e.g. 'n_iterations' or 'n_estimators' for a gradient\n    boosting estimator. In this case ``max_resources`` cannot be 'auto'\n    and must be set explicitly.\n\nmax_resources : int, default='auto'\n    The maximum number of resources that any candidate is allowed to use\n    for a given iteration. By default, this is set ``n_samples`` when\n    ``resource='n_samples'`` (default), else an error is raised.\n\nmin_resources : {'exhaust', 'smallest'} or int, default='smallest'\n    The minimum amount of resource that any candidate is allowed to use\n    for a given iteration. Equivalently, this defines the amount of\n    resources `r0` that are allocated for each candidate at the first\n    iteration.\n\n    - 'smallest' is a heuristic that sets `r0` to a small value:\n        - ``n_splits * 2`` when ``resource='n_samples'`` for a regression\n           problem\n        - ``n_classes * n_splits * 2`` when ``resource='n_samples'`` for a\n           classification problem\n        - ``1`` when ``resource != 'n_samples'``\n    - 'exhaust' will set `r0` such that the **last** iteration uses as\n      much resources as possible. Namely, the last iteration will use the\n      highest value smaller than ``max_resources`` that is a multiple of\n      both ``min_resources`` and ``factor``. In general, using 'exhaust'\n      leads to a more accurate estimator, but is slightly more time\n      consuming. 'exhaust' isn't available when `n_candidates='exhaust'`.\n\n    Note that the amount of resources used at each iteration is always a\n    multiple of ``min_resources``.\n\naggressive_elimination : bool, default=False\n    This is only relevant in cases where there isn't enough resources to\n    reduce the remaining candidates to at most `factor` after the last\n    iteration. If ``True``, then the search process will 'replay' the\n    first iteration for as long as needed until the number of candidates\n    is small enough. This is ``False`` by default, which means that the\n    last iteration may evaluate more than ``factor`` candidates. See\n    :ref:`aggressive_elimination` for more details.\n\ncv : int, cross-validation generator or an iterable, default=5\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. note::\n        Due to implementation details, the folds produced by `cv` must be\n        the same across multiple calls to `cv.split()`. For\n        built-in `scikit-learn` iterators, this can be achieved by\n        deactivating shuffling (`shuffle=False`), or by setting the\n        `cv`'s `random_state` parameter to an integer.\n\nscoring : string, callable, or None, default=None\n    A single string (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n    If None, the estimator's score method is used.\n\nrefit : bool, default=True\n    If True, refit an estimator using the best found parameters on the\n    whole dataset.\n\n    The refitted estimator is made available at the ``best_estimator_``\n    attribute and permits using ``predict`` directly on this\n    ``HalvingRandomSearchCV`` instance.\n\nerror_score : 'raise' or numeric\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error. Default is ``np.nan``\n\nreturn_train_score : bool, default=False\n    If ``False``, the ``cv_results_`` attribute will not include training\n    scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo random number generator state used for subsampling the dataset\n    when `resources != 'n_samples'`. Also used for random uniform\n    sampling from lists of possible values instead of scipy.stats\n    distributions.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int or None, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int\n    Controls the verbosity: the higher, the more messages.\n\nAttributes\n----------\nn_resources_ : list of int\n    The amount of resources used at each iteration.\n\nn_candidates_ : list of int\n    The number of candidate parameters that were evaluated at each\n    iteration.\n\nn_remaining_candidates_ : int\n    The number of candidate parameters that are left after the last\n    iteration. It corresponds to `ceil(n_candidates[-1] / factor)`\n\nmax_resources_ : int\n    The maximum number of resources that any candidate is allowed to use\n    for a given iteration. Note that since the number of resources used at\n    each iteration must be a multiple of ``min_resources_``, the actual\n    number of resources used at the last iteration may be smaller than\n    ``max_resources_``.\n\nmin_resources_ : int\n    The amount of resources that are allocated for each candidate at the\n    first iteration.\n\nn_iterations_ : int\n    The actual number of iterations that were run. This is equal to\n    ``n_required_iterations_`` if ``aggressive_elimination`` is ``True``.\n    Else, this is equal to ``min(n_possible_iterations_,\n    n_required_iterations_)``.\n\nn_possible_iterations_ : int\n    The number of iterations that are possible starting with\n    ``min_resources_`` resources and without exceeding\n    ``max_resources_``.\n\nn_required_iterations_ : int\n    The number of iterations that are required to end up with less than\n    ``factor`` candidates at the last iteration, starting with\n    ``min_resources_`` resources. This will be smaller than\n    ``n_possible_iterations_`` when there isn't enough resources.\n\ncv_results_ : dict of numpy (masked) ndarrays\n    A dict with keys as column headers and values as columns, that can be\n    imported into a pandas ``DataFrame``. It contains many informations for\n    analysing the results of a search.\n    Please refer to the :ref:`User guide<successive_halving_cv_results>`\n    for details.\n\nbest_estimator_ : estimator or dict\n    Estimator that was chosen by the search, i.e. estimator\n    which gave highest score (or smallest loss if specified)\n    on the left out data. Not available if ``refit=False``.\n\nbest_score_ : float\n    Mean cross-validated score of the best_estimator.\n\nbest_params_ : dict\n    Parameter setting that gave the best results on the hold out data.\n\nbest_index_ : int\n    The index (of the ``cv_results_`` arrays) which corresponds to the best\n    candidate parameter setting.\n\n    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n    the parameter setting for the best model, that gives the highest\n    mean score (``search.best_score_``).\n\nscorer_ : function or a dict\n    Scorer function used on the held out data to choose the best\n    parameters for the model.\n\nn_splits_ : int\n    The number of cross-validation splits (folds/iterations).\n\nrefit_time_ : float\n    Seconds used for refitting the best model on the whole dataset.\n\n    This is present only if ``refit`` is not False.\n\nSee Also\n--------\n:class:`HalvingGridSearchCV`:\n    Search over a grid of parameters using successive halving.\n\nNotes\n-----\nThe parameters selected are those that maximize the score of the held-out\ndata, according to the scoring parameter.\n\nExamples\n--------\n\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\n>>> from sklearn.model_selection import HalvingRandomSearchCV\n>>> from scipy.stats import randint\n...\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = RandomForestClassifier(random_state=0)\n>>> np.random.seed(0)\n...\n>>> param_distributions = {\"max_depth\": [3, None],\n...                        \"min_samples_split\": randint(2, 11)}\n>>> search = HalvingRandomSearchCV(clf, param_distributions,\n...                                resource='n_estimators',\n...                                max_resources=10,\n...                                random_state=0).fit(X, y)\n>>> search.best_params_  # doctest: +SKIP\n{'max_depth': None, 'min_samples_split': 10, 'n_estimators': 9}"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.model_selection._split",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Iterable",
          "alias": null
        },
        {
          "module": "inspect",
          "declaration": "signature",
          "alias": null
        },
        {
          "module": "itertools",
          "declaration": "chain",
          "alias": null
        },
        {
          "module": "itertools",
          "declaration": "combinations",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "ceil",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "floor",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "comb",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "_pprint",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_approximate_mode",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "indexable",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "type_of_target",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "column_or_1d",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseCrossValidator",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target variable for supervised learning problems."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    The target variable for supervised learning problems.\n\ngroups : array-like of shape (n_samples,), default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split."
            },
            {
              "name": "get_n_splits",
              "decorators": [
                "abstractmethod"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the number of splitting iterations in the cross-validator"
            }
          ],
          "fullDocstring": "Base class for all cross-validators\n\nImplementations must define `_iter_test_masks` or `_iter_test_indices`."
        },
        {
          "name": "BaseShuffleSplit",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target variable for supervised learning problems."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    The target variable for supervised learning problems.\n\ngroups : array-like of shape (n_samples,), default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split.\n\nNotes\n-----\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer."
            },
            {
              "name": "get_n_splits",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                }
              ],
              "results": [
                {
                  "name": "n_splits",
                  "type": "int",
                  "description": "Returns the number of splitting iterations in the cross-validator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the number of splitting iterations in the cross-validator\n\nParameters\n----------\nX : object\n    Always ignored, exists for compatibility.\n\ny : object\n    Always ignored, exists for compatibility.\n\ngroups : object\n    Always ignored, exists for compatibility.\n\nReturns\n-------\nn_splits : int\n    Returns the number of splitting iterations in the cross-validator."
            }
          ],
          "fullDocstring": "Base class for ShuffleSplit and StratifiedShuffleSplit"
        },
        {
          "name": "GroupKFold",
          "decorators": [],
          "superclasses": [
            "_BaseKFold"
          ],
          "methods": [
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target variable for supervised learning problems."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : array-like of shape (n_samples,), default=None\n    The target variable for supervised learning problems.\n\ngroups : array-like of shape (n_samples,)\n    Group labels for the samples used while splitting the dataset into\n    train/test set.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split."
            }
          ],
          "fullDocstring": "K-fold iterator variant with non-overlapping groups.\n\nThe same group will not appear in two different folds (the number of\ndistinct groups has to be at least equal to the number of folds).\n\nThe folds are approximately balanced in the sense that the number of\ndistinct groups is approximately the same in each fold.\n\nRead more in the :ref:`User Guide <group_k_fold>`.\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\n    .. versionchanged:: 0.22\n        ``n_splits`` default value changed from 3 to 5.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import GroupKFold\n>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n>>> y = np.array([1, 2, 3, 4])\n>>> groups = np.array([0, 0, 2, 2])\n>>> group_kfold = GroupKFold(n_splits=2)\n>>> group_kfold.get_n_splits(X, y, groups)\n2\n>>> print(group_kfold)\nGroupKFold(n_splits=2)\n>>> for train_index, test_index in group_kfold.split(X, y, groups):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\n...     print(X_train, X_test, y_train, y_test)\n...\nTRAIN: [0 1] TEST: [2 3]\n[[1 2]\n [3 4]] [[5 6]\n [7 8]] [1 2] [3 4]\nTRAIN: [2 3] TEST: [0 1]\n[[5 6]\n [7 8]] [[1 2]\n [3 4]] [3 4] [1 2]\n\nSee Also\n--------\nLeaveOneGroupOut : For splitting the data according to explicit\n    domain-specific stratification of the dataset."
        },
        {
          "name": "GroupShuffleSplit",
          "decorators": [],
          "superclasses": [
            "ShuffleSplit"
          ],
          "methods": [
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target variable for supervised learning problems."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : array-like of shape (n_samples,), default=None\n    The target variable for supervised learning problems.\n\ngroups : array-like of shape (n_samples,)\n    Group labels for the samples used while splitting the dataset into\n    train/test set.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split.\n\nNotes\n-----\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer."
            }
          ],
          "fullDocstring": "Shuffle-Group(s)-Out cross-validation iterator\n\nProvides randomized train/test indices to split data according to a\nthird-party provided group. This group information can be used to encode\narbitrary domain specific stratifications of the samples as integers.\n\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nThe difference between LeavePGroupsOut and GroupShuffleSplit is that\nthe former generates splits using all subsets of size ``p`` unique groups,\nwhereas GroupShuffleSplit generates a user-determined number of random\ntest splits, each with a user-determined fraction of unique groups.\n\nFor example, a less computationally intensive alternative to\n``LeavePGroupsOut(p=10)`` would be\n``GroupShuffleSplit(test_size=10, n_splits=100)``.\n\nNote: The parameters ``test_size`` and ``train_size`` refer to groups, and\nnot to samples, as in ShuffleSplit.\n\nRead more in the :ref:`User Guide <group_shuffle_split>`.\n\nParameters\n----------\nn_splits : int, default=5\n    Number of re-shuffling & splitting iterations.\n\ntest_size : float, int, default=0.2\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of groups to include in the test split (rounded up). If int,\n    represents the absolute number of test groups. If None, the value is\n    set to the complement of the train size.\n    The default will change in version 0.21. It will remain 0.2 only\n    if ``train_size`` is unspecified, otherwise it will complement\n    the specified ``train_size``.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the groups to include in the train split. If\n    int, represents the absolute number of train groups. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the training and testing indices produced.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import GroupShuffleSplit\n>>> X = np.ones(shape=(8, 2))\n>>> y = np.ones(shape=(8, 1))\n>>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n>>> print(groups.shape)\n(8,)\n>>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n>>> gss.get_n_splits()\n2\n>>> for train_idx, test_idx in gss.split(X, y, groups):\n...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\nTRAIN: [2 3 4 5 6 7] TEST: [0 1]\nTRAIN: [0 1 5 6 7] TEST: [2 3 4]"
        },
        {
          "name": "KFold",
          "decorators": [],
          "superclasses": [
            "_BaseKFold"
          ],
          "methods": [],
          "fullDocstring": "K-Folds cross-validator\n\nProvides train/test indices to split data in train/test sets. Split\ndataset into k consecutive folds (without shuffling by default).\n\nEach fold is then used once as a validation while the k - 1 remaining\nfolds form the training set.\n\nRead more in the :ref:`User Guide <k_fold>`.\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\n    .. versionchanged:: 0.22\n        ``n_splits`` default value changed from 3 to 5.\n\nshuffle : bool, default=False\n    Whether to shuffle the data before splitting into batches.\n    Note that the samples within each split will not be shuffled.\n\nrandom_state : int, RandomState instance or None, default=None\n    When `shuffle` is True, `random_state` affects the ordering of the\n    indices, which controls the randomness of each fold. Otherwise, this\n    parameter has no effect.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import KFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([1, 2, 3, 4])\n>>> kf = KFold(n_splits=2)\n>>> kf.get_n_splits(X)\n2\n>>> print(kf)\nKFold(n_splits=2, random_state=None, shuffle=False)\n>>> for train_index, test_index in kf.split(X):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\nTRAIN: [2 3] TEST: [0 1]\nTRAIN: [0 1] TEST: [2 3]\n\nNotes\n-----\nThe first ``n_samples % n_splits`` folds have size\n``n_samples // n_splits + 1``, other folds have size\n``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer.\n\nSee Also\n--------\nStratifiedKFold : Takes group information into account to avoid building\n    folds with imbalanced class distributions (for binary or multiclass\n    classification tasks).\n\nGroupKFold : K-fold iterator variant with non-overlapping groups.\n\nRepeatedKFold : Repeats K-Fold n times."
        },
        {
          "name": "LeaveOneGroupOut",
          "decorators": [],
          "superclasses": [
            "BaseCrossValidator"
          ],
          "methods": [
            {
              "name": "get_n_splits",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set. This 'groups' parameter must always be specified to\ncalculate the number of splits, though the other parameters can be\nomitted."
                }
              ],
              "results": [
                {
                  "name": "n_splits",
                  "type": "int",
                  "description": "Returns the number of splitting iterations in the cross-validator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the number of splitting iterations in the cross-validator\n\nParameters\n----------\nX : object\n    Always ignored, exists for compatibility.\n\ny : object\n    Always ignored, exists for compatibility.\n\ngroups : array-like of shape (n_samples,)\n    Group labels for the samples used while splitting the dataset into\n    train/test set. This 'groups' parameter must always be specified to\n    calculate the number of splits, though the other parameters can be\n    omitted.\n\nReturns\n-------\nn_splits : int\n    Returns the number of splitting iterations in the cross-validator."
            },
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target variable for supervised learning problems."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : array-like of shape (n_samples,), default=None\n    The target variable for supervised learning problems.\n\ngroups : array-like of shape (n_samples,)\n    Group labels for the samples used while splitting the dataset into\n    train/test set.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split."
            }
          ],
          "fullDocstring": "Leave One Group Out cross-validator\n\nProvides train/test indices to split data according to a third-party\nprovided group. This group information can be used to encode arbitrary\ndomain specific stratifications of the samples as integers.\n\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nRead more in the :ref:`User Guide <leave_one_group_out>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import LeaveOneGroupOut\n>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n>>> y = np.array([1, 2, 1, 2])\n>>> groups = np.array([1, 1, 2, 2])\n>>> logo = LeaveOneGroupOut()\n>>> logo.get_n_splits(X, y, groups)\n2\n>>> logo.get_n_splits(groups=groups)  # 'groups' is always required\n2\n>>> print(logo)\nLeaveOneGroupOut()\n>>> for train_index, test_index in logo.split(X, y, groups):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\n...     print(X_train, X_test, y_train, y_test)\nTRAIN: [2 3] TEST: [0 1]\n[[5 6]\n [7 8]] [[1 2]\n [3 4]] [1 2] [1 2]\nTRAIN: [0 1] TEST: [2 3]\n[[1 2]\n [3 4]] [[5 6]\n [7 8]] [1 2] [1 2]"
        },
        {
          "name": "LeaveOneOut",
          "decorators": [],
          "superclasses": [
            "BaseCrossValidator"
          ],
          "methods": [
            {
              "name": "get_n_splits",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                }
              ],
              "results": [
                {
                  "name": "n_splits",
                  "type": "int",
                  "description": "Returns the number of splitting iterations in the cross-validator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the number of splitting iterations in the cross-validator\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : object\n    Always ignored, exists for compatibility.\n\ngroups : object\n    Always ignored, exists for compatibility.\n\nReturns\n-------\nn_splits : int\n    Returns the number of splitting iterations in the cross-validator."
            }
          ],
          "fullDocstring": "Leave-One-Out cross-validator\n\nProvides train/test indices to split data in train/test sets. Each\nsample is used once as a test set (singleton) while the remaining\nsamples form the training set.\n\nNote: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and\n``LeavePOut(p=1)`` where ``n`` is the number of samples.\n\nDue to the high number of test sets (which is the same as the\nnumber of samples) this cross-validation method can be very costly.\nFor large datasets one should favor :class:`KFold`, :class:`ShuffleSplit`\nor :class:`StratifiedKFold`.\n\nRead more in the :ref:`User Guide <leave_one_out>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import LeaveOneOut\n>>> X = np.array([[1, 2], [3, 4]])\n>>> y = np.array([1, 2])\n>>> loo = LeaveOneOut()\n>>> loo.get_n_splits(X)\n2\n>>> print(loo)\nLeaveOneOut()\n>>> for train_index, test_index in loo.split(X):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\n...     print(X_train, X_test, y_train, y_test)\nTRAIN: [1] TEST: [0]\n[[3 4]] [[1 2]] [2] [1]\nTRAIN: [0] TEST: [1]\n[[1 2]] [[3 4]] [1] [2]\n\nSee Also\n--------\nLeaveOneGroupOut : For splitting the data according to explicit,\n    domain-specific stratification of the dataset.\nGroupKFold : K-fold iterator variant with non-overlapping groups."
        },
        {
          "name": "LeavePGroupsOut",
          "decorators": [],
          "superclasses": [
            "BaseCrossValidator"
          ],
          "methods": [
            {
              "name": "get_n_splits",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set. This 'groups' parameter must always be specified to\ncalculate the number of splits, though the other parameters can be\nomitted."
                }
              ],
              "results": [
                {
                  "name": "n_splits",
                  "type": "int",
                  "description": "Returns the number of splitting iterations in the cross-validator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the number of splitting iterations in the cross-validator\n\nParameters\n----------\nX : object\n    Always ignored, exists for compatibility.\n\ny : object\n    Always ignored, exists for compatibility.\n\ngroups : array-like of shape (n_samples,)\n    Group labels for the samples used while splitting the dataset into\n    train/test set. This 'groups' parameter must always be specified to\n    calculate the number of splits, though the other parameters can be\n    omitted.\n\nReturns\n-------\nn_splits : int\n    Returns the number of splitting iterations in the cross-validator."
            },
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target variable for supervised learning problems."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : array-like of shape (n_samples,), default=None\n    The target variable for supervised learning problems.\n\ngroups : array-like of shape (n_samples,)\n    Group labels for the samples used while splitting the dataset into\n    train/test set.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split."
            }
          ],
          "fullDocstring": "Leave P Group(s) Out cross-validator\n\nProvides train/test indices to split data according to a third-party\nprovided group. This group information can be used to encode arbitrary\ndomain specific stratifications of the samples as integers.\n\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nThe difference between LeavePGroupsOut and LeaveOneGroupOut is that\nthe former builds the test sets with all the samples assigned to\n``p`` different values of the groups while the latter uses samples\nall assigned the same groups.\n\nRead more in the :ref:`User Guide <leave_p_groups_out>`.\n\nParameters\n----------\nn_groups : int\n    Number of groups (``p``) to leave out in the test split.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import LeavePGroupsOut\n>>> X = np.array([[1, 2], [3, 4], [5, 6]])\n>>> y = np.array([1, 2, 1])\n>>> groups = np.array([1, 2, 3])\n>>> lpgo = LeavePGroupsOut(n_groups=2)\n>>> lpgo.get_n_splits(X, y, groups)\n3\n>>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n3\n>>> print(lpgo)\nLeavePGroupsOut(n_groups=2)\n>>> for train_index, test_index in lpgo.split(X, y, groups):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\n...     print(X_train, X_test, y_train, y_test)\nTRAIN: [2] TEST: [0 1]\n[[5 6]] [[1 2]\n [3 4]] [1] [1 2]\nTRAIN: [1] TEST: [0 2]\n[[3 4]] [[1 2]\n [5 6]] [2] [1 1]\nTRAIN: [0] TEST: [1 2]\n[[1 2]] [[3 4]\n [5 6]] [1] [2 1]\n\nSee Also\n--------\nGroupKFold : K-fold iterator variant with non-overlapping groups."
        },
        {
          "name": "LeavePOut",
          "decorators": [],
          "superclasses": [
            "BaseCrossValidator"
          ],
          "methods": [
            {
              "name": "get_n_splits",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the number of splitting iterations in the cross-validator\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : object\n    Always ignored, exists for compatibility.\n\ngroups : object\n    Always ignored, exists for compatibility."
            }
          ],
          "fullDocstring": "Leave-P-Out cross-validator\n\nProvides train/test indices to split data in train/test sets. This results\nin testing on all distinct samples of size p, while the remaining n - p\nsamples form the training set in each iteration.\n\nNote: ``LeavePOut(p)`` is NOT equivalent to\n``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.\n\nDue to the high number of iterations which grows combinatorically with the\nnumber of samples this cross-validation method can be very costly. For\nlarge datasets one should favor :class:`KFold`, :class:`StratifiedKFold`\nor :class:`ShuffleSplit`.\n\nRead more in the :ref:`User Guide <leave_p_out>`.\n\nParameters\n----------\np : int\n    Size of the test sets. Must be strictly less than the number of\n    samples.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import LeavePOut\n>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n>>> y = np.array([1, 2, 3, 4])\n>>> lpo = LeavePOut(2)\n>>> lpo.get_n_splits(X)\n6\n>>> print(lpo)\nLeavePOut(p=2)\n>>> for train_index, test_index in lpo.split(X):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\nTRAIN: [2 3] TEST: [0 1]\nTRAIN: [1 3] TEST: [0 2]\nTRAIN: [1 2] TEST: [0 3]\nTRAIN: [0 3] TEST: [1 2]\nTRAIN: [0 2] TEST: [1 3]\nTRAIN: [0 1] TEST: [2 3]"
        },
        {
          "name": "PredefinedSplit",
          "decorators": [],
          "superclasses": [
            "BaseCrossValidator"
          ],
          "methods": [
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : object\n    Always ignored, exists for compatibility.\n\ny : object\n    Always ignored, exists for compatibility.\n\ngroups : object\n    Always ignored, exists for compatibility.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split."
            },
            {
              "name": "get_n_splits",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                }
              ],
              "results": [
                {
                  "name": "n_splits",
                  "type": "int",
                  "description": "Returns the number of splitting iterations in the cross-validator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the number of splitting iterations in the cross-validator\n\nParameters\n----------\nX : object\n    Always ignored, exists for compatibility.\n\ny : object\n    Always ignored, exists for compatibility.\n\ngroups : object\n    Always ignored, exists for compatibility.\n\nReturns\n-------\nn_splits : int\n    Returns the number of splitting iterations in the cross-validator."
            }
          ],
          "fullDocstring": "Predefined split cross-validator\n\nProvides train/test indices to split data into train/test sets using a\npredefined scheme specified by the user with the ``test_fold`` parameter.\n\nRead more in the :ref:`User Guide <predefined_split>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\ntest_fold : array-like of shape (n_samples,)\n    The entry ``test_fold[i]`` represents the index of the test set that\n    sample ``i`` belongs to. It is possible to exclude sample ``i`` from\n    any test set (i.e. include sample ``i`` in every training set) by\n    setting ``test_fold[i]`` equal to -1.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import PredefinedSplit\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 1, 1])\n>>> test_fold = [0, 1, -1, 1]\n>>> ps = PredefinedSplit(test_fold)\n>>> ps.get_n_splits()\n2\n>>> print(ps)\nPredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n>>> for train_index, test_index in ps.split():\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\nTRAIN: [1 2 3] TEST: [0]\nTRAIN: [0 2] TEST: [1 3]"
        },
        {
          "name": "RepeatedKFold",
          "decorators": [],
          "superclasses": [
            "_RepeatedSplits"
          ],
          "methods": [],
          "fullDocstring": "Repeated K-Fold cross validator.\n\nRepeats K-Fold n times with different randomization in each repetition.\n\nRead more in the :ref:`User Guide <repeated_k_fold>`.\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\nn_repeats : int, default=10\n    Number of times cross-validator needs to be repeated.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of each repeated cross-validation instance.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import RepeatedKFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 1, 1])\n>>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n>>> for train_index, test_index in rkf.split(X):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\n...\nTRAIN: [0 1] TEST: [2 3]\nTRAIN: [2 3] TEST: [0 1]\nTRAIN: [1 2] TEST: [0 3]\nTRAIN: [0 3] TEST: [1 2]\n\nNotes\n-----\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer.\n\nSee Also\n--------\nRepeatedStratifiedKFold : Repeats Stratified K-Fold n times."
        },
        {
          "name": "RepeatedStratifiedKFold",
          "decorators": [],
          "superclasses": [
            "_RepeatedSplits"
          ],
          "methods": [],
          "fullDocstring": "Repeated Stratified K-Fold cross validator.\n\nRepeats Stratified K-Fold n times with different randomization in each\nrepetition.\n\nRead more in the :ref:`User Guide <repeated_k_fold>`.\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\nn_repeats : int, default=10\n    Number of times cross-validator needs to be repeated.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the generation of the random states for each repetition.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import RepeatedStratifiedKFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 1, 1])\n>>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n...     random_state=36851234)\n>>> for train_index, test_index in rskf.split(X, y):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\n...\nTRAIN: [1 2] TEST: [0 3]\nTRAIN: [0 3] TEST: [1 2]\nTRAIN: [1 3] TEST: [0 2]\nTRAIN: [0 2] TEST: [1 3]\n\nNotes\n-----\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer.\n\nSee Also\n--------\nRepeatedKFold : Repeats K-Fold n times."
        },
        {
          "name": "ShuffleSplit",
          "decorators": [],
          "superclasses": [
            "BaseShuffleSplit"
          ],
          "methods": [],
          "fullDocstring": "Random permutation cross-validator\n\nYields indices to split data into training and test sets.\n\nNote: contrary to other cross-validation strategies, random splits\ndo not guarantee that all folds will be different, although this is\nstill very likely for sizeable datasets.\n\nRead more in the :ref:`User Guide <ShuffleSplit>`.\n\nParameters\n----------\nn_splits : int, default=10\n    Number of re-shuffling & splitting iterations.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.1.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the training and testing indices produced.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import ShuffleSplit\n>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n>>> y = np.array([1, 2, 1, 2, 1, 2])\n>>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n>>> rs.get_n_splits(X)\n5\n>>> print(rs)\nShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n>>> for train_index, test_index in rs.split(X):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\nTRAIN: [1 3 0 4] TEST: [5 2]\nTRAIN: [4 0 2 5] TEST: [1 3]\nTRAIN: [1 2 4 0] TEST: [3 5]\nTRAIN: [3 4 1 0] TEST: [5 2]\nTRAIN: [3 5 1 0] TEST: [2 4]\n>>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n...                   random_state=0)\n>>> for train_index, test_index in rs.split(X):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\nTRAIN: [1 3 0] TEST: [5 2]\nTRAIN: [4 0 2] TEST: [1 3]\nTRAIN: [1 2 4] TEST: [3 5]\nTRAIN: [3 4 1] TEST: [5 2]\nTRAIN: [3 5 1] TEST: [2 4]"
        },
        {
          "name": "StratifiedKFold",
          "decorators": [],
          "superclasses": [
            "_BaseKFold"
          ],
          "methods": [
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features.\n\nNote that providing ``y`` is sufficient to generate the splits and\nhence ``np.zeros(n_samples)`` may be used as a placeholder for\n``X`` instead of actual training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target variable for supervised learning problems.\nStratification is done based on the y labels."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\n    Note that providing ``y`` is sufficient to generate the splits and\n    hence ``np.zeros(n_samples)`` may be used as a placeholder for\n    ``X`` instead of actual training data.\n\ny : array-like of shape (n_samples,)\n    The target variable for supervised learning problems.\n    Stratification is done based on the y labels.\n\ngroups : object\n    Always ignored, exists for compatibility.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split.\n\nNotes\n-----\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer."
            }
          ],
          "fullDocstring": "Stratified K-Folds cross-validator.\n\nProvides train/test indices to split data in train/test sets.\n\nThis cross-validation object is a variation of KFold that returns\nstratified folds. The folds are made by preserving the percentage of\nsamples for each class.\n\nRead more in the :ref:`User Guide <stratified_k_fold>`.\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\n    .. versionchanged:: 0.22\n        ``n_splits`` default value changed from 3 to 5.\n\nshuffle : bool, default=False\n    Whether to shuffle each class's samples before splitting into batches.\n    Note that the samples within each split will not be shuffled.\n\nrandom_state : int, RandomState instance or None, default=None\n    When `shuffle` is True, `random_state` affects the ordering of the\n    indices, which controls the randomness of each fold for each class.\n    Otherwise, leave `random_state` as `None`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import StratifiedKFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 1, 1])\n>>> skf = StratifiedKFold(n_splits=2)\n>>> skf.get_n_splits(X, y)\n2\n>>> print(skf)\nStratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n>>> for train_index, test_index in skf.split(X, y):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\nTRAIN: [1 3] TEST: [0 2]\nTRAIN: [0 2] TEST: [1 3]\n\nNotes\n-----\nThe implementation is designed to:\n\n* Generate test sets such that all contain the same distribution of\n  classes, or as close as possible.\n* Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n  ``y = [1, 0]`` should not change the indices generated.\n* Preserve order dependencies in the dataset ordering, when\n  ``shuffle=False``: all samples from class k in some test set were\n  contiguous in y, or separated in y by samples from classes other than k.\n* Generate test sets where the smallest and largest differ by at most one\n  sample.\n\n.. versionchanged:: 0.22\n    The previous implementation did not follow the last constraint.\n\nSee Also\n--------\nRepeatedStratifiedKFold : Repeats Stratified K-Fold n times."
        },
        {
          "name": "StratifiedShuffleSplit",
          "decorators": [],
          "superclasses": [
            "BaseShuffleSplit"
          ],
          "methods": [
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features.\n\nNote that providing ``y`` is sufficient to generate the splits and\nhence ``np.zeros(n_samples)`` may be used as a placeholder for\n``X`` instead of actual training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target variable for supervised learning problems.\nStratification is done based on the y labels."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\n    Note that providing ``y`` is sufficient to generate the splits and\n    hence ``np.zeros(n_samples)`` may be used as a placeholder for\n    ``X`` instead of actual training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_labels)\n    The target variable for supervised learning problems.\n    Stratification is done based on the y labels.\n\ngroups : object\n    Always ignored, exists for compatibility.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split.\n\nNotes\n-----\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer."
            }
          ],
          "fullDocstring": "Stratified ShuffleSplit cross-validator\n\nProvides train/test indices to split data in train/test sets.\n\nThis cross-validation object is a merge of StratifiedKFold and\nShuffleSplit, which returns stratified randomized folds. The folds\nare made by preserving the percentage of samples for each class.\n\nNote: like the ShuffleSplit strategy, stratified random splits\ndo not guarantee that all folds will be different, although this is\nstill very likely for sizeable datasets.\n\nRead more in the :ref:`User Guide <stratified_shuffle_split>`.\n\nParameters\n----------\nn_splits : int, default=10\n    Number of re-shuffling & splitting iterations.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.1.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the training and testing indices produced.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import StratifiedShuffleSplit\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 0, 1, 1, 1])\n>>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n>>> sss.get_n_splits(X, y)\n5\n>>> print(sss)\nStratifiedShuffleSplit(n_splits=5, random_state=0, ...)\n>>> for train_index, test_index in sss.split(X, y):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\nTRAIN: [5 2 3] TEST: [4 1 0]\nTRAIN: [5 1 4] TEST: [0 2 3]\nTRAIN: [5 0 2] TEST: [4 3 1]\nTRAIN: [4 1 0] TEST: [2 3 5]\nTRAIN: [0 5 1] TEST: [3 4 2]"
        },
        {
          "name": "TimeSeriesSplit",
          "decorators": [],
          "superclasses": [
            "_BaseKFold"
          ],
          "methods": [
            {
              "name": "split",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                },
                {
                  "name": "groups",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Always ignored, exists for compatibility."
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate indices to split data into training and test set.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Always ignored, exists for compatibility.\n\ngroups : array-like of shape (n_samples,)\n    Always ignored, exists for compatibility.\n\nYields\n------\ntrain : ndarray\n    The training set indices for that split.\n\ntest : ndarray\n    The testing set indices for that split."
            }
          ],
          "fullDocstring": "Time Series cross-validator\n\nProvides train/test indices to split time series data samples\nthat are observed at fixed time intervals, in train/test sets.\nIn each split, test indices must be higher than before, and thus shuffling\nin cross validator is inappropriate.\n\nThis cross-validation object is a variation of :class:`KFold`.\nIn the kth split, it returns first k folds as train set and the\n(k+1)th fold as test set.\n\nNote that unlike standard cross-validation methods, successive\ntraining sets are supersets of those that come before them.\n\nRead more in the :ref:`User Guide <time_series_split>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nn_splits : int, default=5\n    Number of splits. Must be at least 2.\n\n    .. versionchanged:: 0.22\n        ``n_splits`` default value changed from 3 to 5.\n\nmax_train_size : int, default=None\n    Maximum size for a single training set.\n\ntest_size : int, default=None\n    Used to limit the size of the test set. Defaults to\n    ``n_samples // (n_splits + 1)``, which is the maximum allowed value\n    with ``gap=0``.\n\n    .. versionadded:: 0.24\n\ngap : int, default=0\n    Number of samples to exclude from the end of each train set before\n    the test set.\n\n    .. versionadded:: 0.24\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import TimeSeriesSplit\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> tscv = TimeSeriesSplit()\n>>> print(tscv)\nTimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n>>> for train_index, test_index in tscv.split(X):\n...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...     X_train, X_test = X[train_index], X[test_index]\n...     y_train, y_test = y[train_index], y[test_index]\nTRAIN: [0] TEST: [1]\nTRAIN: [0 1] TEST: [2]\nTRAIN: [0 1 2] TEST: [3]\nTRAIN: [0 1 2 3] TEST: [4]\nTRAIN: [0 1 2 3 4] TEST: [5]\n>>> # Fix test_size to 2 with 12 samples\n>>> X = np.random.randn(12, 2)\n>>> y = np.random.randint(0, 2, 12)\n>>> tscv = TimeSeriesSplit(n_splits=3, test_size=2)\n>>> for train_index, test_index in tscv.split(X):\n...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...    X_train, X_test = X[train_index], X[test_index]\n...    y_train, y_test = y[train_index], y[test_index]\nTRAIN: [0 1 2 3 4 5] TEST: [6 7]\nTRAIN: [0 1 2 3 4 5 6 7] TEST: [8 9]\nTRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [10 11]\n>>> # Add in a 2 period gap\n>>> tscv = TimeSeriesSplit(n_splits=3, test_size=2, gap=2)\n>>> for train_index, test_index in tscv.split(X):\n...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...    X_train, X_test = X[train_index], X[test_index]\n...    y_train, y_test = y[train_index], y[test_index]\nTRAIN: [0 1 2 3] TEST: [6 7]\nTRAIN: [0 1 2 3 4 5] TEST: [8 9]\nTRAIN: [0 1 2 3 4 5 6 7] TEST: [10 11]\n\nNotes\n-----\nThe training set has size ``i * n_samples // (n_splits + 1)\n+ n_samples % (n_splits + 1)`` in the ``i`` th split,\nwith a test set of size ``n_samples//(n_splits + 1)`` by default,\nwhere ``n_samples`` is the number of samples."
        }
      ],
      "functions": [
        {
          "name": "check_cv",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "cv",
              "type": "Any",
              "hasDefault": true,
              "default": "5",
              "limitation": null,
              "ignored": false,
              "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n- None, to use the default 5-fold cross validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices.\n\nFor integer/None inputs, if classifier is True and ``y`` is either\nbinary or multiclass, :class:`StratifiedKFold` is used. In all other\ncases, :class:`KFold` is used.\n\nRefer :ref:`User Guide <cross_validation>` for the various\ncross-validation strategies that can be used here.\n\n.. versionchanged:: 0.22\n    ``cv`` default value changed from 3-fold to 5-fold."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The target variable for supervised learning problems."
            }
          ],
          "results": [
            {
              "name": "checked_cv",
              "type": null,
              "description": "The return value is a cross-validator which generates the train/test\nsplits via the ``split`` method."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Input checker utility for building a cross-validator\n\nParameters\n----------\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if classifier is True and ``y`` is either\n    binary or multiclass, :class:`StratifiedKFold` is used. In all other\n    cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value changed from 3-fold to 5-fold.\n\ny : array-like, default=None\n    The target variable for supervised learning problems.\n\nclassifier : bool, default=False\n    Whether the task is a classification task, in which case\n    stratified KFold will be used.\n\nReturns\n-------\nchecked_cv : a cross-validator instance.\n    The return value is a cross-validator which generates the train/test\n    splits via the ``split`` method."
        },
        {
          "name": "train_test_split",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "splitting",
              "type": null,
              "description": "List containing train-test split of inputs.\n\n.. versionadded:: 0.16\n    If the input is sparse, the output will be a\n    ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n    input type."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Split arrays or matrices into random train and test subsets\n\nQuick utility that wraps input validation and\n``next(ShuffleSplit().split(X, y))`` and application to input data\ninto a single call for splitting (and optionally subsampling) data in a\noneliner.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nParameters\n----------\n*arrays : sequence of indexables with same length / shape[0]\n    Allowed inputs are lists, numpy arrays, scipy-sparse\n    matrices or pandas dataframes.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.25.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the shuffling applied to the data before applying the split.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data before splitting. If shuffle=False\n    then stratify must be None.\n\nstratify : array-like, default=None\n    If not None, data is split in a stratified fashion, using this as\n    the class labels.\n    Read more in the :ref:`User Guide <stratification>`.\n\nReturns\n-------\nsplitting : list, length=2 * len(arrays)\n    List containing train-test split of inputs.\n\n    .. versionadded:: 0.16\n        If the input is sparse, the output will be a\n        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n        input type.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = np.arange(10).reshape((5, 2)), range(5)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n>>> list(y)\n[0, 1, 2, 3, 4]\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.33, random_state=42)\n...\n>>> X_train\narray([[4, 5],\n       [0, 1],\n       [6, 7]])\n>>> y_train\n[2, 0, 3]\n>>> X_test\narray([[2, 3],\n       [8, 9]])\n>>> y_test\n[1, 4]\n\n>>> train_test_split(y, shuffle=False)\n[[0, 1, 2], [3, 4]]"
        }
      ]
    },
    {
      "name": "sklearn.model_selection._validation",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "time",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "contextlib",
          "declaration": "suppress",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "logger",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "FitFailedWarning",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "check_scoring",
          "alias": null
        },
        {
          "module": "sklearn.metrics._scorer",
          "declaration": "_MultimetricScorer",
          "alias": null
        },
        {
          "module": "sklearn.metrics._scorer",
          "declaration": "_check_multimetric_scoring",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._split",
          "declaration": "check_cv",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "indexable",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "_safe_split",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_fit_params",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "traceback",
          "declaration": "format_exc",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "cross_val_predict",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The object to use to fit the data."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to fit. Can be, for example a list, or an array at least 2d."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The target variable to try to predict in the case of\nsupervised learning."
            }
          ],
          "results": [
            {
              "name": "predictions",
              "type": "NDArray",
              "description": "This is the result of calling `method`. Shape:\n\n    - When `method` is 'predict' and in special case where `method` is\n      'decision_function' and the target is binary: (n_samples,)\n    - When `method` is one of {'predict_proba', 'predict_log_proba',\n      'decision_function'} (unless special case above):\n      (n_samples, n_classes)\n    - If `estimator` is :term:`multioutput`, an extra dimension\n      'n_outputs' is added to the end of each shape above."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate cross-validated estimates for each input data point\n\nThe data is split according to the cv parameter. Each sample belongs\nto exactly one test set, and its prediction is computed with an\nestimator fitted on the corresponding training set.\n\nPassing these predictions into an evaluation metric may not be a valid\nway to measure generalization performance. Results can differ from\n:func:`cross_validate` and :func:`cross_val_score` unless all tests sets\nhave equal size and the metric decomposes over samples.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nParameters\n----------\nestimator : estimator object implementing 'fit' and 'predict'\n    The object to use to fit the data.\n\nX : array-like of shape (n_samples, n_features)\n    The data to fit. Can be, for example a list, or an array at least 2d.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n    The target variable to try to predict in the case of\n    supervised learning.\n\ngroups : array-like of shape (n_samples,), default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`GroupKFold`).\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - int, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel. Training the estimator and\n    predicting are parallelized over the cross-validation splits.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int, default=0\n    The verbosity level.\n\nfit_params : dict, defualt=None\n    Parameters to pass to the fit method of the estimator.\n\npre_dispatch : int or str, default='2*n_jobs'\n    Controls the number of jobs that get dispatched during parallel\n    execution. Reducing this number can be useful to avoid an\n    explosion of memory consumption when more jobs get dispatched\n    than CPUs can process. This parameter can be:\n\n        - None, in which case all the jobs are immediately\n          created and spawned. Use this for lightweight and\n          fast-running jobs, to avoid delays due to on-demand\n          spawning of the jobs\n\n        - An int, giving the exact number of total jobs that are\n          spawned\n\n        - A str, giving an expression as a function of n_jobs,\n          as in '2*n_jobs'\n\nmethod : {'predict', 'predict_proba', 'predict_log_proba',               'decision_function'}, default='predict'\n    The method to be invoked by `estimator`.\n\nReturns\n-------\npredictions : ndarray\n    This is the result of calling `method`. Shape:\n\n        - When `method` is 'predict' and in special case where `method` is\n          'decision_function' and the target is binary: (n_samples,)\n        - When `method` is one of {'predict_proba', 'predict_log_proba',\n          'decision_function'} (unless special case above):\n          (n_samples, n_classes)\n        - If `estimator` is :term:`multioutput`, an extra dimension\n          'n_outputs' is added to the end of each shape above.\n\nSee Also\n--------\ncross_val_score : Calculate score for each CV split.\ncross_validate : Calculate one or more scores and timings for each CV\n    split.\n\nNotes\n-----\nIn the case that one or more classes are absent in a training portion, a\ndefault score needs to be assigned to all instances for that class if\n``method`` produces columns per class, as in {'decision_function',\n'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n0.  In order to ensure finite output, we approximate negative infinity by\nthe minimum finite float value for the dtype in other cases.\n\nExamples\n--------\n>>> from sklearn import datasets, linear_model\n>>> from sklearn.model_selection import cross_val_predict\n>>> diabetes = datasets.load_diabetes()\n>>> X = diabetes.data[:150]\n>>> y = diabetes.target[:150]\n>>> lasso = linear_model.Lasso()\n>>> y_pred = cross_val_predict(lasso, X, y, cv=3)"
        },
        {
          "name": "cross_val_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The object to use to fit the data."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to fit. Can be for example a list, or an array."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The target variable to try to predict in the case of\nsupervised learning."
            }
          ],
          "results": [
            {
              "name": "scores",
              "type": null,
              "description": "Array of scores of the estimator for each run of the cross validation."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Evaluate a score by cross-validation\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nParameters\n----------\nestimator : estimator object implementing 'fit'\n    The object to use to fit the data.\n\nX : array-like of shape (n_samples, n_features)\n    The data to fit. Can be for example a list, or an array.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n    The target variable to try to predict in the case of\n    supervised learning.\n\ngroups : array-like of shape (n_samples,), default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`GroupKFold`).\n\nscoring : str or callable, default=None\n    A str (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)`` which should return only\n    a single value.\n\n    Similar to :func:`cross_validate`\n    but only a single metric is permitted.\n\n    If None, the estimator's default scorer (if available) is used.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - int, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel. Training the estimator and computing\n    the score are parallelized over the cross-validation splits.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int, default=0\n    The verbosity level.\n\nfit_params : dict, default=None\n    Parameters to pass to the fit method of the estimator.\n\npre_dispatch : int or str, default='2*n_jobs'\n    Controls the number of jobs that get dispatched during parallel\n    execution. Reducing this number can be useful to avoid an\n    explosion of memory consumption when more jobs get dispatched\n    than CPUs can process. This parameter can be:\n\n        - None, in which case all the jobs are immediately\n          created and spawned. Use this for lightweight and\n          fast-running jobs, to avoid delays due to on-demand\n          spawning of the jobs\n\n        - An int, giving the exact number of total jobs that are\n          spawned\n\n        - A str, giving an expression as a function of n_jobs,\n          as in '2*n_jobs'\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised.\n    If a numeric value is given, FitFailedWarning is raised.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nscores : ndarray of float of shape=(len(list(cv)),)\n    Array of scores of the estimator for each run of the cross validation.\n\nExamples\n--------\n>>> from sklearn import datasets, linear_model\n>>> from sklearn.model_selection import cross_val_score\n>>> diabetes = datasets.load_diabetes()\n>>> X = diabetes.data[:150]\n>>> y = diabetes.target[:150]\n>>> lasso = linear_model.Lasso()\n>>> print(cross_val_score(lasso, X, y, cv=3))\n[0.33150734 0.08022311 0.03531764]\n\nSee Also\n---------\ncross_validate : To run cross-validation on multiple metrics and also to\n    return train scores, fit times and score times.\n\ncross_val_predict : Get predictions from each split of cross-validation for\n    diagnostic purposes.\n\nsklearn.metrics.make_scorer : Make a scorer from a performance metric or\n    loss function."
        },
        {
          "name": "cross_validate",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The object to use to fit the data."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to fit. Can be for example a list, or an array."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The target variable to try to predict in the case of\nsupervised learning."
            }
          ],
          "results": [
            {
              "name": "scores",
              "type": null,
              "description": "Array of scores of the estimator for each run of the cross validation.\n\nA dict of arrays containing the score/time arrays for each scorer is\nreturned. The possible keys for this ``dict`` are:\n\n    ``test_score``\n        The score array for test scores on each cv split.\n        Suffix ``_score`` in ``test_score`` changes to a specific\n        metric like ``test_r2`` or ``test_auc`` if there are\n        multiple scoring metrics in the scoring parameter.\n    ``train_score``\n        The score array for train scores on each cv split.\n        Suffix ``_score`` in ``train_score`` changes to a specific\n        metric like ``train_r2`` or ``train_auc`` if there are\n        multiple scoring metrics in the scoring parameter.\n        This is available only if ``return_train_score`` parameter\n        is ``True``.\n    ``fit_time``\n        The time for fitting the estimator on the train\n        set for each cv split.\n    ``score_time``\n        The time for scoring the estimator on the test set for each\n        cv split. (Note time for scoring on the train set is not\n        included even if ``return_train_score`` is set to ``True``\n    ``estimator``\n        The estimator objects for each cv split.\n        This is available only if ``return_estimator`` parameter\n        is set to ``True``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Evaluate metric(s) by cross-validation and also record fit/score times.\n\nRead more in the :ref:`User Guide <multimetric_cross_validation>`.\n\nParameters\n----------\nestimator : estimator object implementing 'fit'\n    The object to use to fit the data.\n\nX : array-like of shape (n_samples, n_features)\n    The data to fit. Can be for example a list, or an array.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n    The target variable to try to predict in the case of\n    supervised learning.\n\ngroups : array-like of shape (n_samples,), default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`GroupKFold`).\n\nscoring : str, callable, list/tuple, or dict, default=None\n    A single str (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n    For evaluating multiple metrics, either give a list of (unique) strings\n    or a dict with names as keys and callables as values.\n\n    NOTE that when using custom scorers, each scorer should return a single\n    value. Metric functions returning a list/array of values can be wrapped\n    into multiple scorers that return one value each.\n\n    See :ref:`multimetric_grid_search` for an example.\n\n    If None, the estimator's score method is used.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - int, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel. Training the estimator and computing\n    the score are parallelized over the cross-validation splits.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int, default=0\n    The verbosity level.\n\nfit_params : dict, default=None\n    Parameters to pass to the fit method of the estimator.\n\npre_dispatch : int or str, default='2*n_jobs'\n    Controls the number of jobs that get dispatched during parallel\n    execution. Reducing this number can be useful to avoid an\n    explosion of memory consumption when more jobs get dispatched\n    than CPUs can process. This parameter can be:\n\n        - None, in which case all the jobs are immediately\n          created and spawned. Use this for lightweight and\n          fast-running jobs, to avoid delays due to on-demand\n          spawning of the jobs\n\n        - An int, giving the exact number of total jobs that are\n          spawned\n\n        - A str, giving an expression as a function of n_jobs,\n          as in '2*n_jobs'\n\nreturn_train_score : bool, default=False\n    Whether to include train scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\n    .. versionadded:: 0.19\n\n    .. versionchanged:: 0.21\n        Default value was changed from ``True`` to ``False``\n\nreturn_estimator : bool, default=False\n    Whether to return the estimators fitted on each split.\n\n    .. versionadded:: 0.20\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised.\n    If a numeric value is given, FitFailedWarning is raised.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nscores : dict of float arrays of shape (n_splits,)\n    Array of scores of the estimator for each run of the cross validation.\n\n    A dict of arrays containing the score/time arrays for each scorer is\n    returned. The possible keys for this ``dict`` are:\n\n        ``test_score``\n            The score array for test scores on each cv split.\n            Suffix ``_score`` in ``test_score`` changes to a specific\n            metric like ``test_r2`` or ``test_auc`` if there are\n            multiple scoring metrics in the scoring parameter.\n        ``train_score``\n            The score array for train scores on each cv split.\n            Suffix ``_score`` in ``train_score`` changes to a specific\n            metric like ``train_r2`` or ``train_auc`` if there are\n            multiple scoring metrics in the scoring parameter.\n            This is available only if ``return_train_score`` parameter\n            is ``True``.\n        ``fit_time``\n            The time for fitting the estimator on the train\n            set for each cv split.\n        ``score_time``\n            The time for scoring the estimator on the test set for each\n            cv split. (Note time for scoring on the train set is not\n            included even if ``return_train_score`` is set to ``True``\n        ``estimator``\n            The estimator objects for each cv split.\n            This is available only if ``return_estimator`` parameter\n            is set to ``True``.\n\nExamples\n--------\n>>> from sklearn import datasets, linear_model\n>>> from sklearn.model_selection import cross_validate\n>>> from sklearn.metrics import make_scorer\n>>> from sklearn.metrics import confusion_matrix\n>>> from sklearn.svm import LinearSVC\n>>> diabetes = datasets.load_diabetes()\n>>> X = diabetes.data[:150]\n>>> y = diabetes.target[:150]\n>>> lasso = linear_model.Lasso()\n\nSingle metric evaluation using ``cross_validate``\n\n>>> cv_results = cross_validate(lasso, X, y, cv=3)\n>>> sorted(cv_results.keys())\n['fit_time', 'score_time', 'test_score']\n>>> cv_results['test_score']\narray([0.33150734, 0.08022311, 0.03531764])\n\nMultiple metric evaluation using ``cross_validate``\n(please refer the ``scoring`` parameter doc for more information)\n\n>>> scores = cross_validate(lasso, X, y, cv=3,\n...                         scoring=('r2', 'neg_mean_squared_error'),\n...                         return_train_score=True)\n>>> print(scores['test_neg_mean_squared_error'])\n[-3635.5... -3573.3... -6114.7...]\n>>> print(scores['train_r2'])\n[0.28010158 0.39088426 0.22784852]\n\nSee Also\n---------\ncross_val_score : Run cross-validation for single metric evaluation.\n\ncross_val_predict : Get predictions from each split of cross-validation for\n    diagnostic purposes.\n\nsklearn.metrics.make_scorer : Make a scorer from a performance metric or\n    loss function."
        },
        {
          "name": "learning_curve",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "An object of that type which is cloned for each validation."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target relative to X for classification or regression;\nNone for unsupervised learning."
            }
          ],
          "results": [
            {
              "name": "train_sizes_abs",
              "type": null,
              "description": "Numbers of training examples that has been used to generate the\nlearning curve. Note that the number of ticks might be less\nthan n_ticks because duplicate entries will be removed."
            },
            {
              "name": "train_scores",
              "type": null,
              "description": "Scores on training sets."
            },
            {
              "name": "test_scores",
              "type": null,
              "description": "Scores on test set."
            },
            {
              "name": "fit_times",
              "type": null,
              "description": "Times spent for fitting in seconds. Only present if ``return_times``\nis True."
            },
            {
              "name": "score_times",
              "type": null,
              "description": "Times spent for scoring in seconds. Only present if ``return_times``\nis True."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Learning curve.\n\nDetermines cross-validated training and test scores for different training\nset sizes.\n\nA cross-validation generator splits the whole dataset k times in training\nand test data. Subsets of the training set with varying sizes will be used\nto train the estimator and a score for each training subset size and the\ntest set will be computed. Afterwards, the scores will be averaged over\nall k runs for each training subset size.\n\nRead more in the :ref:`User Guide <learning_curve>`.\n\nParameters\n----------\nestimator : object type that implements the \"fit\" and \"predict\" methods\n    An object of that type which is cloned for each validation.\n\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Target relative to X for classification or regression;\n    None for unsupervised learning.\n\ngroups : array-like of  shape (n_samples,), default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`GroupKFold`).\n\ntrain_sizes : array-like of shape (n_ticks,),             default=np.linspace(0.1, 1.0, 5)\n    Relative or absolute numbers of training examples that will be used to\n    generate the learning curve. If the dtype is float, it is regarded as a\n    fraction of the maximum size of the training set (that is determined\n    by the selected validation method), i.e. it has to be within (0, 1].\n    Otherwise it is interpreted as absolute sizes of the training sets.\n    Note that for classification the number of samples usually have to\n    be big enough to contain at least one sample from each class.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - int, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nscoring : str or callable, default=None\n    A str (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\nexploit_incremental_learning : bool, default=False\n    If the estimator supports incremental learning, this will be\n    used to speed up fitting for different training set sizes.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel. Training the estimator and computing\n    the score are parallelized over the different training and test sets.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npre_dispatch : int or str, default='all'\n    Number of predispatched jobs for parallel execution (default is\n    all). The option can reduce the allocated memory. The str can\n    be an expression like '2*n_jobs'.\n\nverbose : int, default=0\n    Controls the verbosity: the higher, the more messages.\n\nshuffle : bool, default=False\n    Whether to shuffle training data before taking prefixes of it\n    based on``train_sizes``.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when ``shuffle`` is True. Pass an int for reproducible\n    output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised.\n    If a numeric value is given, FitFailedWarning is raised.\n\n    .. versionadded:: 0.20\n\nreturn_times : bool, default=False\n    Whether to return the fit and score times.\n\nfit_params : dict, default=None\n    Parameters to pass to the fit method of the estimator.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ntrain_sizes_abs : array of shape (n_unique_ticks,)\n    Numbers of training examples that has been used to generate the\n    learning curve. Note that the number of ticks might be less\n    than n_ticks because duplicate entries will be removed.\n\ntrain_scores : array of shape (n_ticks, n_cv_folds)\n    Scores on training sets.\n\ntest_scores : array of shape (n_ticks, n_cv_folds)\n    Scores on test set.\n\nfit_times : array of shape (n_ticks, n_cv_folds)\n    Times spent for fitting in seconds. Only present if ``return_times``\n    is True.\n\nscore_times : array of shape (n_ticks, n_cv_folds)\n    Times spent for scoring in seconds. Only present if ``return_times``\n    is True.\n\nNotes\n-----\nSee :ref:`examples/model_selection/plot_learning_curve.py\n<sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`"
        },
        {
          "name": "permutation_test_score",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The object to use to fit the data."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to fit."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The target variable to try to predict in the case of\nsupervised learning."
            }
          ],
          "results": [
            {
              "name": "score",
              "type": "float",
              "description": "The true score without permuting targets."
            },
            {
              "name": "permutation_scores",
              "type": null,
              "description": "The scores obtained for each permutations."
            },
            {
              "name": "pvalue",
              "type": "float",
              "description": "The p-value, which approximates the probability that the score would\nbe obtained by chance. This is calculated as:\n\n`(C + 1) / (n_permutations + 1)`\n\nWhere C is the number of permutations whose score >= the true score.\n\nThe best possible p-value is 1/(n_permutations + 1), the worst is 1.0."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Evaluate the significance of a cross-validated score with permutations\n\nPermutes targets to generate 'randomized data' and compute the empirical\np-value against the null hypothesis that features and targets are\nindependent.\n\nThe p-value represents the fraction of randomized data sets where the\nestimator performed as well or better than in the original data. A small\np-value suggests that there is a real dependency between features and\ntargets which has been used by the estimator to give good predictions.\nA large p-value may be due to lack of real dependency between features\nand targets or the estimator was not able to use the dependency to\ngive good predictions.\n\nRead more in the :ref:`User Guide <permutation_test_score>`.\n\nParameters\n----------\nestimator : estimator object implementing 'fit'\n    The object to use to fit the data.\n\nX : array-like of shape at least 2D\n    The data to fit.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n    The target variable to try to predict in the case of\n    supervised learning.\n\ngroups : array-like of shape (n_samples,), default=None\n    Labels to constrain permutation within groups, i.e. ``y`` values\n    are permuted among samples with the same group identifier.\n    When not specified, ``y`` values are permuted among all samples.\n\n    When a grouped cross-validator is used, the group labels are\n    also passed on to the ``split`` method of the cross-validator. The\n    cross-validator uses them for grouping the samples  while splitting\n    the dataset into train/test set.\n\nscoring : str or callable, default=None\n    A single str (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n    If None the estimator's score method is used.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - int, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nn_permutations : int, default=100\n    Number of times to permute ``y``.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel. Training the estimator and computing\n    the cross-validated score are parallelized over the permutations.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance or None, default=0\n    Pass an int for reproducible output for permutation of\n    ``y`` values among samples. See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    The verbosity level.\n\nfit_params : dict, default=None\n    Parameters to pass to the fit method of the estimator.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nscore : float\n    The true score without permuting targets.\n\npermutation_scores : array of shape (n_permutations,)\n    The scores obtained for each permutations.\n\npvalue : float\n    The p-value, which approximates the probability that the score would\n    be obtained by chance. This is calculated as:\n\n    `(C + 1) / (n_permutations + 1)`\n\n    Where C is the number of permutations whose score >= the true score.\n\n    The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.\n\nNotes\n-----\nThis function implements Test 1 in:\n\n    Ojala and Garriga. `Permutation Tests for Studying Classifier\n    Performance\n    <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The\n    Journal of Machine Learning Research (2010) vol. 11"
        },
        {
          "name": "validation_curve",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "An object of that type which is cloned for each validation."
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target relative to X for classification or regression;\nNone for unsupervised learning."
            }
          ],
          "results": [
            {
              "name": "train_scores",
              "type": null,
              "description": "Scores on training sets."
            },
            {
              "name": "test_scores",
              "type": null,
              "description": "Scores on test set."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Validation curve.\n\nDetermine training and test scores for varying parameter values.\n\nCompute scores for an estimator with different values of a specified\nparameter. This is similar to grid search with one parameter. However, this\nwill also compute training scores and is merely a utility for plotting the\nresults.\n\nRead more in the :ref:`User Guide <validation_curve>`.\n\nParameters\n----------\nestimator : object type that implements the \"fit\" and \"predict\" methods\n    An object of that type which is cloned for each validation.\n\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n    Target relative to X for classification or regression;\n    None for unsupervised learning.\n\nparam_name : str\n    Name of the parameter that will be varied.\n\nparam_range : array-like of shape (n_values,)\n    The values of the parameter that will be evaluated.\n\ngroups : array-like of shape (n_samples,), default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`GroupKFold`).\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - int, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nscoring : str or callable, default=None\n    A str (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel. Training the estimator and computing\n    the score are parallelized over the combinations of each parameter\n    value and each cross-validation split.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npre_dispatch : int or str, default='all'\n    Number of predispatched jobs for parallel execution (default is\n    all). The option can reduce the allocated memory. The str can\n    be an expression like '2*n_jobs'.\n\nverbose : int, default=0\n    Controls the verbosity: the higher, the more messages.\n\nfit_params : dict, default=None\n    Parameters to pass to the fit method of the estimator.\n\n    .. versionadded:: 0.24\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised.\n    If a numeric value is given, FitFailedWarning is raised.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ntrain_scores : array of shape (n_ticks, n_cv_folds)\n    Scores on training sets.\n\ntest_scores : array of shape (n_ticks, n_cv_folds)\n    Scores on test set.\n\nNotes\n-----\nSee :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`"
        }
      ]
    },
    {
      "name": "sklearn.multiclass",
      "imports": [
        {
          "module": "array",
          "alias": null
        },
        {
          "module": "itertools",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "_is_pairwise",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_regressor",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "euclidean_distances",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_safe_tags",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "_safe_split",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "if_delegate_has_method",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "_check_partial_fit_first_call",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "_ovr_decision_function",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "OneVsOneClassifier",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "ClassifierMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Multi-class targets."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit underlying estimators.\n\nParameters\n----------\nX : (sparse) array-like of shape (n_samples, n_features)\n    Data.\n\ny : array-like of shape (n_samples,)\n    Multi-class targets.\n\nReturns\n-------\nself"
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Multi-class targets."
                },
                {
                  "name": "classes",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Classes across all calls to partial_fit.\nCan be obtained via `np.unique(y_all)`, where y_all is the\ntarget vector of the entire dataset.\nThis argument is only required in the first call of partial_fit\nand can be omitted in the subsequent calls."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Partially fit underlying estimators\n\nShould be used when memory is inefficient to train all data. Chunks\nof data can be passed in several iteration, where the first call\nshould have an array of all target variables.\n\n\nParameters\n----------\nX : (sparse) array-like of shape (n_samples, n_features)\n    Data.\n\ny : array-like of shape (n_samples,)\n    Multi-class targets.\n\nclasses : array, shape (n_classes, )\n    Classes across all calls to partial_fit.\n    Can be obtained via `np.unique(y_all)`, where y_all is the\n    target vector of the entire dataset.\n    This argument is only required in the first call of partial_fit\n    and can be omitted in the subsequent calls.\n\nReturns\n-------\nself"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Predicted multi-class targets."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Estimate the best class label for each sample in X.\n\nThis is implemented as ``argmax(decision_function(X), axis=1)`` which\nwill return the label of the class with most votes by estimators\npredicting the outcome of a decision for each possible class pair.\n\nParameters\n----------\nX : (sparse) array-like of shape (n_samples, n_features)\n    Data.\n\nReturns\n-------\ny : numpy array of shape [n_samples]\n    Predicted multi-class targets."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Y",
                  "type": null,
                  "description": ".. versionchanged:: 0.19\n    output shape changed to ``(n_samples,)`` to conform to\n    scikit-learn conventions for binary classification."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Decision function for the OneVsOneClassifier.\n\nThe decision values for the samples are computed by adding the\nnormalized sum of pair-wise classification confidence levels to the\nvotes in order to disambiguate between the decision values when the\nvotes for all the classes are equal leading to a tie.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nReturns\n-------\nY : array-like of shape (n_samples, n_classes) or (n_samples,) for             binary classification.\n\n    .. versionchanged:: 0.19\n        output shape changed to ``(n_samples,)`` to conform to\n        scikit-learn conventions for binary classification."
            },
            {
              "name": "n_classes_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "One-vs-one multiclass strategy\n\nThis strategy consists in fitting one classifier per class pair.\nAt prediction time, the class which received the most votes is selected.\nSince it requires to fit `n_classes * (n_classes - 1) / 2` classifiers,\nthis method is usually slower than one-vs-the-rest, due to its\nO(n_classes^2) complexity. However, this method may be advantageous for\nalgorithms such as kernel algorithms which don't scale well with\n`n_samples`. This is because each individual learning problem only involves\na small subset of the data whereas, with one-vs-the-rest, the complete\ndataset is used `n_classes` times.\n\nRead more in the :ref:`User Guide <ovo_classification>`.\n\nParameters\n----------\nestimator : estimator object\n    An estimator object implementing :term:`fit` and one of\n    :term:`decision_function` or :term:`predict_proba`.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation: the `n_classes * (\n    n_classes - 1) / 2` OVO problems are computed in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nestimators_ : list of ``n_classes * (n_classes - 1) / 2`` estimators\n    Estimators used for predictions.\n\nclasses_ : numpy array of shape [n_classes]\n    Array containing labels.\n\nn_classes_ : int\n    Number of classes\n\npairwise_indices_ : list, length = ``len(estimators_)``, or ``None``\n    Indices of samples used when training the estimators.\n    ``None`` when ``estimator``'s `pairwise` tag is False.\n\n    .. deprecated:: 0.24\n\n        The _pairwise attribute is deprecated in 0.24. From 1.1\n        (renaming of 0.25) and onward, `pairwise_indices_` will use the\n        pairwise estimator tag instead.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.multiclass import OneVsOneClassifier\n>>> from sklearn.svm import LinearSVC\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.33, shuffle=True, random_state=0)\n>>> clf = OneVsOneClassifier(\n...     LinearSVC(random_state=0)).fit(X_train, y_train)\n>>> clf.predict(X_test[:10])\narray([2, 1, 0, 2, 0, 2, 0, 1, 1, 1])"
        },
        {
          "name": "OneVsRestClassifier",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "ClassifierMixin",
            "BaseEstimator",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Multi-class targets. An indicator matrix turns on multilabel\nclassification."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit underlying estimators.\n\nParameters\n----------\nX : (sparse) array-like of shape (n_samples, n_features)\n    Data.\n\ny : (sparse) array-like of shape (n_samples,) or (n_samples, n_classes)\n    Multi-class targets. An indicator matrix turns on multilabel\n    classification.\n\nReturns\n-------\nself"
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Multi-class targets. An indicator matrix turns on multilabel\nclassification."
                },
                {
                  "name": "classes",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Classes across all calls to partial_fit.\nCan be obtained via `np.unique(y_all)`, where y_all is the\ntarget vector of the entire dataset.\nThis argument is only required in the first call of partial_fit\nand can be omitted in the subsequent calls."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Partially fit underlying estimators\n\nShould be used when memory is inefficient to train all data.\nChunks of data can be passed in several iteration.\n\nParameters\n----------\nX : (sparse) array-like of shape (n_samples, n_features)\n    Data.\n\ny : (sparse) array-like of shape (n_samples,) or (n_samples, n_classes)\n    Multi-class targets. An indicator matrix turns on multilabel\n    classification.\n\nclasses : array, shape (n_classes, )\n    Classes across all calls to partial_fit.\n    Can be obtained via `np.unique(y_all)`, where y_all is the\n    target vector of the entire dataset.\n    This argument is only required in the first call of partial_fit\n    and can be omitted in the subsequent calls.\n\nReturns\n-------\nself"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Predicted multi-class targets."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict multi-class targets using underlying estimators.\n\nParameters\n----------\nX : (sparse) array-like of shape (n_samples, n_features)\n    Data.\n\nReturns\n-------\ny : (sparse) array-like of shape (n_samples,) or (n_samples, n_classes)\n    Predicted multi-class targets."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "T",
                  "type": null,
                  "description": "Returns the probability of the sample for each class in the model,\nwhere classes are ordered as they are in `self.classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Probability estimates.\n\nThe returned estimates for all classes are ordered by label of classes.\n\nNote that in the multilabel case, each sample can have any number of\nlabels. This returns the marginal probability that the given sample has\nthe label in question. For example, it is entirely consistent that two\nlabels both have a 90% probability of applying to a given sample.\n\nIn the single label multiclass case, the rows of the returned matrix\nsum to 1.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nReturns\n-------\nT : (sparse) array-like of shape (n_samples, n_classes)\n    Returns the probability of the sample for each class in the model,\n    where classes are ordered as they are in `self.classes_`."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "T",
                  "type": null,
                  "description": ".. versionchanged:: 0.19\n    output shape changed to ``(n_samples,)`` to conform to\n    scikit-learn conventions for binary classification."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the distance of each sample from the decision boundary for\neach class. This can only be used with estimators which implement the\ndecision_function method.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nReturns\n-------\nT : array-like of shape (n_samples, n_classes) or (n_samples,) for             binary classification.\n\n    .. versionchanged:: 0.19\n        output shape changed to ``(n_samples,)`` to conform to\n        scikit-learn conventions for binary classification."
            },
            {
              "name": "multilabel_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Whether this is a multilabel classifier"
            },
            {
              "name": "n_classes_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "coef_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "intercept_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "n_features_in_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "One-vs-the-rest (OvR) multiclass strategy.\n\nAlso known as one-vs-all, this strategy consists in fitting one classifier\nper class. For each classifier, the class is fitted against all the other\nclasses. In addition to its computational efficiency (only `n_classes`\nclassifiers are needed), one advantage of this approach is its\ninterpretability. Since each class is represented by one and one classifier\nonly, it is possible to gain knowledge about the class by inspecting its\ncorresponding classifier. This is the most commonly used strategy for\nmulticlass classification and is a fair default choice.\n\nOneVsRestClassifier can also be used for multilabel classification. To use\nthis feature, provide an indicator matrix for the target `y` when calling\n`.fit`. In other words, the target labels should be formatted as a 2D\nbinary (0/1) matrix, where [i, j] == 1 indicates the presence of label j\nin sample i. This estimator uses the binary relevance method to perform\nmultilabel classification, which involves training one binary classifier\nindependently for each label.\n\nRead more in the :ref:`User Guide <ovr_classification>`.\n\nParameters\n----------\nestimator : estimator object\n    An estimator object implementing :term:`fit` and one of\n    :term:`decision_function` or :term:`predict_proba`.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation: the `n_classes`\n    one-vs-rest problems are computed in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\nAttributes\n----------\nestimators_ : list of `n_classes` estimators\n    Estimators used for predictions.\n\ncoef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n    Coefficient of the features in the decision function. This attribute\n    exists only if the ``estimators_`` defines ``coef_``.\n\n    .. deprecated:: 0.24\n        This attribute is deprecated in 0.24 and will\n        be removed in 1.1 (renaming of 0.26). If you use this attribute\n        in :class:`~sklearn.feature_selection.RFE` or\n        :class:`~sklearn.feature_selection.SelectFromModel`,\n        you may pass a callable to the `importance_getter`\n        parameter that extracts feature the importances\n        from `estimators_`.\n\nintercept_ : ndarray of shape (1, 1) or (n_classes, 1)\n    If ``y`` is binary, the shape is ``(1, 1)`` else ``(n_classes, 1)``\n    This attribute exists only if the ``estimators_`` defines\n    ``intercept_``.\n\n    .. deprecated:: 0.24\n        This attribute is deprecated in 0.24 and will\n        be removed in 1.1 (renaming of 0.26). If you use this attribute\n        in :class:`~sklearn.feature_selection.RFE` or\n        :class:`~sklearn.feature_selection.SelectFromModel`,\n        you may pass a callable to the `importance_getter`\n        parameter that extracts feature the importances\n        from `estimators_`.\n\nclasses_ : array, shape = [`n_classes`]\n    Class labels.\n\nn_classes_ : int\n    Number of classes.\n\nlabel_binarizer_ : LabelBinarizer object\n    Object used to transform multiclass labels to binary labels and\n    vice-versa.\n\nmultilabel_ : boolean\n    Whether a OneVsRestClassifier is a multilabel classifier.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.multiclass import OneVsRestClassifier\n>>> from sklearn.svm import SVC\n>>> X = np.array([\n...     [10, 10],\n...     [8, 10],\n...     [-5, 5.5],\n...     [-5.4, 5.5],\n...     [-20, -20],\n...     [-15, -20]\n... ])\n>>> y = np.array([0, 0, 1, 1, 2, 2])\n>>> clf = OneVsRestClassifier(SVC()).fit(X, y)\n>>> clf.predict([[-19, -20], [9, 9], [-5, 5]])\narray([2, 0, 1])\n\nSee Also\n--------\nsklearn.multioutput.MultiOutputClassifier : Alternate way of extending an\n    estimator for multilabel classification.\nsklearn.preprocessing.MultiLabelBinarizer : Transform iterable of iterables\n    to binary indicator matrix."
        },
        {
          "name": "OutputCodeClassifier",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "ClassifierMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Multi-class targets."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit underlying estimators.\n\nParameters\n----------\nX : (sparse) array-like of shape (n_samples, n_features)\n    Data.\n\ny : numpy array of shape [n_samples]\n    Multi-class targets.\n\nReturns\n-------\nself"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Predicted multi-class targets."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict multi-class targets using underlying estimators.\n\nParameters\n----------\nX : (sparse) array-like of shape (n_samples, n_features)\n    Data.\n\nReturns\n-------\ny : numpy array of shape [n_samples]\n    Predicted multi-class targets."
            }
          ],
          "fullDocstring": "(Error-Correcting) Output-Code multiclass strategy\n\nOutput-code based strategies consist in representing each class with a\nbinary code (an array of 0s and 1s). At fitting time, one binary\nclassifier per bit in the code book is fitted.  At prediction time, the\nclassifiers are used to project new points in the class space and the class\nclosest to the points is chosen. The main advantage of these strategies is\nthat the number of classifiers used can be controlled by the user, either\nfor compressing the model (0 < code_size < 1) or for making the model more\nrobust to errors (code_size > 1). See the documentation for more details.\n\nRead more in the :ref:`User Guide <ecoc>`.\n\nParameters\n----------\nestimator : estimator object\n    An estimator object implementing :term:`fit` and one of\n    :term:`decision_function` or :term:`predict_proba`.\n\ncode_size : float\n    Percentage of the number of classes to be used to create the code book.\n    A number between 0 and 1 will require fewer classifiers than\n    one-vs-the-rest. A number greater than 1 will require more classifiers\n    than one-vs-the-rest.\n\nrandom_state : int, RandomState instance, default=None\n    The generator used to initialize the codebook.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation: the multiclass problems\n    are computed in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nestimators_ : list of `int(n_classes * code_size)` estimators\n    Estimators used for predictions.\n\nclasses_ : numpy array of shape [n_classes]\n    Array containing labels.\n\ncode_book_ : numpy array of shape [n_classes, code_size]\n    Binary array containing the code of each class.\n\nExamples\n--------\n>>> from sklearn.multiclass import OutputCodeClassifier\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=100, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = OutputCodeClassifier(\n...     estimator=RandomForestClassifier(random_state=0),\n...     random_state=0).fit(X, y)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])\n\nReferences\n----------\n\n.. [1] \"Solving multiclass learning problems via error-correcting output\n   codes\",\n   Dietterich T., Bakiri G.,\n   Journal of Artificial Intelligence Research 2,\n   1995.\n\n.. [2] \"The error coding method and PICTs\",\n   James G., Hastie T.,\n   Journal of Computational and Graphical statistics 7,\n   1998.\n\n.. [3] \"The Elements of Statistical Learning\",\n   Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)\n   2008."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.multioutput",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "cross_val_predict",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "if_delegate_has_method",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_fit_params",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "has_fit_parameter",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "ClassifierChain",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "ClassifierMixin",
            "_BaseChain"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                },
                {
                  "name": "Y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model to data matrix X and targets Y.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input data.\nY : array-like of shape (n_samples, n_classes)\n    The target values.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Y_prob",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict probability estimates.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n\nReturns\n-------\nY_prob : array-like of shape (n_samples, n_classes)"
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Y_decision",
                  "type": null,
                  "description": "Returns the decision function of the sample for each model\nin the chain."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Evaluate the decision_function of the models in the chain.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nReturns\n-------\nY_decision : array-like of shape (n_samples, n_classes)\n    Returns the decision function of the sample for each model\n    in the chain."
            }
          ],
          "fullDocstring": "A multi-label model that arranges binary classifiers into a chain.\n\nEach model makes a prediction in the order specified by the chain using\nall of the available features provided to the model plus the predictions\nof models that are earlier in the chain.\n\nRead more in the :ref:`User Guide <classifierchain>`.\n\n.. versionadded:: 0.19\n\nParameters\n----------\nbase_estimator : estimator\n    The base estimator from which the classifier chain is built.\n\norder : array-like of shape (n_outputs,) or 'random', default=None\n    If None, the order will be determined by the order of columns in\n    the label matrix Y.::\n\n        order = [0, 1, 2, ..., Y.shape[1] - 1]\n\n    The order of the chain can be explicitly set by providing a list of\n    integers. For example, for a chain of length 5.::\n\n        order = [1, 3, 2, 4, 0]\n\n    means that the first model in the chain will make predictions for\n    column 1 in the Y matrix, the second model will make predictions\n    for column 3, etc.\n\n    If order is 'random' a random ordering will be used.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines whether to use cross validated predictions or true\n    labels for the results of previous estimators in the chain.\n    Possible inputs for cv are:\n\n    - None, to use true labels when fitting,\n    - integer, to specify the number of folds in a (Stratified)KFold,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\nrandom_state : int, RandomState instance or None, optional (default=None)\n    If ``order='random'``, determines random number generation for the\n    chain order.\n    In addition, it controls the random seed given at each `base_estimator`\n    at each chaining iteration. Thus, it is only used when `base_estimator`\n    exposes a `random_state`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nclasses_ : list\n    A list of arrays of length ``len(estimators_)`` containing the\n    class labels for each estimator in the chain.\n\nestimators_ : list\n    A list of clones of base_estimator.\n\norder_ : list\n    The order of labels in the classifier chain.\n\nExamples\n--------\n>>> from sklearn.datasets import make_multilabel_classification\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.multioutput import ClassifierChain\n>>> X, Y = make_multilabel_classification(\n...    n_samples=12, n_classes=3, random_state=0\n... )\n>>> X_train, X_test, Y_train, Y_test = train_test_split(\n...    X, Y, random_state=0\n... )\n>>> base_lr = LogisticRegression(solver='lbfgs', random_state=0)\n>>> chain = ClassifierChain(base_lr, order='random', random_state=0)\n>>> chain.fit(X_train, Y_train).predict(X_test)\narray([[1., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.]])\n>>> chain.predict_proba(X_test)\narray([[0.8387..., 0.9431..., 0.4576...],\n       [0.8878..., 0.3684..., 0.2640...],\n       [0.0321..., 0.9935..., 0.0625...]])\n\nSee Also\n--------\nRegressorChain : Equivalent for regression.\nMultioutputClassifier : Classifies each output independently rather than\n    chaining.\n\nReferences\n----------\nJesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, \"Classifier\nChains for Multi-label Classification\", 2009."
        },
        {
          "name": "MultiOutputClassifier",
          "decorators": [],
          "superclasses": [
            "_MultiOutputEstimator",
            "ClassifierMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                },
                {
                  "name": "Y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted.\nOnly supported if the underlying classifier supports sample\nweights."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model to data matrix X and targets Y.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input data.\nY : array-like of shape (n_samples, n_classes)\n    The target values.\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Only supported if the underlying classifier supports sample\n    weights.\n**fit_params : dict of string -> object\n    Parameters passed to the ``estimator.fit`` method of each step.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\nself : object"
            },
            {
              "name": "predict_proba",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "The class probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`.\n\n.. versionchanged:: 0.19\n    This function now returns a list of arrays where the length of\n    the list is ``n_outputs``, and each array is (``n_samples``,\n    ``n_classes``) for that particular output."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Probability estimates.\nReturns prediction probabilities for each class of each output.\n\nThis method will raise a ``ValueError`` if any of the\nestimators do not have ``predict_proba``.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data\n\nReturns\n-------\np : array of shape (n_samples, n_classes), or a list of n_outputs             such arrays if n_outputs > 1.\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.\n\n    .. versionchanged:: 0.19\n        This function now returns a list of arrays where the length of\n        the list is ``n_outputs``, and each array is (``n_samples``,\n        ``n_classes``) for that particular output."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples"
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "True values for X"
                }
              ],
              "results": [
                {
                  "name": "scores",
                  "type": "float",
                  "description": "accuracy_score of self.predict(X) versus y"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Returns the mean accuracy on the given test data and labels.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples\n\ny : array-like of shape (n_samples, n_outputs)\n    True values for X\n\nReturns\n-------\nscores : float\n    accuracy_score of self.predict(X) versus y"
            }
          ],
          "fullDocstring": "Multi target classification\n\nThis strategy consists of fitting one classifier per target. This is a\nsimple strategy for extending classifiers that do not natively support\nmulti-target classification\n\nParameters\n----------\nestimator : estimator object\n    An estimator object implementing :term:`fit`, :term:`score` and\n    :term:`predict_proba`.\n\nn_jobs : int or None, optional (default=None)\n    The number of jobs to run in parallel.\n    :meth:`fit`, :meth:`predict` and :meth:`partial_fit` (if supported\n    by the passed estimator) will be parallelized for each target.\n\n    When individual estimators are fast to train or predict,\n    using ``n_jobs > 1`` can result in slower performance due\n    to the parallelism overhead.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all available processes / threads.\n    See :term:`Glossary <n_jobs>` for more details.\n\n    .. versionchanged:: 0.20\n       `n_jobs` default changed from 1 to None\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    Class labels.\n\nestimators_ : list of ``n_output`` estimators\n    Estimators used for predictions.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_multilabel_classification\n>>> from sklearn.multioutput import MultiOutputClassifier\n>>> from sklearn.neighbors import KNeighborsClassifier\n\n>>> X, y = make_multilabel_classification(n_classes=3, random_state=0)\n>>> clf = MultiOutputClassifier(KNeighborsClassifier()).fit(X, y)\n>>> clf.predict(X[-2:])\narray([[1, 1, 0], [1, 1, 1]])"
        },
        {
          "name": "MultiOutputRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "_MultiOutputEstimator"
          ],
          "methods": [
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Multi-output targets."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted.\nOnly supported if the underlying regressor supports sample\nweights."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Incrementally fit the model to data.\nFit a separate model for each output variable.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Data.\n\ny : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n    Multi-output targets.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Only supported if the underlying regressor supports sample\n    weights.\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "Multi target regression\n\nThis strategy consists of fitting one regressor per target. This is a\nsimple strategy for extending regressors that do not natively support\nmulti-target regression.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nestimator : estimator object\n    An estimator object implementing :term:`fit` and :term:`predict`.\n\nn_jobs : int or None, optional (default=None)\n    The number of jobs to run in parallel.\n    :meth:`fit`, :meth:`predict` and :meth:`partial_fit` (if supported\n    by the passed estimator) will be parallelized for each target.\n\n    When individual estimators are fast to train or predict,\n    using ``n_jobs > 1`` can result in slower performance due\n    to the parallelism overhead.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all available processes / threads.\n    See :term:`Glossary <n_jobs>` for more details.\n\n    .. versionchanged:: 0.20\n       `n_jobs` default changed from 1 to None\n\nAttributes\n----------\nestimators_ : list of ``n_output`` estimators\n    Estimators used for predictions.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import load_linnerud\n>>> from sklearn.multioutput import MultiOutputRegressor\n>>> from sklearn.linear_model import Ridge\n>>> X, y = load_linnerud(return_X_y=True)\n>>> clf = MultiOutputRegressor(Ridge(random_state=123)).fit(X, y)\n>>> clf.predict(X[[0]])\narray([[176..., 35..., 57...]])"
        },
        {
          "name": "RegressorChain",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "MetaEstimatorMixin",
            "_BaseChain"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                },
                {
                  "name": "Y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model to data matrix X and targets Y.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input data.\nY : array-like of shape (n_samples, n_classes)\n    The target values.\n\n**fit_params : dict of string -> object\n    Parameters passed to the `fit` method at each step\n    of the regressor chain.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "A multi-label model that arranges regressions into a chain.\n\nEach model makes a prediction in the order specified by the chain using\nall of the available features provided to the model plus the predictions\nof models that are earlier in the chain.\n\nRead more in the :ref:`User Guide <regressorchain>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nbase_estimator : estimator\n    The base estimator from which the classifier chain is built.\n\norder : array-like of shape (n_outputs,) or 'random', default=None\n    If None, the order will be determined by the order of columns in\n    the label matrix Y.::\n\n        order = [0, 1, 2, ..., Y.shape[1] - 1]\n\n    The order of the chain can be explicitly set by providing a list of\n    integers. For example, for a chain of length 5.::\n\n        order = [1, 3, 2, 4, 0]\n\n    means that the first model in the chain will make predictions for\n    column 1 in the Y matrix, the second model will make predictions\n    for column 3, etc.\n\n    If order is 'random' a random ordering will be used.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines whether to use cross validated predictions or true\n    labels for the results of previous estimators in the chain.\n    Possible inputs for cv are:\n\n    - None, to use true labels when fitting,\n    - integer, to specify the number of folds in a (Stratified)KFold,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\nrandom_state : int, RandomState instance or None, optional (default=None)\n    If ``order='random'``, determines random number generation for the\n    chain order.\n    In addition, it controls the random seed given at each `base_estimator`\n    at each chaining iteration. Thus, it is only used when `base_estimator`\n    exposes a `random_state`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nestimators_ : list\n    A list of clones of base_estimator.\n\norder_ : list\n    The order of labels in the classifier chain.\n\nExamples\n--------\n>>> from sklearn.multioutput import RegressorChain\n>>> from sklearn.linear_model import LogisticRegression\n>>> logreg = LogisticRegression(solver='lbfgs',multi_class='multinomial')\n>>> X, Y = [[1, 0], [0, 1], [1, 1]], [[0, 2], [1, 1], [2, 0]]\n>>> chain = RegressorChain(base_estimator=logreg, order=[0, 1]).fit(X, Y)\n>>> chain.predict(X)\narray([[0., 2.],\n       [1., 1.],\n       [2., 0.]])\n\nSee Also\n--------\nClassifierChain : Equivalent for classification.\nMultioutputRegressor : Learns each output independently rather than\n    chaining."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.naive_bayes",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "logsumexp",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "binarize",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "label_binarize",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "_check_partial_fit_first_call",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_non_negative",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "column_or_1d",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BernoulliNB",
          "decorators": [],
          "superclasses": [
            "_BaseDiscreteNB"
          ],
          "methods": [],
          "fullDocstring": "Naive Bayes classifier for multivariate Bernoulli models.\n\nLike MultinomialNB, this classifier is suitable for discrete data. The\ndifference is that while MultinomialNB works with occurrence counts,\nBernoulliNB is designed for binary/boolean features.\n\nRead more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Additive (Laplace/Lidstone) smoothing parameter\n    (0 for no smoothing).\n\nbinarize : float or None, default=0.0\n    Threshold for binarizing (mapping to booleans) of sample features.\n    If None, input is presumed to already consist of binary vectors.\n\nfit_prior : bool, default=True\n    Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used.\n\nclass_prior : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. If specified the priors are not\n    adjusted according to the data.\n\nAttributes\n----------\nclass_count_ : ndarray of shape (n_classes)\n    Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided.\n\nclass_log_prior_ : ndarray of shape (n_classes)\n    Log probability of each class (smoothed).\n\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier\n\ncoef_ : ndarray of shape (n_classes, n_features)\n    Mirrors ``feature_log_prob_`` for interpreting `BernoulliNB`\n    as a linear model.\n\nfeature_count_ : ndarray of shape (n_classes, n_features)\n    Number of samples encountered for each (class, feature)\n    during fitting. This value is weighted by the sample weight when\n    provided.\n\nfeature_log_prob_ : ndarray of shape (n_classes, n_features)\n    Empirical log probability of features given a class, P(x_i|y).\n\nintercept_ : ndarray of shape (n_classes,)\n    Mirrors ``class_log_prior_`` for interpreting `BernoulliNB`\n    as a linear model.\n\nn_features_ : int\n    Number of features of each sample.\n\nExamples\n--------\n>>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> Y = np.array([1, 2, 3, 4, 4, 5])\n>>> from sklearn.naive_bayes import BernoulliNB\n>>> clf = BernoulliNB()\n>>> clf.fit(X, Y)\nBernoulliNB()\n>>> print(clf.predict(X[2:3]))\n[3]\n\nReferences\n----------\nC.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\nInformation Retrieval. Cambridge University Press, pp. 234-265.\nhttps://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n\nA. McCallum and K. Nigam (1998). A comparison of event models for naive\nBayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\nText Categorization, pp. 41-48.\n\nV. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\nnaive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS)."
        },
        {
          "name": "CategoricalNB",
          "decorators": [],
          "superclasses": [
            "_BaseDiscreteNB"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features. Here, each feature of X is\nassumed to be from a different categorical distribution.\nIt is further assumed that all categories of each feature are\nrepresented by the numbers 0, ..., n - 1, where n refers to the\ntotal number of categories for the given feature. This can, for\ninstance, be achieved with the help of OrdinalEncoder."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights applied to individual samples (1. for unweighted)."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit Naive Bayes classifier according to X, y\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features. Here, each feature of X is\n    assumed to be from a different categorical distribution.\n    It is further assumed that all categories of each feature are\n    represented by the numbers 0, ..., n - 1, where n refers to the\n    total number of categories for the given feature. This can, for\n    instance, be achieved with the help of OrdinalEncoder.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples), default=None\n    Weights applied to individual samples (1. for unweighted).\n\nReturns\n-------\nself : object"
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features. Here, each feature of X is\nassumed to be from a different categorical distribution.\nIt is further assumed that all categories of each feature are\nrepresented by the numbers 0, ..., n - 1, where n refers to the\ntotal number of categories for the given feature. This can, for\ninstance, be achieved with the help of OrdinalEncoder."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "classes",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of all the classes that can possibly appear in the y vector.\n\nMust be provided at the first call to partial_fit, can be omitted\nin subsequent calls."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights applied to individual samples (1. for unweighted)."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance overhead hence it is better to call\npartial_fit on chunks of data that are as large as possible\n(as long as fitting in the memory budget) to hide the overhead.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features. Here, each feature of X is\n    assumed to be from a different categorical distribution.\n    It is further assumed that all categories of each feature are\n    represented by the numbers 0, ..., n - 1, where n refers to the\n    total number of categories for the given feature. This can, for\n    instance, be achieved with the help of OrdinalEncoder.\n\ny : array-like of shape (n_samples)\n    Target values.\n\nclasses : array-like of shape (n_classes), default=None\n    List of all the classes that can possibly appear in the y vector.\n\n    Must be provided at the first call to partial_fit, can be omitted\n    in subsequent calls.\n\nsample_weight : array-like of shape (n_samples), default=None\n    Weights applied to individual samples (1. for unweighted).\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "Naive Bayes classifier for categorical features\n\nThe categorical Naive Bayes classifier is suitable for classification with\ndiscrete features that are categorically distributed. The categories of\neach feature are drawn from a categorical distribution.\n\nRead more in the :ref:`User Guide <categorical_naive_bayes>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Additive (Laplace/Lidstone) smoothing parameter\n    (0 for no smoothing).\n\nfit_prior : bool, default=True\n    Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used.\n\nclass_prior : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. If specified the priors are not\n    adjusted according to the data.\n\nmin_categories : int or array-like of shape (n_features,), default=None\n    Minimum number of categories per feature.\n\n    - integer: Sets the minimum number of categories per feature to\n      `n_categories` for each features.\n    - array-like: shape (n_features,) where `n_categories[i]` holds the\n      minimum number of categories for the ith column of the input.\n    - None (default): Determines the number of categories automatically\n      from the training data.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncategory_count_ : list of arrays of shape (n_features,)\n    Holds arrays of shape (n_classes, n_categories of respective feature)\n    for each feature. Each array provides the number of samples\n    encountered for each class and category of the specific feature.\n\nclass_count_ : ndarray of shape (n_classes,)\n    Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided.\n\nclass_log_prior_ : ndarray of shape (n_classes,)\n    Smoothed empirical log probability for each class.\n\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier\n\nfeature_log_prob_ : list of arrays of shape (n_features,)\n    Holds arrays of shape (n_classes, n_categories of respective feature)\n    for each feature. Each array provides the empirical log probability\n    of categories given the respective feature and class, ``P(x_i|y)``.\n\nn_features_ : int\n    Number of features of each sample.\n\nn_categories_ : ndarray of shape (n_features,), dtype=np.int64\n    Number of categories for each feature. This value is\n    inferred from the data or set by the minimum number of categories.\n\n    .. versionadded:: 0.24\n\nExamples\n--------\n>>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import CategoricalNB\n>>> clf = CategoricalNB()\n>>> clf.fit(X, y)\nCategoricalNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
        },
        {
          "name": "ComplementNB",
          "decorators": [],
          "superclasses": [
            "_BaseDiscreteNB"
          ],
          "methods": [],
          "fullDocstring": "The Complement Naive Bayes classifier described in Rennie et al. (2003).\n\nThe Complement Naive Bayes classifier was designed to correct the \"severe\nassumptions\" made by the standard Multinomial Naive Bayes classifier. It is\nparticularly suited for imbalanced data sets.\n\nRead more in the :ref:`User Guide <complement_naive_bayes>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nalpha : float, default=1.0\n    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n\nfit_prior : bool, default=True\n    Only used in edge case with a single class in the training set.\n\nclass_prior : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. Not used.\n\nnorm : bool, default=False\n    Whether or not a second normalization of the weights is performed. The\n    default behavior mirrors the implementations found in Mahout and Weka,\n    which do not follow the full algorithm described in Table 9 of the\n    paper.\n\nAttributes\n----------\nclass_count_ : ndarray of shape (n_classes,)\n    Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided.\n\nclass_log_prior_ : ndarray of shape (n_classes,)\n    Smoothed empirical log probability for each class. Only used in edge\n    case with a single class in the training set.\n\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier\n\ncoef_ : ndarray of shape (n_classes, n_features)\n    Mirrors ``feature_log_prob_`` for interpreting `ComplementNB`\n    as a linear model.\n\n    .. deprecated:: 0.24\n        ``coef_`` is deprecated in 0.24 and will be removed in 1.1\n        (renaming of 0.26).\n\nfeature_all_ : ndarray of shape (n_features,)\n    Number of samples encountered for each feature during fitting. This\n    value is weighted by the sample weight when provided.\n\nfeature_count_ : ndarray of shape (n_classes, n_features)\n    Number of samples encountered for each (class, feature) during fitting.\n    This value is weighted by the sample weight when provided.\n\nfeature_log_prob_ : ndarray of shape (n_classes, n_features)\n    Empirical weights for class complements.\n\nintercept_ : ndarray of shape (n_classes,)\n    Mirrors ``class_log_prior_`` for interpreting `ComplementNB`\n    as a linear model.\n\n    .. deprecated:: 0.24\n        ``coef_`` is deprecated in 0.24 and will be removed in 1.1\n        (renaming of 0.26).\n\nn_features_ : int\n    Number of features of each sample.\n\nExamples\n--------\n>>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import ComplementNB\n>>> clf = ComplementNB()\n>>> clf.fit(X, y)\nComplementNB()\n>>> print(clf.predict(X[2:3]))\n[3]\n\nReferences\n----------\nRennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\nTackling the poor assumptions of naive bayes text classifiers. In ICML\n(Vol. 3, pp. 616-623).\nhttps://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf"
        },
        {
          "name": "GaussianNB",
          "decorators": [],
          "superclasses": [
            "_BaseNB"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples\nand n_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights applied to individual samples (1. for unweighted).\n\n.. versionadded:: 0.17\n   Gaussian Naive Bayes supports fitting with *sample_weight*."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit Gaussian Naive Bayes according to X, y\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weights applied to individual samples (1. for unweighted).\n\n    .. versionadded:: 0.17\n       Gaussian Naive Bayes supports fitting with *sample_weight*.\n\nReturns\n-------\nself : object"
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                },
                {
                  "name": "classes",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of all the classes that can possibly appear in the y vector.\n\nMust be provided at the first call to partial_fit, can be omitted\nin subsequent calls."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Weights applied to individual samples (1. for unweighted).\n\n.. versionadded:: 0.17"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance and numerical stability overhead,\nhence it is better to call partial_fit on chunks of data that are\nas large as possible (as long as fitting in the memory budget) to\nhide the overhead.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nclasses : array-like of shape (n_classes,), default=None\n    List of all the classes that can possibly appear in the y vector.\n\n    Must be provided at the first call to partial_fit, can be omitted\n    in subsequent calls.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weights applied to individual samples (1. for unweighted).\n\n    .. versionadded:: 0.17\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "Gaussian Naive Bayes (GaussianNB)\n\nCan perform online updates to model parameters via :meth:`partial_fit`.\nFor details on algorithm used to update feature means and variance online,\nsee Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n\n    http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n\nRead more in the :ref:`User Guide <gaussian_naive_bayes>`.\n\nParameters\n----------\npriors : array-like of shape (n_classes,)\n    Prior probabilities of the classes. If specified the priors are not\n    adjusted according to the data.\n\nvar_smoothing : float, default=1e-9\n    Portion of the largest variance of all features that is added to\n    variances for calculation stability.\n\n    .. versionadded:: 0.20\n\nAttributes\n----------\nclass_count_ : ndarray of shape (n_classes,)\n    number of training samples observed in each class.\n\nclass_prior_ : ndarray of shape (n_classes,)\n    probability of each class.\n\nclasses_ : ndarray of shape (n_classes,)\n    class labels known to the classifier\n\nepsilon_ : float\n    absolute additive value to variances\n\nsigma_ : ndarray of shape (n_classes, n_features)\n    variance of each feature per class\n\ntheta_ : ndarray of shape (n_classes, n_features)\n    mean of each feature per class\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> Y = np.array([1, 1, 1, 2, 2, 2])\n>>> from sklearn.naive_bayes import GaussianNB\n>>> clf = GaussianNB()\n>>> clf.fit(X, Y)\nGaussianNB()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]\n>>> clf_pf = GaussianNB()\n>>> clf_pf.partial_fit(X, Y, np.unique(Y))\nGaussianNB()\n>>> print(clf_pf.predict([[-0.8, -1]]))\n[1]"
        },
        {
          "name": "MultinomialNB",
          "decorators": [],
          "superclasses": [
            "_BaseDiscreteNB"
          ],
          "methods": [],
          "fullDocstring": "Naive Bayes classifier for multinomial models\n\nThe multinomial Naive Bayes classifier is suitable for classification with\ndiscrete features (e.g., word counts for text classification). The\nmultinomial distribution normally requires integer feature counts. However,\nin practice, fractional counts such as tf-idf may also work.\n\nRead more in the :ref:`User Guide <multinomial_naive_bayes>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Additive (Laplace/Lidstone) smoothing parameter\n    (0 for no smoothing).\n\nfit_prior : bool, default=True\n    Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used.\n\nclass_prior : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. If specified the priors are not\n    adjusted according to the data.\n\nAttributes\n----------\nclass_count_ : ndarray of shape (n_classes,)\n    Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided.\n\nclass_log_prior_ : ndarray of shape (n_classes, )\n    Smoothed empirical log probability for each class.\n\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier\n\ncoef_ : ndarray of shape (n_classes, n_features)\n    Mirrors ``feature_log_prob_`` for interpreting `MultinomialNB`\n    as a linear model.\n\n    .. deprecated:: 0.24\n        ``coef_`` is deprecated in 0.24 and will be removed in 1.1\n        (renaming of 0.26).\n\nfeature_count_ : ndarray of shape (n_classes, n_features)\n    Number of samples encountered for each (class, feature)\n    during fitting. This value is weighted by the sample weight when\n    provided.\n\nfeature_log_prob_ : ndarray of shape (n_classes, n_features)\n    Empirical log probability of features\n    given a class, ``P(x_i|y)``.\n\nintercept_ : ndarray of shape (n_classes,)\n    Mirrors ``class_log_prior_`` for interpreting `MultinomialNB`\n    as a linear model.\n\n    .. deprecated:: 0.24\n        ``intercept_`` is deprecated in 0.24 and will be removed in 1.1\n        (renaming of 0.26).\n\nn_features_ : int\n    Number of features of each sample.\n\nExamples\n--------\n>>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import MultinomialNB\n>>> clf = MultinomialNB()\n>>> clf.fit(X, y)\nMultinomialNB()\n>>> print(clf.predict(X[2:3]))\n[3]\n\nNotes\n-----\nFor the rationale behind the names `coef_` and `intercept_`, i.e.\nnaive Bayes as a linear classifier, see J. Rennie et al. (2003),\nTackling the poor assumptions of naive Bayes text classifiers, ICML.\n\nReferences\n----------\nC.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\nInformation Retrieval. Cambridge University Press, pp. 234-265.\nhttps://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neighbors",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._ball_tree",
          "declaration": "BallTree",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "VALID_METRICS",
          "alias": null
        },
        {
          "module": "sklearn._base",
          "declaration": "VALID_METRICS_SPARSE",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "KNeighborsClassifier",
          "alias": null
        },
        {
          "module": "sklearn._classification",
          "declaration": "RadiusNeighborsClassifier",
          "alias": null
        },
        {
          "module": "sklearn._dist_metrics",
          "declaration": "DistanceMetric",
          "alias": null
        },
        {
          "module": "sklearn._graph",
          "declaration": "KNeighborsTransformer",
          "alias": null
        },
        {
          "module": "sklearn._graph",
          "declaration": "RadiusNeighborsTransformer",
          "alias": null
        },
        {
          "module": "sklearn._graph",
          "declaration": "kneighbors_graph",
          "alias": null
        },
        {
          "module": "sklearn._graph",
          "declaration": "radius_neighbors_graph",
          "alias": null
        },
        {
          "module": "sklearn._kd_tree",
          "declaration": "KDTree",
          "alias": null
        },
        {
          "module": "sklearn._kde",
          "declaration": "KernelDensity",
          "alias": null
        },
        {
          "module": "sklearn._lof",
          "declaration": "LocalOutlierFactor",
          "alias": null
        },
        {
          "module": "sklearn._nca",
          "declaration": "NeighborhoodComponentsAnalysis",
          "alias": null
        },
        {
          "module": "sklearn._nearest_centroid",
          "declaration": "NearestCentroid",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "KNeighborsRegressor",
          "alias": null
        },
        {
          "module": "sklearn._regression",
          "declaration": "RadiusNeighborsRegressor",
          "alias": null
        },
        {
          "module": "sklearn._unsupervised",
          "declaration": "NearestNeighbors",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.neighbors._base",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csr_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "DataConversionWarning",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "EfficiencyWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "pairwise_distances_chunked",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "PAIRWISE_DISTANCE_FUNCTIONS",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._ball_tree",
          "declaration": "BallTree",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._kd_tree",
          "declaration": "KDTree",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_to_object_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_even_slices",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "parse_version",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_non_negative",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KNeighborsMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "kneighbors",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The query point or points.\nIf not provided, neighbors of each indexed point are returned.\nIn this case, the query point is not considered its own neighbor."
                },
                {
                  "name": "n_neighbors",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Number of neighbors required for each sample. The default is the\nvalue passed to the constructor."
                },
                {
                  "name": "return_distance",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Whether or not to return the distances."
                }
              ],
              "results": [
                {
                  "name": "neigh_dist",
                  "type": null,
                  "description": "Array representing the lengths to points, only present if\nreturn_distance=True"
                },
                {
                  "name": "neigh_ind",
                  "type": null,
                  "description": "Indices of the nearest points in the population matrix."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Finds the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.\n\nParameters\n----------\nX : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n    The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor.\n\nn_neighbors : int, default=None\n    Number of neighbors required for each sample. The default is the\n    value passed to the constructor.\n\nreturn_distance : bool, default=True\n    Whether or not to return the distances.\n\nReturns\n-------\nneigh_dist : ndarray of shape (n_queries, n_neighbors)\n    Array representing the lengths to points, only present if\n    return_distance=True\n\nneigh_ind : ndarray of shape (n_queries, n_neighbors)\n    Indices of the nearest points in the population matrix.\n\nExamples\n--------\nIn the following example, we construct a NearestNeighbors\nclass from an array representing our data set and ask who's\nthe closest point to [1,1,1]\n\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=1)\n>>> neigh.fit(samples)\nNearestNeighbors(n_neighbors=1)\n>>> print(neigh.kneighbors([[1., 1., 1.]]))\n(array([[0.5]]), array([[2]]))\n\nAs you can see, it returns [[0.5]], and [[2]], which means that the\nelement is at distance 0.5 and is the third element of samples\n(indexes start at 0). You can also query for multiple points:\n\n>>> X = [[0., 1., 0.], [1., 0., 1.]]\n>>> neigh.kneighbors(X, return_distance=False)\narray([[1],\n       [2]]...)"
            },
            {
              "name": "kneighbors_graph",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The query point or points.\nIf not provided, neighbors of each indexed point are returned.\nIn this case, the query point is not considered its own neighbor.\nFor ``metric='precomputed'`` the shape should be\n(n_queries, n_indexed). Otherwise the shape should be\n(n_queries, n_features)."
                },
                {
                  "name": "n_neighbors",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Number of neighbors for each sample. The default is the value\npassed to the constructor."
                },
                {
                  "name": "mode",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "connectivity",
                  "limitation": null,
                  "ignored": false,
                  "description": "Type of returned matrix: 'connectivity' will return the\nconnectivity matrix with ones and zeros, in 'distance' the\nedges are Euclidean distance between points."
                }
              ],
              "results": [
                {
                  "name": "A",
                  "type": null,
                  "description": "`n_samples_fit` is the number of samples in the fitted data\n`A[i, j]` is assigned the weight of edge that connects `i` to `j`.\nThe matrix is of CSR format."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Computes the (weighted) graph of k-Neighbors for points in X\n\nParameters\n----------\nX : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n    The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor.\n    For ``metric='precomputed'`` the shape should be\n    (n_queries, n_indexed). Otherwise the shape should be\n    (n_queries, n_features).\n\nn_neighbors : int, default=None\n    Number of neighbors for each sample. The default is the value\n    passed to the constructor.\n\nmode : {'connectivity', 'distance'}, default='connectivity'\n    Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are Euclidean distance between points.\n\nReturns\n-------\nA : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data\n    `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n    The matrix is of CSR format.\n\nExamples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(n_neighbors=2)\n>>> neigh.fit(X)\nNearestNeighbors(n_neighbors=2)\n>>> A = neigh.kneighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 1.],\n       [1., 0., 1.]])\n\nSee Also\n--------\nNearestNeighbors.radius_neighbors_graph"
            }
          ],
          "fullDocstring": "Mixin for k-neighbors searches"
        },
        {
          "name": "NeighborsBase",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "MultiOutputMixin"
          ],
          "methods": [],
          "fullDocstring": "Base class for nearest neighbors estimators."
        },
        {
          "name": "RadiusNeighborsMixin",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "radius_neighbors",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The query point or points.\nIf not provided, neighbors of each indexed point are returned.\nIn this case, the query point is not considered its own neighbor."
                },
                {
                  "name": "radius",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Limiting distance of neighbors to return. The default is the value\npassed to the constructor."
                },
                {
                  "name": "return_distance",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Whether or not to return the distances."
                },
                {
                  "name": "sort_results",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, the distances and indices will be sorted by increasing\ndistances before being returned. If False, the results may not\nbe sorted. If `return_distance=False`, setting `sort_results=True`\nwill result in an error.\n\n.. versionadded:: 0.22"
                }
              ],
              "results": [
                {
                  "name": "neigh_dist",
                  "type": null,
                  "description": "Array representing the distances to each point, only present if\n`return_distance=True`. The distance values are computed according\nto the ``metric`` constructor parameter."
                },
                {
                  "name": "neigh_ind",
                  "type": null,
                  "description": "An array of arrays of indices of the approximate nearest points\nfrom the population matrix that lie within a ball of size\n``radius`` around the query points."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Finds the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of each point from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.\n\nParameters\n----------\nX : array-like of (n_samples, n_features), default=None\n    The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor.\n\nradius : float, default=None\n    Limiting distance of neighbors to return. The default is the value\n    passed to the constructor.\n\nreturn_distance : bool, default=True\n    Whether or not to return the distances.\n\nsort_results : bool, default=False\n    If True, the distances and indices will be sorted by increasing\n    distances before being returned. If False, the results may not\n    be sorted. If `return_distance=False`, setting `sort_results=True`\n    will result in an error.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\nneigh_dist : ndarray of shape (n_samples,) of arrays\n    Array representing the distances to each point, only present if\n    `return_distance=True`. The distance values are computed according\n    to the ``metric`` constructor parameter.\n\nneigh_ind : ndarray of shape (n_samples,) of arrays\n    An array of arrays of indices of the approximate nearest points\n    from the population matrix that lie within a ball of size\n    ``radius`` around the query points.\n\nExamples\n--------\nIn the following example, we construct a NeighborsClassifier\nclass from an array representing our data set and ask who's\nthe closest point to [1, 1, 1]:\n\n>>> import numpy as np\n>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.6)\n>>> neigh.fit(samples)\nNearestNeighbors(radius=1.6)\n>>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n>>> print(np.asarray(rng[0][0]))\n[1.5 0.5]\n>>> print(np.asarray(rng[1][0]))\n[1 2]\n\nThe first array returned contains the distances to all points which\nare closer than 1.6, while the second array returned contains their\nindices.  In general, multiple points can be queried at the same time.\n\nNotes\n-----\nBecause the number of neighbors of each point is not necessarily\nequal, the results for multiple query points cannot be fit in a\nstandard data array.\nFor efficiency, `radius_neighbors` returns arrays of objects, where\neach object is a 1D array of indices or distances."
            },
            {
              "name": "radius_neighbors_graph",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The query point or points.\nIf not provided, neighbors of each indexed point are returned.\nIn this case, the query point is not considered its own neighbor."
                },
                {
                  "name": "radius",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Radius of neighborhoods. The default is the value passed to the\nconstructor."
                },
                {
                  "name": "mode",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "connectivity",
                  "limitation": null,
                  "ignored": false,
                  "description": "Type of returned matrix: 'connectivity' will return the\nconnectivity matrix with ones and zeros, in 'distance' the\nedges are Euclidean distance between points."
                },
                {
                  "name": "sort_results",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "False",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, in each row of the result, the non-zero entries will be\nsorted by increasing distances. If False, the non-zero entries may\nnot be sorted. Only used with mode='distance'.\n\n.. versionadded:: 0.22"
                }
              ],
              "results": [
                {
                  "name": "A",
                  "type": null,
                  "description": "`n_samples_fit` is the number of samples in the fitted data\n`A[i, j]` is assigned the weight of edge that connects `i` to `j`.\nThe matrix if of format CSR."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Computes the (weighted) graph of Neighbors for points in X\n\nNeighborhoods are restricted the points at a distance lower than\nradius.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features), default=None\n    The query point or points.\n    If not provided, neighbors of each indexed point are returned.\n    In this case, the query point is not considered its own neighbor.\n\nradius : float, default=None\n    Radius of neighborhoods. The default is the value passed to the\n    constructor.\n\nmode : {'connectivity', 'distance'}, default='connectivity'\n    Type of returned matrix: 'connectivity' will return the\n    connectivity matrix with ones and zeros, in 'distance' the\n    edges are Euclidean distance between points.\n\nsort_results : bool, default=False\n    If True, in each row of the result, the non-zero entries will be\n    sorted by increasing distances. If False, the non-zero entries may\n    not be sorted. Only used with mode='distance'.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\nA : sparse-matrix of shape (n_queries, n_samples_fit)\n    `n_samples_fit` is the number of samples in the fitted data\n    `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n    The matrix if of format CSR.\n\nExamples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import NearestNeighbors\n>>> neigh = NearestNeighbors(radius=1.5)\n>>> neigh.fit(X)\nNearestNeighbors(radius=1.5)\n>>> A = neigh.radius_neighbors_graph(X)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 1.]])\n\nSee Also\n--------\nkneighbors_graph"
            }
          ],
          "fullDocstring": "Mixin for radius-based neighbors searches"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neighbors._classification",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "stats",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "KNeighborsMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "NeighborsBase",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "RadiusNeighborsMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "_check_weights",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "_get_weights",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "weighted_mode",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_is_arraylike",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KNeighborsClassifier",
          "decorators": [],
          "superclasses": [
            "KNeighborsMixin",
            "ClassifierMixin",
            "NeighborsBase"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted k-nearest neighbors classifier."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the k-nearest neighbors classifier from the training dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n    Training data.\n\ny : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n    Target values.\n\nReturns\n-------\nself : KNeighborsClassifier\n    The fitted k-nearest neighbors classifier."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Class labels for each data sample."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the class labels for the provided data.\n\nParameters\n----------\nX : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n    Test samples.\n\nReturns\n-------\ny : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n    Class labels for each data sample."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "of such arrays if n_outputs > 1.\nThe class probabilities of the input samples. Classes are ordered\nby lexicographic order."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return probability estimates for the test data X.\n\nParameters\n----------\nX : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n    Test samples.\n\nReturns\n-------\np : ndarray of shape (n_queries, n_classes), or a list of n_outputs\n    of such arrays if n_outputs > 1.\n    The class probabilities of the input samples. Classes are ordered\n    by lexicographic order."
            }
          ],
          "fullDocstring": "Classifier implementing the k-nearest neighbors vote.\n\nRead more in the :ref:`User Guide <classification>`.\n\nParameters\n----------\nn_neighbors : int, default=5\n    Number of neighbors to use by default for :meth:`kneighbors` queries.\n\nweights : {'uniform', 'distance'} or callable, default='uniform'\n    weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\np : int, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric : str or callable, default='minkowski'\n    the distance metric to use for the tree.  The default metric is\n    minkowski, and with p=2 is equivalent to the standard Euclidean\n    metric. See the documentation of :class:`DistanceMetric` for a\n    list of available metrics.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n\nAttributes\n----------\nclasses_ : array of shape (n_classes,)\n    Class labels known to the classifier\n\neffective_metric_ : str or callble\n    The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\noutputs_2d_ : bool\n    False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n    otherwise True.\n\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> neigh = KNeighborsClassifier(n_neighbors=3)\n>>> neigh.fit(X, y)\nKNeighborsClassifier(...)\n>>> print(neigh.predict([[1.1]]))\n[0]\n>>> print(neigh.predict_proba([[0.9]]))\n[[0.66666667 0.33333333]]\n\nSee Also\n--------\nRadiusNeighborsClassifier\nKNeighborsRegressor\nRadiusNeighborsRegressor\nNearestNeighbors\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n.. warning::\n\n   Regarding the Nearest Neighbors algorithms, if it is found that two\n   neighbors, neighbor `k+1` and `k`, have identical distances\n   but different labels, the results will depend on the ordering of the\n   training data.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm"
        },
        {
          "name": "RadiusNeighborsClassifier",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "NeighborsBase",
            "RadiusNeighborsMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted radius neighbors classifier."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the radius neighbors classifier from the training dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n    Training data.\n\ny : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n    Target values.\n\nReturns\n-------\nself : RadiusNeighborsClassifier\n    The fitted radius neighbors classifier."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Class labels for each data sample."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the class labels for the provided data.\n\nParameters\n----------\nX : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n    Test samples.\n\nReturns\n-------\ny : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n    Class labels for each data sample."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                }
              ],
              "results": [
                {
                  "name": "p",
                  "type": null,
                  "description": "of such arrays if n_outputs > 1.\nThe class probabilities of the input samples. Classes are ordered\nby lexicographic order."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return probability estimates for the test data X.\n\nParameters\n----------\nX : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n    Test samples.\n\nReturns\n-------\np : ndarray of shape (n_queries, n_classes), or a list of n_outputs\n    of such arrays if n_outputs > 1.\n    The class probabilities of the input samples. Classes are ordered\n    by lexicographic order."
            }
          ],
          "fullDocstring": "Classifier implementing a vote among neighbors within a given radius\n\nRead more in the :ref:`User Guide <classification>`.\n\nParameters\n----------\nradius : float, default=1.0\n    Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries.\n\nweights : {'uniform', 'distance'} or callable, default='uniform'\n    weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\np : int, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric : str or callable, default='minkowski'\n    the distance metric to use for the tree.  The default metric is\n    minkowski, and with p=2 is equivalent to the standard Euclidean\n    metric. See the documentation of :class:`DistanceMetric` for a\n    list of available metrics.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n\noutlier_label : {manual label, 'most_frequent'}, default=None\n    label for outlier samples (samples with no neighbors in given radius).\n\n    - manual label: str or int label (should be the same type as y)\n      or list of manual labels if multi-output is used.\n    - 'most_frequent' : assign the most frequent label of y to outliers.\n    - None : when any outlier is detected, ValueError will be raised.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier.\n\neffective_metric_ : str or callable\n    The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\noutlier_label_ : int or array-like of shape (n_class,)\n    Label which is given for outlier samples (samples with no neighbors\n    on given radius).\n\noutputs_2d_ : bool\n    False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n    otherwise True.\n\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsClassifier\n>>> neigh = RadiusNeighborsClassifier(radius=1.0)\n>>> neigh.fit(X, y)\nRadiusNeighborsClassifier(...)\n>>> print(neigh.predict([[1.5]]))\n[0]\n>>> print(neigh.predict_proba([[1.0]]))\n[[0.66666667 0.33333333]]\n\nSee Also\n--------\nKNeighborsClassifier\nRadiusNeighborsRegressor\nKNeighborsRegressor\nNearestNeighbors\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neighbors._graph",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "KNeighborsMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "NeighborsBase",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "RadiusNeighborsMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._unsupervised",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KNeighborsTransformer",
          "decorators": [],
          "superclasses": [
            "KNeighborsMixin",
            "NeighborsBase",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted k-nearest neighbors transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the k-nearest neighbors transformer from the training dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n    Training data.\n\nReturns\n-------\nself : KNeighborsTransformer\n    The fitted k-nearest neighbors transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample data."
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "Xt[i, j] is assigned the weight of edge that connects i to j.\nOnly the neighbors have an explicit value.\nThe diagonal is always explicit.\nThe matrix is of CSR format."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Computes the (weighted) graph of Neighbors for points in X\n\nParameters\n----------\nX : array-like of shape (n_samples_transform, n_features)\n    Sample data.\n\nReturns\n-------\nXt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n    Xt[i, j] is assigned the weight of edge that connects i to j.\n    Only the neighbors have an explicit value.\n    The diagonal is always explicit.\n    The matrix is of CSR format."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training set."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "Xt[i, j] is assigned the weight of edge that connects i to j.\nOnly the neighbors have an explicit value.\nThe diagonal is always explicit.\nThe matrix is of CSR format."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training set.\n\ny : ignored\n\nReturns\n-------\nXt : sparse matrix of shape (n_samples, n_samples)\n    Xt[i, j] is assigned the weight of edge that connects i to j.\n    Only the neighbors have an explicit value.\n    The diagonal is always explicit.\n    The matrix is of CSR format."
            }
          ],
          "fullDocstring": "Transform X into a (weighted) graph of k nearest neighbors\n\nThe transformed data is a sparse graph as returned by kneighbors_graph.\n\nRead more in the :ref:`User Guide <neighbors_transformer>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nmode : {'distance', 'connectivity'}, default='distance'\n    Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric.\n\nn_neighbors : int, default=5\n    Number of neighbors for each sample in the transformed sparse graph.\n    For compatibility reasons, as each sample is considered as its own\n    neighbor, one extra neighbor will be computed when mode == 'distance'.\n    In this case, the sparse graph contains (n_neighbors + 1) neighbors.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\nmetric : str or callable, default='minkowski'\n    metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=1\n    The number of parallel jobs to run for neighbors search.\n    If ``-1``, then the number of jobs is set to the number of CPU cores.\n\nAttributes\n----------\neffective_metric_ : str or callable\n    The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nExamples\n--------\n>>> from sklearn.manifold import Isomap\n>>> from sklearn.neighbors import KNeighborsTransformer\n>>> from sklearn.pipeline import make_pipeline\n>>> estimator = make_pipeline(\n...     KNeighborsTransformer(n_neighbors=5, mode='distance'),\n...     Isomap(neighbors_algorithm='precomputed'))"
        },
        {
          "name": "RadiusNeighborsTransformer",
          "decorators": [],
          "superclasses": [
            "NeighborsBase",
            "RadiusNeighborsMixin",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted radius neighbors transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the radius neighbors transformer from the training dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n    Training data.\n\nReturns\n-------\nself : RadiusNeighborsTransformer\n    The fitted radius neighbors transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample data"
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "Xt[i, j] is assigned the weight of edge that connects i to j.\nOnly the neighbors have an explicit value.\nThe diagonal is always explicit.\nThe matrix is of CSR format."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Computes the (weighted) graph of Neighbors for points in X\n\nParameters\n----------\nX : array-like of shape (n_samples_transform, n_features)\n    Sample data\n\nReturns\n-------\nXt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n    Xt[i, j] is assigned the weight of edge that connects i to j.\n    Only the neighbors have an explicit value.\n    The diagonal is always explicit.\n    The matrix is of CSR format."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training set."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "Xt[i, j] is assigned the weight of edge that connects i to j.\nOnly the neighbors have an explicit value.\nThe diagonal is always explicit.\nThe matrix is of CSR format."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training set.\n\ny : ignored\n\nReturns\n-------\nXt : sparse matrix of shape (n_samples, n_samples)\n    Xt[i, j] is assigned the weight of edge that connects i to j.\n    Only the neighbors have an explicit value.\n    The diagonal is always explicit.\n    The matrix is of CSR format."
            }
          ],
          "fullDocstring": "Transform X into a (weighted) graph of neighbors nearer than a radius\n\nThe transformed data is a sparse graph as returned by\nradius_neighbors_graph.\n\nRead more in the :ref:`User Guide <neighbors_transformer>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nmode : {'distance', 'connectivity'}, default='distance'\n    Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric.\n\nradius : float, default=1.\n    Radius of neighborhood in the transformed sparse graph.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\nmetric : str or callable, default='minkowski'\n    metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=1\n    The number of parallel jobs to run for neighbors search.\n    If ``-1``, then the number of jobs is set to the number of CPU cores.\n\nAttributes\n----------\neffective_metric_ : str or callable\n    The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nExamples\n--------\n>>> from sklearn.cluster import DBSCAN\n>>> from sklearn.neighbors import RadiusNeighborsTransformer\n>>> from sklearn.pipeline import make_pipeline\n>>> estimator = make_pipeline(\n...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n...     DBSCAN(min_samples=30, metric='precomputed'))"
        }
      ],
      "functions": [
        {
          "name": "kneighbors_graph",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Sample data, in the form of a numpy array or a precomputed\n:class:`BallTree`."
            },
            {
              "name": "n_neighbors",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of neighbors for each sample."
            }
          ],
          "results": [
            {
              "name": "A",
              "type": null,
              "description": "Graph where A[i, j] is assigned the weight of edge that\nconnects i to j. The matrix is of CSR format."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the (weighted) graph of k-Neighbors for points in X\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or BallTree\n    Sample data, in the form of a numpy array or a precomputed\n    :class:`BallTree`.\n\nn_neighbors : int\n    Number of neighbors for each sample.\n\nmode : {'connectivity', 'distance'}, default='connectivity'\n    Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric.\n\nmetric : str, default='minkowski'\n    The distance metric used to calculate the k-Neighbors for each sample\n    point. The DistanceMetric class gives a list of available metrics.\n    The default distance is 'euclidean' ('minkowski' metric with the p\n    param equal to 2.)\n\np : int, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    additional keyword arguments for the metric function.\n\ninclude_self : bool or 'auto', default=False\n    Whether or not to mark each sample as the first nearest neighbor to\n    itself. If 'auto', then True is used for mode='connectivity' and False\n    for mode='distance'.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nA : sparse matrix of shape (n_samples, n_samples)\n    Graph where A[i, j] is assigned the weight of edge that\n    connects i to j. The matrix is of CSR format.\n\nExamples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import kneighbors_graph\n>>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 1.],\n       [1., 0., 1.]])\n\nSee Also\n--------\nradius_neighbors_graph"
        },
        {
          "name": "radius_neighbors_graph",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Sample data, in the form of a numpy array or a precomputed\n:class:`BallTree`."
            },
            {
              "name": "radius",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Radius of neighborhoods."
            }
          ],
          "results": [
            {
              "name": "A",
              "type": null,
              "description": "Graph where A[i, j] is assigned the weight of edge that connects\ni to j. The matrix is of CSR format."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes the (weighted) graph of Neighbors for points in X\n\nNeighborhoods are restricted the points at a distance lower than\nradius.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or BallTree\n    Sample data, in the form of a numpy array or a precomputed\n    :class:`BallTree`.\n\nradius : float\n    Radius of neighborhoods.\n\nmode : {'connectivity', 'distance'}, default='connectivity'\n    Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric.\n\nmetric : str, default='minkowski'\n    The distance metric used to calculate the neighbors within a\n    given radius for each sample point. The DistanceMetric class\n    gives a list of available metrics. The default distance is\n    'euclidean' ('minkowski' metric with the param equal to 2.)\n\np : int, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    additional keyword arguments for the metric function.\n\ninclude_self : bool or 'auto', default=False\n    Whether or not to mark each sample as the first nearest neighbor to\n    itself. If 'auto', then True is used for mode='connectivity' and False\n    for mode='distance'.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nA : sparse matrix of shape (n_samples, n_samples)\n    Graph where A[i, j] is assigned the weight of edge that connects\n    i to j. The matrix is of CSR format.\n\nExamples\n--------\n>>> X = [[0], [3], [1]]\n>>> from sklearn.neighbors import radius_neighbors_graph\n>>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',\n...                            include_self=True)\n>>> A.toarray()\narray([[1., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 1.]])\n\nSee Also\n--------\nkneighbors_graph"
        }
      ]
    },
    {
      "name": "sklearn.neighbors._kde",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy.special",
          "declaration": "gammainc",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._ball_tree",
          "declaration": "BallTree",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._ball_tree",
          "declaration": "DTYPE",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._kd_tree",
          "declaration": "KDTree",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KernelDensity",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of n_features-dimensional data points.  Each row\ncorresponds to a single data point."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored. This parameter exists only for compatibility with\n:class:`~sklearn.pipeline.Pipeline`."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of sample weights attached to the data X.\n\n.. versionadded:: 0.20"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns instance of object."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the Kernel Density model on the data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    List of n_features-dimensional data points.  Each row\n    corresponds to a single data point.\n\ny : None\n    Ignored. This parameter exists only for compatibility with\n    :class:`~sklearn.pipeline.Pipeline`.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    List of sample weights attached to the data X.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nself : object\n    Returns instance of object."
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "An array of points to query.  Last dimension should match dimension\nof training data (n_features)."
                }
              ],
              "results": [
                {
                  "name": "density",
                  "type": null,
                  "description": "The array of log(density) evaluations. These are normalized to be\nprobability densities, so values will be low for high-dimensional\ndata."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Evaluate the log density model on the data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    An array of points to query.  Last dimension should match dimension\n    of training data (n_features).\n\nReturns\n-------\ndensity : ndarray of shape (n_samples,)\n    The array of log(density) evaluations. These are normalized to be\n    probability densities, so values will be low for high-dimensional\n    data."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "List of n_features-dimensional data points.  Each row\ncorresponds to a single data point."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored. This parameter exists only for compatibility with\n:class:`~sklearn.pipeline.Pipeline`."
                }
              ],
              "results": [
                {
                  "name": "logprob",
                  "type": "float",
                  "description": "Total log-likelihood of the data in X. This is normalized to be a\nprobability density, so the value will be low for high-dimensional\ndata."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the total log probability density under the model.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    List of n_features-dimensional data points.  Each row\n    corresponds to a single data point.\n\ny : None\n    Ignored. This parameter exists only for compatibility with\n    :class:`~sklearn.pipeline.Pipeline`.\n\nReturns\n-------\nlogprob : float\n    Total log-likelihood of the data in X. This is normalized to be a\n    probability density, so the value will be low for high-dimensional\n    data."
            },
            {
              "name": "sample",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "n_samples",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "1",
                  "limitation": null,
                  "ignored": false,
                  "description": "Number of samples to generate."
                },
                {
                  "name": "random_state",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Determines random number generation used to generate\nrandom samples. Pass an int for reproducible results\nacross multiple function calls.\nSee :term: `Glossary <random_state>`."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "List of samples."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate random samples from the model.\n\nCurrently, this is implemented only for gaussian and tophat kernels.\n\nParameters\n----------\nn_samples : int, default=1\n    Number of samples to generate.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation used to generate\n    random samples. Pass an int for reproducible results\n    across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nReturns\n-------\nX : array-like of shape (n_samples, n_features)\n    List of samples."
            }
          ],
          "fullDocstring": "Kernel Density Estimation.\n\nRead more in the :ref:`User Guide <kernel_density>`.\n\nParameters\n----------\nbandwidth : float, default=1.0\n    The bandwidth of the kernel.\n\nalgorithm : {'kd_tree', 'ball_tree', 'auto'}, default='auto'\n    The tree algorithm to use.\n\nkernel : {'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian'\n    The kernel to use.\n\nmetric : str, default='euclidian'\n    The distance metric to use.  Note that not all metrics are\n    valid with all algorithms.  Refer to the documentation of\n    :class:`BallTree` and :class:`KDTree` for a description of\n    available algorithms.  Note that the normalization of the density\n    output is correct only for the Euclidean distance metric. Default\n    is 'euclidean'.\n\natol : float, default=0\n    The desired absolute tolerance of the result.  A larger tolerance will\n    generally lead to faster execution.\n\nrtol : float, default=0\n    The desired relative tolerance of the result.  A larger tolerance will\n    generally lead to faster execution.\n\nbreadth_first : bool, default=True\n    If true (default), use a breadth-first approach to the problem.\n    Otherwise use a depth-first approach.\n\nleaf_size : int, default=40\n    Specify the leaf size of the underlying tree.  See :class:`BallTree`\n    or :class:`KDTree` for details.\n\nmetric_params : dict, default=None\n    Additional parameters to be passed to the tree for use with the\n    metric.  For more information, see the documentation of\n    :class:`BallTree` or :class:`KDTree`.\n\nAttributes\n----------\ntree_ : ``BinaryTree`` instance\n    The tree algorithm for fast generalized N-point problems.\n\nSee Also\n--------\nsklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\n    problems.\nsklearn.neighbors.BallTree : Ball tree for fast generalized N-point\n    problems.\n\nExamples\n--------\nCompute a gaussian kernel density estimate with a fixed bandwidth.\n\n>>> import numpy as np\n>>> rng = np.random.RandomState(42)\n>>> X = rng.random_sample((100, 3))\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n>>> log_density = kde.score_samples(X[:3])\n>>> log_density\narray([-1.52955942, -1.51462041, -1.60244657])"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neighbors._lof",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "OutlierMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "KNeighborsMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "NeighborsBase",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "LocalOutlierFactor",
          "decorators": [],
          "superclasses": [
            "KNeighborsMixin",
            "OutlierMixin",
            "NeighborsBase"
          ],
          "methods": [
            {
              "name": "fit_predict",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "is_inlier",
                  "type": null,
                  "description": "Returns -1 for anomalies/outliers and 1 for inliers."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fits the model to the training set X and returns the labels.\n\n**Not available for novelty detection (when novelty is set to True).**\nLabel is 1 for an inlier and -1 for an outlier according to the LOF\nscore and the contamination parameter.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features), default=None\n    The query sample or samples to compute the Local Outlier Factor\n    w.r.t. to the training samples.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nis_inlier : ndarray of shape (n_samples,)\n    Returns -1 for anomalies/outliers and 1 for inliers."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted local outlier factor detector."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the local outlier factor detector from the training dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n    Training data.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : LocalOutlierFactor\n    The fitted local outlier factor detector."
            },
            {
              "name": "predict",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "is_inlier",
                  "type": null,
                  "description": "Returns -1 for anomalies/outliers and +1 for inliers."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the labels (1 inlier, -1 outlier) of X according to LOF.\n\n**Only available for novelty detection (when novelty is set to True).**\nThis method allows to generalize prediction to *new observations* (not\nin the training set).\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The query sample or samples to compute the Local Outlier Factor\n    w.r.t. to the training samples.\n\nReturns\n-------\nis_inlier : ndarray of shape (n_samples,)\n    Returns -1 for anomalies/outliers and +1 for inliers."
            },
            {
              "name": "decision_function",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "shifted_opposite_lof_scores",
                  "type": null,
                  "description": "The shifted opposite of the Local Outlier Factor of each input\nsamples. The lower, the more abnormal. Negative scores represent\noutliers, positive scores represent inliers."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Shifted opposite of the Local Outlier Factor of X.\n\nBigger is better, i.e. large values correspond to inliers.\n\n**Only available for novelty detection (when novelty is set to True).**\nThe shift offset allows a zero threshold for being an outlier.\nThe argument X is supposed to contain *new data*: if X contains a\npoint from training, it considers the later in its own neighborhood.\nAlso, the samples in X are not considered in the neighborhood of any\npoint.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The query sample or samples to compute the Local Outlier Factor\n    w.r.t. the training samples.\n\nReturns\n-------\nshifted_opposite_lof_scores : ndarray of shape (n_samples,)\n    The shifted opposite of the Local Outlier Factor of each input\n    samples. The lower, the more abnormal. Negative scores represent\n    outliers, positive scores represent inliers."
            },
            {
              "name": "score_samples",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "opposite_lof_scores",
                  "type": null,
                  "description": "The opposite of the Local Outlier Factor of each input samples.\nThe lower, the more abnormal."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Opposite of the Local Outlier Factor of X.\n\nIt is the opposite as bigger is better, i.e. large values correspond\nto inliers.\n\n**Only available for novelty detection (when novelty is set to True).**\nThe argument X is supposed to contain *new data*: if X contains a\npoint from training, it considers the later in its own neighborhood.\nAlso, the samples in X are not considered in the neighborhood of any\npoint.\nThe score_samples on training data is available by considering the\nthe ``negative_outlier_factor_`` attribute.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The query sample or samples to compute the Local Outlier Factor\n    w.r.t. the training samples.\n\nReturns\n-------\nopposite_lof_scores : ndarray of shape (n_samples,)\n    The opposite of the Local Outlier Factor of each input samples.\n    The lower, the more abnormal."
            }
          ],
          "fullDocstring": "Unsupervised Outlier Detection using Local Outlier Factor (LOF)\n\nThe anomaly score of each sample is called Local Outlier Factor.\nIt measures the local deviation of density of a given sample with\nrespect to its neighbors.\nIt is local in that the anomaly score depends on how isolated the object\nis with respect to the surrounding neighborhood.\nMore precisely, locality is given by k-nearest neighbors, whose distance\nis used to estimate the local density.\nBy comparing the local density of a sample to the local densities of\nits neighbors, one can identify samples that have a substantially lower\ndensity than their neighbors. These are considered outliers.\n\n.. versionadded:: 0.19\n\nParameters\n----------\nn_neighbors : int, default=20\n    Number of neighbors to use by default for :meth:`kneighbors` queries.\n    If n_neighbors is larger than the number of samples provided,\n    all samples will be used.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem.\n\nmetric : str or callable, default='minkowski'\n    metric used for the distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square. X may be a sparse matrix, in which case only \"nonzero\"\n    elements may be considered neighbors.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics:\n    https://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    :func:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this\n    is equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\ncontamination : 'auto' or float, default='auto'\n    The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. When fitting this is used to define the\n    threshold on the scores of the samples.\n\n    - if 'auto', the threshold is determined as in the\n      original paper,\n    - if a float, the contamination should be in the range [0, 0.5].\n\n    .. versionchanged:: 0.22\n       The default value of ``contamination`` changed from 0.1\n       to ``'auto'``.\n\nnovelty : bool, default=False\n    By default, LocalOutlierFactor is only meant to be used for outlier\n    detection (novelty=False). Set novelty to True if you want to use\n    LocalOutlierFactor for novelty detection. In this case be aware that\n    that you should only use predict, decision_function and score_samples\n    on new unseen data and not on the training set.\n\n    .. versionadded:: 0.20\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nnegative_outlier_factor_ : ndarray of shape (n_samples,)\n    The opposite LOF of the training samples. The higher, the more normal.\n    Inliers tend to have a LOF score close to 1\n    (``negative_outlier_factor_`` close to -1), while outliers tend to have\n    a larger LOF score.\n\n    The local outlier factor (LOF) of a sample captures its\n    supposed 'degree of abnormality'.\n    It is the average of the ratio of the local reachability density of\n    a sample and those of its k-nearest neighbors.\n\nn_neighbors_ : int\n    The actual number of neighbors used for :meth:`kneighbors` queries.\n\noffset_ : float\n    Offset used to obtain binary labels from the raw scores.\n    Observations having a negative_outlier_factor smaller than `offset_`\n    are detected as abnormal.\n    The offset is set to -1.5 (inliers score around -1), except when a\n    contamination parameter different than \"auto\" is provided. In that\n    case, the offset is defined in such a way we obtain the expected\n    number of outliers in training.\n\n    .. versionadded:: 0.20\n\neffective_metric_ : str\n    The effective metric used for the distance computation.\n\neffective_metric_params_ : dict\n    The effective additional keyword arguments for the metric function.\n\nn_samples_fit_ : int\n    It is the number of samples in the fitted data.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.neighbors import LocalOutlierFactor\n>>> X = [[-1.1], [0.2], [101.1], [0.3]]\n>>> clf = LocalOutlierFactor(n_neighbors=2)\n>>> clf.fit_predict(X)\narray([ 1,  1, -1,  1])\n>>> clf.negative_outlier_factor_\narray([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])\n\nReferences\n----------\n.. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n       LOF: identifying density-based local outliers. In ACM sigmod record."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neighbors._nca",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "sys",
          "alias": null
        },
        {
          "module": "time",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "__future__",
          "declaration": "print_function",
          "alias": null
        },
        {
          "module": "scipy.optimize",
          "declaration": "minimize",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.decomposition",
          "declaration": "PCA",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "pairwise_distances",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "softmax",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.random",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_scalar",
          "alias": null
        },
        {
          "module": "warnings",
          "declaration": "warn",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "NeighborhoodComponentsAnalysis",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training samples."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The corresponding training labels."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "returns a trained NeighborhoodComponentsAnalysis model."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model according to the given training data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The training samples.\n\ny : array-like of shape (n_samples,)\n    The corresponding training labels.\n\nReturns\n-------\nself : object\n    returns a trained NeighborhoodComponentsAnalysis model."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data samples."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": "The data samples transformed."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Applies the learned transformation to the given data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data samples.\n\nReturns\n-------\nX_embedded: ndarray of shape (n_samples, n_components)\n    The data samples transformed.\n\nRaises\n------\nNotFittedError\n    If :meth:`fit` has not been called before."
            }
          ],
          "fullDocstring": "Neighborhood Components Analysis\n\nNeighborhood Component Analysis (NCA) is a machine learning algorithm for\nmetric learning. It learns a linear transformation in a supervised fashion\nto improve the classification accuracy of a stochastic nearest neighbors\nrule in the transformed space.\n\nRead more in the :ref:`User Guide <nca>`.\n\nParameters\n----------\nn_components : int, default=None\n    Preferred dimensionality of the projected space.\n    If None it will be set to ``n_features``.\n\ninit : {'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto'\n    Initialization of the linear transformation. Possible options are\n    'auto', 'pca', 'lda', 'identity', 'random', and a numpy array of shape\n    (n_features_a, n_features_b).\n\n    'auto'\n        Depending on ``n_components``, the most reasonable initialization\n        will be chosen. If ``n_components <= n_classes`` we use 'lda', as\n        it uses labels information. If not, but\n        ``n_components < min(n_features, n_samples)``, we use 'pca', as\n        it projects data in meaningful directions (those of higher\n        variance). Otherwise, we just use 'identity'.\n\n    'pca'\n        ``n_components`` principal components of the inputs passed\n        to :meth:`fit` will be used to initialize the transformation.\n        (See :class:`~sklearn.decomposition.PCA`)\n\n    'lda'\n        ``min(n_components, n_classes)`` most discriminative\n        components of the inputs passed to :meth:`fit` will be used to\n        initialize the transformation. (If ``n_components > n_classes``,\n        the rest of the components will be zero.) (See\n        :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\n\n    'identity'\n        If ``n_components`` is strictly smaller than the\n        dimensionality of the inputs passed to :meth:`fit`, the identity\n        matrix will be truncated to the first ``n_components`` rows.\n\n    'random'\n        The initial transformation will be a random array of shape\n        `(n_components, n_features)`. Each value is sampled from the\n        standard normal distribution.\n\n    numpy array\n        n_features_b must match the dimensionality of the inputs passed to\n        :meth:`fit` and n_features_a must be less than or equal to that.\n        If ``n_components`` is not None, n_features_a must match it.\n\nwarm_start : bool, default=False\n    If True and :meth:`fit` has been called before, the solution of the\n    previous call to :meth:`fit` is used as the initial linear\n    transformation (``n_components`` and ``init`` will be ignored).\n\nmax_iter : int, default=50\n    Maximum number of iterations in the optimization.\n\ntol : float, default=1e-5\n    Convergence tolerance for the optimization.\n\ncallback : callable, default=None\n    If not None, this function is called after every iteration of the\n    optimizer, taking as arguments the current solution (flattened\n    transformation matrix) and the number of iterations. This might be\n    useful in case one wants to examine or store the transformation\n    found after each iteration.\n\nverbose : int, default=0\n    If 0, no progress messages will be printed.\n    If 1, progress messages will be printed to stdout.\n    If > 1, progress messages will be printed and the ``disp``\n    parameter of :func:`scipy.optimize.minimize` will be set to\n    ``verbose - 2``.\n\nrandom_state : int or numpy.RandomState, default=None\n    A pseudo random number generator object or a seed for it if int. If\n    ``init='random'``, ``random_state`` is used to initialize the random\n    transformation. If ``init='pca'``, ``random_state`` is passed as an\n    argument to PCA when initializing the transformation. Pass an int\n    for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    The linear transformation learned during fitting.\n\nn_iter_ : int\n    Counts the number of iterations performed by the optimizer.\n\nrandom_state_ : numpy.RandomState\n    Pseudo random number generator object used during initialization.\n\nExamples\n--------\n>>> from sklearn.neighbors import NeighborhoodComponentsAnalysis\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n... stratify=y, test_size=0.7, random_state=42)\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n>>> nca.fit(X_train, y_train)\nNeighborhoodComponentsAnalysis(...)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> knn.fit(X_train, y_train)\nKNeighborsClassifier(...)\n>>> print(knn.score(X_test, y_test))\n0.933333...\n>>> knn.fit(nca.transform(X_train), y_train)\nKNeighborsClassifier(...)\n>>> print(knn.score(nca.transform(X_test), y_test))\n0.961904...\n\nReferences\n----------\n.. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n       \"Neighbourhood Components Analysis\". Advances in Neural Information\n       Processing Systems. 17, 513-520, 2005.\n       http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n\n.. [2] Wikipedia entry on Neighborhood Components Analysis\n       https://en.wikipedia.org/wiki/Neighbourhood_components_analysis"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neighbors._nearest_centroid",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": "sp"
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_distances",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "csc_median_axis_0",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "NearestCentroid",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features.\nNote that centroid shrinking cannot be used with sparse matrices."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values (integers)"
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the NearestCentroid model according to the given training data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n    Note that centroid shrinking cannot be used with sparse matrices.\ny : array-like of shape (n_samples,)\n    Target values (integers)"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "C",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform classification on an array of test vectors X.\n\nThe predicted class C for each sample in X is returned.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nReturns\n-------\nC : ndarray of shape (n_samples,)\n\nNotes\n-----\nIf the metric constructor parameter is \"precomputed\", X is assumed to\nbe the distance matrix between the data to be predicted and\n``self.centroids_``."
            }
          ],
          "fullDocstring": "Nearest centroid classifier.\n\nEach class is represented by its centroid, with test samples classified to\nthe class with the nearest centroid.\n\nRead more in the :ref:`User Guide <nearest_centroid_classifier>`.\n\nParameters\n----------\nmetric : str or callable\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string or callable, it must be one of\n    the options allowed by metrics.pairwise.pairwise_distances for its\n    metric parameter.\n    The centroids for the samples corresponding to each class is the point\n    from which the sum of the distances (according to the metric) of all\n    samples that belong to that particular class are minimized.\n    If the \"manhattan\" metric is provided, this centroid is the median and\n    for all other metrics, the centroid is now set to be the mean.\n\n    .. versionchanged:: 0.19\n        ``metric='precomputed'`` was deprecated and now raises an error\n\nshrink_threshold : float, default=None\n    Threshold for shrinking centroids to remove features.\n\nAttributes\n----------\ncentroids_ : array-like of shape (n_classes, n_features)\n    Centroid of each class.\n\nclasses_ : array of shape (n_classes,)\n    The unique classes labels.\n\nExamples\n--------\n>>> from sklearn.neighbors import NearestCentroid\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = NearestCentroid()\n>>> clf.fit(X, y)\nNearestCentroid()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]\n\nSee Also\n--------\nKNeighborsClassifier : Nearest neighbors classifier.\n\nNotes\n-----\nWhen used for text classification with tf-idf vectors, this classifier is\nalso known as the Rocchio classifier.\n\nReferences\n----------\nTibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\nmultiple cancer types by shrunken centroids of gene expression. Proceedings\nof the National Academy of Sciences of the United States of America,\n99(10), 6567-6572. The National Academy of Sciences."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neighbors._regression",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "KNeighborsMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "NeighborsBase",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "RadiusNeighborsMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "_check_weights",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "_get_weights",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KNeighborsRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "KNeighborsMixin",
            "NeighborsBase"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted k-nearest neighbors regressor."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the k-nearest neighbors regressor from the training dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n    Training data.\n\ny : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n    Target values.\n\nReturns\n-------\nself : KNeighborsRegressor\n    The fitted k-nearest neighbors regressor."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Target values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the target for the provided data\n\nParameters\n----------\nX : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n    Test samples.\n\nReturns\n-------\ny : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n    Target values."
            }
          ],
          "fullDocstring": "Regression based on k-nearest neighbors.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n\n.. versionadded:: 0.9\n\nParameters\n----------\nn_neighbors : int, default=5\n    Number of neighbors to use by default for :meth:`kneighbors` queries.\n\nweights : {'uniform', 'distance'} or callable, default='uniform'\n    weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\np : int, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric : str or callable, default='minkowski'\n    the distance metric to use for the tree.  The default metric is\n    minkowski, and with p=2 is equivalent to the standard Euclidean\n    metric. See the documentation of :class:`DistanceMetric` for a\n    list of available metrics.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n\nAttributes\n----------\neffective_metric_ : str or callable\n    The distance metric to use. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsRegressor\n>>> neigh = KNeighborsRegressor(n_neighbors=2)\n>>> neigh.fit(X, y)\nKNeighborsRegressor(...)\n>>> print(neigh.predict([[1.5]]))\n[0.5]\n\nSee Also\n--------\nNearestNeighbors\nRadiusNeighborsRegressor\nKNeighborsClassifier\nRadiusNeighborsClassifier\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n.. warning::\n\n   Regarding the Nearest Neighbors algorithms, if it is found that two\n   neighbors, neighbor `k+1` and `k`, have identical distances but\n   different labels, the results will depend on the ordering of the\n   training data.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm"
        },
        {
          "name": "RadiusNeighborsRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "NeighborsBase",
            "RadiusNeighborsMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted radius neighbors regressor."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the radius neighbors regressor from the training dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n    Training data.\n\ny : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n    Target values.\n\nReturns\n-------\nself : RadiusNeighborsRegressor\n    The fitted radius neighbors regressor."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Test samples."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Target values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the target for the provided data\n\nParameters\n----------\nX : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n    Test samples.\n\nReturns\n-------\ny : ndarray of shape (n_queries,) or (n_queries, n_outputs),                 dtype=double\n    Target values."
            }
          ],
          "fullDocstring": "Regression based on neighbors within a fixed radius.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n\n.. versionadded:: 0.9\n\nParameters\n----------\nradius : float, default=1.0\n    Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries.\n\nweights : {'uniform', 'distance'} or callable, default='uniform'\n    weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\np : int, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric : str or callable, default='minkowski'\n    the distance metric to use for the tree.  The default metric is\n    minkowski, and with p=2 is equivalent to the standard Euclidean\n    metric. See the documentation of :class:`DistanceMetric` for a\n    list of available metrics.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\neffective_metric_ : str or callable\n    The distance metric to use. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsRegressor\n>>> neigh = RadiusNeighborsRegressor(radius=1.0)\n>>> neigh.fit(X, y)\nRadiusNeighborsRegressor(...)\n>>> print(neigh.predict([[1.5]]))\n[0.5]\n\nSee Also\n--------\nNearestNeighbors\nKNeighborsRegressor\nKNeighborsClassifier\nRadiusNeighborsClassifier\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neighbors._unsupervised",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.neighbors._base",
          "declaration": "KNeighborsMixin",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "NeighborsBase",
          "alias": null
        },
        {
          "module": "sklearn.neighbors._base",
          "declaration": "RadiusNeighborsMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "NearestNeighbors",
          "decorators": [],
          "superclasses": [
            "KNeighborsMixin",
            "NeighborsBase",
            "RadiusNeighborsMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Not used, present for API consistency by convention."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted nearest neighbors estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the nearest neighbors estimator from the training dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n    Training data.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : NearestNeighbors\n    The fitted nearest neighbors estimator."
            }
          ],
          "fullDocstring": "Unsupervised learner for implementing neighbor searches.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\n.. versionadded:: 0.9\n\nParameters\n----------\nn_neighbors : int, default=5\n    Number of neighbors to use by default for :meth:`kneighbors` queries.\n\nradius : float, default=1.0\n    Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\nmetric : str or callable, default='minkowski'\n    the distance metric to use for the tree.  The default metric is\n    minkowski, and with p=2 is equivalent to the standard Euclidean\n    metric. See the documentation of :class:`DistanceMetric` for a\n    list of available metrics.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\neffective_metric_ : str\n    Metric used to compute distances to neighbors.\n\neffective_metric_params_ : dict\n    Parameters for the metric used to compute distances to neighbors.\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.neighbors import NearestNeighbors\n>>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n\n>>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n>>> neigh.fit(samples)\nNearestNeighbors(...)\n\n>>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\narray([[2, 0]]...)\n\n>>> nbrs = neigh.radius_neighbors(\n...    [[0, 0, 1.3]], 0.4, return_distance=False\n... )\n>>> np.asarray(nbrs[0][0])\narray(2)\n\nSee Also\n--------\nKNeighborsClassifier\nRadiusNeighborsClassifier\nKNeighborsRegressor\nRadiusNeighborsRegressor\nBallTree\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neighbors.setup",
      "imports": [
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.neural_network",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._multilayer_perceptron",
          "declaration": "MLPClassifier",
          "alias": null
        },
        {
          "module": "sklearn._multilayer_perceptron",
          "declaration": "MLPRegressor",
          "alias": null
        },
        {
          "module": "sklearn._rbm",
          "declaration": "BernoulliRBM",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.neural_network._base",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": "logistic_sigmoid"
        },
        {
          "module": "scipy.special",
          "declaration": "xlogy",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "binary_log_loss",
          "decorators": [],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) labels."
            },
            {
              "name": "y_prob",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted probabilities, as returned by a classifier's\npredict_proba method."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": "float",
              "description": "The degree to which the samples are correctly predicted."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute binary logistic loss for classification.\n\nThis is identical to log_loss in binary classification case,\nbut is kept for its use in multilabel case.\n\nParameters\n----------\ny_true : array-like or label indicator matrix\n    Ground truth (correct) labels.\n\ny_prob : array-like of float, shape = (n_samples, 1)\n    Predicted probabilities, as returned by a classifier's\n    predict_proba method.\n\nReturns\n-------\nloss : float\n    The degree to which the samples are correctly predicted."
        },
        {
          "name": "inplace_identity",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data, where n_samples is the number of samples\nand n_features is the number of features."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Simply leave the input array unchanged.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Data, where n_samples is the number of samples\n    and n_features is the number of features."
        },
        {
          "name": "inplace_identity_derivative",
          "decorators": [],
          "parameters": [
            {
              "name": "Z",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data which was output from the identity activation function during\nthe forward pass."
            },
            {
              "name": "delta",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The backpropagated error signal to be modified inplace."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Apply the derivative of the identity function: do nothing.\n\nParameters\n----------\nZ : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The data which was output from the identity activation function during\n    the forward pass.\n\ndelta : {array-like}, shape (n_samples, n_features)\n     The backpropagated error signal to be modified inplace."
        },
        {
          "name": "inplace_logistic",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The input data."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the logistic function inplace.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The input data."
        },
        {
          "name": "inplace_logistic_derivative",
          "decorators": [],
          "parameters": [
            {
              "name": "Z",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data which was output from the logistic activation function during\nthe forward pass."
            },
            {
              "name": "delta",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The backpropagated error signal to be modified inplace."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Apply the derivative of the logistic sigmoid function.\n\nIt exploits the fact that the derivative is a simple function of the output\nvalue from logistic function.\n\nParameters\n----------\nZ : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The data which was output from the logistic activation function during\n    the forward pass.\n\ndelta : {array-like}, shape (n_samples, n_features)\n     The backpropagated error signal to be modified inplace."
        },
        {
          "name": "inplace_relu",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The input data."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the rectified linear unit function inplace.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The input data."
        },
        {
          "name": "inplace_relu_derivative",
          "decorators": [],
          "parameters": [
            {
              "name": "Z",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data which was output from the rectified linear units activation\nfunction during the forward pass."
            },
            {
              "name": "delta",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The backpropagated error signal to be modified inplace."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Apply the derivative of the relu function.\n\nIt exploits the fact that the derivative is a simple function of the output\nvalue from rectified linear units activation function.\n\nParameters\n----------\nZ : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The data which was output from the rectified linear units activation\n    function during the forward pass.\n\ndelta : {array-like}, shape (n_samples, n_features)\n     The backpropagated error signal to be modified inplace."
        },
        {
          "name": "inplace_softmax",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The input data."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the K-way softmax function inplace.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The input data."
        },
        {
          "name": "inplace_tanh",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The input data."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the hyperbolic tan function inplace.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The input data."
        },
        {
          "name": "inplace_tanh_derivative",
          "decorators": [],
          "parameters": [
            {
              "name": "Z",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data which was output from the hyperbolic tangent activation\nfunction during the forward pass."
            },
            {
              "name": "delta",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The backpropagated error signal to be modified inplace."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Apply the derivative of the hyperbolic tanh function.\n\nIt exploits the fact that the derivative is a simple function of the output\nvalue from hyperbolic tangent.\n\nParameters\n----------\nZ : {array-like, sparse matrix}, shape (n_samples, n_features)\n    The data which was output from the hyperbolic tangent activation\n    function during the forward pass.\n\ndelta : {array-like}, shape (n_samples, n_features)\n     The backpropagated error signal to be modified inplace."
        },
        {
          "name": "log_loss",
          "decorators": [],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) labels."
            },
            {
              "name": "y_prob",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted probabilities, as returned by a classifier's\npredict_proba method."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": "float",
              "description": "The degree to which the samples are correctly predicted."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute Logistic loss for classification.\n\nParameters\n----------\ny_true : array-like or label indicator matrix\n    Ground truth (correct) labels.\n\ny_prob : array-like of float, shape = (n_samples, n_classes)\n    Predicted probabilities, as returned by a classifier's\n    predict_proba method.\n\nReturns\n-------\nloss : float\n    The degree to which the samples are correctly predicted."
        },
        {
          "name": "squared_loss",
          "decorators": [],
          "parameters": [
            {
              "name": "y_true",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Ground truth (correct) values."
            },
            {
              "name": "y_pred",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Predicted values, as returned by a regression estimator."
            }
          ],
          "results": [
            {
              "name": "loss",
              "type": "float",
              "description": "The degree to which the samples are correctly predicted."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the squared loss for regression.\n\nParameters\n----------\ny_true : array-like or label indicator matrix\n    Ground truth (correct) values.\n\ny_pred : array-like or label indicator matrix\n    Predicted values, as returned by a regression estimator.\n\nReturns\n-------\nloss : float\n    The degree to which the samples are correctly predicted."
        }
      ]
    },
    {
      "name": "sklearn.neural_network._multilayer_perceptron",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.optimize",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "train_test_split",
          "alias": null
        },
        {
          "module": "sklearn.neural_network._base",
          "declaration": "ACTIVATIONS",
          "alias": null
        },
        {
          "module": "sklearn.neural_network._base",
          "declaration": "DERIVATIVES",
          "alias": null
        },
        {
          "module": "sklearn.neural_network._base",
          "declaration": "LOSS_FUNCTIONS",
          "alias": null
        },
        {
          "module": "sklearn.neural_network._stochastic_optimizers",
          "declaration": "AdamOptimizer",
          "alias": null
        },
        {
          "module": "sklearn.neural_network._stochastic_optimizers",
          "declaration": "SGDOptimizer",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_batches",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "shuffle",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "_check_partial_fit_first_call",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "type_of_target",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "unique_labels",
          "alias": null
        },
        {
          "module": "sklearn.utils.optimize",
          "declaration": "_check_optimize_result",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseMultilayerPerceptron",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (class labels in classification, real numbers in\nregression)."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model to data matrix X and target(s) y.\n\nParameters\n----------\nX : ndarray or sparse matrix of shape (n_samples, n_features)\n    The input data.\n\ny : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n    The target values (class labels in classification, real numbers in\n    regression).\n\nReturns\n-------\nself : returns a trained MLP model."
            },
            {
              "name": "partial_fit",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Update the model with a single iteration over the given data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input data.\n\ny : ndarray of shape (n_samples,)\n    The target values.\n\nReturns\n-------\nself : returns a trained MLP model."
            }
          ],
          "fullDocstring": "Base class for MLP classification and regression.\n\nWarning: This class should not be used directly.\nUse derived classes instead.\n\n.. versionadded:: 0.18"
        },
        {
          "name": "MLPClassifier",
          "decorators": [],
          "superclasses": [
            "BaseMultilayerPerceptron",
            "ClassifierMixin"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted classes."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the multi-layer perceptron classifier\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input data.\n\nReturns\n-------\ny : ndarray, shape (n_samples,) or (n_samples, n_classes)\n    The predicted classes."
            },
            {
              "name": "partial_fit",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Update the model with a single iteration over the given data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input data.\n\ny : array-like of shape (n_samples,)\n    The target values.\n\nclasses : array of shape (n_classes,), default=None\n    Classes across all calls to partial_fit.\n    Can be obtained via `np.unique(y_all)`, where y_all is the\n    target vector of the entire dataset.\n    This argument is required for the first call to partial_fit\n    and can be omitted in the subsequent calls.\n    Note that y doesn't need to contain all labels in `classes`.\n\nReturns\n-------\nself : returns a trained MLP model."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                }
              ],
              "results": [
                {
                  "name": "log_y_prob",
                  "type": null,
                  "description": "The predicted log-probability of the sample for each class\nin the model, where classes are ordered as they are in\n`self.classes_`. Equivalent to log(predict_proba(X))"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the log of probability estimates.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    The input data.\n\nReturns\n-------\nlog_y_prob : ndarray of shape (n_samples, n_classes)\n    The predicted log-probability of the sample for each class\n    in the model, where classes are ordered as they are in\n    `self.classes_`. Equivalent to log(predict_proba(X))"
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                }
              ],
              "results": [
                {
                  "name": "y_prob",
                  "type": null,
                  "description": "The predicted probability of the sample for each class in the\nmodel, where classes are ordered as they are in `self.classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Probability estimates.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input data.\n\nReturns\n-------\ny_prob : ndarray of shape (n_samples, n_classes)\n    The predicted probability of the sample for each class in the\n    model, where classes are ordered as they are in `self.classes_`."
            }
          ],
          "fullDocstring": "Multi-layer Perceptron classifier.\n\nThis model optimizes the log-loss function using LBFGS or stochastic\ngradient descent.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nhidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)\n    The ith element represents the number of neurons in the ith\n    hidden layer.\n\nactivation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n    Activation function for the hidden layer.\n\n    - 'identity', no-op activation, useful to implement linear bottleneck,\n      returns f(x) = x\n\n    - 'logistic', the logistic sigmoid function,\n      returns f(x) = 1 / (1 + exp(-x)).\n\n    - 'tanh', the hyperbolic tan function,\n      returns f(x) = tanh(x).\n\n    - 'relu', the rectified linear unit function,\n      returns f(x) = max(0, x)\n\nsolver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n    The solver for weight optimization.\n\n    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n\n    - 'sgd' refers to stochastic gradient descent.\n\n    - 'adam' refers to a stochastic gradient-based optimizer proposed\n      by Kingma, Diederik, and Jimmy Ba\n\n    Note: The default solver 'adam' works pretty well on relatively\n    large datasets (with thousands of training samples or more) in terms of\n    both training time and validation score.\n    For small datasets, however, 'lbfgs' can converge faster and perform\n    better.\n\nalpha : float, default=0.0001\n    L2 penalty (regularization term) parameter.\n\nbatch_size : int, default='auto'\n    Size of minibatches for stochastic optimizers.\n    If the solver is 'lbfgs', the classifier will not use minibatch.\n    When set to \"auto\", `batch_size=min(200, n_samples)`\n\nlearning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n    Learning rate schedule for weight updates.\n\n    - 'constant' is a constant learning rate given by\n      'learning_rate_init'.\n\n    - 'invscaling' gradually decreases the learning rate at each\n      time step 't' using an inverse scaling exponent of 'power_t'.\n      effective_learning_rate = learning_rate_init / pow(t, power_t)\n\n    - 'adaptive' keeps the learning rate constant to\n      'learning_rate_init' as long as training loss keeps decreasing.\n      Each time two consecutive epochs fail to decrease training loss by at\n      least tol, or fail to increase validation score by at least tol if\n      'early_stopping' is on, the current learning rate is divided by 5.\n\n    Only used when ``solver='sgd'``.\n\nlearning_rate_init : double, default=0.001\n    The initial learning rate used. It controls the step-size\n    in updating the weights. Only used when solver='sgd' or 'adam'.\n\npower_t : double, default=0.5\n    The exponent for inverse scaling learning rate.\n    It is used in updating effective learning rate when the learning_rate\n    is set to 'invscaling'. Only used when solver='sgd'.\n\nmax_iter : int, default=200\n    Maximum number of iterations. The solver iterates until convergence\n    (determined by 'tol') or this number of iterations. For stochastic\n    solvers ('sgd', 'adam'), note that this determines the number of epochs\n    (how many times each data point will be used), not the number of\n    gradient steps.\n\nshuffle : bool, default=True\n    Whether to shuffle samples in each iteration. Only used when\n    solver='sgd' or 'adam'.\n\nrandom_state : int, RandomState instance, default=None\n    Determines random number generation for weights and bias\n    initialization, train-test split if early stopping is used, and batch\n    sampling when solver='sgd' or 'adam'.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=1e-4\n    Tolerance for the optimization. When the loss or score is not improving\n    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n    unless ``learning_rate`` is set to 'adaptive', convergence is\n    considered to be reached and training stops.\n\nverbose : bool, default=False\n    Whether to print progress messages to stdout.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous\n    call to fit as initialization, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nmomentum : float, default=0.9\n    Momentum for gradient descent update. Should be between 0 and 1. Only\n    used when solver='sgd'.\n\nnesterovs_momentum : bool, default=True\n    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n    momentum > 0.\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to true, it will automatically set\n    aside 10% of training data as validation and terminate training when\n    validation score is not improving by at least tol for\n    ``n_iter_no_change`` consecutive epochs. The split is stratified,\n    except in a multilabel setting.\n    Only effective when solver='sgd' or 'adam'\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True\n\nbeta_1 : float, default=0.9\n    Exponential decay rate for estimates of first moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n\nbeta_2 : float, default=0.999\n    Exponential decay rate for estimates of second moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n\nepsilon : float, default=1e-8\n    Value for numerical stability in adam. Only used when solver='adam'\n\nn_iter_no_change : int, default=10\n    Maximum number of epochs to not meet ``tol`` improvement.\n    Only effective when solver='sgd' or 'adam'\n\n    .. versionadded:: 0.20\n\nmax_fun : int, default=15000\n    Only used when solver='lbfgs'. Maximum number of loss function calls.\n    The solver iterates until convergence (determined by 'tol'), number\n    of iterations reaches max_iter, or this number of loss function calls.\n    Note that number of loss function calls will be greater than or equal\n    to the number of iterations for the `MLPClassifier`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nclasses_ : ndarray or list of ndarray of shape (n_classes,)\n    Class labels for each output.\n\nloss_ : float\n    The current loss computed with the loss function.\n\nbest_loss_ : float\n    The minimum loss reached by the solver throughout fitting.\n\nloss_curve_ : list of shape (`n_iter_`,)\n    The ith element in the list represents the loss at the ith iteration.\n\nt_ : int\n    The number of training samples seen by the solver during fitting.\n\ncoefs_ : list of shape (n_layers - 1,)\n    The ith element in the list represents the weight matrix corresponding\n    to layer i.\n\nintercepts_ : list of shape (n_layers - 1,)\n    The ith element in the list represents the bias vector corresponding to\n    layer i + 1.\n\nn_iter_ : int\n    The number of iterations the solver has ran.\n\nn_layers_ : int\n    Number of layers.\n\nn_outputs_ : int\n    Number of outputs.\n\nout_activation_ : str\n    Name of the output activation function.\n\nExamples\n--------\n>>> from sklearn.neural_network import MLPClassifier\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = make_classification(n_samples=100, random_state=1)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n...                                                     random_state=1)\n>>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n>>> clf.predict_proba(X_test[:1])\narray([[0.038..., 0.961...]])\n>>> clf.predict(X_test[:5, :])\narray([1, 0, 1, 0, 1])\n>>> clf.score(X_test, y_test)\n0.8...\n\nNotes\n-----\nMLPClassifier trains iteratively since at each time step\nthe partial derivatives of the loss function with respect to the model\nparameters are computed to update the parameters.\n\nIt can also have a regularization term added to the loss function\nthat shrinks model parameters to prevent overfitting.\n\nThis implementation works with data represented as dense numpy arrays or\nsparse scipy arrays of floating point values.\n\nReferences\n----------\nHinton, Geoffrey E.\n    \"Connectionist learning procedures.\" Artificial intelligence 40.1\n    (1989): 185-234.\n\nGlorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n    training deep feedforward neural networks.\" International Conference\n    on Artificial Intelligence and Statistics. 2010.\n\nHe, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n    performance on imagenet classification.\" arXiv preprint\n    arXiv:1502.01852 (2015).\n\nKingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n    optimization.\" arXiv preprint arXiv:1412.6980 (2014)."
        },
        {
          "name": "MLPRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseMultilayerPerceptron"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict using the multi-layer perceptron model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input data.\n\nReturns\n-------\ny : ndarray of shape (n_samples, n_outputs)\n    The predicted values."
            }
          ],
          "fullDocstring": "Multi-layer Perceptron regressor.\n\nThis model optimizes the squared-loss using LBFGS or stochastic gradient\ndescent.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nhidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)\n    The ith element represents the number of neurons in the ith\n    hidden layer.\n\nactivation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n    Activation function for the hidden layer.\n\n    - 'identity', no-op activation, useful to implement linear bottleneck,\n      returns f(x) = x\n\n    - 'logistic', the logistic sigmoid function,\n      returns f(x) = 1 / (1 + exp(-x)).\n\n    - 'tanh', the hyperbolic tan function,\n      returns f(x) = tanh(x).\n\n    - 'relu', the rectified linear unit function,\n      returns f(x) = max(0, x)\n\nsolver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n    The solver for weight optimization.\n\n    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n\n    - 'sgd' refers to stochastic gradient descent.\n\n    - 'adam' refers to a stochastic gradient-based optimizer proposed by\n      Kingma, Diederik, and Jimmy Ba\n\n    Note: The default solver 'adam' works pretty well on relatively\n    large datasets (with thousands of training samples or more) in terms of\n    both training time and validation score.\n    For small datasets, however, 'lbfgs' can converge faster and perform\n    better.\n\nalpha : float, default=0.0001\n    L2 penalty (regularization term) parameter.\n\nbatch_size : int, default='auto'\n    Size of minibatches for stochastic optimizers.\n    If the solver is 'lbfgs', the classifier will not use minibatch.\n    When set to \"auto\", `batch_size=min(200, n_samples)`\n\nlearning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n    Learning rate schedule for weight updates.\n\n    - 'constant' is a constant learning rate given by\n      'learning_rate_init'.\n\n    - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n      at each time step 't' using an inverse scaling exponent of 'power_t'.\n      effective_learning_rate = learning_rate_init / pow(t, power_t)\n\n    - 'adaptive' keeps the learning rate constant to\n      'learning_rate_init' as long as training loss keeps decreasing.\n      Each time two consecutive epochs fail to decrease training loss by at\n      least tol, or fail to increase validation score by at least tol if\n      'early_stopping' is on, the current learning rate is divided by 5.\n\n    Only used when solver='sgd'.\n\nlearning_rate_init : double, default=0.001\n    The initial learning rate used. It controls the step-size\n    in updating the weights. Only used when solver='sgd' or 'adam'.\n\npower_t : double, default=0.5\n    The exponent for inverse scaling learning rate.\n    It is used in updating effective learning rate when the learning_rate\n    is set to 'invscaling'. Only used when solver='sgd'.\n\nmax_iter : int, default=200\n    Maximum number of iterations. The solver iterates until convergence\n    (determined by 'tol') or this number of iterations. For stochastic\n    solvers ('sgd', 'adam'), note that this determines the number of epochs\n    (how many times each data point will be used), not the number of\n    gradient steps.\n\nshuffle : bool, default=True\n    Whether to shuffle samples in each iteration. Only used when\n    solver='sgd' or 'adam'.\n\nrandom_state : int, RandomState instance, default=None\n    Determines random number generation for weights and bias\n    initialization, train-test split if early stopping is used, and batch\n    sampling when solver='sgd' or 'adam'.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=1e-4\n    Tolerance for the optimization. When the loss or score is not improving\n    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n    unless ``learning_rate`` is set to 'adaptive', convergence is\n    considered to be reached and training stops.\n\nverbose : bool, default=False\n    Whether to print progress messages to stdout.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous\n    call to fit as initialization, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nmomentum : float, default=0.9\n    Momentum for gradient descent update.  Should be between 0 and 1. Only\n    used when solver='sgd'.\n\nnesterovs_momentum : bool, default=True\n    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n    momentum > 0.\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to true, it will automatically set\n    aside 10% of training data as validation and terminate training when\n    validation score is not improving by at least ``tol`` for\n    ``n_iter_no_change`` consecutive epochs.\n    Only effective when solver='sgd' or 'adam'\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True\n\nbeta_1 : float, default=0.9\n    Exponential decay rate for estimates of first moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n\nbeta_2 : float, default=0.999\n    Exponential decay rate for estimates of second moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'\n\nepsilon : float, default=1e-8\n    Value for numerical stability in adam. Only used when solver='adam'\n\nn_iter_no_change : int, default=10\n    Maximum number of epochs to not meet ``tol`` improvement.\n    Only effective when solver='sgd' or 'adam'\n\n    .. versionadded:: 0.20\n\nmax_fun : int, default=15000\n    Only used when solver='lbfgs'. Maximum number of function calls.\n    The solver iterates until convergence (determined by 'tol'), number\n    of iterations reaches max_iter, or this number of function calls.\n    Note that number of function calls will be greater than or equal to\n    the number of iterations for the MLPRegressor.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nloss_ : float\n    The current loss computed with the loss function.\n\nbest_loss_ : float\n    The minimum loss reached by the solver throughout fitting.\n\nloss_curve_ : list of shape (`n_iter_`,)\n    The ith element in the list represents the loss at the ith iteration.\n\nt_ : int\n    The number of training samples seen by the solver during fitting.\n\ncoefs_ : list of shape (n_layers - 1,)\n    The ith element in the list represents the weight matrix corresponding\n    to layer i.\n\nintercepts_ : list of shape (n_layers - 1,)\n    The ith element in the list represents the bias vector corresponding to\n    layer i + 1.\n\nn_iter_ : int\n    The number of iterations the solver has ran.\n\nn_layers_ : int\n    Number of layers.\n\nn_outputs_ : int\n    Number of outputs.\n\nout_activation_ : str\n    Name of the output activation function.\n\nloss_curve_ : list of shape (n_iters,)\n    Loss value evaluated at the end of each training step.\n\nt_ : int\n    Mathematically equals `n_iters * X.shape[0]`, it means\n    `time_step` and it is used by optimizer's learning rate scheduler.\n\nExamples\n--------\n>>> from sklearn.neural_network import MLPRegressor\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = make_regression(n_samples=200, random_state=1)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n...                                                     random_state=1)\n>>> regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n>>> regr.predict(X_test[:2])\narray([-0.9..., -7.1...])\n>>> regr.score(X_test, y_test)\n0.4...\n\nNotes\n-----\nMLPRegressor trains iteratively since at each time step\nthe partial derivatives of the loss function with respect to the model\nparameters are computed to update the parameters.\n\nIt can also have a regularization term added to the loss function\nthat shrinks model parameters to prevent overfitting.\n\nThis implementation works with data represented as dense and sparse numpy\narrays of floating point values.\n\nReferences\n----------\nHinton, Geoffrey E.\n    \"Connectionist learning procedures.\" Artificial intelligence 40.1\n    (1989): 185-234.\n\nGlorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n    training deep feedforward neural networks.\" International Conference\n    on Artificial Intelligence and Statistics. 2010.\n\nHe, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n    performance on imagenet classification.\" arXiv preprint\n    arXiv:1502.01852 (2015).\n\nKingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n    optimization.\" arXiv preprint arXiv:1412.6980 (2014)."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neural_network._rbm",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "time",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy.special",
          "declaration": "expit",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "gen_even_slices",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "log_logistic",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BernoulliRBM",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to be transformed."
                }
              ],
              "results": [
                {
                  "name": "h",
                  "type": null,
                  "description": "Latent representations of the data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the hidden layer activation probabilities, P(h=1|v=X).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to be transformed.\n\nReturns\n-------\nh : ndarray of shape (n_samples, n_components)\n    Latent representations of the data."
            },
            {
              "name": "gibbs",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "v",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Values of the visible layer to start from."
                }
              ],
              "results": [
                {
                  "name": "v_new",
                  "type": null,
                  "description": "Values of the visible layer after one Gibbs step."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform one Gibbs sampling step.\n\nParameters\n----------\nv : ndarray of shape (n_samples, n_features)\n    Values of the visible layer to start from.\n\nReturns\n-------\nv_new : ndarray of shape (n_samples, n_features)\n    Values of the visible layer after one Gibbs step."
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted model."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model to the data X which should contain a partial\nsegment of the data.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Training data.\n\nReturns\n-------\nself : BernoulliRBM\n    The fitted model."
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Values of the visible layer. Must be all-boolean (not checked)."
                }
              ],
              "results": [
                {
                  "name": "pseudo_likelihood",
                  "type": null,
                  "description": "Value of the pseudo-likelihood (proxy for likelihood)."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the pseudo-likelihood of X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Values of the visible layer. Must be all-boolean (not checked).\n\nReturns\n-------\npseudo_likelihood : ndarray of shape (n_samples,)\n    Value of the pseudo-likelihood (proxy for likelihood).\n\nNotes\n-----\nThis method is not deterministic: it computes a quantity called the\nfree energy on X, then on a randomly corrupted version of X, and\nreturns the log of the logistic function of the difference."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "The fitted model."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model to the data X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\nReturns\n-------\nself : BernoulliRBM\n    The fitted model."
            }
          ],
          "fullDocstring": "Bernoulli Restricted Boltzmann Machine (RBM).\n\nA Restricted Boltzmann Machine with binary visible units and\nbinary hidden units. Parameters are estimated using Stochastic Maximum\nLikelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n[2].\n\nThe time complexity of this implementation is ``O(d ** 2)`` assuming\nd ~ n_features ~ n_components.\n\nRead more in the :ref:`User Guide <rbm>`.\n\nParameters\n----------\nn_components : int, default=256\n    Number of binary hidden units.\n\nlearning_rate : float, default=0.1\n    The learning rate for weight updates. It is *highly* recommended\n    to tune this hyper-parameter. Reasonable values are in the\n    10**[0., -3.] range.\n\nbatch_size : int, default=10\n    Number of examples per minibatch.\n\nn_iter : int, default=10\n    Number of iterations/sweeps over the training dataset to perform\n    during training.\n\nverbose : int, default=0\n    The verbosity level. The default, zero, means silent mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for:\n\n    - Gibbs sampling from visible and hidden layers.\n\n    - Initializing components, sampling from layers during fit.\n\n    - Corrupting the data when scoring samples.\n\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nintercept_hidden_ : array-like of shape (n_components,)\n    Biases of the hidden units.\n\nintercept_visible_ : array-like of shape (n_features,)\n    Biases of the visible units.\n\ncomponents_ : array-like of shape (n_components, n_features)\n    Weight matrix, where n_features in the number of\n    visible units and n_components is the number of hidden units.\n\nh_samples_ : array-like of shape (batch_size, n_components)\n    Hidden Activation sampled from the model distribution,\n    where batch_size in the number of examples per minibatch and\n    n_components is the number of hidden units.\n\nExamples\n--------\n\n>>> import numpy as np\n>>> from sklearn.neural_network import BernoulliRBM\n>>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n>>> model = BernoulliRBM(n_components=2)\n>>> model.fit(X)\nBernoulliRBM(n_components=2)\n\nReferences\n----------\n\n[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for\n    deep belief nets. Neural Computation 18, pp 1527-1554.\n    https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\n\n[2] Tieleman, T. Training Restricted Boltzmann Machines using\n    Approximations to the Likelihood Gradient. International Conference\n    on Machine Learning (ICML) 2008"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.neural_network._stochastic_optimizers",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [],
      "classes": [
        {
          "name": "AdamOptimizer",
          "decorators": [],
          "superclasses": [
            "BaseOptimizer"
          ],
          "methods": [],
          "fullDocstring": "Stochastic gradient descent optimizer with Adam\n\nNote: All default values are from the original Adam paper\n\nParameters\n----------\nparams : list, length = len(coefs_) + len(intercepts_)\n    The concatenated list containing coefs_ and intercepts_ in MLP model.\n    Used for initializing velocities and updating params\n\nlearning_rate_init : float, default=0.001\n    The initial learning rate used. It controls the step-size in updating\n    the weights\n\nbeta_1 : float, default=0.9\n    Exponential decay rate for estimates of first moment vector, should be\n    in [0, 1)\n\nbeta_2 : float, default=0.999\n    Exponential decay rate for estimates of second moment vector, should be\n    in [0, 1)\n\nepsilon : float, default=1e-8\n    Value for numerical stability\n\nAttributes\n----------\nlearning_rate : float\n    The current learning rate\n\nt : int\n    Timestep\n\nms : list, length = len(params)\n    First moment vectors\n\nvs : list, length = len(params)\n    Second moment vectors\n\nReferences\n----------\nKingma, Diederik, and Jimmy Ba.\n\"Adam: A method for stochastic optimization.\"\narXiv preprint arXiv:1412.6980 (2014)."
        },
        {
          "name": "BaseOptimizer",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "update_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "grads",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Containing gradients with respect to coefs_ and intercepts_ in MLP\nmodel. So length should be aligned with params"
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Update parameters with given gradients\n\nParameters\n----------\ngrads : list, length = len(params)\n    Containing gradients with respect to coefs_ and intercepts_ in MLP\n    model. So length should be aligned with params"
            },
            {
              "name": "iteration_ends",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "time_step",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform update to learning rate and potentially other states at the\nend of an iteration"
            },
            {
              "name": "trigger_stopping",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "msg",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Message passed in for verbose output"
                },
                {
                  "name": "verbose",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Print message to stdin if True"
                }
              ],
              "results": [
                {
                  "name": "is_stopping",
                  "type": "bool",
                  "description": "True if training needs to stop"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Decides whether it is time to stop training\n\nParameters\n----------\nmsg : str\n    Message passed in for verbose output\n\nverbose : bool\n    Print message to stdin if True\n\nReturns\n-------\nis_stopping : bool\n    True if training needs to stop"
            }
          ],
          "fullDocstring": "Base (Stochastic) gradient descent optimizer\n\nParameters\n----------\nparams : list, length = len(coefs_) + len(intercepts_)\n    The concatenated list containing coefs_ and intercepts_ in MLP model.\n    Used for initializing velocities and updating params\n\nlearning_rate_init : float, default=0.1\n    The initial learning rate used. It controls the step-size in updating\n    the weights\n\nAttributes\n----------\nlearning_rate : float\n    the current learning rate"
        },
        {
          "name": "SGDOptimizer",
          "decorators": [],
          "superclasses": [
            "BaseOptimizer"
          ],
          "methods": [
            {
              "name": "iteration_ends",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "time_step",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "number of training samples trained on so far, used to update\nlearning rate for 'invscaling'"
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform updates to learning rate and potential other states at the\nend of an iteration\n\nParameters\n----------\ntime_step : int\n    number of training samples trained on so far, used to update\n    learning rate for 'invscaling'"
            },
            {
              "name": "trigger_stopping",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "msg",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "verbose",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Stochastic gradient descent optimizer with momentum\n\nParameters\n----------\nparams : list, length = len(coefs_) + len(intercepts_)\n    The concatenated list containing coefs_ and intercepts_ in MLP model.\n    Used for initializing velocities and updating params\n\nlearning_rate_init : float, default=0.1\n    The initial learning rate used. It controls the step-size in updating\n    the weights\n\nlr_schedule : {'constant', 'adaptive', 'invscaling'}, default='constant'\n    Learning rate schedule for weight updates.\n\n    -'constant', is a constant learning rate given by\n     'learning_rate_init'.\n\n    -'invscaling' gradually decreases the learning rate 'learning_rate_' at\n      each time step 't' using an inverse scaling exponent of 'power_t'.\n      learning_rate_ = learning_rate_init / pow(t, power_t)\n\n    -'adaptive', keeps the learning rate constant to\n     'learning_rate_init' as long as the training keeps decreasing.\n     Each time 2 consecutive epochs fail to decrease the training loss by\n     tol, or fail to increase validation score by tol if 'early_stopping'\n     is on, the current learning rate is divided by 5.\n\nmomentum : float, default=0.9\n    Value of momentum used, must be larger than or equal to 0\n\nnesterov : bool, default=True\n    Whether to use nesterov's momentum or not. Use nesterov's if True\n\npower_t : float, default=0.5\n    Power of time step 't' in inverse scaling. See `lr_schedule` for\n    more details.\n\nAttributes\n----------\nlearning_rate : float\n    the current learning rate\n\nvelocities : list, length = len(params)\n    velocities that are used to update params"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.pipeline",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "defaultdict",
          "alias": null
        },
        {
          "module": "itertools",
          "declaration": "islice",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_print_elapsed_time",
          "alias": null
        },
        {
          "module": "sklearn.utils._estimator_html_repr",
          "declaration": "_VisualBlock",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_safe_tags",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "_BaseComposition",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "if_delegate_has_method",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_memory",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "FeatureUnion",
          "decorators": [],
          "superclasses": [
            "_BaseComposition",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
                }
              ],
              "results": [
                {
                  "name": "params",
                  "type": null,
                  "description": "Parameter names mapped to their values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the\nestimators contained within the `transformer_list` of the\n`FeatureUnion`.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : mapping of string to any\n    Parameter names mapped to their values."
            },
            {
              "name": "set_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``. Note that\nyou can directly set the parameters of the estimators contained in\n`tranformer_list`.\n\nReturns\n-------\nself"
            },
            {
              "name": "get_feature_names",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "feature_names",
                  "type": null,
                  "description": "Names of the features produced by transform."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get feature names from all transformers.\n\nReturns\n-------\nfeature_names : list of strings\n    Names of the features produced by transform."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, used to fit transformers."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Targets for supervised learning."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "This estimator"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit all transformers using X.\n\nParameters\n----------\nX : iterable or array-like, depending on transformers\n    Input data, used to fit transformers.\n\ny : array-like of shape (n_samples, n_outputs), default=None\n    Targets for supervised learning.\n\nReturns\n-------\nself : FeatureUnion\n    This estimator"
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data to be transformed."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Targets for supervised learning."
                }
              ],
              "results": [
                {
                  "name": "X_t",
                  "type": null,
                  "description": "hstack of results of transformers. sum_n_components is the\nsum of n_components (output dimension) over transformers."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit all transformers, transform the data and concatenate results.\n\nParameters\n----------\nX : iterable or array-like, depending on transformers\n    Input data to be transformed.\n\ny : array-like of shape (n_samples, n_outputs), default=None\n    Targets for supervised learning.\n\nReturns\n-------\nX_t : array-like or sparse matrix of                 shape (n_samples, sum_n_components)\n    hstack of results of transformers. sum_n_components is the\n    sum of n_components (output dimension) over transformers."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data to be transformed."
                }
              ],
              "results": [
                {
                  "name": "X_t",
                  "type": null,
                  "description": "hstack of results of transformers. sum_n_components is the\nsum of n_components (output dimension) over transformers."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X separately by each transformer, concatenate results.\n\nParameters\n----------\nX : iterable or array-like, depending on transformers\n    Input data to be transformed.\n\nReturns\n-------\nX_t : array-like or sparse matrix of                 shape (n_samples, sum_n_components)\n    hstack of results of transformers. sum_n_components is the\n    sum of n_components (output dimension) over transformers."
            },
            {
              "name": "n_features_in_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Concatenates results of multiple transformer objects.\n\nThis estimator applies a list of transformer objects in parallel to the\ninput data, then concatenates the results. This is useful to combine\nseveral feature extraction mechanisms into a single transformer.\n\nParameters of the transformers may be set using its name and the parameter\nname separated by a '__'. A transformer may be replaced entirely by\nsetting the parameter with its name to another transformer,\nor removed by setting to 'drop'.\n\nRead more in the :ref:`User Guide <feature_union>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\ntransformer_list : list of (string, transformer) tuples\n    List of transformer objects to be applied to the data. The first\n    half of each tuple is the name of the transformer. The tranformer can\n    be 'drop' for it to be ignored.\n\n    .. versionchanged:: 0.22\n       Deprecated `None` as a transformer in favor of 'drop'.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\ntransformer_weights : dict, default=None\n    Multiplicative weights for features per transformer.\n    Keys are transformer names, values the weights.\n    Raises ValueError if key not present in ``transformer_list``.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each transformer will be\n    printed as it is completed.\n\nSee Also\n--------\nmake_union : Convenience function for simplified feature union\n    construction.\n\nExamples\n--------\n>>> from sklearn.pipeline import FeatureUnion\n>>> from sklearn.decomposition import PCA, TruncatedSVD\n>>> union = FeatureUnion([(\"pca\", PCA(n_components=1)),\n...                       (\"svd\", TruncatedSVD(n_components=2))])\n>>> X = [[0., 1., 3], [2., 2., 5]]\n>>> union.fit_transform(X)\narray([[ 1.5       ,  3.0...,  0.8...],\n       [-1.5       ,  5.7..., -0.4...]])"
        },
        {
          "name": "Pipeline",
          "decorators": [],
          "superclasses": [
            "_BaseComposition"
          ],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
                }
              ],
              "results": [
                {
                  "name": "params",
                  "type": null,
                  "description": "Parameter names mapped to their values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Get parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the\nestimators contained within the `steps` of the `Pipeline`.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : mapping of string to any\n    Parameter names mapped to their values."
            },
            {
              "name": "set_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``. Note that\nyou can directly set the parameters of the estimators contained in\n`steps`.\n\nReturns\n-------\nself"
            },
            {
              "name": "named_steps",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data. Must fulfill input requirements of first step of the\npipeline."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training targets. Must fulfill label requirements for all steps of\nthe pipeline."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "This estimator"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model\n\nFit all the transforms one after the other and transform the\ndata, then fit the transformed data using the final estimator.\n\nParameters\n----------\nX : iterable\n    Training data. Must fulfill input requirements of first step of the\n    pipeline.\n\ny : iterable, default=None\n    Training targets. Must fulfill label requirements for all steps of\n    the pipeline.\n\n**fit_params : dict of string -> object\n    Parameters passed to the ``fit`` method of each step, where\n    each parameter name is prefixed such that parameter ``p`` for step\n    ``s`` has key ``s__p``.\n\nReturns\n-------\nself : Pipeline\n    This estimator"
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data. Must fulfill input requirements of first step of the\npipeline."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training targets. Must fulfill label requirements for all steps of\nthe pipeline."
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "Transformed samples"
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model and transform with the final estimator\n\nFits all the transforms one after the other and transforms the\ndata, then uses fit_transform on transformed data with the final\nestimator.\n\nParameters\n----------\nX : iterable\n    Training data. Must fulfill input requirements of first step of the\n    pipeline.\n\ny : iterable, default=None\n    Training targets. Must fulfill label requirements for all steps of\n    the pipeline.\n\n**fit_params : dict of string -> object\n    Parameters passed to the ``fit`` method of each step, where\n    each parameter name is prefixed such that parameter ``p`` for step\n    ``s`` has key ``s__p``.\n\nReturns\n-------\nXt : array-like of shape  (n_samples, n_transformed_features)\n    Transformed samples"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to predict on. Must fulfill input requirements of first step\nof the pipeline."
                }
              ],
              "results": [
                {
                  "name": "y_pred",
                  "type": "ArrayLike",
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply transforms to the data, and predict with the final estimator\n\nParameters\n----------\nX : iterable\n    Data to predict on. Must fulfill input requirements of first step\n    of the pipeline.\n\n**predict_params : dict of string -> object\n    Parameters to the ``predict`` called at the end of all\n    transformations in the pipeline. Note that while this may be\n    used to return uncertainties from some models with return_std\n    or return_cov, uncertainties that are generated by the\n    transformations in the pipeline are not propagated to the\n    final estimator.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ny_pred : array-like"
            },
            {
              "name": "fit_predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training data. Must fulfill input requirements of first step of\nthe pipeline."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training targets. Must fulfill label requirements for all steps\nof the pipeline."
                }
              ],
              "results": [
                {
                  "name": "y_pred",
                  "type": "ArrayLike",
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Applies fit_predict of last step in pipeline after transforms.\n\nApplies fit_transforms of a pipeline to the data, followed by the\nfit_predict method of the final estimator in the pipeline. Valid\nonly if the final estimator implements fit_predict.\n\nParameters\n----------\nX : iterable\n    Training data. Must fulfill input requirements of first step of\n    the pipeline.\n\ny : iterable, default=None\n    Training targets. Must fulfill label requirements for all steps\n    of the pipeline.\n\n**fit_params : dict of string -> object\n    Parameters passed to the ``fit`` method of each step, where\n    each parameter name is prefixed such that parameter ``p`` for step\n    ``s`` has key ``s__p``.\n\nReturns\n-------\ny_pred : array-like"
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to predict on. Must fulfill input requirements of first step\nof the pipeline."
                }
              ],
              "results": [
                {
                  "name": "y_proba",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply transforms, and predict_proba of the final estimator\n\nParameters\n----------\nX : iterable\n    Data to predict on. Must fulfill input requirements of first step\n    of the pipeline.\n\nReturns\n-------\ny_proba : array-like of shape (n_samples, n_classes)"
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to predict on. Must fulfill input requirements of first step\nof the pipeline."
                }
              ],
              "results": [
                {
                  "name": "y_score",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply transforms, and decision_function of the final estimator\n\nParameters\n----------\nX : iterable\n    Data to predict on. Must fulfill input requirements of first step\n    of the pipeline.\n\nReturns\n-------\ny_score : array-like of shape (n_samples, n_classes)"
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to predict on. Must fulfill input requirements of first step\nof the pipeline."
                }
              ],
              "results": [
                {
                  "name": "y_score",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply transforms, and score_samples of the final estimator.\n\nParameters\n----------\nX : iterable\n    Data to predict on. Must fulfill input requirements of first step\n    of the pipeline.\n\nReturns\n-------\ny_score : ndarray of shape (n_samples,)"
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to predict on. Must fulfill input requirements of first step\nof the pipeline."
                }
              ],
              "results": [
                {
                  "name": "y_score",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply transforms, and predict_log_proba of the final estimator\n\nParameters\n----------\nX : iterable\n    Data to predict on. Must fulfill input requirements of first step\n    of the pipeline.\n\nReturns\n-------\ny_score : array-like of shape (n_samples, n_classes)"
            },
            {
              "name": "transform",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply transforms, and transform with the final estimator\n\nThis also works where final estimator is ``None``: all prior\ntransformations are applied.\n\nParameters\n----------\nX : iterable\n    Data to transform. Must fulfill input requirements of first step\n    of the pipeline.\n\nReturns\n-------\nXt : array-like of shape  (n_samples, n_transformed_features)"
            },
            {
              "name": "inverse_transform",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply inverse transformations in reverse order\n\nAll estimators in the pipeline must support ``inverse_transform``.\n\nParameters\n----------\nXt : array-like of shape  (n_samples, n_transformed_features)\n    Data samples, where ``n_samples`` is the number of samples and\n    ``n_features`` is the number of features. Must fulfill\n    input requirements of last step of pipeline's\n    ``inverse_transform`` method.\n\nReturns\n-------\nXt : array-like of shape (n_samples, n_features)"
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to predict on. Must fulfill input requirements of first step\nof the pipeline."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Targets used for scoring. Must fulfill label requirements for all\nsteps of the pipeline."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "If not None, this argument is passed as ``sample_weight`` keyword\nargument to the ``score`` method of the final estimator."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply transforms, and score with the final estimator\n\nParameters\n----------\nX : iterable\n    Data to predict on. Must fulfill input requirements of first step\n    of the pipeline.\n\ny : iterable, default=None\n    Targets used for scoring. Must fulfill label requirements for all\n    steps of the pipeline.\n\nsample_weight : array-like, default=None\n    If not None, this argument is passed as ``sample_weight`` keyword\n    argument to the ``score`` method of the final estimator.\n\nReturns\n-------\nscore : float"
            },
            {
              "name": "classes_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "n_features_in_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Pipeline of transforms with a final estimator.\n\nSequentially apply a list of transforms and a final estimator.\nIntermediate steps of the pipeline must be 'transforms', that is, they\nmust implement fit and transform methods.\nThe final estimator only needs to implement fit.\nThe transformers in the pipeline can be cached using ``memory`` argument.\n\nThe purpose of the pipeline is to assemble several steps that can be\ncross-validated together while setting different parameters.\nFor this, it enables setting parameters of the various steps using their\nnames and the parameter name separated by a '__', as in the example below.\nA step's estimator may be replaced entirely by setting the parameter\nwith its name to another estimator, or a transformer removed by setting\nit to 'passthrough' or ``None``.\n\nRead more in the :ref:`User Guide <pipeline>`.\n\n.. versionadded:: 0.5\n\nParameters\n----------\nsteps : list\n    List of (name, transform) tuples (implementing fit/transform) that are\n    chained, in the order in which they are chained, with the last object\n    an estimator.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the fitted transformers of the pipeline. By default,\n    no caching is performed. If a string is given, it is the path to\n    the caching directory. Enabling caching triggers a clone of\n    the transformers before fitting. Therefore, the transformer\n    instance given to the pipeline cannot be inspected\n    directly. Use the attribute ``named_steps`` or ``steps`` to\n    inspect estimators within the pipeline. Caching the\n    transformers is advantageous when fitting is time consuming.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each step will be printed as it\n    is completed.\n\nAttributes\n----------\nnamed_steps : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n    Read-only attribute to access any step parameter by user given name.\n    Keys are step names and values are steps parameters.\n\nSee Also\n--------\nmake_pipeline : Convenience function for simplified pipeline construction.\n\nExamples\n--------\n>>> from sklearn.svm import SVC\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.pipeline import Pipeline\n>>> X, y = make_classification(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n...                                                     random_state=0)\n>>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n>>> # The pipeline can be used as any other estimator\n>>> # and avoids leaking the test set into the train set\n>>> pipe.fit(X_train, y_train)\nPipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\n>>> pipe.score(X_test, y_test)\n0.88"
        }
      ],
      "functions": [
        {
          "name": "make_pipeline",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "p",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Construct a Pipeline from the given estimators.\n\nThis is a shorthand for the Pipeline constructor; it does not require, and\ndoes not permit, naming the estimators. Instead, their names will be set\nto the lowercase of their types automatically.\n\nParameters\n----------\n*steps : list of estimators.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the fitted transformers of the pipeline. By default,\n    no caching is performed. If a string is given, it is the path to\n    the caching directory. Enabling caching triggers a clone of\n    the transformers before fitting. Therefore, the transformer\n    instance given to the pipeline cannot be inspected\n    directly. Use the attribute ``named_steps`` or ``steps`` to\n    inspect estimators within the pipeline. Caching the\n    transformers is advantageous when fitting is time consuming.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each step will be printed as it\n    is completed.\n\nSee Also\n--------\nPipeline : Class for creating a pipeline of transforms with a final\n    estimator.\n\nExamples\n--------\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.preprocessing import StandardScaler\n>>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('gaussiannb', GaussianNB())])\n\nReturns\n-------\np : Pipeline"
        },
        {
          "name": "make_union",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "f",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Construct a FeatureUnion from the given transformers.\n\nThis is a shorthand for the FeatureUnion constructor; it does not require,\nand does not permit, naming the transformers. Instead, they will be given\nnames automatically based on their types. It also does not allow weighting.\n\nParameters\n----------\n*transformers : list of estimators\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each transformer will be\n    printed as it is completed.\n\nReturns\n-------\nf : FeatureUnion\n\nSee Also\n--------\nFeatureUnion : Class for concatenating the results of multiple transformer\n    objects.\n\nExamples\n--------\n>>> from sklearn.decomposition import PCA, TruncatedSVD\n>>> from sklearn.pipeline import make_union\n>>> make_union(PCA(), TruncatedSVD())\n FeatureUnion(transformer_list=[('pca', PCA()),\n                               ('truncatedsvd', TruncatedSVD())])"
        }
      ]
    },
    {
      "name": "sklearn.preprocessing",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._data",
          "declaration": "Binarizer",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "KernelCenterer",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "MaxAbsScaler",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "MinMaxScaler",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "Normalizer",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "PolynomialFeatures",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "PowerTransformer",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "QuantileTransformer",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "RobustScaler",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "StandardScaler",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "add_dummy_feature",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "binarize",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "maxabs_scale",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "minmax_scale",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "normalize",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "power_transform",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "quantile_transform",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "robust_scale",
          "alias": null
        },
        {
          "module": "sklearn._data",
          "declaration": "scale",
          "alias": null
        },
        {
          "module": "sklearn._discretization",
          "declaration": "KBinsDiscretizer",
          "alias": null
        },
        {
          "module": "sklearn._encoders",
          "declaration": "OneHotEncoder",
          "alias": null
        },
        {
          "module": "sklearn._encoders",
          "declaration": "OrdinalEncoder",
          "alias": null
        },
        {
          "module": "sklearn._function_transformer",
          "declaration": "FunctionTransformer",
          "alias": null
        },
        {
          "module": "sklearn._label",
          "declaration": "LabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn._label",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn._label",
          "declaration": "MultiLabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn._label",
          "declaration": "label_binarize",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.preprocessing._data",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "itertools",
          "declaration": "chain",
          "alias": null
        },
        {
          "module": "itertools",
          "declaration": "combinations",
          "alias": null
        },
        {
          "module": "itertools",
          "declaration": "combinations_with_replacement",
          "alias": "combinations_w_r"
        },
        {
          "module": "scipy",
          "declaration": "optimize",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "stats",
          "alias": null
        },
        {
          "module": "scipy.special",
          "declaration": "boxcox",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing._csr_polynomial_expansion",
          "declaration": "_csr_polynomial_expansion",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing._encoders",
          "declaration": "OneHotEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "_incremental_mean_and_var",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "_incremental_weighted_mean_and_var",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "incr_mean_variance_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "inplace_column_scale",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "mean_variance_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "min_max_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs_fast",
          "declaration": "inplace_csr_row_normalize_l1",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs_fast",
          "declaration": "inplace_csr_row_normalize_l2",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "FLOAT_DTYPES",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_random_state",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "Binarizer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Do nothing and return the estimator unchanged.\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to binarize, element by element.\nscipy.sparse matrices should be in CSR format to avoid an\nun-necessary copy."
                },
                {
                  "name": "copy",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Copy the input X or not."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Binarize each element of X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to binarize, element by element.\n    scipy.sparse matrices should be in CSR format to avoid an\n    un-necessary copy.\n\ncopy : bool\n    Copy the input X or not.\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Transformed array."
            }
          ],
          "fullDocstring": "Binarize data (set feature values to 0 or 1) according to a threshold.\n\nValues greater than the threshold map to 1, while values less than\nor equal to the threshold map to 0. With the default threshold of 0,\nonly positive values map to 1.\n\nBinarization is a common operation on text count data where the\nanalyst can decide to only consider the presence or absence of a\nfeature rather than a quantified number of occurrences for instance.\n\nIt can also be used as a pre-processing step for estimators that\nconsider boolean random variables (e.g. modelled using the Bernoulli\ndistribution in a Bayesian setting).\n\nRead more in the :ref:`User Guide <preprocessing_binarization>`.\n\nParameters\n----------\nthreshold : float, default=0.0\n    Feature values below or equal to this are replaced by 0, above it by 1.\n    Threshold may not be less than 0 for operations on sparse matrices.\n\ncopy : bool, default=True\n    set to False to perform inplace binarization and avoid a copy (if\n    the input is already a numpy array or a scipy.sparse CSR matrix).\n\nExamples\n--------\n>>> from sklearn.preprocessing import Binarizer\n>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n>>> transformer = Binarizer().fit(X)  # fit does nothing.\n>>> transformer\nBinarizer()\n>>> transformer.transform(X)\narray([[1., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.]])\n\nNotes\n-----\nIf the input is a sparse matrix, only the non-zero values are subject\nto update by the Binarizer class.\n\nThis estimator is stateless (besides constructor parameters), the\nfit method does nothing but is useful when used in a pipeline.\n\nSee Also\n--------\nbinarize : Equivalent function without the estimator API."
        },
        {
          "name": "KernelCenterer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "K",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Kernel matrix."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit KernelCenterer\n\nParameters\n----------\nK : ndarray of shape (n_samples, n_samples)\n    Kernel matrix.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "K",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Kernel matrix."
                },
                {
                  "name": "copy",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Set to False to perform inplace computation."
                }
              ],
              "results": [
                {
                  "name": "K_new",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Center kernel matrix.\n\nParameters\n----------\nK : ndarray of shape (n_samples1, n_samples2)\n    Kernel matrix.\n\ncopy : bool, default=True\n    Set to False to perform inplace computation.\n\nReturns\n-------\nK_new : ndarray of shape (n_samples1, n_samples2)"
            }
          ],
          "fullDocstring": "Center a kernel matrix.\n\nLet K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a\nfunction mapping x to a Hilbert space. KernelCenterer centers (i.e.,\nnormalize to have zero mean) the data without explicitly computing phi(x).\nIt is equivalent to centering phi(x) with\nsklearn.preprocessing.StandardScaler(with_std=False).\n\nRead more in the :ref:`User Guide <kernel_centering>`.\n\nAttributes\n----------\nK_fit_rows_ : array of shape (n_samples,)\n    Average of each column of kernel matrix.\n\nK_fit_all_ : float\n    Average of kernel matrix.\n\nExamples\n--------\n>>> from sklearn.preprocessing import KernelCenterer\n>>> from sklearn.metrics.pairwise import pairwise_kernels\n>>> X = [[ 1., -2.,  2.],\n...      [ -2.,  1.,  3.],\n...      [ 4.,  1., -2.]]\n>>> K = pairwise_kernels(X, metric='linear')\n>>> K\narray([[  9.,   2.,  -2.],\n       [  2.,  14., -13.],\n       [ -2., -13.,  21.]])\n>>> transformer = KernelCenterer().fit(K)\n>>> transformer\nKernelCenterer()\n>>> transformer.transform(K)\narray([[  5.,   0.,  -5.],\n       [  0.,  14., -14.],\n       [ -5., -14.,  19.]])"
        },
        {
          "name": "MaxAbsScaler",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to compute the per-feature minimum and maximum\nused for later scaling along the features axis."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted scaler."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the maximum absolute value to be used for later scaling.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to compute the per-feature minimum and maximum\n    used for later scaling along the features axis.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted scaler."
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to compute the mean and standard deviation\nused for later scaling along the features axis."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted scaler."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Online computation of max absolute value of X for later scaling.\n\nAll of X is processed as a single batch. This is intended for cases\nwhen :meth:`fit` is not feasible due to very large number of\n`n_samples` or because X is read from a continuous stream.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to compute the mean and standard deviation\n    used for later scaling along the features axis.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted scaler."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data that should be scaled."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Scale the data\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data that should be scaled.\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Transformed array."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data that should be transformed back."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Scale back the data to the original representation\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data that should be transformed back.\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Transformed array."
            }
          ],
          "fullDocstring": "Scale each feature by its maximum absolute value.\n\nThis estimator scales and translates each feature individually such\nthat the maximal absolute value of each feature in the\ntraining set will be 1.0. It does not shift/center the data, and\nthus does not destroy any sparsity.\n\nThis scaler can also be applied to sparse CSR or CSC matrices.\n\n.. versionadded:: 0.17\n\nParameters\n----------\ncopy : bool, default=True\n    Set to False to perform inplace scaling and avoid a copy (if the input\n    is already a numpy array).\n\nAttributes\n----------\nscale_ : ndarray of shape (n_features,)\n    Per feature relative scaling of the data.\n\n    .. versionadded:: 0.17\n       *scale_* attribute.\n\nmax_abs_ : ndarray of shape (n_features,)\n    Per feature maximum absolute value.\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator. Will be reset on\n    new calls to fit, but increments across ``partial_fit`` calls.\n\nExamples\n--------\n>>> from sklearn.preprocessing import MaxAbsScaler\n>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n>>> transformer = MaxAbsScaler().fit(X)\n>>> transformer\nMaxAbsScaler()\n>>> transformer.transform(X)\narray([[ 0.5, -1. ,  1. ],\n       [ 1. ,  0. ,  0. ],\n       [ 0. ,  1. , -0.5]])\n\nSee Also\n--------\nmaxabs_scale : Equivalent function without the estimator API.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`."
        },
        {
          "name": "MinMaxScaler",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to compute the per-feature minimum and maximum\nused for later scaling along the features axis."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted scaler."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the minimum and maximum to be used for later scaling.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data used to compute the per-feature minimum and maximum\n    used for later scaling along the features axis.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted scaler."
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to compute the mean and standard deviation\nused for later scaling along the features axis."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted scaler."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Online computation of min and max on X for later scaling.\n\nAll of X is processed as a single batch. This is intended for cases\nwhen :meth:`fit` is not feasible due to very large number of\n`n_samples` or because X is read from a continuous stream.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data used to compute the mean and standard deviation\n    used for later scaling along the features axis.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted scaler."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data that will be transformed."
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Scale features of X according to feature_range.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input data that will be transformed.\n\nReturns\n-------\nXt : ndarray of shape (n_samples, n_features)\n    Transformed data."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data that will be transformed. It cannot be sparse."
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "Transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Undo the scaling of X according to feature_range.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input data that will be transformed. It cannot be sparse.\n\nReturns\n-------\nXt : ndarray of shape (n_samples, n_features)\n    Transformed data."
            }
          ],
          "fullDocstring": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such\nthat it is in the given range on the training set, e.g. between\nzero and one.\n\nThe transformation is given by::\n\n    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n    X_scaled = X_std * (max - min) + min\n\nwhere min, max = feature_range.\n\nThis transformation is often used as an alternative to zero mean,\nunit variance scaling.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\nfeature_range : tuple (min, max), default=(0, 1)\n    Desired range of transformed data.\n\ncopy : bool, default=True\n    Set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array).\n\nclip: bool, default=False\n    Set to True to clip transformed values of held-out data to\n    provided `feature range`.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nmin_ : ndarray of shape (n_features,)\n    Per feature adjustment for minimum. Equivalent to\n    ``min - X.min(axis=0) * self.scale_``\n\nscale_ : ndarray of shape (n_features,)\n    Per feature relative scaling of the data. Equivalent to\n    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n\n    .. versionadded:: 0.17\n       *scale_* attribute.\n\ndata_min_ : ndarray of shape (n_features,)\n    Per feature minimum seen in the data\n\n    .. versionadded:: 0.17\n       *data_min_*\n\ndata_max_ : ndarray of shape (n_features,)\n    Per feature maximum seen in the data\n\n    .. versionadded:: 0.17\n       *data_max_*\n\ndata_range_ : ndarray of shape (n_features,)\n    Per feature range ``(data_max_ - data_min_)`` seen in the data\n\n    .. versionadded:: 0.17\n       *data_range_*\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator.\n    It will be reset on new calls to fit, but increments across\n    ``partial_fit`` calls.\n\nExamples\n--------\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n>>> scaler = MinMaxScaler()\n>>> print(scaler.fit(data))\nMinMaxScaler()\n>>> print(scaler.data_max_)\n[ 1. 18.]\n>>> print(scaler.transform(data))\n[[0.   0.  ]\n [0.25 0.25]\n [0.5  0.5 ]\n [1.   1.  ]]\n>>> print(scaler.transform([[2, 2]]))\n[[1.5 0. ]]\n\nSee Also\n--------\nminmax_scale : Equivalent function without the estimator API.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`."
        },
        {
          "name": "Normalizer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to estimate the normalization parameters."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Do nothing and return the estimator unchanged\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to estimate the normalization parameters.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to normalize, row by row. scipy.sparse matrices should be\nin CSR format to avoid an un-necessary copy."
                },
                {
                  "name": "copy",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Copy the input X or not."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Scale each non zero row of X to unit norm\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to normalize, row by row. scipy.sparse matrices should be\n    in CSR format to avoid an un-necessary copy.\n\ncopy : bool, default=None\n    Copy the input X or not.\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Transformed array."
            }
          ],
          "fullDocstring": "Normalize samples individually to unit norm.\n\nEach sample (i.e. each row of the data matrix) with at least one\nnon zero component is rescaled independently of other samples so\nthat its norm (l1, l2 or inf) equals one.\n\nThis transformer is able to work both with dense numpy arrays and\nscipy.sparse matrix (use CSR format if you want to avoid the burden of\na copy / conversion).\n\nScaling inputs to unit norms is a common operation for text\nclassification or clustering for instance. For instance the dot\nproduct of two l2-normalized TF-IDF vectors is the cosine similarity\nof the vectors and is the base similarity metric for the Vector\nSpace Model commonly used by the Information Retrieval community.\n\nRead more in the :ref:`User Guide <preprocessing_normalization>`.\n\nParameters\n----------\nnorm : {'l1', 'l2', 'max'}, default='l2'\n    The norm to use to normalize each non zero sample. If norm='max'\n    is used, values will be rescaled by the maximum of the absolute\n    values.\n\ncopy : bool, default=True\n    set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array or a scipy.sparse\n    CSR matrix).\n\nExamples\n--------\n>>> from sklearn.preprocessing import Normalizer\n>>> X = [[4, 1, 2, 2],\n...      [1, 3, 9, 3],\n...      [5, 7, 5, 1]]\n>>> transformer = Normalizer().fit(X)  # fit does nothing.\n>>> transformer\nNormalizer()\n>>> transformer.transform(X)\narray([[0.8, 0.2, 0.4, 0.4],\n       [0.1, 0.3, 0.9, 0.3],\n       [0.5, 0.7, 0.5, 0.1]])\n\nNotes\n-----\nThis estimator is stateless (besides constructor parameters), the\nfit method does nothing but is useful when used in a pipeline.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\nSee Also\n--------\nnormalize : Equivalent function without the estimator API."
        },
        {
          "name": "PolynomialFeatures",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "powers_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "get_feature_names",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "input_features",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "String names for input features if available. By default,\n\"x0\", \"x1\", ... \"xn_features\" is used."
                }
              ],
              "results": [
                {
                  "name": "output_feature_names",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return feature names for output features\n\nParameters\n----------\ninput_features : list of str of shape (n_features,), default=None\n    String names for input features if available. By default,\n    \"x0\", \"x1\", ... \"xn_features\" is used.\n\nReturns\n-------\noutput_feature_names : list of str of shape (n_output_features,)"
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute number of output features.\n\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to transform, row by row.\n\nPrefer CSR over CSC for sparse input (for speed), but CSC is\nrequired if the degree is 4 or higher. If the degree is less than\n4 and the input format is CSC, it will be converted to CSR, have\nits polynomial features generated, then converted back to CSC.\n\nIf the degree is 2 or 3, the method described in \"Leveraging\nSparsity to Speed Up Polynomial Feature Expansions of CSR Matrices\nUsing K-Simplex Numbers\" by Andrew Nystrom and John Hughes is\nused, which is much faster than the method used on CSC input. For\nthis reason, a CSC input will be converted to CSR, and the output\nwill be converted back to CSC prior to being returned, hence the\npreference of CSR."
                }
              ],
              "results": [
                {
                  "name": "XP",
                  "type": null,
                  "description": "The matrix of features, where NP is the number of polynomial\nfeatures generated from the combination of inputs. If a sparse\nmatrix is provided, it will be converted into a sparse\n``csr_matrix``."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform data to polynomial features\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to transform, row by row.\n\n    Prefer CSR over CSC for sparse input (for speed), but CSC is\n    required if the degree is 4 or higher. If the degree is less than\n    4 and the input format is CSC, it will be converted to CSR, have\n    its polynomial features generated, then converted back to CSC.\n\n    If the degree is 2 or 3, the method described in \"Leveraging\n    Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices\n    Using K-Simplex Numbers\" by Andrew Nystrom and John Hughes is\n    used, which is much faster than the method used on CSC input. For\n    this reason, a CSC input will be converted to CSR, and the output\n    will be converted back to CSC prior to being returned, hence the\n    preference of CSR.\n\nReturns\n-------\nXP : {ndarray, sparse matrix} of shape (n_samples, NP)\n    The matrix of features, where NP is the number of polynomial\n    features generated from the combination of inputs. If a sparse\n    matrix is provided, it will be converted into a sparse\n    ``csr_matrix``."
            }
          ],
          "fullDocstring": "Generate polynomial and interaction features.\n\nGenerate a new feature matrix consisting of all polynomial combinations\nof the features with degree less than or equal to the specified degree.\nFor example, if an input sample is two dimensional and of the form\n[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n\nParameters\n----------\ndegree : int, default=2\n    The degree of the polynomial features.\n\ninteraction_only : bool, default=False\n    If true, only interaction features are produced: features that are\n    products of at most ``degree`` *distinct* input features (so not\n    ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).\n\ninclude_bias : bool, default=True\n    If True (default), then include a bias column, the feature in which\n    all polynomial powers are zero (i.e. a column of ones - acts as an\n    intercept term in a linear model).\n\norder : {'C', 'F'}, default='C'\n    Order of output array in the dense case. 'F' order is faster to\n    compute, but may slow down subsequent estimators.\n\n    .. versionadded:: 0.21\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> X = np.arange(6).reshape(3, 2)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n>>> poly = PolynomialFeatures(2)\n>>> poly.fit_transform(X)\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\n       [ 1.,  4.,  5., 16., 20., 25.]])\n>>> poly = PolynomialFeatures(interaction_only=True)\n>>> poly.fit_transform(X)\narray([[ 1.,  0.,  1.,  0.],\n       [ 1.,  2.,  3.,  6.],\n       [ 1.,  4.,  5., 20.]])\n\nAttributes\n----------\npowers_ : ndarray of shape (n_output_features, n_input_features)\n    powers_[i, j] is the exponent of the jth input in the ith output.\n\nn_input_features_ : int\n    The total number of input features.\n\nn_output_features_ : int\n    The total number of polynomial output features. The number of output\n    features is computed by iterating over all suitably sized combinations\n    of input features.\n\nNotes\n-----\nBe aware that the number of features in the output array scales\npolynomially in the number of features of the input array, and\nexponentially in the degree. High degrees can cause overfitting.\n\nSee :ref:`examples/linear_model/plot_polynomial_interpolation.py\n<sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`"
        },
        {
          "name": "PowerTransformer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to estimate the optimal transformation parameters."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Estimate the optimal parameter lambda for each feature.\n\nThe optimal lambda parameter for minimizing skewness is estimated on\neach feature independently using maximum likelihood.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data used to estimate the optimal transformation parameters.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted transformer."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to be transformed using a power transformation."
                }
              ],
              "results": [
                {
                  "name": "X_trans",
                  "type": null,
                  "description": "The transformed data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply the power transform to each feature using the fitted lambdas.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data to be transformed using a power transformation.\n\nReturns\n-------\nX_trans : ndarray of shape (n_samples, n_features)\n    The transformed data."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The transformed data."
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "The original data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Apply the inverse power transformation using the fitted lambdas.\n\nThe inverse of the Box-Cox transformation is given by::\n\n    if lambda_ == 0:\n        X = exp(X_trans)\n    else:\n        X = (X_trans * lambda_ + 1) ** (1 / lambda_)\n\nThe inverse of the Yeo-Johnson transformation is given by::\n\n    if X >= 0 and lambda_ == 0:\n        X = exp(X_trans) - 1\n    elif X >= 0 and lambda_ != 0:\n        X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1\n    elif X < 0 and lambda_ != 2:\n        X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))\n    elif X < 0 and lambda_ == 2:\n        X = 1 - exp(-X_trans)\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The transformed data.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The original data."
            }
          ],
          "fullDocstring": "Apply a power transform featurewise to make data more Gaussian-like.\n\nPower transforms are a family of parametric, monotonic transformations\nthat are applied to make data more Gaussian-like. This is useful for\nmodeling issues related to heteroscedasticity (non-constant variance),\nor other situations where normality is desired.\n\nCurrently, PowerTransformer supports the Box-Cox transform and the\nYeo-Johnson transform. The optimal parameter for stabilizing variance and\nminimizing skewness is estimated through maximum likelihood.\n\nBox-Cox requires input data to be strictly positive, while Yeo-Johnson\nsupports both positive or negative data.\n\nBy default, zero-mean, unit-variance normalization is applied to the\ntransformed data.\n\nRead more in the :ref:`User Guide <preprocessing_transformer>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nmethod : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n    The power transform method. Available methods are:\n\n    - 'yeo-johnson' [1]_, works with positive and negative values\n    - 'box-cox' [2]_, only works with strictly positive values\n\nstandardize : bool, default=True\n    Set to True to apply zero-mean, unit-variance normalization to the\n    transformed output.\n\ncopy : bool, default=True\n    Set to False to perform inplace computation during transformation.\n\nAttributes\n----------\nlambdas_ : ndarray of float of shape (n_features,)\n    The parameters of the power transformation for the selected features.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import PowerTransformer\n>>> pt = PowerTransformer()\n>>> data = [[1, 2], [3, 2], [4, 5]]\n>>> print(pt.fit(data))\nPowerTransformer()\n>>> print(pt.lambdas_)\n[ 1.386... -3.100...]\n>>> print(pt.transform(data))\n[[-1.316... -0.707...]\n [ 0.209... -0.707...]\n [ 1.106...  1.414...]]\n\nSee Also\n--------\npower_transform : Equivalent function without the estimator API.\n\nQuantileTransformer : Maps data to a standard normal distribution with\n    the parameter `output_distribution='normal'`.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in ``fit``, and maintained\nin ``transform``.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\nReferences\n----------\n\n.. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n       improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n       (2000).\n\n.. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n       of the Royal Statistical Society B, 26, 211-252 (1964)."
        },
        {
          "name": "QuantileTransformer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to scale along the features axis. If a sparse\nmatrix is provided, it will be converted into a sparse\n``csc_matrix``. Additionally, the sparse matrix needs to be\nnonnegative if `ignore_implicit_zeros` is False."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted transformer."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the quantiles used for transforming.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to scale along the features axis. If a sparse\n    matrix is provided, it will be converted into a sparse\n    ``csc_matrix``. Additionally, the sparse matrix needs to be\n    nonnegative if `ignore_implicit_zeros` is False.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n   Fitted transformer."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to scale along the features axis. If a sparse\nmatrix is provided, it will be converted into a sparse\n``csc_matrix``. Additionally, the sparse matrix needs to be\nnonnegative if `ignore_implicit_zeros` is False."
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "The projected data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Feature-wise transformation of the data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to scale along the features axis. If a sparse\n    matrix is provided, it will be converted into a sparse\n    ``csc_matrix``. Additionally, the sparse matrix needs to be\n    nonnegative if `ignore_implicit_zeros` is False.\n\nReturns\n-------\nXt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    The projected data."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to scale along the features axis. If a sparse\nmatrix is provided, it will be converted into a sparse\n``csc_matrix``. Additionally, the sparse matrix needs to be\nnonnegative if `ignore_implicit_zeros` is False."
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "The projected data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Back-projection to the original space.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to scale along the features axis. If a sparse\n    matrix is provided, it will be converted into a sparse\n    ``csc_matrix``. Additionally, the sparse matrix needs to be\n    nonnegative if `ignore_implicit_zeros` is False.\n\nReturns\n-------\nXt : {ndarray, sparse matrix} of (n_samples, n_features)\n    The projected data."
            }
          ],
          "fullDocstring": "Transform features using quantiles information.\n\nThis method transforms the features to follow a uniform or a normal\ndistribution. Therefore, for a given feature, this transformation tends\nto spread out the most frequent values. It also reduces the impact of\n(marginal) outliers: this is therefore a robust preprocessing scheme.\n\nThe transformation is applied on each feature independently. First an\nestimate of the cumulative distribution function of a feature is\nused to map the original values to a uniform distribution. The obtained\nvalues are then mapped to the desired output distribution using the\nassociated quantile function. Features values of new/unseen data that fall\nbelow or above the fitted range will be mapped to the bounds of the output\ndistribution. Note that this transform is non-linear. It may distort linear\ncorrelations between variables measured at the same scale but renders\nvariables measured at different scales more directly comparable.\n\nRead more in the :ref:`User Guide <preprocessing_transformer>`.\n\n.. versionadded:: 0.19\n\nParameters\n----------\nn_quantiles : int, default=1000 or n_samples\n    Number of quantiles to be computed. It corresponds to the number\n    of landmarks used to discretize the cumulative distribution function.\n    If n_quantiles is larger than the number of samples, n_quantiles is set\n    to the number of samples as a larger number of quantiles does not give\n    a better approximation of the cumulative distribution function\n    estimator.\n\noutput_distribution : {'uniform', 'normal'}, default='uniform'\n    Marginal distribution for the transformed data. The choices are\n    'uniform' (default) or 'normal'.\n\nignore_implicit_zeros : bool, default=False\n    Only applies to sparse matrices. If True, the sparse entries of the\n    matrix are discarded to compute the quantile statistics. If False,\n    these entries are treated as zeros.\n\nsubsample : int, default=1e5\n    Maximum number of samples used to estimate the quantiles for\n    computational efficiency. Note that the subsampling procedure may\n    differ for value-identical sparse and dense matrices.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for subsampling and smoothing\n    noise.\n    Please see ``subsample`` for more details.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`\n\ncopy : bool, default=True\n    Set to False to perform inplace transformation and avoid a copy (if the\n    input is already a numpy array).\n\nAttributes\n----------\nn_quantiles_ : int\n    The actual number of quantiles used to discretize the cumulative\n    distribution function.\n\nquantiles_ : ndarray of shape (n_quantiles, n_features)\n    The values corresponding the quantiles of reference.\n\nreferences_ : ndarray of shape (n_quantiles, )\n    Quantiles of references.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import QuantileTransformer\n>>> rng = np.random.RandomState(0)\n>>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n>>> qt = QuantileTransformer(n_quantiles=10, random_state=0)\n>>> qt.fit_transform(X)\narray([...])\n\nSee Also\n--------\nquantile_transform : Equivalent function without the estimator API.\nPowerTransformer : Perform mapping to a normal distribution using a power\n    transform.\nStandardScaler : Perform standardization that is faster, but less robust\n    to outliers.\nRobustScaler : Perform robust standardization that removes the influence\n    of outliers but does not put outliers and inliers on the same scale.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`."
        },
        {
          "name": "RobustScaler",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to compute the median and quantiles\nused for later scaling along the features axis."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted scaler."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the median and quantiles to be used for scaling.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to compute the median and quantiles\n    used for later scaling along the features axis.\n\ny : None\n    Ignored.\n\nReturns\n-------\nself : object\n    Fitted scaler."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to scale along the specified axis."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Center and scale the data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to scale along the specified axis.\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Transformed array."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The rescaled data to be transformed back."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Scale back the data to the original representation\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The rescaled data to be transformed back.\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Transformed array."
            }
          ],
          "fullDocstring": "Scale features using statistics that are robust to outliers.\n\nThis Scaler removes the median and scales the data according to\nthe quantile range (defaults to IQR: Interquartile Range).\nThe IQR is the range between the 1st quartile (25th quantile)\nand the 3rd quartile (75th quantile).\n\nCentering and scaling happen independently on each feature by\ncomputing the relevant statistics on the samples in the training\nset. Median and interquartile range are then stored to be used on\nlater data using the ``transform`` method.\n\nStandardization of a dataset is a common requirement for many\nmachine learning estimators. Typically this is done by removing the mean\nand scaling to unit variance. However, outliers can often influence the\nsample mean / variance in a negative way. In such cases, the median and\nthe interquartile range often give better results.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\nwith_centering : bool, default=True\n    If True, center the data before scaling.\n    This will cause ``transform`` to raise an exception when attempted on\n    sparse matrices, because centering them entails building a dense\n    matrix which in common use cases is likely to be too large to fit in\n    memory.\n\nwith_scaling : bool, default=True\n    If True, scale the data to interquartile range.\n\nquantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,         default=(25.0, 75.0), == (1st quantile, 3rd quantile), == IQR\n    Quantile range used to calculate ``scale_``.\n\n    .. versionadded:: 0.18\n\ncopy : bool, default=True\n    If False, try to avoid a copy and do inplace scaling instead.\n    This is not guaranteed to always work inplace; e.g. if the data is\n    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n    returned.\n\nunit_variance : bool, default=False\n    If True, scale data so that normally distributed features have a\n    variance of 1. In general, if the difference between the x-values of\n    ``q_max`` and ``q_min`` for a standard normal distribution is greater\n    than 1, the dataset will be scaled down. If less than 1, the dataset\n    will be scaled up.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncenter_ : array of floats\n    The median value for each feature in the training set.\n\nscale_ : array of floats\n    The (scaled) interquartile range for each feature in the training set.\n\n    .. versionadded:: 0.17\n       *scale_* attribute.\n\nExamples\n--------\n>>> from sklearn.preprocessing import RobustScaler\n>>> X = [[ 1., -2.,  2.],\n...      [ -2.,  1.,  3.],\n...      [ 4.,  1., -2.]]\n>>> transformer = RobustScaler().fit(X)\n>>> transformer\nRobustScaler()\n>>> transformer.transform(X)\narray([[ 0. , -2. ,  0. ],\n       [-1. ,  0. ,  0.4],\n       [ 1. ,  0. , -1.6]])\n\nSee Also\n--------\nrobust_scale : Equivalent function without the estimator API.\n\n:class:`~sklearn.decomposition.PCA`\n    Further removes the linear correlation across features with\n    'whiten=True'.\n\nNotes\n-----\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\nhttps://en.wikipedia.org/wiki/Median\nhttps://en.wikipedia.org/wiki/Interquartile_range"
        },
        {
          "name": "StandardScaler",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to compute the mean and standard deviation\nused for later scaling along the features axis."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Individual weights for each sample.\n\n.. versionadded:: 0.24\n   parameter *sample_weight* support to StandardScaler."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted scaler."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the mean and std to be used for later scaling.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to compute the mean and standard deviation\n    used for later scaling along the features axis.\n\ny : None\n    Ignored.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample.\n\n    .. versionadded:: 0.24\n       parameter *sample_weight* support to StandardScaler.\n\nReturns\n-------\nself : object\n    Fitted scaler."
            },
            {
              "name": "partial_fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to compute the mean and standard deviation\nused for later scaling along the features axis."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Individual weights for each sample.\n\n.. versionadded:: 0.24\n   parameter *sample_weight* support to StandardScaler."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted scaler."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Online computation of mean and std on X for later scaling.\n\nAll of X is processed as a single batch. This is intended for cases\nwhen :meth:`fit` is not feasible due to very large number of\n`n_samples` or because X is read from a continuous stream.\n\nThe algorithm for incremental mean and std is given in Equation 1.5a,b\nin Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\nfor computing the sample variance: Analysis and recommendations.\"\nThe American Statistician 37.3 (1983): 242-247:\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to compute the mean and standard deviation\n    used for later scaling along the features axis.\n\ny : None\n    Ignored.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample.\n\n    .. versionadded:: 0.24\n       parameter *sample_weight* support to StandardScaler.\n\nReturns\n-------\nself : object\n    Fitted scaler."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to scale along the features axis."
                },
                {
                  "name": "copy",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Copy the input X or not."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform standardization by centering and scaling\n\nParameters\n----------\nX : {array-like, sparse matrix of shape (n_samples, n_features)\n    The data used to scale along the features axis.\ncopy : bool, default=None\n    Copy the input X or not.\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Transformed array."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data used to scale along the features axis."
                },
                {
                  "name": "copy",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Copy the input X or not."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Scale back the data to the original representation\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data used to scale along the features axis.\ncopy : bool, default=None\n    Copy the input X or not.\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Transformed array."
            }
          ],
          "fullDocstring": "Standardize features by removing the mean and scaling to unit variance\n\nThe standard score of a sample `x` is calculated as:\n\n    z = (x - u) / s\n\nwhere `u` is the mean of the training samples or zero if `with_mean=False`,\nand `s` is the standard deviation of the training samples or one if\n`with_std=False`.\n\nCentering and scaling happen independently on each feature by computing\nthe relevant statistics on the samples in the training set. Mean and\nstandard deviation are then stored to be used on later data using\n:meth:`transform`.\n\nStandardization of a dataset is a common requirement for many\nmachine learning estimators: they might behave badly if the\nindividual features do not more or less look like standard normally\ndistributed data (e.g. Gaussian with 0 mean and unit variance).\n\nFor instance many elements used in the objective function of\na learning algorithm (such as the RBF kernel of Support Vector\nMachines or the L1 and L2 regularizers of linear models) assume that\nall features are centered around 0 and have variance in the same\norder. If a feature has a variance that is orders of magnitude larger\nthat others, it might dominate the objective function and make the\nestimator unable to learn from other features correctly as expected.\n\nThis scaler can also be applied to sparse CSR or CSC matrices by passing\n`with_mean=False` to avoid breaking the sparsity structure of the data.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\ncopy : bool, default=True\n    If False, try to avoid a copy and do inplace scaling instead.\n    This is not guaranteed to always work inplace; e.g. if the data is\n    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n    returned.\n\nwith_mean : bool, default=True\n    If True, center the data before scaling.\n    This does not work (and will raise an exception) when attempted on\n    sparse matrices, because centering them entails building a dense\n    matrix which in common use cases is likely to be too large to fit in\n    memory.\n\nwith_std : bool, default=True\n    If True, scale the data to unit variance (or equivalently,\n    unit standard deviation).\n\nAttributes\n----------\nscale_ : ndarray of shape (n_features,) or None\n    Per feature relative scaling of the data to achieve zero mean and unit\n    variance. Generally this is calculated using `np.sqrt(var_)`. If a\n    variance is zero, we can't achieve unit variance, and the data is left\n    as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n    when `with_std=False`.\n\n    .. versionadded:: 0.17\n       *scale_*\n\nmean_ : ndarray of shape (n_features,) or None\n    The mean value for each feature in the training set.\n    Equal to ``None`` when ``with_mean=False``.\n\nvar_ : ndarray of shape (n_features,) or None\n    The variance for each feature in the training set. Used to compute\n    `scale_`. Equal to ``None`` when ``with_std=False``.\n\nn_samples_seen_ : int or ndarray of shape (n_features,)\n    The number of samples processed by the estimator for each feature.\n    If there are no missing samples, the ``n_samples_seen`` will be an\n    integer, otherwise it will be an array of dtype int. If\n    `sample_weights` are used it will be a float (if no missing data)\n    or an array of dtype float that sums the weights seen so far.\n    Will be reset on new calls to fit, but increments across\n    ``partial_fit`` calls.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler\n>>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n>>> scaler = StandardScaler()\n>>> print(scaler.fit(data))\nStandardScaler()\n>>> print(scaler.mean_)\n[0.5 0.5]\n>>> print(scaler.transform(data))\n[[-1. -1.]\n [-1. -1.]\n [ 1.  1.]\n [ 1.  1.]]\n>>> print(scaler.transform([[2, 2]]))\n[[3. 3.]]\n\nSee Also\n--------\nscale : Equivalent function without the estimator API.\n\n:class:`~sklearn.decomposition.PCA` : Further removes the linear\n    correlation across features with 'whiten=True'.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nWe use a biased estimator for the standard deviation, equivalent to\n`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\naffect model performance.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`."
        }
      ],
      "functions": [
        {
          "name": "add_dummy_feature",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data."
            },
            {
              "name": "value",
              "type": "Any",
              "hasDefault": true,
              "default": "1.0",
              "limitation": null,
              "ignored": false,
              "description": "Value to use for the dummy feature."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "Same data with dummy feature added as first column."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Augment dataset with an additional dummy feature.\n\nThis is useful for fitting an intercept term with implementations which\ncannot otherwise fit it directly.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Data.\n\nvalue : float\n    Value to use for the dummy feature.\n\nReturns\n-------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features + 1)\n    Same data with dummy feature added as first column.\n\nExamples\n--------\n>>> from sklearn.preprocessing import add_dummy_feature\n>>> add_dummy_feature([[0, 1], [1, 0]])\narray([[1., 0., 1.],\n       [1., 1., 0.]])"
        },
        {
          "name": "binarize",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to binarize, element by element.\nscipy.sparse matrices should be in CSR or CSC format to avoid an\nun-necessary copy."
            }
          ],
          "results": [
            {
              "name": "X_tr",
              "type": null,
              "description": "The transformed data."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Boolean thresholding of array-like or scipy.sparse matrix.\n\nRead more in the :ref:`User Guide <preprocessing_binarization>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to binarize, element by element.\n    scipy.sparse matrices should be in CSR or CSC format to avoid an\n    un-necessary copy.\n\nthreshold : float, default=0.0\n    Feature values below or equal to this are replaced by 0, above it by 1.\n    Threshold may not be less than 0 for operations on sparse matrices.\n\ncopy : bool, default=True\n    set to False to perform inplace binarization and avoid a copy\n    (if the input is already a numpy array or a scipy.sparse CSR / CSC\n    matrix and if axis is 1).\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    The transformed data.\n\nSee Also\n--------\nBinarizer : Performs binarization using the Transformer API\n    (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`)."
        },
        {
          "name": "maxabs_scale",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data."
            }
          ],
          "results": [
            {
              "name": "X_tr",
              "type": null,
              "description": "The transformed data."
            },
            {
              "name": "",
              "type": null,
              "description": "Do not use :func:`~sklearn.preprocessing.maxabs_scale` unless you know what\nyou are doing. A common mistake is to apply it to the entire data\n*before* splitting into training and test sets. This will bias the\nmodel evaluation because information would have leaked from the test\nset to the training set.\nIn general, we recommend using\n:class:`~sklearn.preprocessing.MaxAbsScaler` within a\n:ref:`Pipeline <pipeline>` in order to prevent most risks of data\nleaking: `pipe = make_pipeline(MaxAbsScaler(), LogisticRegression())`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Scale each feature to the [-1, 1] range without breaking the sparsity.\n\nThis estimator scales each feature individually such\nthat the maximal absolute value of each feature in the\ntraining set will be 1.0.\n\nThis scaler can also be applied to sparse CSR or CSC matrices.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data.\n\naxis : int, default=0\n    axis used to scale along. If 0, independently scale each feature,\n    otherwise (if 1) scale each sample.\n\ncopy : bool, default=True\n    Set to False to perform inplace scaling and avoid a copy (if the input\n    is already a numpy array).\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    The transformed data.\n\n.. warning:: Risk of data leak\n\n    Do not use :func:`~sklearn.preprocessing.maxabs_scale` unless you know what\n    you are doing. A common mistake is to apply it to the entire data\n    *before* splitting into training and test sets. This will bias the\n    model evaluation because information would have leaked from the test\n    set to the training set.\n    In general, we recommend using\n    :class:`~sklearn.preprocessing.MaxAbsScaler` within a\n    :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n    leaking: `pipe = make_pipeline(MaxAbsScaler(), LogisticRegression())`.\n\nSee Also\n--------\nMaxAbsScaler : Performs scaling to the [-1, 1] range using\n    the Transformer API (e.g. as part of a preprocessing\n    :class:`~sklearn.pipeline.Pipeline`).\n\nNotes\n-----\nNaNs are treated as missing values: disregarded to compute the statistics,\nand maintained during the data transformation.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`."
        },
        {
          "name": "minmax_scale",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data."
            },
            {
              "name": "feature_range",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Desired range of transformed data."
            }
          ],
          "results": [
            {
              "name": "X_tr",
              "type": null,
              "description": "The transformed data."
            },
            {
              "name": "",
              "type": null,
              "description": "Do not use :func:`~sklearn.preprocessing.minmax_scale` unless you know\nwhat you are doing. A common mistake is to apply it to the entire data\n*before* splitting into training and test sets. This will bias the\nmodel evaluation because information would have leaked from the test\nset to the training set.\nIn general, we recommend using\n:class:`~sklearn.preprocessing.MinMaxScaler` within a\n:ref:`Pipeline <pipeline>` in order to prevent most risks of data\nleaking: `pipe = make_pipeline(MinMaxScaler(), LogisticRegression())`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such\nthat it is in the given range on the training set, i.e. between\nzero and one.\n\nThe transformation is given by (when ``axis=0``)::\n\n    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n    X_scaled = X_std * (max - min) + min\n\nwhere min, max = feature_range.\n\nThe transformation is calculated as (when ``axis=0``)::\n\n   X_scaled = scale * X + min - X.min(axis=0) * scale\n   where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n\nThis transformation is often used as an alternative to zero mean,\nunit variance scaling.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\n.. versionadded:: 0.17\n   *minmax_scale* function interface\n   to :class:`~sklearn.preprocessing.MinMaxScaler`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data.\n\nfeature_range : tuple (min, max), default=(0, 1)\n    Desired range of transformed data.\n\naxis : int, default=0\n    Axis used to scale along. If 0, independently scale each feature,\n    otherwise (if 1) scale each sample.\n\ncopy : bool, default=True\n    Set to False to perform inplace scaling and avoid a copy (if the input\n    is already a numpy array).\n\nReturns\n-------\nX_tr : ndarray of shape (n_samples, n_features)\n    The transformed data.\n\n.. warning:: Risk of data leak\n\n    Do not use :func:`~sklearn.preprocessing.minmax_scale` unless you know\n    what you are doing. A common mistake is to apply it to the entire data\n    *before* splitting into training and test sets. This will bias the\n    model evaluation because information would have leaked from the test\n    set to the training set.\n    In general, we recommend using\n    :class:`~sklearn.preprocessing.MinMaxScaler` within a\n    :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n    leaking: `pipe = make_pipeline(MinMaxScaler(), LogisticRegression())`.\n\nSee Also\n--------\nMinMaxScaler : Performs scaling to a given range using the Transformer\n    API (e.g. as part of a preprocessing\n    :class:`~sklearn.pipeline.Pipeline`).\n\nNotes\n-----\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`."
        },
        {
          "name": "normalize",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to normalize, element by element.\nscipy.sparse matrices should be in CSR format to avoid an\nun-necessary copy."
            },
            {
              "name": "norm",
              "type": "Any",
              "hasDefault": true,
              "default": "l2",
              "limitation": null,
              "ignored": false,
              "description": "The norm to use to normalize each non zero sample (or each non-zero\nfeature if axis is 0)."
            }
          ],
          "results": [
            {
              "name": "X",
              "type": null,
              "description": "Normalized input X."
            },
            {
              "name": "norms",
              "type": null,
              "description": "An array of norms along given axis for X.\nWhen X is sparse, a NotImplementedError will be raised\nfor norm 'l1' or 'l2'."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Scale input vectors individually to unit norm (vector length).\n\nRead more in the :ref:`User Guide <preprocessing_normalization>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to normalize, element by element.\n    scipy.sparse matrices should be in CSR format to avoid an\n    un-necessary copy.\n\nnorm : {'l1', 'l2', 'max'}, default='l2'\n    The norm to use to normalize each non zero sample (or each non-zero\n    feature if axis is 0).\n\naxis : {0, 1}, default=1\n    axis used to normalize the data along. If 1, independently normalize\n    each sample, otherwise (if 0) normalize each feature.\n\ncopy : bool, default=True\n    set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array or a scipy.sparse\n    CSR matrix and if axis is 1).\n\nreturn_norm : bool, default=False\n    whether to return the computed norms\n\nReturns\n-------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Normalized input X.\n\nnorms : ndarray of shape (n_samples, ) if axis=1 else (n_features, )\n    An array of norms along given axis for X.\n    When X is sparse, a NotImplementedError will be raised\n    for norm 'l1' or 'l2'.\n\nSee Also\n--------\nNormalizer : Performs normalization using the Transformer API\n    (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n\nNotes\n-----\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`."
        },
        {
          "name": "power_transform",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to be transformed using a power transformation."
            },
            {
              "name": "method",
              "type": "Any",
              "hasDefault": true,
              "default": "yeo-johnson",
              "limitation": null,
              "ignored": false,
              "description": "The power transform method. Available methods are:\n\n- 'yeo-johnson' [1]_, works with positive and negative values\n- 'box-cox' [2]_, only works with strictly positive values\n\n.. versionchanged:: 0.23\n    The default value of the `method` parameter changed from\n    'box-cox' to 'yeo-johnson' in 0.23."
            }
          ],
          "results": [
            {
              "name": "X_trans",
              "type": null,
              "description": "The transformed data."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Power transforms are a family of parametric, monotonic transformations\nthat are applied to make data more Gaussian-like. This is useful for\nmodeling issues related to heteroscedasticity (non-constant variance),\nor other situations where normality is desired.\n\nCurrently, power_transform supports the Box-Cox transform and the\nYeo-Johnson transform. The optimal parameter for stabilizing variance and\nminimizing skewness is estimated through maximum likelihood.\n\nBox-Cox requires input data to be strictly positive, while Yeo-Johnson\nsupports both positive or negative data.\n\nBy default, zero-mean, unit-variance normalization is applied to the\ntransformed data.\n\nRead more in the :ref:`User Guide <preprocessing_transformer>`.\n\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data to be transformed using a power transformation.\n\nmethod : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n    The power transform method. Available methods are:\n\n    - 'yeo-johnson' [1]_, works with positive and negative values\n    - 'box-cox' [2]_, only works with strictly positive values\n\n    .. versionchanged:: 0.23\n        The default value of the `method` parameter changed from\n        'box-cox' to 'yeo-johnson' in 0.23.\n\nstandardize : bool, default=True\n    Set to True to apply zero-mean, unit-variance normalization to the\n    transformed output.\n\ncopy : bool, default=True\n    Set to False to perform inplace computation during transformation.\n\nReturns\n-------\nX_trans : ndarray of shape (n_samples, n_features)\n    The transformed data.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import power_transform\n>>> data = [[1, 2], [3, 2], [4, 5]]\n>>> print(power_transform(data, method='box-cox'))\n[[-1.332... -0.707...]\n [ 0.256... -0.707...]\n [ 1.076...  1.414...]]\n\n.. warning:: Risk of data leak.\n    Do not use :func:`~sklearn.preprocessing.power_transform` unless you\n    know what you are doing. A common mistake is to apply it to the entire\n    data *before* splitting into training and test sets. This will bias the\n    model evaluation because information would have leaked from the test\n    set to the training set.\n    In general, we recommend using\n    :class:`~sklearn.preprocessing.PowerTransformer` within a\n    :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n    leaking, e.g.: `pipe = make_pipeline(PowerTransformer(),\n    LogisticRegression())`.\n\nSee Also\n--------\nPowerTransformer : Equivalent transformation with the\n    Transformer API (e.g. as part of a preprocessing\n    :class:`~sklearn.pipeline.Pipeline`).\n\nquantile_transform : Maps data to a standard normal distribution with\n    the parameter `output_distribution='normal'`.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in ``fit``, and maintained\nin ``transform``.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\nReferences\n----------\n\n.. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n       improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n       (2000).\n\n.. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n       of the Royal Statistical Society B, 26, 211-252 (1964)."
        },
        {
          "name": "quantile_transform",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to transform."
            }
          ],
          "results": [
            {
              "name": "Xt",
              "type": null,
              "description": "The transformed data."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Transform features using quantiles information.\n\nThis method transforms the features to follow a uniform or a normal\ndistribution. Therefore, for a given feature, this transformation tends\nto spread out the most frequent values. It also reduces the impact of\n(marginal) outliers: this is therefore a robust preprocessing scheme.\n\nThe transformation is applied on each feature independently. First an\nestimate of the cumulative distribution function of a feature is\nused to map the original values to a uniform distribution. The obtained\nvalues are then mapped to the desired output distribution using the\nassociated quantile function. Features values of new/unseen data that fall\nbelow or above the fitted range will be mapped to the bounds of the output\ndistribution. Note that this transform is non-linear. It may distort linear\ncorrelations between variables measured at the same scale but renders\nvariables measured at different scales more directly comparable.\n\nRead more in the :ref:`User Guide <preprocessing_transformer>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to transform.\n\naxis : int, default=0\n    Axis used to compute the means and standard deviations along. If 0,\n    transform each feature, otherwise (if 1) transform each sample.\n\nn_quantiles : int, default=1000 or n_samples\n    Number of quantiles to be computed. It corresponds to the number\n    of landmarks used to discretize the cumulative distribution function.\n    If n_quantiles is larger than the number of samples, n_quantiles is set\n    to the number of samples as a larger number of quantiles does not give\n    a better approximation of the cumulative distribution function\n    estimator.\n\noutput_distribution : {'uniform', 'normal'}, default='uniform'\n    Marginal distribution for the transformed data. The choices are\n    'uniform' (default) or 'normal'.\n\nignore_implicit_zeros : bool, default=False\n    Only applies to sparse matrices. If True, the sparse entries of the\n    matrix are discarded to compute the quantile statistics. If False,\n    these entries are treated as zeros.\n\nsubsample : int, default=1e5\n    Maximum number of samples used to estimate the quantiles for\n    computational efficiency. Note that the subsampling procedure may\n    differ for value-identical sparse and dense matrices.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for subsampling and smoothing\n    noise.\n    Please see ``subsample`` for more details.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`\n\ncopy : bool, default=True\n    Set to False to perform inplace transformation and avoid a copy (if the\n    input is already a numpy array). If True, a copy of `X` is transformed,\n    leaving the original `X` unchanged\n\n    ..versionchanged:: 0.23\n        The default value of `copy` changed from False to True in 0.23.\n\nReturns\n-------\nXt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    The transformed data.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import quantile_transform\n>>> rng = np.random.RandomState(0)\n>>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n>>> quantile_transform(X, n_quantiles=10, random_state=0, copy=True)\narray([...])\n\nSee Also\n--------\nQuantileTransformer : Performs quantile-based scaling using the\n    Transformer API (e.g. as part of a preprocessing\n    :class:`~sklearn.pipeline.Pipeline`).\npower_transform : Maps data to a normal distribution using a\n    power transformation.\nscale : Performs standardization that is faster, but less robust\n    to outliers.\nrobust_scale : Performs robust standardization that removes the influence\n    of outliers but does not put outliers and inliers on the same scale.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\n.. warning:: Risk of data leak\n\n    Do not use :func:`~sklearn.preprocessing.quantile_transform` unless\n    you know what you are doing. A common mistake is to apply it\n    to the entire data *before* splitting into training and\n    test sets. This will bias the model evaluation because\n    information would have leaked from the test set to the\n    training set.\n    In general, we recommend using\n    :class:`~sklearn.preprocessing.QuantileTransformer` within a\n    :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n    leaking:`pipe = make_pipeline(QuantileTransformer(),\n    LogisticRegression())`.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`."
        },
        {
          "name": "robust_scale",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to center and scale."
            }
          ],
          "results": [
            {
              "name": "X_tr",
              "type": null,
              "description": "The transformed data."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Standardize a dataset along any axis\n\nCenter to the median and component wise scale\naccording to the interquartile range.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_sample, n_features)\n    The data to center and scale.\n\naxis : int, default=0\n    axis used to compute the medians and IQR along. If 0,\n    independently scale each feature, otherwise (if 1) scale\n    each sample.\n\nwith_centering : bool, default=True\n    If True, center the data before scaling.\n\nwith_scaling : bool, default=True\n    If True, scale the data to unit variance (or equivalently,\n    unit standard deviation).\n\nquantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0\n    default=(25.0, 75.0), == (1st quantile, 3rd quantile), == IQR\n    Quantile range used to calculate ``scale_``.\n\n    .. versionadded:: 0.18\n\ncopy : bool, default=True\n    set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array or a scipy.sparse\n    CSR matrix and if axis is 1).\n\nunit_variance : bool, default=False\n    If True, scale data so that normally distributed features have a\n    variance of 1. In general, if the difference between the x-values of\n    ``q_max`` and ``q_min`` for a standard normal distribution is greater\n    than 1, the dataset will be scaled down. If less than 1, the dataset\n    will be scaled up.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    The transformed data.\n\nNotes\n-----\nThis implementation will refuse to center scipy.sparse matrices\nsince it would make them non-sparse and would potentially crash the\nprogram with memory exhaustion problems.\n\nInstead the caller is expected to either set explicitly\n`with_centering=False` (in that case, only variance scaling will be\nperformed on the features of the CSR matrix) or to call `X.toarray()`\nif he/she expects the materialized dense array to fit in memory.\n\nTo avoid memory copy the caller should pass a CSR matrix.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n.. warning:: Risk of data leak\n\n    Do not use :func:`~sklearn.preprocessing.robust_scale` unless you know\n    what you are doing. A common mistake is to apply it to the entire data\n    *before* splitting into training and test sets. This will bias the\n    model evaluation because information would have leaked from the test\n    set to the training set.\n    In general, we recommend using\n    :class:`~sklearn.preprocessing.RobustScaler` within a\n    :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n    leaking: `pipe = make_pipeline(RobustScaler(), LogisticRegression())`.\n\nSee Also\n--------\nRobustScaler : Performs centering and scaling using the Transformer API\n    (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`)."
        },
        {
          "name": "scale",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The data to center and scale."
            }
          ],
          "results": [
            {
              "name": "X_tr",
              "type": null,
              "description": "The transformed data."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Standardize a dataset along any axis.\n\nCenter to the mean and component wise scale to unit variance.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to center and scale.\n\naxis : int, default=0\n    axis used to compute the means and standard deviations along. If 0,\n    independently standardize each feature, otherwise (if 1) standardize\n    each sample.\n\nwith_mean : bool, default=True\n    If True, center the data before scaling.\n\nwith_std : bool, default=True\n    If True, scale the data to unit variance (or equivalently,\n    unit standard deviation).\n\ncopy : bool, default=True\n    set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array or a scipy.sparse\n    CSC matrix and if axis is 1).\n\nReturns\n-------\nX_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    The transformed data.\n\nNotes\n-----\nThis implementation will refuse to center scipy.sparse matrices\nsince it would make them non-sparse and would potentially crash the\nprogram with memory exhaustion problems.\n\nInstead the caller is expected to either set explicitly\n`with_mean=False` (in that case, only variance scaling will be\nperformed on the features of the CSC matrix) or to call `X.toarray()`\nif he/she expects the materialized dense array to fit in memory.\n\nTo avoid memory copy the caller should pass a CSC matrix.\n\nNaNs are treated as missing values: disregarded to compute the statistics,\nand maintained during the data transformation.\n\nWe use a biased estimator for the standard deviation, equivalent to\n`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\naffect model performance.\n\nFor a comparison of the different scalers, transformers, and normalizers,\nsee :ref:`examples/preprocessing/plot_all_scaling.py\n<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n.. warning:: Risk of data leak\n\n    Do not use :func:`~sklearn.preprocessing.scale` unless you know\n    what you are doing. A common mistake is to apply it to the entire data\n    *before* splitting into training and test sets. This will bias the\n    model evaluation because information would have leaked from the test\n    set to the training set.\n    In general, we recommend using\n    :class:`~sklearn.preprocessing.StandardScaler` within a\n    :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n    leaking: `pipe = make_pipeline(StandardScaler(), LogisticRegression())`.\n\nSee Also\n--------\nStandardScaler : Performs scaling to unit variance using the Transformer\n    API (e.g. as part of a preprocessing\n    :class:`~sklearn.pipeline.Pipeline`)."
        }
      ]
    },
    {
      "name": "sklearn.preprocessing._discretization",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "OneHotEncoder",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KBinsDiscretizer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to be discretized."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored. This parameter exists only for compatibility with\n:class:`~sklearn.pipeline.Pipeline`."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the estimator.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data to be discretized.\n\ny : None\n    Ignored. This parameter exists only for compatibility with\n    :class:`~sklearn.pipeline.Pipeline`.\n\nReturns\n-------\nself"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Data to be discretized."
                }
              ],
              "results": [
                {
                  "name": "Xt",
                  "type": null,
                  "description": "Data in the binned space. Will be a sparse matrix if\n`self.encode='onehot'` and ndarray otherwise."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Discretize the data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data to be discretized.\n\nReturns\n-------\nXt : {ndarray, sparse matrix}, dtype={np.float32, np.float64}\n    Data in the binned space. Will be a sparse matrix if\n    `self.encode='onehot'` and ndarray otherwise."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "Xt",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Transformed data in the binned space."
                }
              ],
              "results": [
                {
                  "name": "Xinv",
                  "type": null,
                  "description": "Data in the original feature space."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform discretized data back to original feature space.\n\nNote that this function does not regenerate the original data\ndue to discretization rounding.\n\nParameters\n----------\nXt : array-like of shape (n_samples, n_features)\n    Transformed data in the binned space.\n\nReturns\n-------\nXinv : ndarray, dtype={np.float32, np.float64}\n    Data in the original feature space."
            }
          ],
          "fullDocstring": "Bin continuous data into intervals.\n\nRead more in the :ref:`User Guide <preprocessing_discretization>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nn_bins : int or array-like of shape (n_features,), default=5\n    The number of bins to produce. Raises ValueError if ``n_bins < 2``.\n\nencode : {'onehot', 'onehot-dense', 'ordinal'}, default='onehot'\n    Method used to encode the transformed result.\n\n    onehot\n        Encode the transformed result with one-hot encoding\n        and return a sparse matrix. Ignored features are always\n        stacked to the right.\n    onehot-dense\n        Encode the transformed result with one-hot encoding\n        and return a dense array. Ignored features are always\n        stacked to the right.\n    ordinal\n        Return the bin identifier encoded as an integer value.\n\nstrategy : {'uniform', 'quantile', 'kmeans'}, default='quantile'\n    Strategy used to define the widths of the bins.\n\n    uniform\n        All bins in each feature have identical widths.\n    quantile\n        All bins in each feature have the same number of points.\n    kmeans\n        Values in each bin have the same nearest center of a 1D k-means\n        cluster.\n\ndtype : {np.float32, np.float64}, default=None\n    The desired data-type for the output. If None, output dtype is\n    consistent with input dtype. Only np.float32 and np.float64 are\n    supported.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nn_bins_ : ndarray of shape (n_features,), dtype=np.int_\n    Number of bins per feature. Bins whose width are too small\n    (i.e., <= 1e-8) are removed with a warning.\n\nbin_edges_ : ndarray of ndarray of shape (n_features,)\n    The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``\n    Ignored features will have empty arrays.\n\nSee Also\n--------\nBinarizer : Class used to bin values as ``0`` or\n    ``1`` based on a parameter ``threshold``.\n\nNotes\n-----\nIn bin edges for feature ``i``, the first and last values are used only for\n``inverse_transform``. During transform, bin edges are extended to::\n\n  np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])\n\nYou can combine ``KBinsDiscretizer`` with\n:class:`~sklearn.compose.ColumnTransformer` if you only want to preprocess\npart of the features.\n\n``KBinsDiscretizer`` might produce constant features (e.g., when\n``encode = 'onehot'`` and certain bins do not contain any data).\nThese features can be removed with feature selection algorithms\n(e.g., :class:`~sklearn.feature_selection.VarianceThreshold`).\n\nExamples\n--------\n>>> X = [[-2, 1, -4,   -1],\n...      [-1, 2, -3, -0.5],\n...      [ 0, 3, -2,  0.5],\n...      [ 1, 4, -1,    2]]\n>>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n>>> est.fit(X)\nKBinsDiscretizer(...)\n>>> Xt = est.transform(X)\n>>> Xt  # doctest: +SKIP\narray([[ 0., 0., 0., 0.],\n       [ 1., 1., 1., 0.],\n       [ 2., 2., 2., 1.],\n       [ 2., 2., 2., 2.]])\n\nSometimes it may be useful to convert the data back into the original\nfeature space. The ``inverse_transform`` function converts the binned\ndata into the original feature space. Each value will be equal to the mean\nof the two bin edges.\n\n>>> est.bin_edges_[0]\narray([-2., -1.,  0.,  1.])\n>>> est.inverse_transform(Xt)\narray([[-1.5,  1.5, -3.5, -0.5],\n       [-0.5,  2.5, -2.5, -0.5],\n       [ 0.5,  3.5, -1.5,  0.5],\n       [ 0.5,  3.5, -1.5,  1.5]])"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.preprocessing._encoders",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "is_scalar_nan",
          "alias": null
        },
        {
          "module": "sklearn.utils._encode",
          "declaration": "_check_unknown",
          "alias": null
        },
        {
          "module": "sklearn.utils._encode",
          "declaration": "_encode",
          "alias": null
        },
        {
          "module": "sklearn.utils._encode",
          "declaration": "_unique",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "OneHotEncoder",
          "decorators": [],
          "superclasses": [
            "_BaseEncoder"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to determine the categories of each feature."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored. This parameter exists only for compatibility with\n:class:`~sklearn.pipeline.Pipeline`."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit OneHotEncoder to X.\n\nParameters\n----------\nX : array-like, shape [n_samples, n_features]\n    The data to determine the categories of each feature.\n\ny : None\n    Ignored. This parameter exists only for compatibility with\n    :class:`~sklearn.pipeline.Pipeline`.\n\nReturns\n-------\nself"
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to encode."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored. This parameter exists only for compatibility with\n:class:`~sklearn.pipeline.Pipeline`."
                }
              ],
              "results": [
                {
                  "name": "X_out",
                  "type": null,
                  "description": "Transformed input."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit OneHotEncoder to X, then transform X.\n\nEquivalent to fit(X).transform(X) but more convenient.\n\nParameters\n----------\nX : array-like, shape [n_samples, n_features]\n    The data to encode.\n\ny : None\n    Ignored. This parameter exists only for compatibility with\n    :class:`~sklearn.pipeline.Pipeline`.\n\nReturns\n-------\nX_out : sparse matrix if sparse=True else a 2-d array\n    Transformed input."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to encode."
                }
              ],
              "results": [
                {
                  "name": "X_out",
                  "type": null,
                  "description": "Transformed input."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X using one-hot encoding.\n\nParameters\n----------\nX : array-like, shape [n_samples, n_features]\n    The data to encode.\n\nReturns\n-------\nX_out : sparse matrix if sparse=True else a 2-d array\n    Transformed input."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The transformed data."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Inverse transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Convert the data back to the original representation.\n\nIn case unknown categories are encountered (all zeros in the\none-hot encoding), ``None`` is used to represent this category.\n\nParameters\n----------\nX : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n    The transformed data.\n\nReturns\n-------\nX_tr : array-like, shape [n_samples, n_features]\n    Inverse transformed array."
            },
            {
              "name": "get_feature_names",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "input_features",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "String names for input features if available. By default,\n\"x0\", \"x1\", ... \"xn_features\" is used."
                }
              ],
              "results": [
                {
                  "name": "output_feature_names",
                  "type": null,
                  "description": "Array of feature names."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return feature names for output features.\n\nParameters\n----------\ninput_features : list of str of shape (n_features,)\n    String names for input features if available. By default,\n    \"x0\", \"x1\", ... \"xn_features\" is used.\n\nReturns\n-------\noutput_feature_names : ndarray of shape (n_output_features,)\n    Array of feature names."
            }
          ],
          "fullDocstring": "Encode categorical features as a one-hot numeric array.\n\nThe input to this transformer should be an array-like of integers or\nstrings, denoting the values taken on by categorical (discrete) features.\nThe features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\nencoding scheme. This creates a binary column for each category and\nreturns a sparse matrix or dense array (depending on the ``sparse``\nparameter)\n\nBy default, the encoder derives the categories based on the unique values\nin each feature. Alternatively, you can also specify the `categories`\nmanually.\n\nThis encoding is needed for feeding categorical data to many scikit-learn\nestimators, notably linear models and SVMs with the standard kernels.\n\nNote: a one-hot encoding of y labels should use a LabelBinarizer\ninstead.\n\nRead more in the :ref:`User Guide <preprocessing_categorical_features>`.\n\n.. versionchanged:: 0.20\n\nParameters\n----------\ncategories : 'auto' or a list of array-like, default='auto'\n    Categories (unique values) per feature:\n\n    - 'auto' : Determine categories automatically from the training data.\n    - list : ``categories[i]`` holds the categories expected in the ith\n      column. The passed categories should not mix strings and numeric\n      values within a single feature, and should be sorted in case of\n      numeric values.\n\n    The used categories can be found in the ``categories_`` attribute.\n\n    .. versionadded:: 0.20\n\ndrop : {'first', 'if_binary'} or a array-like of shape (n_features,),             default=None\n    Specifies a methodology to use to drop one of the categories per\n    feature. This is useful in situations where perfectly collinear\n    features cause problems, such as when feeding the resulting data\n    into a neural network or an unregularized regression.\n\n    However, dropping one category breaks the symmetry of the original\n    representation and can therefore induce a bias in downstream models,\n    for instance for penalized linear classification or regression models.\n\n    - None : retain all features (the default).\n    - 'first' : drop the first category in each feature. If only one\n      category is present, the feature will be dropped entirely.\n    - 'if_binary' : drop the first category in each feature with two\n      categories. Features with 1 or more than 2 categories are\n      left intact.\n    - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n      should be dropped.\n\n    .. versionchanged:: 0.23\n       Added option 'if_binary'.\n\nsparse : bool, default=True\n    Will return sparse matrix if set True else will return an array.\n\ndtype : number type, default=float\n    Desired dtype of output.\n\nhandle_unknown : {'error', 'ignore'}, default='error'\n    Whether to raise an error or ignore if an unknown categorical feature\n    is present during transform (default is to raise). When this parameter\n    is set to 'ignore' and an unknown category is encountered during\n    transform, the resulting one-hot encoded columns for this feature\n    will be all zeros. In the inverse transform, an unknown category\n    will be denoted as None.\n\nAttributes\n----------\ncategories_ : list of arrays\n    The categories of each feature determined during fitting\n    (in order of the features in X and corresponding with the output\n    of ``transform``). This includes the category specified in ``drop``\n    (if any).\n\ndrop_idx_ : array of shape (n_features,)\n    - ``drop_idx_[i]`` is\u00a0the index in ``categories_[i]`` of the category\n      to be dropped for each feature.\n    - ``drop_idx_[i] = None`` if no category is to be dropped from the\n      feature with index ``i``, e.g. when `drop='if_binary'` and the\n      feature isn't binary.\n    - ``drop_idx_ = None`` if all the transformed features will be\n      retained.\n\n    .. versionchanged:: 0.23\n       Added the possibility to contain `None` values.\n\nSee Also\n--------\nOrdinalEncoder : Performs an ordinal (integer)\n  encoding of the categorical features.\nsklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n  dictionary items (also handles string-valued features).\nsklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n  encoding of dictionary items or strings.\nLabelBinarizer : Binarizes labels in a one-vs-all\n  fashion.\nMultiLabelBinarizer : Transforms between iterable of\n  iterables and a multilabel format, e.g. a (samples x classes) binary\n  matrix indicating the presence of a class label.\n\nExamples\n--------\nGiven a dataset with two features, we let the encoder find the unique\nvalues per feature and transform the data to a binary one-hot encoding.\n\n>>> from sklearn.preprocessing import OneHotEncoder\n\nOne can discard categories not seen during `fit`:\n\n>>> enc = OneHotEncoder(handle_unknown='ignore')\n>>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n>>> enc.fit(X)\nOneHotEncoder(handle_unknown='ignore')\n>>> enc.categories_\n[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n>>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\narray([[1., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0.]])\n>>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\narray([['Male', 1],\n       [None, 2]], dtype=object)\n>>> enc.get_feature_names(['gender', 'group'])\narray(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'],\n  dtype=object)\n\nOne can always drop the first column for each feature:\n\n>>> drop_enc = OneHotEncoder(drop='first').fit(X)\n>>> drop_enc.categories_\n[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n>>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\narray([[0., 0., 0.],\n       [1., 1., 0.]])\n\nOr drop a column for feature only having 2 categories:\n\n>>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n>>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\narray([[0., 1., 0., 0.],\n       [1., 0., 1., 0.]])"
        },
        {
          "name": "OrdinalEncoder",
          "decorators": [],
          "superclasses": [
            "_BaseEncoder"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to determine the categories of each feature."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored. This parameter exists only for compatibility with\n:class:`~sklearn.pipeline.Pipeline`."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the OrdinalEncoder to X.\n\nParameters\n----------\nX : array-like, shape [n_samples, n_features]\n    The data to determine the categories of each feature.\n\ny : None\n    Ignored. This parameter exists only for compatibility with\n    :class:`~sklearn.pipeline.Pipeline`.\n\nReturns\n-------\nself"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data to encode."
                }
              ],
              "results": [
                {
                  "name": "X_out",
                  "type": null,
                  "description": "Transformed input."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X to ordinal codes.\n\nParameters\n----------\nX : array-like, shape [n_samples, n_features]\n    The data to encode.\n\nReturns\n-------\nX_out : sparse matrix or a 2-d array\n    Transformed input."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The transformed data."
                }
              ],
              "results": [
                {
                  "name": "X_tr",
                  "type": null,
                  "description": "Inverse transformed array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Convert the data back to the original representation.\n\nParameters\n----------\nX : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n    The transformed data.\n\nReturns\n-------\nX_tr : array-like, shape [n_samples, n_features]\n    Inverse transformed array."
            }
          ],
          "fullDocstring": "Encode categorical features as an integer array.\n\nThe input to this transformer should be an array-like of integers or\nstrings, denoting the values taken on by categorical (discrete) features.\nThe features are converted to ordinal integers. This results in\na single column of integers (0 to n_categories - 1) per feature.\n\nRead more in the :ref:`User Guide <preprocessing_categorical_features>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\ncategories : 'auto' or a list of array-like, default='auto'\n    Categories (unique values) per feature:\n\n    - 'auto' : Determine categories automatically from the training data.\n    - list : ``categories[i]`` holds the categories expected in the ith\n      column. The passed categories should not mix strings and numeric\n      values, and should be sorted in case of numeric values.\n\n    The used categories can be found in the ``categories_`` attribute.\n\ndtype : number type, default np.float64\n    Desired dtype of output.\n\nhandle_unknown : {'error', 'use_encoded_value'}, default='error'\n    When set to 'error' an error will be raised in case an unknown\n    categorical feature is present during transform. When set to\n    'use_encoded_value', the encoded value of unknown categories will be\n    set to the value given for the parameter `unknown_value`. In\n    :meth:`inverse_transform`, an unknown category will be denoted as None.\n\n    .. versionadded:: 0.24\n\nunknown_value : int or np.nan, default=None\n    When the parameter handle_unknown is set to 'use_encoded_value', this\n    parameter is required and will set the encoded value of unknown\n    categories. It has to be distinct from the values used to encode any of\n    the categories in `fit`. If set to np.nan, the `dtype` parameter must\n    be a float dtype.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncategories_ : list of arrays\n    The categories of each feature determined during ``fit`` (in order of\n    the features in X and corresponding with the output of ``transform``).\n    This does not include categories that weren't seen during ``fit``.\n\nSee Also\n--------\nOneHotEncoder : Performs a one-hot encoding of categorical features.\nLabelEncoder : Encodes target labels with values between 0 and\n    ``n_classes-1``.\n\nExamples\n--------\nGiven a dataset with two features, we let the encoder find the unique\nvalues per feature and transform the data to an ordinal encoding.\n\n>>> from sklearn.preprocessing import OrdinalEncoder\n>>> enc = OrdinalEncoder()\n>>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n>>> enc.fit(X)\nOrdinalEncoder()\n>>> enc.categories_\n[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n>>> enc.transform([['Female', 3], ['Male', 1]])\narray([[0., 2.],\n       [1., 0.]])\n\n>>> enc.inverse_transform([[1, 0], [0, 1]])\narray([['Male', 1],\n       ['Female', 2]], dtype=object)"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.preprocessing._function_transformer",
      "imports": [
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_allclose_dense_sparse",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "FunctionTransformer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input array."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit transformer by checking X.\n\nIf ``validate`` is ``True``, ``X`` will be checked.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    Input array.\n\nReturns\n-------\nself"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input array."
                }
              ],
              "results": [
                {
                  "name": "X_out",
                  "type": null,
                  "description": "Transformed input."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X using the forward function.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    Input array.\n\nReturns\n-------\nX_out : array-like, shape (n_samples, n_features)\n    Transformed input."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input array."
                }
              ],
              "results": [
                {
                  "name": "X_out",
                  "type": null,
                  "description": "Transformed input."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform X using the inverse function.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    Input array.\n\nReturns\n-------\nX_out : array-like, shape (n_samples, n_features)\n    Transformed input."
            }
          ],
          "fullDocstring": "Constructs a transformer from an arbitrary callable.\n\nA FunctionTransformer forwards its X (and optionally y) arguments to a\nuser-defined function or function object and returns the result of this\nfunction. This is useful for stateless transformations such as taking the\nlog of frequencies, doing custom scaling, etc.\n\nNote: If a lambda is used as the function, then the resulting\ntransformer will not be pickleable.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <function_transformer>`.\n\nParameters\n----------\nfunc : callable, default=None\n    The callable to use for the transformation. This will be passed\n    the same arguments as transform, with args and kwargs forwarded.\n    If func is None, then func will be the identity function.\n\ninverse_func : callable, default=None\n    The callable to use for the inverse transformation. This will be\n    passed the same arguments as inverse transform, with args and\n    kwargs forwarded. If inverse_func is None, then inverse_func\n    will be the identity function.\n\nvalidate : bool, default=False\n    Indicate that the input X array should be checked before calling\n    ``func``. The possibilities are:\n\n    - If False, there is no input validation.\n    - If True, then X will be converted to a 2-dimensional NumPy array or\n      sparse matrix. If the conversion is not possible an exception is\n      raised.\n\n    .. versionchanged:: 0.22\n       The default of ``validate`` changed from True to False.\n\naccept_sparse : bool, default=False\n    Indicate that func accepts a sparse matrix as input. If validate is\n    False, this has no effect. Otherwise, if accept_sparse is false,\n    sparse matrix inputs will cause an exception to be raised.\n\ncheck_inverse : bool, default=True\n   Whether to check that or ``func`` followed by ``inverse_func`` leads to\n   the original inputs. It can be used for a sanity check, raising a\n   warning when the condition is not fulfilled.\n\n   .. versionadded:: 0.20\n\nkw_args : dict, default=None\n    Dictionary of additional keyword arguments to pass to func.\n\n    .. versionadded:: 0.18\n\ninv_kw_args : dict, default=None\n    Dictionary of additional keyword arguments to pass to inverse_func.\n\n    .. versionadded:: 0.18\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import FunctionTransformer\n>>> transformer = FunctionTransformer(np.log1p)\n>>> X = np.array([[0, 1], [2, 3]])\n>>> transformer.transform(X)\narray([[0.       , 0.6931...],\n       [1.0986..., 1.3862...]])"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.preprocessing._label",
      "imports": [
        {
          "module": "array",
          "alias": null
        },
        {
          "module": "itertools",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "defaultdict",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.utils._encode",
          "declaration": "_encode",
          "alias": null
        },
        {
          "module": "sklearn.utils._encode",
          "declaration": "_unique",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "type_of_target",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "unique_labels",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs",
          "declaration": "min_max_axis",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "LabelBinarizer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values. The 2-d matrix should only contain 0 and 1,\nrepresents multilabel classification."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit label binarizer.\n\nParameters\n----------\ny : ndarray of shape (n_samples,) or (n_samples, n_classes)\n    Target values. The 2-d matrix should only contain 0 and 1,\n    represents multilabel classification.\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values. The 2-d matrix should only contain 0 and 1,\nrepresents multilabel classification. Sparse matrix can be\nCSR, CSC, COO, DOK, or LIL."
                }
              ],
              "results": [
                {
                  "name": "Y",
                  "type": null,
                  "description": "Shape will be (n_samples, 1) for binary problems. Sparse matrix\nwill be of CSR format."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit label binarizer and transform multi-class labels to binary\nlabels.\n\nThe output of transform is sometimes referred to as\nthe 1-of-K coding scheme.\n\nParameters\n----------\ny : {ndarray, sparse matrix} of shape (n_samples,) or                 (n_samples, n_classes)\n    Target values. The 2-d matrix should only contain 0 and 1,\n    represents multilabel classification. Sparse matrix can be\n    CSR, CSC, COO, DOK, or LIL.\n\nReturns\n-------\nY : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    Shape will be (n_samples, 1) for binary problems. Sparse matrix\n    will be of CSR format."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values. The 2-d matrix should only contain 0 and 1,\nrepresents multilabel classification. Sparse matrix can be\nCSR, CSC, COO, DOK, or LIL."
                }
              ],
              "results": [
                {
                  "name": "Y",
                  "type": null,
                  "description": "Shape will be (n_samples, 1) for binary problems. Sparse matrix\nwill be of CSR format."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform multi-class labels to binary labels.\n\nThe output of transform is sometimes referred to by some authors as\nthe 1-of-K coding scheme.\n\nParameters\n----------\ny : {array, sparse matrix} of shape (n_samples,) or                 (n_samples, n_classes)\n    Target values. The 2-d matrix should only contain 0 and 1,\n    represents multilabel classification. Sparse matrix can be\n    CSR, CSC, COO, DOK, or LIL.\n\nReturns\n-------\nY : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    Shape will be (n_samples, 1) for binary problems. Sparse matrix\n    will be of CSR format."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "Y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values. All sparse matrices are converted to CSR before\ninverse transformation."
                },
                {
                  "name": "threshold",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Threshold used in the binary and multi-label cases.\n\nUse 0 when ``Y`` contains the output of decision_function\n(classifier).\nUse 0.5 when ``Y`` contains the output of predict_proba.\n\nIf None, the threshold is assumed to be half way between\nneg_label and pos_label."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Target values. Sparse matrix will be of CSR format."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform binary labels back to multi-class labels.\n\nParameters\n----------\nY : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    Target values. All sparse matrices are converted to CSR before\n    inverse transformation.\n\nthreshold : float, default=None\n    Threshold used in the binary and multi-label cases.\n\n    Use 0 when ``Y`` contains the output of decision_function\n    (classifier).\n    Use 0.5 when ``Y`` contains the output of predict_proba.\n\n    If None, the threshold is assumed to be half way between\n    neg_label and pos_label.\n\nReturns\n-------\ny : {ndarray, sparse matrix} of shape (n_samples,)\n    Target values. Sparse matrix will be of CSR format.\n\nNotes\n-----\nIn the case when the binary labels are fractional\n(probabilistic), inverse_transform chooses the class with the\ngreatest value. Typically, this allows to use the output of a\nlinear model's decision_function method directly as the input\nof inverse_transform."
            }
          ],
          "fullDocstring": "Binarize labels in a one-vs-all fashion.\n\nSeveral regression and binary classification algorithms are\navailable in scikit-learn. A simple way to extend these algorithms\nto the multi-class classification case is to use the so-called\none-vs-all scheme.\n\nAt learning time, this simply consists in learning one regressor\nor binary classifier per class. In doing so, one needs to convert\nmulti-class labels to binary labels (belong or does not belong\nto the class). LabelBinarizer makes this process easy with the\ntransform method.\n\nAt prediction time, one assigns the class for which the corresponding\nmodel gave the greatest confidence. LabelBinarizer makes this easy\nwith the inverse_transform method.\n\nRead more in the :ref:`User Guide <preprocessing_targets>`.\n\nParameters\n----------\n\nneg_label : int, default=0\n    Value with which negative labels must be encoded.\n\npos_label : int, default=1\n    Value with which positive labels must be encoded.\n\nsparse_output : bool, default=False\n    True if the returned array from transform is desired to be in sparse\n    CSR format.\n\nAttributes\n----------\n\nclasses_ : ndarray of shape (n_classes,)\n    Holds the label for each class.\n\ny_type_ : str\n    Represents the type of the target data as evaluated by\n    utils.multiclass.type_of_target. Possible type are 'continuous',\n    'continuous-multioutput', 'binary', 'multiclass',\n    'multiclass-multioutput', 'multilabel-indicator', and 'unknown'.\n\nsparse_input_ : bool\n    True if the input data to transform is given as a sparse matrix, False\n    otherwise.\n\nExamples\n--------\n>>> from sklearn import preprocessing\n>>> lb = preprocessing.LabelBinarizer()\n>>> lb.fit([1, 2, 6, 4, 2])\nLabelBinarizer()\n>>> lb.classes_\narray([1, 2, 4, 6])\n>>> lb.transform([1, 6])\narray([[1, 0, 0, 0],\n       [0, 0, 0, 1]])\n\nBinary targets transform to a column vector\n\n>>> lb = preprocessing.LabelBinarizer()\n>>> lb.fit_transform(['yes', 'no', 'no', 'yes'])\narray([[1],\n       [0],\n       [0],\n       [1]])\n\nPassing a 2D matrix for multilabel classification\n\n>>> import numpy as np\n>>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))\nLabelBinarizer()\n>>> lb.classes_\narray([0, 1, 2])\n>>> lb.transform([0, 1, 2, 1])\narray([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0]])\n\nSee Also\n--------\nlabel_binarize : Function to perform the transform operation of\n    LabelBinarizer with fixed classes.\nOneHotEncoder : Encode categorical features using a one-hot aka one-of-K\n    scheme."
        },
        {
          "name": "LabelEncoder",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit label encoder.\n\nParameters\n----------\ny : array-like of shape (n_samples,)\n    Target values.\n\nReturns\n-------\nself : returns an instance of self."
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit label encoder and return encoded labels.\n\nParameters\n----------\ny : array-like of shape (n_samples,)\n    Target values.\n\nReturns\n-------\ny : array-like of shape (n_samples,)"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform labels to normalized encoding.\n\nParameters\n----------\ny : array-like of shape (n_samples,)\n    Target values.\n\nReturns\n-------\ny : array-like of shape (n_samples,)"
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform labels back to original encoding.\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    Target values.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)"
            }
          ],
          "fullDocstring": "Encode target labels with value between 0 and n_classes-1.\n\nThis transformer should be used to encode target values, *i.e.* `y`, and\nnot the input `X`.\n\nRead more in the :ref:`User Guide <preprocessing_targets>`.\n\n.. versionadded:: 0.12\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    Holds the label for each class.\n\nExamples\n--------\n`LabelEncoder` can be used to normalize labels.\n\n>>> from sklearn import preprocessing\n>>> le = preprocessing.LabelEncoder()\n>>> le.fit([1, 2, 2, 6])\nLabelEncoder()\n>>> le.classes_\narray([1, 2, 6])\n>>> le.transform([1, 1, 2, 6])\narray([0, 0, 1, 2]...)\n>>> le.inverse_transform([0, 0, 1, 2])\narray([1, 1, 2, 6])\n\nIt can also be used to transform non-numerical labels (as long as they are\nhashable and comparable) to numerical labels.\n\n>>> le = preprocessing.LabelEncoder()\n>>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nLabelEncoder()\n>>> list(le.classes_)\n['amsterdam', 'paris', 'tokyo']\n>>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\narray([2, 2, 1]...)\n>>> list(le.inverse_transform([2, 2, 1]))\n['tokyo', 'tokyo', 'paris']\n\nSee Also\n--------\nOrdinalEncoder : Encode categorical features using an ordinal encoding\n    scheme.\nOneHotEncoder : Encode categorical features as a one-hot numeric array."
        },
        {
          "name": "MultiLabelBinarizer",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "A set of labels (any orderable and hashable object) for each\nsample. If the `classes` parameter is set, `y` will not be\niterated."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the label sets binarizer, storing :term:`classes_`.\n\nParameters\n----------\ny : iterable of iterables\n    A set of labels (any orderable and hashable object) for each\n    sample. If the `classes` parameter is set, `y` will not be\n    iterated.\n\nReturns\n-------\nself : returns this MultiLabelBinarizer instance"
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "A set of labels (any orderable and hashable object) for each\nsample. If the `classes` parameter is set, `y` will not be\niterated."
                }
              ],
              "results": [
                {
                  "name": "y_indicator",
                  "type": null,
                  "description": "A matrix such that `y_indicator[i, j] = 1` i.f.f. `classes_[j]`\nis in `y[i]`, and 0 otherwise. Sparse matrix will be of CSR\nformat."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the label sets binarizer and transform the given label sets.\n\nParameters\n----------\ny : iterable of iterables\n    A set of labels (any orderable and hashable object) for each\n    sample. If the `classes` parameter is set, `y` will not be\n    iterated.\n\nReturns\n-------\ny_indicator : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    A matrix such that `y_indicator[i, j] = 1` i.f.f. `classes_[j]`\n    is in `y[i]`, and 0 otherwise. Sparse matrix will be of CSR\n    format."
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "A set of labels (any orderable and hashable object) for each\nsample. If the `classes` parameter is set, `y` will not be\niterated."
                }
              ],
              "results": [
                {
                  "name": "y_indicator",
                  "type": null,
                  "description": "A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in\n`y[i]`, and 0 otherwise."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform the given label sets.\n\nParameters\n----------\ny : iterable of iterables\n    A set of labels (any orderable and hashable object) for each\n    sample. If the `classes` parameter is set, `y` will not be\n    iterated.\n\nReturns\n-------\ny_indicator : array or CSR matrix, shape (n_samples, n_classes)\n    A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in\n    `y[i]`, and 0 otherwise."
            },
            {
              "name": "inverse_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "yt",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "A matrix containing only 1s ands 0s."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The set of labels for each sample such that `y[i]` consists of\n`classes_[j]` for each `yt[i, j] == 1`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Transform the given indicator matrix into label sets.\n\nParameters\n----------\nyt : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    A matrix containing only 1s ands 0s.\n\nReturns\n-------\ny : list of tuples\n    The set of labels for each sample such that `y[i]` consists of\n    `classes_[j]` for each `yt[i, j] == 1`."
            }
          ],
          "fullDocstring": "Transform between iterable of iterables and a multilabel format.\n\nAlthough a list of sets or tuples is a very intuitive format for multilabel\ndata, it is unwieldy to process. This transformer converts between this\nintuitive format and the supported multilabel format: a (samples x classes)\nbinary matrix indicating the presence of a class label.\n\nParameters\n----------\nclasses : array-like of shape (n_classes,), default=None\n    Indicates an ordering for the class labels.\n    All entries should be unique (cannot contain duplicate classes).\n\nsparse_output : bool, default=False\n    Set to True if output binary array is desired in CSR sparse format.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    A copy of the `classes` parameter when provided.\n    Otherwise it corresponds to the sorted set of classes found\n    when fitting.\n\nExamples\n--------\n>>> from sklearn.preprocessing import MultiLabelBinarizer\n>>> mlb = MultiLabelBinarizer()\n>>> mlb.fit_transform([(1, 2), (3,)])\narray([[1, 1, 0],\n       [0, 0, 1]])\n>>> mlb.classes_\narray([1, 2, 3])\n\n>>> mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}])\narray([[0, 1, 1],\n       [1, 0, 0]])\n>>> list(mlb.classes_)\n['comedy', 'sci-fi', 'thriller']\n\nA common mistake is to pass in a list, which leads to the following issue:\n\n>>> mlb = MultiLabelBinarizer()\n>>> mlb.fit(['sci-fi', 'thriller', 'comedy'])\nMultiLabelBinarizer()\n>>> mlb.classes_\narray(['-', 'c', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'o', 'r', 's', 't',\n    'y'], dtype=object)\n\nTo correct this, the list of labels should be passed in as:\n\n>>> mlb = MultiLabelBinarizer()\n>>> mlb.fit([['sci-fi', 'thriller', 'comedy']])\nMultiLabelBinarizer()\n>>> mlb.classes_\narray(['comedy', 'sci-fi', 'thriller'], dtype=object)\n\nSee Also\n--------\nOneHotEncoder : Encode categorical features using a one-hot aka one-of-K\n    scheme."
        }
      ],
      "functions": [
        {
          "name": "label_binarize",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Sequence of integer labels or multilabel data to encode."
            }
          ],
          "results": [
            {
              "name": "Y",
              "type": null,
              "description": "Shape will be (n_samples, 1) for binary problems. Sparse matrix will\nbe of CSR format."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Binarize labels in a one-vs-all fashion.\n\nSeveral regression and binary classification algorithms are\navailable in scikit-learn. A simple way to extend these algorithms\nto the multi-class classification case is to use the so-called\none-vs-all scheme.\n\nThis function makes it possible to compute this transformation for a\nfixed set of class labels known ahead of time.\n\nParameters\n----------\ny : array-like\n    Sequence of integer labels or multilabel data to encode.\n\nclasses : array-like of shape (n_classes,)\n    Uniquely holds the label for each class.\n\nneg_label : int, default=0\n    Value with which negative labels must be encoded.\n\npos_label : int, default=1\n    Value with which positive labels must be encoded.\n\nsparse_output : bool, default=False,\n    Set to true if output binary array is desired in CSR sparse format.\n\nReturns\n-------\nY : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    Shape will be (n_samples, 1) for binary problems. Sparse matrix will\n    be of CSR format.\n\nExamples\n--------\n>>> from sklearn.preprocessing import label_binarize\n>>> label_binarize([1, 6], classes=[1, 2, 4, 6])\narray([[1, 0, 0, 0],\n       [0, 0, 0, 1]])\n\nThe class ordering is preserved:\n\n>>> label_binarize([1, 6], classes=[1, 6, 4, 2])\narray([[1, 0, 0, 0],\n       [0, 1, 0, 0]])\n\nBinary targets transform to a column vector\n\n>>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\narray([[1],\n       [0],\n       [0],\n       [1]])\n\nSee Also\n--------\nLabelBinarizer : Class used to wrap the functionality of label_binarize and\n    allow for fitting to classes independently of the transform operation."
        }
      ]
    },
    {
      "name": "sklearn.preprocessing.setup",
      "imports": [
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.random_projection",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "TransformerMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "DataDimensionalityWarning",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.random",
          "declaration": "sample_without_replacement",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseRandomProjection",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "TransformerMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training set: only the shape is used to find optimal random\nmatrix dimensions based on the theory referenced in the\nafore mentioned papers."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Ignored"
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Generate a sparse random projection matrix.\n\nParameters\n----------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Training set: only the shape is used to find optimal random\n    matrix dimensions based on the theory referenced in the\n    afore mentioned papers.\n\ny\n    Ignored\n\nReturns\n-------\nself"
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data to project into a smaller dimensional space."
                }
              ],
              "results": [
                {
                  "name": "X_new",
                  "type": null,
                  "description": "Projected array."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Project the data by using matrix product with the random matrix\n\nParameters\n----------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    The input data to project into a smaller dimensional space.\n\nReturns\n-------\nX_new : {ndarray, sparse matrix} of shape (n_samples, n_components)\n    Projected array."
            }
          ],
          "fullDocstring": "Base class for random projections.\n\nWarning: This class should not be used directly.\nUse derived classes instead."
        },
        {
          "name": "GaussianRandomProjection",
          "decorators": [],
          "superclasses": [
            "BaseRandomProjection"
          ],
          "methods": [],
          "fullDocstring": "Reduce dimensionality through Gaussian random projection.\n\nThe components of the random matrix are drawn from N(0, 1 / n_components).\n\nRead more in the :ref:`User Guide <gaussian_random_matrix>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nn_components : int or 'auto', default='auto'\n    Dimensionality of the target projection space.\n\n    n_components can be automatically adjusted according to the\n    number of samples in the dataset and the bound given by the\n    Johnson-Lindenstrauss lemma. In that case the quality of the\n    embedding is controlled by the ``eps`` parameter.\n\n    It should be noted that Johnson-Lindenstrauss lemma can yield\n    very conservative estimated of the required number of components\n    as it makes no assumption on the structure of the dataset.\n\neps : float, default=0.1\n    Parameter to control the quality of the embedding according to\n    the Johnson-Lindenstrauss lemma when `n_components` is set to\n    'auto'. The value should be strictly positive.\n\n    Smaller values lead to better embedding and higher number of\n    dimensions (n_components) in the target projection space.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generator used to generate the\n    projection matrix at fit time.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nn_components_ : int\n    Concrete number of components computed when n_components=\"auto\".\n\ncomponents_ : ndarray of shape (n_components, n_features)\n    Random matrix used for the projection.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.random_projection import GaussianRandomProjection\n>>> rng = np.random.RandomState(42)\n>>> X = rng.rand(100, 10000)\n>>> transformer = GaussianRandomProjection(random_state=rng)\n>>> X_new = transformer.fit_transform(X)\n>>> X_new.shape\n(100, 3947)\n\nSee Also\n--------\nSparseRandomProjection"
        },
        {
          "name": "SparseRandomProjection",
          "decorators": [],
          "superclasses": [
            "BaseRandomProjection"
          ],
          "methods": [],
          "fullDocstring": "Reduce dimensionality through sparse random projection.\n\nSparse random matrix is an alternative to dense random\nprojection matrix that guarantees similar embedding quality while being\nmuch more memory efficient and allowing faster computation of the\nprojected data.\n\nIf we note `s = 1 / density` the components of the random matrix are\ndrawn from:\n\n  - -sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n  -  0                              with probability 1 - 1 / s\n  - +sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n\nRead more in the :ref:`User Guide <sparse_random_matrix>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nn_components : int or 'auto', default='auto'\n    Dimensionality of the target projection space.\n\n    n_components can be automatically adjusted according to the\n    number of samples in the dataset and the bound given by the\n    Johnson-Lindenstrauss lemma. In that case the quality of the\n    embedding is controlled by the ``eps`` parameter.\n\n    It should be noted that Johnson-Lindenstrauss lemma can yield\n    very conservative estimated of the required number of components\n    as it makes no assumption on the structure of the dataset.\n\ndensity : float or 'auto', default='auto'\n    Ratio in the range (0, 1] of non-zero component in the random\n    projection matrix.\n\n    If density = 'auto', the value is set to the minimum density\n    as recommended by Ping Li et al.: 1 / sqrt(n_features).\n\n    Use density = 1 / 3.0 if you want to reproduce the results from\n    Achlioptas, 2001.\n\neps : float, default=0.1\n    Parameter to control the quality of the embedding according to\n    the Johnson-Lindenstrauss lemma when n_components is set to\n    'auto'. This value should be strictly positive.\n\n    Smaller values lead to better embedding and higher number of\n    dimensions (n_components) in the target projection space.\n\ndense_output : bool, default=False\n    If True, ensure that the output of the random projection is a\n    dense numpy array even if the input and random projection matrix\n    are both sparse. In practice, if the number of components is\n    small the number of zero components in the projected data will\n    be very small and it will be more CPU and memory efficient to\n    use a dense representation.\n\n    If False, the projected data uses a sparse representation if\n    the input is sparse.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generator used to generate the\n    projection matrix at fit time.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nn_components_ : int\n    Concrete number of components computed when n_components=\"auto\".\n\ncomponents_ : sparse matrix of shape (n_components, n_features)\n    Random matrix used for the projection. Sparse matrix will be of CSR\n    format.\n\ndensity_ : float in range 0.0 - 1.0\n    Concrete density computed from when density = \"auto\".\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.random_projection import SparseRandomProjection\n>>> rng = np.random.RandomState(42)\n>>> X = rng.rand(100, 10000)\n>>> transformer = SparseRandomProjection(random_state=rng)\n>>> X_new = transformer.fit_transform(X)\n>>> X_new.shape\n(100, 3947)\n>>> # very few components are non-zero\n>>> np.mean(transformer.components_ != 0)\n0.0100...\n\nSee Also\n--------\nGaussianRandomProjection\n\nReferences\n----------\n\n.. [1] Ping Li, T. Hastie and K. W. Church, 2006,\n       \"Very Sparse Random Projections\".\n       https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf\n\n.. [2] D. Achlioptas, 2001, \"Database-friendly random projections\",\n       https://users.soe.ucsc.edu/~optas/papers/jl.pdf"
        }
      ],
      "functions": [
        {
          "name": "johnson_lindenstrauss_min_dim",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n_samples",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of samples that should be a integer greater than 0. If an array\nis given, it will compute a safe number of components array-wise."
            }
          ],
          "results": [
            {
              "name": "n_components",
              "type": null,
              "description": "The minimal number of components to guarantee with good probability\nan eps-embedding with n_samples."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Find a 'safe' number of components to randomly project to.\n\nThe distortion introduced by a random projection `p` only changes the\ndistance between two points by a factor (1 +- eps) in an euclidean space\nwith good probability. The projection `p` is an eps-embedding as defined\nby:\n\n  (1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2\n\nWhere u and v are any rows taken from a dataset of shape (n_samples,\nn_features), eps is in ]0, 1[ and p is a projection by a random Gaussian\nN(0, 1) matrix of shape (n_components, n_features) (or a sparse\nAchlioptas matrix).\n\nThe minimum number of components to guarantee the eps-embedding is\ngiven by:\n\n  n_components >= 4 log(n_samples) / (eps^2 / 2 - eps^3 / 3)\n\nNote that the number of dimensions is independent of the original\nnumber of features but instead depends on the size of the dataset:\nthe larger the dataset, the higher is the minimal dimensionality of\nan eps-embedding.\n\nRead more in the :ref:`User Guide <johnson_lindenstrauss>`.\n\nParameters\n----------\nn_samples : int or array-like of int\n    Number of samples that should be a integer greater than 0. If an array\n    is given, it will compute a safe number of components array-wise.\n\neps : float or ndarray of shape (n_components,), dtype=float,             default=0.1\n    Maximum distortion rate in the range (0,1 ) as defined by the\n    Johnson-Lindenstrauss lemma. If an array is given, it will compute a\n    safe number of components array-wise.\n\nReturns\n-------\nn_components : int or ndarray of int\n    The minimal number of components to guarantee with good probability\n    an eps-embedding with n_samples.\n\nExamples\n--------\n\n>>> johnson_lindenstrauss_min_dim(1e6, eps=0.5)\n663\n\n>>> johnson_lindenstrauss_min_dim(1e6, eps=[0.5, 0.1, 0.01])\narray([    663,   11841, 1112658])\n\n>>> johnson_lindenstrauss_min_dim([1e4, 1e5, 1e6], eps=0.1)\narray([ 7894,  9868, 11841])\n\nReferences\n----------\n\n.. [1] https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma\n\n.. [2] Sanjoy Dasgupta and Anupam Gupta, 1999,\n       \"An elementary proof of the Johnson-Lindenstrauss Lemma.\"\n       http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.3654"
        }
      ]
    },
    {
      "name": "sklearn.semi_supervised",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._label_propagation",
          "declaration": "LabelPropagation",
          "alias": null
        },
        {
          "module": "sklearn._label_propagation",
          "declaration": "LabelSpreading",
          "alias": null
        },
        {
          "module": "sklearn._self_training",
          "declaration": "SelfTrainingClassifier",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.semi_supervised._label_propagation",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "csgraph",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "rbf_kernel",
          "alias": null
        },
        {
          "module": "sklearn.neighbors",
          "declaration": "NearestNeighbors",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseLabelPropagation",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data matrix."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Predictions for input data."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Performs inductive inference across the model.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    Predictions for input data."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data matrix."
                }
              ],
              "results": [
                {
                  "name": "probabilities",
                  "type": null,
                  "description": "Normalized probability distributions across\nclass labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict probability for each possible outcome.\n\nCompute the probability estimates for each single sample in X\nand each possible outcome seen during training (categorical\ndistribution).\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\nprobabilities : ndarray of shape (n_samples, n_classes)\n    Normalized probability distributions across\n    class labels."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "A matrix of shape (n_samples, n_samples) will be created from this."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "`n_labeled_samples` (unlabeled points are marked as -1)\nAll unlabeled samples will be transductively assigned labels."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit a semi-supervised label propagation model based\n\nAll the input data is provided matrix X (labeled and unlabeled)\nand corresponding label matrix y with a dedicated marker value for\nunlabeled samples.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    A matrix of shape (n_samples, n_samples) will be created from this.\n\ny : array-like of shape (n_samples,)\n    `n_labeled_samples` (unlabeled points are marked as -1)\n    All unlabeled samples will be transductively assigned labels.\n\nReturns\n-------\nself : object"
            }
          ],
          "fullDocstring": "Base class for label propagation module.\n\n Parameters\n ----------\n kernel : {'knn', 'rbf'} or callable, default='rbf'\n     String identifier for kernel function to use or the kernel function\n     itself. Only 'rbf' and 'knn' strings are valid inputs. The function\n     passed should take two inputs, each of shape (n_samples, n_features),\n     and return a (n_samples, n_samples) shaped weight matrix.\n\n gamma : float, default=20\n     Parameter for rbf kernel.\n\n n_neighbors : int, default=7\n     Parameter for knn kernel. Need to be strictly positive.\n\n alpha : float, default=1.0\n     Clamping factor.\n\n max_iter : int, default=30\n     Change maximum number of iterations allowed.\n\n tol : float, default=1e-3\n     Convergence tolerance: threshold to consider the system at steady\n     state.\n\nn_jobs : int, default=None\n     The number of parallel jobs to run.\n     ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n     ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n     for more details.\n "
        },
        {
          "name": "LabelPropagation",
          "decorators": [],
          "superclasses": [
            "BaseLabelPropagation"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Label Propagation classifier\n\nRead more in the :ref:`User Guide <label_propagation>`.\n\nParameters\n----------\nkernel : {'knn', 'rbf'} or callable, default='rbf'\n    String identifier for kernel function to use or the kernel function\n    itself. Only 'rbf' and 'knn' strings are valid inputs. The function\n    passed should take two inputs, each of shape (n_samples, n_features),\n    and return a (n_samples, n_samples) shaped weight matrix.\n\ngamma : float, default=20\n    Parameter for rbf kernel.\n\nn_neighbors : int, default=7\n    Parameter for knn kernel which need to be strictly positive.\n\nmax_iter : int, default=1000\n    Change maximum number of iterations allowed.\n\ntol : float, 1e-3\n    Convergence tolerance: threshold to consider the system at steady\n    state.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nX_ : ndarray of shape (n_samples, n_features)\n    Input array.\n\nclasses_ : ndarray of shape (n_classes,)\n    The distinct labels used in classifying instances.\n\nlabel_distributions_ : ndarray of shape (n_samples, n_classes)\n    Categorical distribution for each item.\n\ntransduction_ : ndarray of shape (n_samples)\n    Label assigned to each item via the transduction.\n\nn_iter_ : int\n    Number of iterations run.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import datasets\n>>> from sklearn.semi_supervised import LabelPropagation\n>>> label_prop_model = LabelPropagation()\n>>> iris = datasets.load_iris()\n>>> rng = np.random.RandomState(42)\n>>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\n>>> labels = np.copy(iris.target)\n>>> labels[random_unlabeled_points] = -1\n>>> label_prop_model.fit(iris.data, labels)\nLabelPropagation(...)\n\nReferences\n----------\nXiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data\nwith label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon\nUniversity, 2002 http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf\n\nSee Also\n--------\nLabelSpreading : Alternate label propagation strategy more robust to noise."
        },
        {
          "name": "LabelSpreading",
          "decorators": [],
          "superclasses": [
            "BaseLabelPropagation"
          ],
          "methods": [],
          "fullDocstring": "LabelSpreading model for semi-supervised learning\n\nThis model is similar to the basic Label Propagation algorithm,\nbut uses affinity matrix based on the normalized graph Laplacian\nand soft clamping across the labels.\n\nRead more in the :ref:`User Guide <label_propagation>`.\n\nParameters\n----------\nkernel : {'knn', 'rbf'} or callable, default='rbf'\n    String identifier for kernel function to use or the kernel function\n    itself. Only 'rbf' and 'knn' strings are valid inputs. The function\n    passed should take two inputs, each of shape (n_samples, n_features),\n    and return a (n_samples, n_samples) shaped weight matrix.\n\ngamma : float, default=20\n  Parameter for rbf kernel.\n\nn_neighbors : int, default=7\n  Parameter for knn kernel which is a strictly positive integer.\n\nalpha : float, default=0.2\n  Clamping factor. A value in (0, 1) that specifies the relative amount\n  that an instance should adopt the information from its neighbors as\n  opposed to its initial label.\n  alpha=0 means keeping the initial label information; alpha=1 means\n  replacing all initial information.\n\nmax_iter : int, default=30\n  Maximum number of iterations allowed.\n\ntol : float, default=1e-3\n  Convergence tolerance: threshold to consider the system at steady\n  state.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nX_ : ndarray of shape (n_samples, n_features)\n    Input array.\n\nclasses_ : ndarray of shape (n_classes,)\n    The distinct labels used in classifying instances.\n\nlabel_distributions_ : ndarray of shape (n_samples, n_classes)\n    Categorical distribution for each item.\n\ntransduction_ : ndarray of shape (n_samples,)\n    Label assigned to each item via the transduction.\n\nn_iter_ : int\n    Number of iterations run.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import datasets\n>>> from sklearn.semi_supervised import LabelSpreading\n>>> label_prop_model = LabelSpreading()\n>>> iris = datasets.load_iris()\n>>> rng = np.random.RandomState(42)\n>>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\n>>> labels = np.copy(iris.target)\n>>> labels[random_unlabeled_points] = -1\n>>> label_prop_model.fit(iris.data, labels)\nLabelSpreading(...)\n\nReferences\n----------\nDengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston,\nBernhard Schoelkopf. Learning with local and global consistency (2004)\nhttp://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219\n\nSee Also\n--------\nLabelPropagation : Unregularized graph based semi-supervised learning."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.semi_supervised._self_training",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MetaEstimatorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "safe_mask",
          "alias": null
        },
        {
          "module": "sklearn.utils.metaestimators",
          "declaration": "if_delegate_has_method",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "SelfTrainingClassifier",
          "decorators": [],
          "superclasses": [
            "MetaEstimatorMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array representing the data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array representing the labels. Unlabeled samples should have the\nlabel -1."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Returns an instance of self."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fits this ``SelfTrainingClassifier`` to a dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Array representing the data.\n\ny : {array-like, sparse matrix} of shape (n_samples,)\n    Array representing the labels. Unlabeled samples should have the\n    label -1.\n\nReturns\n-------\nself : object\n    Returns an instance of self."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array representing the data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Array with predicted labels."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the classes of X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Array representing the data.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    Array with predicted labels."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array representing the data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Array with prediction probabilities."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict probability for each possible outcome.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Array representing the data.\n\nReturns\n-------\ny : ndarray of shape (n_samples, n_features)\n    Array with prediction probabilities."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array representing the data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Result of the decision function of the `base_estimator`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Calls decision function of the `base_estimator`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Array representing the data.\n\nReturns\n-------\ny : ndarray of shape (n_samples, n_features)\n    Result of the decision function of the `base_estimator`."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array representing the data."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "Array with log prediction probabilities."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict log probability for each possible outcome.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Array representing the data.\n\nReturns\n-------\ny : ndarray of shape (n_samples, n_features)\n    Array with log prediction probabilities."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array representing the data."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array representing the labels."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "Result of calling score on the `base_estimator`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Calls score on the `base_estimator`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Array representing the data.\n\ny : array-like of shape (n_samples,)\n    Array representing the labels.\n\nReturns\n-------\nscore : float\n    Result of calling score on the `base_estimator`."
            }
          ],
          "fullDocstring": "Self-training classifier.\n\nThis class allows a given supervised classifier to function as a\nsemi-supervised classifier, allowing it to learn from unlabeled data. It\ndoes this by iteratively predicting pseudo-labels for the unlabeled data\nand adding them to the training set.\n\nThe classifier will continue iterating until either max_iter is reached, or\nno pseudo-labels were added to the training set in the previous iteration.\n\nRead more in the :ref:`User Guide <self_training>`.\n\nParameters\n----------\nbase_estimator : estimator object\n    An estimator object implementing ``fit`` and ``predict_proba``.\n    Invoking the ``fit`` method will fit a clone of the passed estimator,\n    which will be stored in the ``base_estimator_`` attribute.\n\ncriterion : {'threshold', 'k_best'}, default='threshold'\n    The selection criterion used to select which labels to add to the\n    training set. If 'threshold', pseudo-labels with prediction\n    probabilities above `threshold` are added to the dataset. If 'k_best',\n    the `k_best` pseudo-labels with highest prediction probabilities are\n    added to the dataset. When using the 'threshold' criterion, a\n    :ref:`well calibrated classifier <calibration>` should be used.\n\nthreshold : float, default=0.75\n    The decision threshold for use with `criterion='threshold'`.\n    Should be in [0, 1). When using the 'threshold' criterion, a\n    :ref:`well calibrated classifier <calibration>` should be used.\n\nk_best : int, default=10\n    The amount of samples to add in each iteration. Only used when\n    `criterion` is k_best'.\n\nmax_iter : int or None, default=10\n    Maximum number of iterations allowed. Should be greater than or equal\n    to 0. If it is ``None``, the classifier will continue to predict labels\n    until no new pseudo-labels are added, or all unlabeled samples have\n    been labeled.\n\nverbose: bool, default=False\n    Enable verbose output.\n\nAttributes\n----------\nbase_estimator_ : estimator object\n    The fitted estimator.\n\nclasses_ : ndarray or list of ndarray of shape (n_classes,)\n    Class labels for each output. (Taken from the trained\n    ``base_estimator_``).\n\ntransduction_ : ndarray of shape (n_samples,)\n    The labels used for the final fit of the classifier, including\n    pseudo-labels added during fit.\n\nlabeled_iter_ : ndarray of shape (n_samples,)\n    The iteration in which each sample was labeled. When a sample has\n    iteration 0, the sample was already labeled in the original dataset.\n    When a sample has iteration -1, the sample was not labeled in any\n    iteration.\n\nn_iter_ : int\n    The number of rounds of self-training, that is the number of times the\n    base estimator is fitted on relabeled variants of the training set.\n\ntermination_condition_ : {'max_iter', 'no_change', 'all_labeled'}\n    The reason that fitting was stopped.\n\n    - 'max_iter': `n_iter_` reached `max_iter`.\n    - 'no_change': no new labels were predicted.\n    - 'all_labeled': all unlabeled samples were labeled before `max_iter`\n      was reached.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import datasets\n>>> from sklearn.semi_supervised import SelfTrainingClassifier\n>>> from sklearn.svm import SVC\n>>> rng = np.random.RandomState(42)\n>>> iris = datasets.load_iris()\n>>> random_unlabeled_points = rng.rand(iris.target.shape[0]) < 0.3\n>>> iris.target[random_unlabeled_points] = -1\n>>> svc = SVC(probability=True, gamma=\"auto\")\n>>> self_training_model = SelfTrainingClassifier(svc)\n>>> self_training_model.fit(iris.data, iris.target)\nSelfTrainingClassifier(...)\n\nReferences\n----------\nDavid Yarowsky. 1995. Unsupervised word sense disambiguation rivaling\nsupervised methods. In Proceedings of the 33rd annual meeting on\nAssociation for Computational Linguistics (ACL '95). Association for\nComputational Linguistics, Stroudsburg, PA, USA, 189-196. DOI:\nhttps://doi.org/10.3115/981658.981684"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.setup",
      "imports": [
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "sys",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "sklearn._build_utils",
          "declaration": "cythonize_extensions",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.svm",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._bounds",
          "declaration": "l1_min_c",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "LinearSVC",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "LinearSVR",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "NuSVC",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "NuSVR",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "OneClassSVM",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "SVC",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "SVR",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.svm._base",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelEncoder",
          "alias": null
        },
        {
          "module": "sklearn.svm",
          "declaration": "_liblinear",
          "alias": "liblinear"
        },
        {
          "module": "sklearn.svm",
          "declaration": "_libsvm",
          "alias": "libsvm"
        },
        {
          "module": "sklearn.svm",
          "declaration": "_libsvm_sparse",
          "alias": "libsvm_sparse"
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "compute_class_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "_ovr_decision_function",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_large_sparse",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseLibSVM",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vectors, where n_samples is the number of samples\nand n_features is the number of features.\nFor kernel=\"precomputed\", the expected shape of X is\n(n_samples, n_samples)."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target values (class labels in classification, real numbers in\nregression)."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Per-sample weights. Rescale C per sample. Higher weights\nforce the classifier to put more emphasis on these points."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the SVM model according to the given training data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n    Training vectors, where n_samples is the number of samples\n    and n_features is the number of features.\n    For kernel=\"precomputed\", the expected shape of X is\n    (n_samples, n_samples).\n\ny : array-like of shape (n_samples,)\n    Target values (class labels in classification, real numbers in\n    regression).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Per-sample weights. Rescale C per sample. Higher weights\n    force the classifier to put more emphasis on these points.\n\nReturns\n-------\nself : object\n\nNotes\n-----\nIf X and y are not C-ordered and contiguous arrays of np.float64 and\nX is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n\nIf X is a dense array, then the other methods will not support sparse\nmatrices as input."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "For kernel=\"precomputed\", the expected shape of X is\n(n_samples_test, n_samples_train)."
                }
              ],
              "results": [
                {
                  "name": "y_pred",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform regression on samples in X.\n\nFor an one-class model, +1 (inlier) or -1 (outlier) is returned.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    For kernel=\"precomputed\", the expected shape of X is\n    (n_samples_test, n_samples_train).\n\nReturns\n-------\ny_pred : ndarray of shape (n_samples,)"
            },
            {
              "name": "coef_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "n_support_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Base class for estimators that use libsvm as backing library.\n\nThis implements support vector machine classification and regression.\n\nParameter documentation is in the derived `SVC` class."
        },
        {
          "name": "BaseSVC",
          "decorators": [],
          "superclasses": [
            "BaseLibSVM",
            "ClassifierMixin"
          ],
          "methods": [
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "X",
                  "type": null,
                  "description": "Returns the decision function of the sample for each class\nin the model.\nIf decision_function_shape='ovr', the shape is (n_samples,\nn_classes)."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Evaluates the decision function for the samples in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n    Returns the decision function of the sample for each class\n    in the model.\n    If decision_function_shape='ovr', the shape is (n_samples,\n    n_classes).\n\nNotes\n-----\nIf decision_function_shape='ovo', the function values are proportional\nto the distance of the samples X to the separating hyperplane. If the\nexact distances are required, divide the function values by the norm of\nthe weight vector (``coef_``). See also `this question\n<https://stats.stackexchange.com/questions/14876/\ninterpreting-distance-from-hyperplane-in-svm>`_ for further details.\nIf decision_function_shape='ovr', the decision function is a monotonic\ntransformation of ovo decision function."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "For kernel=\"precomputed\", the expected shape of X is\n(n_samples_test, n_samples_train)."
                }
              ],
              "results": [
                {
                  "name": "y_pred",
                  "type": null,
                  "description": "Class labels for samples in X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform classification on samples in X.\n\nFor an one-class model, +1 or -1 is returned.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n    For kernel=\"precomputed\", the expected shape of X is\n    (n_samples_test, n_samples_train).\n\nReturns\n-------\ny_pred : ndarray of shape (n_samples,)\n    Class labels for samples in X."
            },
            {
              "name": "predict_proba",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "T",
                  "type": null,
                  "description": "Returns the probability of the sample for each class in\nthe model. The columns correspond to the classes in sorted\norder, as they appear in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute probabilities of possible outcomes for samples in X.\n\nThe model need to have probability information computed at training\ntime: fit with attribute `probability` set to True.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    For kernel=\"precomputed\", the expected shape of X is\n    (n_samples_test, n_samples_train).\n\nReturns\n-------\nT : ndarray of shape (n_samples, n_classes)\n    Returns the probability of the sample for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.\n\nNotes\n-----\nThe probability model is created using cross validation, so\nthe results can be slightly different than those obtained by\npredict. Also, it will produce meaningless results on very small\ndatasets."
            },
            {
              "name": "predict_log_proba",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "T",
                  "type": null,
                  "description": "Returns the log-probabilities of the sample for each class in\nthe model. The columns correspond to the classes in sorted\norder, as they appear in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute log probabilities of possible outcomes for samples in X.\n\nThe model need to have probability information computed at training\ntime: fit with attribute `probability` set to True.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n    For kernel=\"precomputed\", the expected shape of X is\n    (n_samples_test, n_samples_train).\n\nReturns\n-------\nT : ndarray of shape (n_samples, n_classes)\n    Returns the log-probabilities of the sample for each class in\n    the model. The columns correspond to the classes in sorted\n    order, as they appear in the attribute :term:`classes_`.\n\nNotes\n-----\nThe probability model is created using cross validation, so\nthe results can be slightly different than those obtained by\npredict. Also, it will produce meaningless results on very small\ndatasets."
            },
            {
              "name": "probA_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "probB_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "ABC for LibSVM-based classifiers."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.svm._bounds",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.preprocessing",
          "declaration": "LabelBinarizer",
          "alias": null
        },
        {
          "module": "sklearn.utils.extmath",
          "declaration": "safe_sparse_dot",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_consistent_length",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "l1_min_c",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Training vector, where n_samples in the number of samples and\nn_features is the number of features."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target vector relative to X."
            }
          ],
          "results": [
            {
              "name": "l1_min_c",
              "type": "float",
              "description": "minimum value for C"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Return the lowest bound for C such that for C in (l1_min_C, infinity)\nthe model is guaranteed not to be empty. This applies to l1 penalized\nclassifiers, such as LinearSVC with penalty='l1' and\nlinear_model.LogisticRegression with penalty='l1'.\n\nThis value is valid if class_weight parameter in fit() is not set.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target vector relative to X.\n\nloss : {'squared_hinge', 'log'}, default='squared_hinge'\n    Specifies the loss function.\n    With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss).\n    With 'log' it is the loss of logistic regression models.\n\nfit_intercept : bool, default=True\n    Specifies if the intercept should be fitted by the model.\n    It must match the fit() method parameter.\n\nintercept_scaling : float, default=1.0\n    when fit_intercept is True, instance vector x becomes\n    [x, intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equals to\n    intercept_scaling is appended to the instance vector.\n    It must match the fit() method parameter.\n\nReturns\n-------\nl1_min_c : float\n    minimum value for C"
        }
      ]
    },
    {
      "name": "sklearn.svm._classes",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "OutlierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "LinearModel",
          "alias": null
        },
        {
          "module": "sklearn.linear_model._base",
          "declaration": "SparseCoefMixin",
          "alias": null
        },
        {
          "module": "sklearn.svm._base",
          "declaration": "BaseLibSVM",
          "alias": null
        },
        {
          "module": "sklearn.svm._base",
          "declaration": "BaseSVC",
          "alias": null
        },
        {
          "module": "sklearn.svm._base",
          "declaration": "_fit_liblinear",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "LinearSVC",
          "decorators": [],
          "superclasses": [
            "LinearClassifierMixin",
            "SparseCoefMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target vector relative to X."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of weights that are assigned to individual\nsamples. If not provided,\nthen each sample is given unit weight.\n\n.. versionadded:: 0.18"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "An instance of the estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model according to the given training data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target vector relative to X.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Array of weights that are assigned to individual\n    samples. If not provided,\n    then each sample is given unit weight.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nself : object\n    An instance of the estimator."
            }
          ],
          "fullDocstring": "Linear Support Vector Classification.\n\nSimilar to SVC with parameter kernel='linear', but implemented in terms of\nliblinear rather than libsvm, so it has more flexibility in the choice of\npenalties and loss functions and should scale better to large numbers of\nsamples.\n\nThis class supports both dense and sparse input and the multiclass support\nis handled according to a one-vs-the-rest scheme.\n\nRead more in the :ref:`User Guide <svm_classification>`.\n\nParameters\n----------\npenalty : {'l1', 'l2'}, default='l2'\n    Specifies the norm used in the penalization. The 'l2'\n    penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n    vectors that are sparse.\n\nloss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n    Specifies the loss function. 'hinge' is the standard SVM loss\n    (used e.g. by the SVC class) while 'squared_hinge' is the\n    square of the hinge loss. The combination of ``penalty='l1'``\n    and ``loss='hinge'`` is not supported.\n\ndual : bool, default=True\n    Select the algorithm to either solve the dual or primal\n    optimization problem. Prefer dual=False when n_samples > n_features.\n\ntol : float, default=1e-4\n    Tolerance for stopping criteria.\n\nC : float, default=1.0\n    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n\nmulti_class : {'ovr', 'crammer_singer'}, default='ovr'\n    Determines the multi-class strategy if `y` contains more than\n    two classes.\n    ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n    ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n    While `crammer_singer` is interesting from a theoretical perspective\n    as it is consistent, it is seldom used in practice as it rarely leads\n    to better accuracy and is more expensive to compute.\n    If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n    will be ignored.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be already centered).\n\nintercept_scaling : float, default=1\n    When self.fit_intercept is True, instance vector x becomes\n    ``[x, self.intercept_scaling]``,\n    i.e. a \"synthetic\" feature with constant value equals to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes intercept_scaling * synthetic feature weight\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n\nclass_weight : dict or 'balanced', default=None\n    Set the parameter C of class i to ``class_weight[i]*C`` for\n    SVC. If not given, all classes are supposed to have\n    weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\nverbose : int, default=0\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in liblinear that, if enabled, may not work\n    properly in a multithreaded context.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generation for shuffling the data for\n    the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n    underlying implementation of :class:`LinearSVC` is not random and\n    ``random_state`` has no effect on the results.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_iter : int, default=1000\n    The maximum number of iterations to be run.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    ``coef_`` is a readonly property derived from ``raw_coef_`` that\n    follows the internal memory layout of liblinear.\n\nintercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n    Constants in decision function.\n\nclasses_ : ndarray of shape (n_classes,)\n    The unique classes labels.\n\nn_iter_ : int\n    Maximum number of iterations run across all classes.\n\nSee Also\n--------\nSVC : Implementation of Support Vector Machine classifier using libsvm:\n    the kernel can be non-linear but its SMO algorithm does not\n    scale to large number of samples as LinearSVC does.\n\n    Furthermore SVC multi-class mode is implemented using one\n    vs one scheme while LinearSVC uses one vs the rest. It is\n    possible to implement one vs the rest with SVC by using the\n    :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n\n    Finally SVC can fit dense data without memory copy if the input\n    is C-contiguous. Sparse data will still incur memory copy though.\n\nsklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n    cost function as LinearSVC\n    by adjusting the penalty and loss parameters. In addition it requires\n    less memory, allows incremental (online) learning, and implements\n    various loss functions and regularization regimes.\n\nNotes\n-----\nThe underlying C implementation uses a random number generator to\nselect features when fitting the model. It is thus not uncommon\nto have slightly different results for the same input data. If\nthat happens, try with a smaller ``tol`` parameter.\n\nThe underlying implementation, liblinear, uses a sparse internal\nrepresentation for the data that will incur a memory copy.\n\nPredict output may not match that of standalone liblinear in certain\ncases. See :ref:`differences from liblinear <liblinear_differences>`\nin the narrative documentation.\n\nReferences\n----------\n`LIBLINEAR: A Library for Large Linear Classification\n<https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n\nExamples\n--------\n>>> from sklearn.svm import LinearSVC\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_features=4, random_state=0)\n>>> clf = make_pipeline(StandardScaler(),\n...                     LinearSVC(random_state=0, tol=1e-5))\n>>> clf.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n\n>>> print(clf.named_steps['linearsvc'].coef_)\n[[0.141...   0.526... 0.679... 0.493...]]\n\n>>> print(clf.named_steps['linearsvc'].intercept_)\n[0.1693...]\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]"
        },
        {
          "name": "LinearSVR",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "LinearModel"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples in the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target vector relative to X"
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Array of weights that are assigned to individual\nsamples. If not provided,\nthen each sample is given unit weight.\n\n.. versionadded:: 0.18"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "An instance of the estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit the model according to the given training data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target vector relative to X\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Array of weights that are assigned to individual\n    samples. If not provided,\n    then each sample is given unit weight.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nself : object\n    An instance of the estimator."
            }
          ],
          "fullDocstring": "Linear Support Vector Regression.\n\nSimilar to SVR with parameter kernel='linear', but implemented in terms of\nliblinear rather than libsvm, so it has more flexibility in the choice of\npenalties and loss functions and should scale better to large numbers of\nsamples.\n\nThis class supports both dense and sparse input.\n\nRead more in the :ref:`User Guide <svm_regression>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\nepsilon : float, default=0.0\n    Epsilon parameter in the epsilon-insensitive loss function. Note\n    that the value of this parameter depends on the scale of the target\n    variable y. If unsure, set ``epsilon=0``.\n\ntol : float, default=1e-4\n    Tolerance for stopping criteria.\n\nC : float, default=1.0\n    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n\nloss : {'epsilon_insensitive', 'squared_epsilon_insensitive'},             default='epsilon_insensitive'\n    Specifies the loss function. The epsilon-insensitive loss\n    (standard SVR) is the L1 loss, while the squared epsilon-insensitive\n    loss ('squared_epsilon_insensitive') is the L2 loss.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be already centered).\n\nintercept_scaling : float, default=1.\n    When self.fit_intercept is True, instance vector x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equals to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes intercept_scaling * synthetic feature weight\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n\ndual : bool, default=True\n    Select the algorithm to either solve the dual or primal\n    optimization problem. Prefer dual=False when n_samples > n_features.\n\nverbose : int, default=0\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in liblinear that, if enabled, may not work\n    properly in a multithreaded context.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generation for shuffling the data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_iter : int, default=1000\n    The maximum number of iterations to be run.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is a readonly property derived from `raw_coef_` that\n    follows the internal memory layout of liblinear.\n\nintercept_ : ndarray of shape (1) if n_classes == 2 else (n_classes)\n    Constants in decision function.\n\nn_iter_ : int\n    Maximum number of iterations run across all classes.\n\nExamples\n--------\n>>> from sklearn.svm import LinearSVR\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=4, random_state=0)\n>>> regr = make_pipeline(StandardScaler(),\n...                      LinearSVR(random_state=0, tol=1e-5))\n>>> regr.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvr', LinearSVR(random_state=0, tol=1e-05))])\n\n>>> print(regr.named_steps['linearsvr'].coef_)\n[18.582... 27.023... 44.357... 64.522...]\n>>> print(regr.named_steps['linearsvr'].intercept_)\n[-4...]\n>>> print(regr.predict([[0, 0, 0, 0]]))\n[-2.384...]\n\n\nSee Also\n--------\nLinearSVC : Implementation of Support Vector Machine classifier using the\n    same library as this class (liblinear).\n\nSVR : Implementation of Support Vector Machine regression using libsvm:\n    the kernel can be non-linear but its SMO algorithm does not\n    scale to large number of samples as LinearSVC does.\n\nsklearn.linear_model.SGDRegressor : SGDRegressor can optimize the same cost\n    function as LinearSVR\n    by adjusting the penalty and loss parameters. In addition it requires\n    less memory, allows incremental (online) learning, and implements\n    various loss functions and regularization regimes."
        },
        {
          "name": "NuSVC",
          "decorators": [],
          "superclasses": [
            "BaseSVC"
          ],
          "methods": [],
          "fullDocstring": "Nu-Support Vector Classification.\n\nSimilar to SVC but uses a parameter to control the number of support\nvectors.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <svm_classification>`.\n\nParameters\n----------\nnu : float, default=0.5\n    An upper bound on the fraction of margin errors (see :ref:`User Guide\n    <nu_svc>`) and a lower bound of the fraction of support vectors.\n    Should be in the interval (0, 1].\n\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n     Specifies the kernel type to be used in the algorithm.\n     It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n     a callable.\n     If none is given, 'rbf' will be used. If a callable is given it is\n     used to precompute the kernel matrix.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\nprobability : bool, default=False\n    Whether to enable probability estimates. This must be enabled prior\n    to calling `fit`, will slow down that method as it internally uses\n    5-fold cross-validation, and `predict_proba` may be inconsistent with\n    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nclass_weight : {dict, 'balanced'}, default=None\n    Set the parameter C of class i to class_weight[i]*C for\n    SVC. If not given, all classes are supposed to have\n    weight one. The \"balanced\" mode uses the values of y to automatically\n    adjust weights inversely proportional to class frequencies as\n    ``n_samples / (n_classes * np.bincount(y))``\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\ndecision_function_shape : {'ovo', 'ovr'}, default='ovr'\n    Whether to return a one-vs-rest ('ovr') decision function of shape\n    (n_samples, n_classes) as all other classifiers, or the original\n    one-vs-one ('ovo') decision function of libsvm which has shape\n    (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n    ('ovo') is always used as multi-class strategy. The parameter is\n    ignored for binary classification.\n\n    .. versionchanged:: 0.19\n        decision_function_shape is 'ovr' by default.\n\n    .. versionadded:: 0.17\n       *decision_function_shape='ovr'* is recommended.\n\n    .. versionchanged:: 0.17\n       Deprecated *decision_function_shape='ovo' and None*.\n\nbreak_ties : bool, default=False\n    If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n    :term:`predict` will break ties according to the confidence values of\n    :term:`decision_function`; otherwise the first class among the tied\n    classes is returned. Please note that breaking ties comes at a\n    relatively high computational cost compared to a simple predict.\n\n    .. versionadded:: 0.22\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generation for shuffling the data for\n    probability estimates. Ignored when `probability` is False.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nclass_weight_ : ndarray of shape (n_classes,)\n    Multipliers of parameter C of each class.\n    Computed based on the ``class_weight`` parameter.\n\nclasses_ : ndarray of shape (n_classes,)\n    The unique classes labels.\n\ncoef_ : ndarray of shape (n_classes * (n_classes -1) / 2, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (n_classes - 1, n_SV)\n    Dual coefficients of the support vector in the decision\n    function (see :ref:`sgd_mathematical_formulation`), multiplied by\n    their targets.\n    For multiclass, coefficient for all 1-vs-1 classifiers.\n    The layout of the coefficients in the multiclass case is somewhat\n    non-trivial. See the :ref:`multi-class section of the User Guide\n    <svm_multi_class>` for details.\n\nfit_status_ : int\n    0 if correctly fitted, 1 if the algorithm did not converge.\n\nintercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n    Constants in decision function.\n\nsupport_ : ndarray of shape (n_SV,)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors.\n\nn_support_ : ndarray of shape (n_classes,), dtype=int32\n    Number of support vectors for each class.\n\nfit_status_ : int\n    0 if correctly fitted, 1 if the algorithm did not converge.\n\nprobA_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\nprobB_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n    If `probability=True`, it corresponds to the parameters learned in\n    Platt scaling to produce probability estimates from decision values.\n    If `probability=False`, it's an empty array. Platt scaling uses the\n    logistic function\n    ``1 / (1 + exp(decision_value * probA_ + probB_))``\n    where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n    more information on the multiclass case and training procedure see\n    section 8 of [1]_.\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> y = np.array([1, 1, 2, 2])\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.svm import NuSVC\n>>> clf = make_pipeline(StandardScaler(), NuSVC())\n>>> clf.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])\n>>> print(clf.predict([[-0.8, -1]]))\n[1]\n\nSee Also\n--------\nSVC : Support Vector Machine for classification using libsvm.\n\nLinearSVC : Scalable linear Support Vector Machine for classification using\n    liblinear.\n\nReferences\n----------\n.. [1] `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n.. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n    machines and comparison to regularizedlikelihood methods.\"\n    <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_"
        },
        {
          "name": "NuSVR",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseLibSVM"
          ],
          "methods": [],
          "fullDocstring": "Nu Support Vector Regression.\n\nSimilar to NuSVC, for regression, uses a parameter nu to control\nthe number of support vectors. However, unlike NuSVC, where nu\nreplaces C, here nu replaces the parameter epsilon of epsilon-SVR.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <svm_regression>`.\n\nParameters\n----------\nnu : float, default=0.5\n    An upper bound on the fraction of training errors and a lower bound of\n    the fraction of support vectors. Should be in the interval (0, 1].  By\n    default 0.5 will be taken.\n\nC : float, default=1.0\n    Penalty parameter C of the error term.\n\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n     Specifies the kernel type to be used in the algorithm.\n     It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n     a callable.\n     If none is given, 'rbf' will be used. If a callable is given it is\n     used to precompute the kernel matrix.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\nAttributes\n----------\nclass_weight_ : ndarray of shape (n_classes,)\n    Multipliers of parameter C for each class.\n    Computed based on the ``class_weight`` parameter.\n\ncoef_ : ndarray of shape (1, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (1, n_SV)\n    Coefficients of the support vector in the decision function.\n\nfit_status_ : int\n    0 if correctly fitted, 1 otherwise (will raise warning)\n\nintercept_ : ndarray of shape (1,)\n    Constants in decision function.\n\nn_support_ : ndarray of shape (n_classes,), dtype=int32\n    Number of support vectors for each class.\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nsupport_ : ndarray of shape (n_SV,)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors.\n\nExamples\n--------\n>>> from sklearn.svm import NuSVR\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> import numpy as np\n>>> n_samples, n_features = 10, 5\n>>> np.random.seed(0)\n>>> y = np.random.randn(n_samples)\n>>> X = np.random.randn(n_samples, n_features)\n>>> regr = make_pipeline(StandardScaler(), NuSVR(C=1.0, nu=0.1))\n>>> regr.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('nusvr', NuSVR(nu=0.1))])\n\nSee Also\n--------\nNuSVC : Support Vector Machine for classification implemented with libsvm\n    with a parameter to control the number of support vectors.\n\nSVR : Epsilon Support Vector Machine for regression implemented with\n    libsvm.\n\nReferences\n----------\n.. [1] `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n.. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n    machines and comparison to regularizedlikelihood methods.\"\n    <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_"
        },
        {
          "name": "OneClassSVM",
          "decorators": [],
          "superclasses": [
            "BaseLibSVM",
            "OutlierMixin"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Set of samples, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "not used, present for API consistency by convention."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Per-sample weights. Rescale C per sample. Higher weights\nforce the classifier to put more emphasis on these points."
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Detects the soft boundary of the set of samples X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Set of samples, where n_samples is the number of samples and\n    n_features is the number of features.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Per-sample weights. Rescale C per sample. Higher weights\n    force the classifier to put more emphasis on these points.\n\ny : Ignored\n    not used, present for API consistency by convention.\n\nReturns\n-------\nself : object\n\nNotes\n-----\nIf X is not a C-ordered contiguous array it is copied."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data matrix."
                }
              ],
              "results": [
                {
                  "name": "dec",
                  "type": null,
                  "description": "Returns the decision function of the samples."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Signed distance to the separating hyperplane.\n\nSigned distance is positive for an inlier and negative for an outlier.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\ndec : ndarray of shape (n_samples,)\n    Returns the decision function of the samples."
            },
            {
              "name": "score_samples",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The data matrix."
                }
              ],
              "results": [
                {
                  "name": "score_samples",
                  "type": null,
                  "description": "Returns the (unshifted) scoring function of the samples."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Raw scoring function of the samples.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\nscore_samples : ndarray of shape (n_samples,)\n    Returns the (unshifted) scoring function of the samples."
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "For kernel=\"precomputed\", the expected shape of X is\n(n_samples_test, n_samples_train)."
                }
              ],
              "results": [
                {
                  "name": "y_pred",
                  "type": null,
                  "description": "Class labels for samples in X."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Perform classification on samples in X.\n\nFor a one-class model, +1 or -1 is returned.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n    For kernel=\"precomputed\", the expected shape of X is\n    (n_samples_test, n_samples_train).\n\nReturns\n-------\ny_pred : ndarray of shape (n_samples,)\n    Class labels for samples in X."
            },
            {
              "name": "probA_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "probB_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Unsupervised Outlier Detection.\n\nEstimate the support of a high-dimensional distribution.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <outlier_detection>`.\n\nParameters\n----------\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n     Specifies the kernel type to be used in the algorithm.\n     It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n     a callable.\n     If none is given, 'rbf' will be used. If a callable is given it is\n     used to precompute the kernel matrix.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\nnu : float, default=0.5\n    An upper bound on the fraction of training\n    errors and a lower bound of the fraction of support\n    vectors. Should be in the interval (0, 1]. By default 0.5\n    will be taken.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\nAttributes\n----------\nclass_weight_ : ndarray of shape (n_classes,)\n    Multipliers of parameter C for each class.\n    Computed based on the ``class_weight`` parameter.\n\ncoef_ : ndarray of shape (1, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (1, n_SV)\n    Coefficients of the support vectors in the decision function.\n\nfit_status_ : int\n    0 if correctly fitted, 1 otherwise (will raise warning)\n\nintercept_ : ndarray of shape (1,)\n    Constant in the decision function.\n\nn_support_ : ndarray of shape (n_classes,), dtype=int32\n    Number of support vectors for each class.\n\noffset_ : float\n    Offset used to define the decision function from the raw scores.\n    We have the relation: decision_function = score_samples - `offset_`.\n    The offset is the opposite of `intercept_` and is provided for\n    consistency with other outlier detection algorithms.\n\n    .. versionadded:: 0.20\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nsupport_ : ndarray of shape (n_SV,)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors.\n\nExamples\n--------\n>>> from sklearn.svm import OneClassSVM\n>>> X = [[0], [0.44], [0.45], [0.46], [1]]\n>>> clf = OneClassSVM(gamma='auto').fit(X)\n>>> clf.predict(X)\narray([-1,  1,  1,  1, -1])\n>>> clf.score_samples(X)\narray([1.7798..., 2.0547..., 2.0556..., 2.0561..., 1.7332...])"
        },
        {
          "name": "SVC",
          "decorators": [],
          "superclasses": [
            "BaseSVC"
          ],
          "methods": [],
          "fullDocstring": "C-Support Vector Classification.\n\nThe implementation is based on libsvm. The fit time scales at least\nquadratically with the number of samples and may be impractical\nbeyond tens of thousands of samples. For large datasets\nconsider using :class:`~sklearn.svm.LinearSVC` or\n:class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n:class:`~sklearn.kernel_approximation.Nystroem` transformer.\n\nThe multiclass support is handled according to a one-vs-one scheme.\n\nFor details on the precise mathematical formulation of the provided\nkernel functions and how `gamma`, `coef0` and `degree` affect each\nother, see the corresponding section in the narrative documentation:\n:ref:`svm_kernels`.\n\nRead more in the :ref:`User Guide <svm_classification>`.\n\nParameters\n----------\nC : float, default=1.0\n    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive. The penalty\n    is a squared l2 penalty.\n\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n    Specifies the kernel type to be used in the algorithm.\n    It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n    a callable.\n    If none is given, 'rbf' will be used. If a callable is given it is\n    used to pre-compute the kernel matrix from data matrices; that matrix\n    should be an array of shape ``(n_samples, n_samples)``.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\nprobability : bool, default=False\n    Whether to enable probability estimates. This must be enabled prior\n    to calling `fit`, will slow down that method as it internally uses\n    5-fold cross-validation, and `predict_proba` may be inconsistent with\n    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nclass_weight : dict or 'balanced', default=None\n    Set the parameter C of class i to class_weight[i]*C for\n    SVC. If not given, all classes are supposed to have\n    weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\ndecision_function_shape : {'ovo', 'ovr'}, default='ovr'\n    Whether to return a one-vs-rest ('ovr') decision function of shape\n    (n_samples, n_classes) as all other classifiers, or the original\n    one-vs-one ('ovo') decision function of libsvm which has shape\n    (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n    ('ovo') is always used as multi-class strategy. The parameter is\n    ignored for binary classification.\n\n    .. versionchanged:: 0.19\n        decision_function_shape is 'ovr' by default.\n\n    .. versionadded:: 0.17\n       *decision_function_shape='ovr'* is recommended.\n\n    .. versionchanged:: 0.17\n       Deprecated *decision_function_shape='ovo' and None*.\n\nbreak_ties : bool, default=False\n    If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n    :term:`predict` will break ties according to the confidence values of\n    :term:`decision_function`; otherwise the first class among the tied\n    classes is returned. Please note that breaking ties comes at a\n    relatively high computational cost compared to a simple predict.\n\n    .. versionadded:: 0.22\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generation for shuffling the data for\n    probability estimates. Ignored when `probability` is False.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nclass_weight_ : ndarray of shape (n_classes,)\n    Multipliers of parameter C for each class.\n    Computed based on the ``class_weight`` parameter.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\ncoef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is a readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (n_classes -1, n_SV)\n    Dual coefficients of the support vector in the decision\n    function (see :ref:`sgd_mathematical_formulation`), multiplied by\n    their targets.\n    For multiclass, coefficient for all 1-vs-1 classifiers.\n    The layout of the coefficients in the multiclass case is somewhat\n    non-trivial. See the :ref:`multi-class section of the User Guide\n    <svm_multi_class>` for details.\n\nfit_status_ : int\n    0 if correctly fitted, 1 otherwise (will raise warning)\n\nintercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n    Constants in decision function.\n\nsupport_ : ndarray of shape (n_SV)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors.\n\nn_support_ : ndarray of shape (n_classes,), dtype=int32\n    Number of support vectors for each class.\n\nprobA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\nprobB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n    If `probability=True`, it corresponds to the parameters learned in\n    Platt scaling to produce probability estimates from decision values.\n    If `probability=False`, it's an empty array. Platt scaling uses the\n    logistic function\n    ``1 / (1 + exp(decision_value * probA_ + probB_))``\n    where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n    more information on the multiclass case and training procedure see\n    section 8 of [1]_.\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> y = np.array([1, 1, 2, 2])\n>>> from sklearn.svm import SVC\n>>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n>>> clf.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc', SVC(gamma='auto'))])\n\n>>> print(clf.predict([[-0.8, -1]]))\n[1]\n\nSee Also\n--------\nSVR : Support Vector Machine for Regression implemented using libsvm.\n\nLinearSVC : Scalable Linear Support Vector Machine for classification\n    implemented using liblinear. Check the See Also section of\n    LinearSVC for more comparison element.\n\nReferences\n----------\n.. [1] `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n.. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n    machines and comparison to regularizedlikelihood methods.\"\n    <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_"
        },
        {
          "name": "SVR",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseLibSVM"
          ],
          "methods": [
            {
              "name": "probA_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "probB_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Epsilon-Support Vector Regression.\n\nThe free parameters in the model are C and epsilon.\n\nThe implementation is based on libsvm. The fit time complexity\nis more than quadratic with the number of samples which makes it hard\nto scale to datasets with more than a couple of 10000 samples. For large\ndatasets consider using :class:`~sklearn.svm.LinearSVR` or\n:class:`~sklearn.linear_model.SGDRegressor` instead, possibly after a\n:class:`~sklearn.kernel_approximation.Nystroem` transformer.\n\nRead more in the :ref:`User Guide <svm_regression>`.\n\nParameters\n----------\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n     Specifies the kernel type to be used in the algorithm.\n     It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n     a callable.\n     If none is given, 'rbf' will be used. If a callable is given it is\n     used to precompute the kernel matrix.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\nC : float, default=1.0\n    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n    The penalty is a squared l2 penalty.\n\nepsilon : float, default=0.1\n     Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n     within which no penalty is associated in the training loss function\n     with points predicted within a distance epsilon from the actual\n     value.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\nAttributes\n----------\nclass_weight_ : ndarray of shape (n_classes,)\n    Multipliers of parameter C for each class.\n    Computed based on the ``class_weight`` parameter.\n\ncoef_ : ndarray of shape (1, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (1, n_SV)\n    Coefficients of the support vector in the decision function.\n\nfit_status_ : int\n    0 if correctly fitted, 1 otherwise (will raise warning)\n\nintercept_ : ndarray of shape (1,)\n    Constants in decision function.\n\nn_support_ : ndarray of shape (n_classes,), dtype=int32\n    Number of support vectors for each class.\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nsupport_ : ndarray of shape (n_SV,)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors.\n\nExamples\n--------\n>>> from sklearn.svm import SVR\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> import numpy as np\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n>>> regr.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('svr', SVR(epsilon=0.2))])\n\nSee Also\n--------\nNuSVR : Support Vector Machine for regression implemented using libsvm\n    using a parameter to control the number of support vectors.\n\nLinearSVR : Scalable Linear Support Vector Machine for regression\n    implemented using liblinear.\n\nReferences\n----------\n.. [1] `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n.. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n    machines and comparison to regularizedlikelihood methods.\"\n    <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.svm.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.tree",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn._classes",
          "declaration": "BaseDecisionTree",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "DecisionTreeClassifier",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "DecisionTreeRegressor",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "ExtraTreeClassifier",
          "alias": null
        },
        {
          "module": "sklearn._classes",
          "declaration": "ExtraTreeRegressor",
          "alias": null
        },
        {
          "module": "sklearn._export",
          "declaration": "export_graphviz",
          "alias": null
        },
        {
          "module": "sklearn._export",
          "declaration": "export_text",
          "alias": null
        },
        {
          "module": "sklearn._export",
          "declaration": "plot_tree",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.tree._classes",
      "imports": [
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "math",
          "declaration": "ceil",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "MultiOutputMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "_criterion",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "_splitter",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "_tree",
          "alias": null
        },
        {
          "module": "sklearn.tree._criterion",
          "declaration": "Criterion",
          "alias": null
        },
        {
          "module": "sklearn.tree._splitter",
          "declaration": "Splitter",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "BestFirstTreeBuilder",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "DepthFirstTreeBuilder",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "Tree",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "_build_pruned_tree_ccp",
          "alias": null
        },
        {
          "module": "sklearn.tree._tree",
          "declaration": "ccp_pruning_path",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "Bunch",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "compute_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "BaseDecisionTree",
          "decorators": [],
          "superclasses": [
            "BaseEstimator",
            "MultiOutputMixin"
          ],
          "methods": [
            {
              "name": "get_depth",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self.tree_.max_depth",
                  "type": "int",
                  "description": "The maximum depth of the tree."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the depth of the decision tree.\n\nThe depth of a tree is the maximum distance between the root\nand any leaf.\n\nReturns\n-------\nself.tree_.max_depth : int\n    The maximum depth of the tree."
            },
            {
              "name": "get_n_leaves",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "self.tree_.n_leaves",
                  "type": "int",
                  "description": "Number of leaves."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the number of leaves of the decision tree.\n\nReturns\n-------\nself.tree_.n_leaves : int\n    Number of leaves."
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X_idx_sorted",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "deprecated",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Allow to bypass several input checking.\nDon't use this parameter unless you know what you do."
                }
              ],
              "results": [
                {
                  "name": "y",
                  "type": null,
                  "description": "The predicted classes, or the predict values."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class or regression value for X.\n\nFor a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\ncheck_input : bool, default=True\n    Allow to bypass several input checking.\n    Don't use this parameter unless you know what you do.\n\nReturns\n-------\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    The predicted classes, or the predict values."
            },
            {
              "name": "apply",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Allow to bypass several input checking.\nDon't use this parameter unless you know what you do."
                }
              ],
              "results": [
                {
                  "name": "X_leaves",
                  "type": null,
                  "description": "For each datapoint x in X, return the index of the leaf x\nends up in. Leaves are numbered within\n``[0; self.tree_.node_count)``, possibly with gaps in the\nnumbering."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the index of the leaf that each sample is predicted as.\n\n.. versionadded:: 0.17\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\ncheck_input : bool, default=True\n    Allow to bypass several input checking.\n    Don't use this parameter unless you know what you do.\n\nReturns\n-------\nX_leaves : array-like of shape (n_samples,)\n    For each datapoint x in X, return the index of the leaf x\n    ends up in. Leaves are numbered within\n    ``[0; self.tree_.node_count)``, possibly with gaps in the\n    numbering."
            },
            {
              "name": "decision_path",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Allow to bypass several input checking.\nDon't use this parameter unless you know what you do."
                }
              ],
              "results": [
                {
                  "name": "indicator",
                  "type": null,
                  "description": "Return a node indicator CSR matrix where non zero elements\nindicates that the samples goes through the nodes."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the decision path in the tree.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\ncheck_input : bool, default=True\n    Allow to bypass several input checking.\n    Don't use this parameter unless you know what you do.\n\nReturns\n-------\nindicator : sparse matrix of shape (n_samples, n_nodes)\n    Return a node indicator CSR matrix where non zero elements\n    indicates that the samples goes through the nodes."
            },
            {
              "name": "cost_complexity_pruning_path",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csc_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (class labels) as integers or strings."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. Splits are also\nignored if they would result in any single class carrying a\nnegative weight in either child node."
                }
              ],
              "results": [
                {
                  "name": "ccp_path",
                  "type": null,
                  "description": "Dictionary-like object, with the following attributes.\n\nccp_alphas : ndarray\n    Effective alphas of subtree during pruning.\n\nimpurities : ndarray\n    Sum of the impurities of the subtree leaves for the\n    corresponding alpha value in ``ccp_alphas``."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Compute the pruning path during Minimal Cost-Complexity Pruning.\n\nSee :ref:`minimal_cost_complexity_pruning` for details on the pruning\nprocess.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csc_matrix``.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    The target values (class labels) as integers or strings.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node. Splits are also\n    ignored if they would result in any single class carrying a\n    negative weight in either child node.\n\nReturns\n-------\nccp_path : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    ccp_alphas : ndarray\n        Effective alphas of subtree during pruning.\n\n    impurities : ndarray\n        Sum of the impurities of the subtree leaves for the\n        corresponding alpha value in ``ccp_alphas``."
            },
            {
              "name": "feature_importances_",
              "decorators": [
                "property"
              ],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [
                {
                  "name": "feature_importances_",
                  "type": null,
                  "description": "Normalized total reduction of criteria by feature\n(Gini importance)."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Return the feature importances.\n\nThe importance of a feature is computed as the (normalized) total\nreduction of the criterion brought by that feature.\nIt is also known as the Gini importance.\n\nWarning: impurity-based feature importances can be misleading for\nhigh cardinality features (many unique values). See\n:func:`sklearn.inspection.permutation_importance` as an alternative.\n\nReturns\n-------\nfeature_importances_ : ndarray of shape (n_features,)\n    Normalized total reduction of criteria by feature\n    (Gini importance)."
            }
          ],
          "fullDocstring": "Base class for decision trees.\n\nWarning: This class should not be used directly.\nUse derived classes instead."
        },
        {
          "name": "DecisionTreeClassifier",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "BaseDecisionTree"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csc_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (class labels) as integers or strings."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. Splits are also\nignored if they would result in any single class carrying a\nnegative weight in either child node."
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Allow to bypass several input checking.\nDon't use this parameter unless you know what you do."
                },
                {
                  "name": "X_idx_sorted",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "deprecated",
                  "limitation": null,
                  "ignored": false,
                  "description": "This parameter is deprecated and has no effect.\nIt will be removed in 1.1 (renaming of 0.26).\n\n.. deprecated :: 0.24"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Build a decision tree classifier from the training set (X, y).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csc_matrix``.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    The target values (class labels) as integers or strings.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node. Splits are also\n    ignored if they would result in any single class carrying a\n    negative weight in either child node.\n\ncheck_input : bool, default=True\n    Allow to bypass several input checking.\n    Don't use this parameter unless you know what you do.\n\nX_idx_sorted : deprecated, default=\"deprecated\"\n    This parameter is deprecated and has no effect.\n    It will be removed in 1.1 (renaming of 0.26).\n\n    .. deprecated :: 0.24\n\nReturns\n-------\nself : DecisionTreeClassifier\n    Fitted estimator."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Allow to bypass several input checking.\nDon't use this parameter unless you know what you do."
                }
              ],
              "results": [
                {
                  "name": "proba",
                  "type": null,
                  "description": "The class probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class probabilities of the input samples X.\n\nThe predicted class probability is the fraction of samples of the same\nclass in a leaf.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\ncheck_input : bool, default=True\n    Allow to bypass several input checking.\n    Don't use this parameter unless you know what you do.\n\nReturns\n-------\nproba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            },
            {
              "name": "predict_log_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
                }
              ],
              "results": [
                {
                  "name": "proba",
                  "type": null,
                  "description": "The class log-probabilities of the input samples. The order of the\nclasses corresponds to that in the attribute :term:`classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict class log-probabilities of the input samples X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nproba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n    The class log-probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`."
            }
          ],
          "fullDocstring": "A decision tree classifier.\n\nRead more in the :ref:`User Guide <tree>`.\n\nParameters\n----------\ncriterion : {\"gini\", \"entropy\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\nsplitter : {\"best\", \"random\"}, default=\"best\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n    The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary <random_state>` for details.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=0\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nclass_weight : dict, list of dict or \"balanced\", default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If None, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,) or list of ndarray\n    The classes labels (single output problem),\n    or a list of arrays of class labels (multi-output problem).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance [4]_.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nn_classes_ : int or list of int\n    The number of classes (for single output problems),\n    or a list containing the number of classes for each\n    output (for multi-output problems).\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nDecisionTreeRegressor : A decision tree regressor.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe :meth:`predict` method operates using the :func:`numpy.argmax`\nfunction on the outputs of :meth:`predict_proba`. This means that in\ncase the highest predicted probabilities are tied, the classifier will\npredict the tied class with the lowest index in :term:`classes_`.\n\nReferences\n----------\n\n.. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n.. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n       Learning\", Springer, 2009.\n\n.. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> clf = DecisionTreeClassifier(random_state=0)\n>>> iris = load_iris()\n>>> cross_val_score(clf, iris.data, iris.target, cv=10)\n...                             # doctest: +SKIP\n...\narray([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])"
        },
        {
          "name": "DecisionTreeRegressor",
          "decorators": [],
          "superclasses": [
            "RegressorMixin",
            "BaseDecisionTree"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The training input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csc_matrix``."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The target values (real numbers). Use ``dtype=np.float64`` and\n``order='C'`` for maximum efficiency."
                },
                {
                  "name": "sample_weight",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node."
                },
                {
                  "name": "check_input",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": "Allow to bypass several input checking.\nDon't use this parameter unless you know what you do."
                },
                {
                  "name": "X_idx_sorted",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "deprecated",
                  "limitation": null,
                  "ignored": false,
                  "description": "This parameter is deprecated and has no effect.\nIt will be removed in 1.1 (renaming of 0.26).\n\n.. deprecated :: 0.24"
                }
              ],
              "results": [
                {
                  "name": "self",
                  "type": null,
                  "description": "Fitted estimator."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Build a decision tree regressor from the training set (X, y).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csc_matrix``.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    The target values (real numbers). Use ``dtype=np.float64`` and\n    ``order='C'`` for maximum efficiency.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node.\n\ncheck_input : bool, default=True\n    Allow to bypass several input checking.\n    Don't use this parameter unless you know what you do.\n\nX_idx_sorted : deprecated, default=\"deprecated\"\n    This parameter is deprecated and has no effect.\n    It will be removed in 1.1 (renaming of 0.26).\n\n    .. deprecated :: 0.24\n\nReturns\n-------\nself : DecisionTreeRegressor\n    Fitted estimator."
            }
          ],
          "fullDocstring": "A decision tree regressor.\n\nRead more in the :ref:`User Guide <tree>`.\n\nParameters\n----------\ncriterion : {\"mse\", \"friedman_mse\", \"mae\", \"poisson\"}, default=\"mse\"\n    The function to measure the quality of a split. Supported criteria\n    are \"mse\" for the mean squared error, which is equal to variance\n    reduction as feature selection criterion and minimizes the L2 loss\n    using the mean of each terminal node, \"friedman_mse\", which uses mean\n    squared error with Friedman's improvement score for potential splits,\n    \"mae\" for the mean absolute error, which minimizes the L1 loss using\n    the median of each terminal node, and \"poisson\" which uses reduction in\n    Poisson deviance to find splits.\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\n    .. versionadded:: 0.24\n        Poisson deviance criterion.\n\nsplitter : {\"best\", \"random\"}, default=\"best\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary <random_state>` for details.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=0\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nfeature_importances_ : ndarray of shape (n_features,)\n    The feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the\n    (normalized) total reduction of the criterion brought\n    by that feature. It is also known as the Gini importance [4]_.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nDecisionTreeClassifier : A decision tree classifier.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n\n.. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n.. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n       Learning\", Springer, 2009.\n\n.. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.tree import DecisionTreeRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> regressor = DecisionTreeRegressor(random_state=0)\n>>> cross_val_score(regressor, X, y, cv=10)\n...                    # doctest: +SKIP\n...\narray([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\n       0.16...,  0.11..., -0.73..., -0.30..., -0.00...])"
        },
        {
          "name": "ExtraTreeClassifier",
          "decorators": [],
          "superclasses": [
            "DecisionTreeClassifier"
          ],
          "methods": [],
          "fullDocstring": "An extremely randomized tree classifier.\n\nExtra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the `max_features` randomly\nselected features and the best split among those is chosen. When\n`max_features` is set 1, this amounts to building a totally random\ndecision tree.\n\nWarning: Extra-trees should only be used within ensemble methods.\n\nRead more in the :ref:`User Guide <tree>`.\n\nParameters\n----------\ncriterion : {\"gini\", \"entropy\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\nsplitter : {\"random\", \"best\"}, default=\"random\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float, {\"auto\", \"sqrt\", \"log2\"} or None, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used to pick randomly the `max_features` used at each split.\n    See :term:`Glossary <random_state>` for details.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nclass_weight : dict, list of dict or \"balanced\", default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If None, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,) or list of ndarray\n    The classes labels (single output problem),\n    or a list of arrays of class labels (multi-output problem).\n\nmax_features_ : int\n    The inferred value of max_features.\n\nn_classes_ : int or list of int\n    The number of classes (for single output problems),\n    or a list containing the number of classes for each\n    output (for multi-output problems).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nExtraTreeRegressor : An extremely randomized tree regressor.\nsklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.\nsklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.ensemble import BaggingClassifier\n>>> from sklearn.tree import ExtraTreeClassifier\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...    X, y, random_state=0)\n>>> extra_tree = ExtraTreeClassifier(random_state=0)\n>>> cls = BaggingClassifier(extra_tree, random_state=0).fit(\n...    X_train, y_train)\n>>> cls.score(X_test, y_test)\n0.8947..."
        },
        {
          "name": "ExtraTreeRegressor",
          "decorators": [],
          "superclasses": [
            "DecisionTreeRegressor"
          ],
          "methods": [],
          "fullDocstring": "An extremely randomized tree regressor.\n\nExtra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the `max_features` randomly\nselected features and the best split among those is chosen. When\n`max_features` is set 1, this amounts to building a totally random\ndecision tree.\n\nWarning: Extra-trees should only be used within ensemble methods.\n\nRead more in the :ref:`User Guide <tree>`.\n\nParameters\n----------\ncriterion : {\"mse\", \"friedman_mse\", \"mae\"}, default=\"mse\"\n    The function to measure the quality of a split. Supported criteria\n    are \"mse\" for the mean squared error, which is equal to variance\n    reduction as feature selection criterion and \"mae\" for the mean\n    absolute error.\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\n    .. versionadded:: 0.24\n        Poisson deviance criterion.\n\nsplitter : {\"random\", \"best\"}, default=\"random\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float, {\"auto\", \"sqrt\", \"log2\"} or None, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used to pick randomly the `max_features` used at each split.\n    See :term:`Glossary <random_state>` for details.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nmax_features_ : int\n    The inferred value of max_features.\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    Return impurity-based feature importances (the higher, the more\n    important the feature).\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nExtraTreeClassifier : An extremely randomized tree classifier.\nsklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.\nsklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.ensemble import BaggingRegressor\n>>> from sklearn.tree import ExtraTreeRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> extra_tree = ExtraTreeRegressor(random_state=0)\n>>> reg = BaggingRegressor(extra_tree, random_state=0).fit(\n...     X_train, y_train)\n>>> reg.score(X_test, y_test)\n0.33..."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.tree._export",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "io",
          "declaration": "StringIO",
          "alias": null
        },
        {
          "module": "numbers",
          "declaration": "Integral",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "DecisionTreeClassifier",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "_criterion",
          "alias": null
        },
        {
          "module": "sklearn.tree",
          "declaration": "_tree",
          "alias": null
        },
        {
          "module": "sklearn.tree._reingold_tilford",
          "declaration": "Tree",
          "alias": null
        },
        {
          "module": "sklearn.tree._reingold_tilford",
          "declaration": "buchheim",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "Sentinel",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": null
        }
      ],
      "functions": [
        {
          "name": "export_graphviz",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "decision_tree",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The decision tree to be exported to GraphViz."
            },
            {
              "name": "out_file",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Handle or name of the output file. If ``None``, the result is\nreturned as a string.\n\n.. versionchanged:: 0.20\n    Default of out_file changed from \"tree.dot\" to None."
            }
          ],
          "results": [
            {
              "name": "dot_data",
              "type": "str",
              "description": "String representation of the input tree in GraphViz dot format.\nOnly returned if ``out_file`` is None.\n\n.. versionadded:: 0.18"
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Export a decision tree in DOT format.\n\nThis function generates a GraphViz representation of the decision tree,\nwhich is then written into `out_file`. Once exported, graphical renderings\ncan be generated using, for example::\n\n    $ dot -Tps tree.dot -o tree.ps      (PostScript format)\n    $ dot -Tpng tree.dot -o tree.png    (PNG format)\n\nThe sample counts that are shown are weighted with any sample_weights that\nmight be present.\n\nRead more in the :ref:`User Guide <tree>`.\n\nParameters\n----------\ndecision_tree : decision tree classifier\n    The decision tree to be exported to GraphViz.\n\nout_file : object or str, default=None\n    Handle or name of the output file. If ``None``, the result is\n    returned as a string.\n\n    .. versionchanged:: 0.20\n        Default of out_file changed from \"tree.dot\" to None.\n\nmax_depth : int, default=None\n    The maximum depth of the representation. If None, the tree is fully\n    generated.\n\nfeature_names : list of str, default=None\n    Names of each of the features.\n    If None generic names will be used (\"feature_0\", \"feature_1\", ...).\n\nclass_names : list of str or bool, default=None\n    Names of each of the target classes in ascending numerical order.\n    Only relevant for classification and not supported for multi-output.\n    If ``True``, shows a symbolic representation of the class name.\n\nlabel : {'all', 'root', 'none'}, default='all'\n    Whether to show informative labels for impurity, etc.\n    Options include 'all' to show at every node, 'root' to show only at\n    the top root node, or 'none' to not show at any node.\n\nfilled : bool, default=False\n    When set to ``True``, paint nodes to indicate majority class for\n    classification, extremity of values for regression, or purity of node\n    for multi-output.\n\nleaves_parallel : bool, default=False\n    When set to ``True``, draw all leaf nodes at the bottom of the tree.\n\nimpurity : bool, default=True\n    When set to ``True``, show the impurity at each node.\n\nnode_ids : bool, default=False\n    When set to ``True``, show the ID number on each node.\n\nproportion : bool, default=False\n    When set to ``True``, change the display of 'values' and/or 'samples'\n    to be proportions and percentages respectively.\n\nrotate : bool, default=False\n    When set to ``True``, orient tree left to right rather than top-down.\n\nrounded : bool, default=False\n    When set to ``True``, draw node boxes with rounded corners and use\n    Helvetica fonts instead of Times-Roman.\n\nspecial_characters : bool, default=False\n    When set to ``False``, ignore special characters for PostScript\n    compatibility.\n\nprecision : int, default=3\n    Number of digits of precision for floating point in the values of\n    impurity, threshold and value attributes of each node.\n\nReturns\n-------\ndot_data : string\n    String representation of the input tree in GraphViz dot format.\n    Only returned if ``out_file`` is None.\n\n    .. versionadded:: 0.18\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn import tree\n\n>>> clf = tree.DecisionTreeClassifier()\n>>> iris = load_iris()\n\n>>> clf = clf.fit(iris.data, iris.target)\n>>> tree.export_graphviz(clf)\n'digraph Tree {..."
        },
        {
          "name": "export_text",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "decision_tree",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The decision tree estimator to be exported.\nIt can be an instance of\nDecisionTreeClassifier or DecisionTreeRegressor."
            }
          ],
          "results": [
            {
              "name": "report",
              "type": "str",
              "description": "Text summary of all the rules in the decision tree."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Build a text report showing the rules of a decision tree.\n\nNote that backwards compatibility may not be supported.\n\nParameters\n----------\ndecision_tree : object\n    The decision tree estimator to be exported.\n    It can be an instance of\n    DecisionTreeClassifier or DecisionTreeRegressor.\n\nfeature_names : list of str, default=None\n    A list of length n_features containing the feature names.\n    If None generic names will be used (\"feature_0\", \"feature_1\", ...).\n\nmax_depth : int, default=10\n    Only the first max_depth levels of the tree are exported.\n    Truncated branches will be marked with \"...\".\n\nspacing : int, default=3\n    Number of spaces between edges. The higher it is, the wider the result.\n\ndecimals : int, default=2\n    Number of decimal digits to display.\n\nshow_weights : bool, default=False\n    If true the classification weights will be exported on each leaf.\n    The classification weights are the number of samples each class.\n\nReturns\n-------\nreport : string\n    Text summary of all the rules in the decision tree.\n\nExamples\n--------\n\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> from sklearn.tree import export_text\n>>> iris = load_iris()\n>>> X = iris['data']\n>>> y = iris['target']\n>>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n>>> decision_tree = decision_tree.fit(X, y)\n>>> r = export_text(decision_tree, feature_names=iris['feature_names'])\n>>> print(r)\n|--- petal width (cm) <= 0.80\n|   |--- class: 0\n|--- petal width (cm) >  0.80\n|   |--- petal width (cm) <= 1.75\n|   |   |--- class: 1\n|   |--- petal width (cm) >  1.75\n|   |   |--- class: 2"
        },
        {
          "name": "plot_tree",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "decision_tree",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The decision tree to be plotted."
            }
          ],
          "results": [
            {
              "name": "annotations",
              "type": null,
              "description": "List containing the artists for the annotation boxes making up the\ntree."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Plot a decision tree.\n\nThe sample counts that are shown are weighted with any sample_weights that\nmight be present.\n\nThe visualization is fit automatically to the size of the axis.\nUse the ``figsize`` or ``dpi`` arguments of ``plt.figure``  to control\nthe size of the rendering.\n\nRead more in the :ref:`User Guide <tree>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\ndecision_tree : decision tree regressor or classifier\n    The decision tree to be plotted.\n\nmax_depth : int, default=None\n    The maximum depth of the representation. If None, the tree is fully\n    generated.\n\nfeature_names : list of strings, default=None\n    Names of each of the features.\n    If None, generic names will be used (\"X[0]\", \"X[1]\", ...).\n\nclass_names : list of str or bool, default=None\n    Names of each of the target classes in ascending numerical order.\n    Only relevant for classification and not supported for multi-output.\n    If ``True``, shows a symbolic representation of the class name.\n\nlabel : {'all', 'root', 'none'}, default='all'\n    Whether to show informative labels for impurity, etc.\n    Options include 'all' to show at every node, 'root' to show only at\n    the top root node, or 'none' to not show at any node.\n\nfilled : bool, default=False\n    When set to ``True``, paint nodes to indicate majority class for\n    classification, extremity of values for regression, or purity of node\n    for multi-output.\n\nimpurity : bool, default=True\n    When set to ``True``, show the impurity at each node.\n\nnode_ids : bool, default=False\n    When set to ``True``, show the ID number on each node.\n\nproportion : bool, default=False\n    When set to ``True``, change the display of 'values' and/or 'samples'\n    to be proportions and percentages respectively.\n\nrotate : bool, default=False\n    This parameter has no effect on the matplotlib tree visualisation and\n    it is kept here for backward compatibility.\n\n    .. deprecated:: 0.23\n       ``rotate`` is deprecated in 0.23 and will be removed in 1.0\n       (renaming of 0.25).\n\nrounded : bool, default=False\n    When set to ``True``, draw node boxes with rounded corners and use\n    Helvetica fonts instead of Times-Roman.\n\nprecision : int, default=3\n    Number of digits of precision for floating point in the values of\n    impurity, threshold and value attributes of each node.\n\nax : matplotlib axis, default=None\n    Axes to plot to. If None, use current axis. Any previous content\n    is cleared.\n\nfontsize : int, default=None\n    Size of text font. If None, determined automatically to fit figure.\n\nReturns\n-------\nannotations : list of artists\n    List containing the artists for the annotation boxes making up the\n    tree.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn import tree\n\n>>> clf = tree.DecisionTreeClassifier(random_state=0)\n>>> iris = load_iris()\n\n>>> clf = clf.fit(iris.data, iris.target)\n>>> tree.plot_tree(clf)  # doctest: +SKIP\n[Text(251.5,345.217,'X[3] <= 0.8..."
        }
      ]
    },
    {
      "name": "sklearn.tree._reingold_tilford",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [],
      "classes": [
        {
          "name": "DrawTree",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "left",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "right",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "lbrother",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "get_lmost_sibling",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "max_extents",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": null
        },
        {
          "name": "Tree",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": null
        }
      ],
      "functions": [
        {
          "name": "ancestor",
          "decorators": [],
          "parameters": [
            {
              "name": "vil",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "v",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "default_ancestor",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "apportion",
          "decorators": [],
          "parameters": [
            {
              "name": "v",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "default_ancestor",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "distance",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "buchheim",
          "decorators": [],
          "parameters": [
            {
              "name": "tree",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "execute_shifts",
          "decorators": [],
          "parameters": [
            {
              "name": "v",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "first_walk",
          "decorators": [],
          "parameters": [
            {
              "name": "v",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "distance",
              "type": "Any",
              "hasDefault": true,
              "default": "1.0",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "move_subtree",
          "decorators": [],
          "parameters": [
            {
              "name": "wl",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "wr",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "shift",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "second_walk",
          "decorators": [],
          "parameters": [
            {
              "name": "v",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "m",
              "type": "Any",
              "hasDefault": false,
              "default": "0",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "depth",
              "type": "Any",
              "hasDefault": false,
              "default": "0",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "min",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "third_walk",
          "decorators": [],
          "parameters": [
            {
              "name": "tree",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "n",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.tree.setup",
      "imports": [
        {
          "module": "numpy",
          "alias": null
        },
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "numpy.distutils.misc_util",
          "declaration": "Configuration",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.utils",
      "imports": [
        {
          "module": "inspect",
          "alias": null
        },
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "pkgutil",
          "alias": null
        },
        {
          "module": "platform",
          "alias": null
        },
        {
          "module": "struct",
          "alias": null
        },
        {
          "module": "timeit",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "",
          "declaration": "get_config",
          "alias": null
        },
        {
          "module": "collections.abc",
          "declaration": "Sequence",
          "alias": null
        },
        {
          "module": "contextlib",
          "declaration": "contextmanager",
          "alias": null
        },
        {
          "module": "exceptions",
          "declaration": "DataConversionWarning",
          "alias": null
        },
        {
          "module": "importlib",
          "declaration": "import_module",
          "alias": null
        },
        {
          "module": "itertools",
          "declaration": "compress",
          "alias": null
        },
        {
          "module": "itertools",
          "declaration": "islice",
          "alias": null
        },
        {
          "module": "operator",
          "declaration": "itemgetter",
          "alias": null
        },
        {
          "module": "pathlib",
          "declaration": "Path",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "sklearn",
          "declaration": "_joblib",
          "alias": null
        },
        {
          "module": "sklearn._estimator_html_repr",
          "declaration": "estimator_html_repr",
          "alias": null
        },
        {
          "module": "sklearn.class_weight",
          "declaration": "compute_class_weight",
          "alias": null
        },
        {
          "module": "sklearn.class_weight",
          "declaration": "compute_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.deprecation",
          "declaration": "deprecated",
          "alias": null
        },
        {
          "module": "sklearn.fixes",
          "declaration": "np_version",
          "alias": null
        },
        {
          "module": "sklearn.fixes",
          "declaration": "parse_version",
          "alias": null
        },
        {
          "module": "sklearn.murmurhash",
          "declaration": "murmurhash3_32",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "as_float_array",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "assert_all_finite",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "check_consistent_length",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "check_scalar",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "check_symmetric",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "column_or_1d",
          "alias": null
        },
        {
          "module": "sklearn.validation",
          "declaration": "indexable",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "Bunch",
          "decorators": [],
          "superclasses": [
            "dict"
          ],
          "methods": [],
          "fullDocstring": "Container object exposing keys as attributes.\n\nBunch objects are sometimes used as an output for functions and methods.\nThey extend dictionaries by enabling values to be accessed by key,\n`bunch[\"value_key\"]`, or by an attribute, `bunch.value_key`.\n\nExamples\n--------\n>>> b = Bunch(a=1, b=2)\n>>> b['b']\n2\n>>> b.b\n2\n>>> b.a = 3\n>>> b['a']\n3\n>>> b.c = 6\n>>> b['c']\n6"
        }
      ],
      "functions": [
        {
          "name": "all_estimators",
          "decorators": [],
          "parameters": [
            {
              "name": "type_filter",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Which kind of estimators should be returned. If None, no filter is\napplied and all estimators are returned.  Possible values are\n'classifier', 'regressor', 'cluster' and 'transformer' to get\nestimators only of these specific types, or a list of these to\nget the estimators that fit at least one of the types."
            }
          ],
          "results": [
            {
              "name": "estimators",
              "type": null,
              "description": "List of (name, class), where ``name`` is the class name as string\nand ``class`` is the actuall type of the class."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Get a list of all estimators from sklearn.\n\nThis function crawls the module and gets all classes that inherit\nfrom BaseEstimator. Classes that are defined in test-modules are not\nincluded.\n\nParameters\n----------\ntype_filter : {\"classifier\", \"regressor\", \"cluster\", \"transformer\"}             or list of such str, default=None\n    Which kind of estimators should be returned. If None, no filter is\n    applied and all estimators are returned.  Possible values are\n    'classifier', 'regressor', 'cluster' and 'transformer' to get\n    estimators only of these specific types, or a list of these to\n    get the estimators that fit at least one of the types.\n\nReturns\n-------\nestimators : list of tuples\n    List of (name, class), where ``name`` is the class name as string\n    and ``class`` is the actuall type of the class."
        },
        {
          "name": "axis0_safe_slice",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data on which to apply mask."
            },
            {
              "name": "mask",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Mask to be used on X."
            },
            {
              "name": "len_mask",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The length of the mask."
            }
          ],
          "results": [
            {
              "name": "",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "This mask is safer than safe_mask since it returns an\nempty array, when a sparse matrix is sliced with a boolean mask\nwith all False, instead of raising an unhelpful error in older\nversions of SciPy.\n\nSee: https://github.com/scipy/scipy/issues/5361\n\nAlso note that we can avoid doing the dot product by checking if\nthe len_mask is not zero in _huber_loss_and_gradient but this\nis not going to be the bottleneck, since the number of outliers\nand non_outliers are typically non-zero and it makes the code\ntougher to follow.\n\nParameters\n----------\nX : {array-like, sparse matrix}\n    Data on which to apply mask.\n\nmask : ndarray\n    Mask to be used on X.\n\nlen_mask : int\n    The length of the mask.\n\nReturns\n-------\n    mask"
        },
        {
          "name": "check_matplotlib_support",
          "decorators": [],
          "parameters": [
            {
              "name": "caller_name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The name of the caller that requires matplotlib."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Raise ImportError with detailed error message if mpl is not installed.\n\nPlot utilities like :func:`plot_partial_dependence` should lazily import\nmatplotlib and call this helper before any computation.\n\nParameters\n----------\ncaller_name : str\n    The name of the caller that requires matplotlib."
        },
        {
          "name": "check_pandas_support",
          "decorators": [],
          "parameters": [
            {
              "name": "caller_name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The name of the caller that requires pandas."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Raise ImportError with detailed error message if pandas is not\ninstalled.\n\nPlot utilities like :func:`fetch_openml` should lazily import\npandas and call this helper before any computation.\n\nParameters\n----------\ncaller_name : str\n    The name of the caller that requires pandas."
        },
        {
          "name": "gen_batches",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "batch_size",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of element in each batch."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generator to create slices containing batch_size elements, from 0 to n.\n\nThe last slice may contain less than batch_size elements, when batch_size\ndoes not divide n.\n\nParameters\n----------\nn : int\nbatch_size : int\n    Number of element in each batch.\nmin_batch_size : int, default=0\n    Minimum batch size to produce.\n\nYields\n------\nslice of batch_size elements\n\nExamples\n--------\n>>> from sklearn.utils import gen_batches\n>>> list(gen_batches(7, 3))\n[slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n>>> list(gen_batches(6, 3))\n[slice(0, 3, None), slice(3, 6, None)]\n>>> list(gen_batches(2, 3))\n[slice(0, 2, None)]\n>>> list(gen_batches(7, 3, min_batch_size=0))\n[slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n>>> list(gen_batches(7, 3, min_batch_size=2))\n[slice(0, 3, None), slice(3, 7, None)]"
        },
        {
          "name": "gen_even_slices",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "n",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "n_packs",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of slices to generate."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generator to create n_packs slices going up to n.\n\nParameters\n----------\nn : int\nn_packs : int\n    Number of slices to generate.\nn_samples : int, default=None\n    Number of samples. Pass n_samples when the slices are to be used for\n    sparse matrix indexing; slicing off-the-end raises an exception, while\n    it works for NumPy arrays.\n\nYields\n------\nslice\n\nExamples\n--------\n>>> from sklearn.utils import gen_even_slices\n>>> list(gen_even_slices(10, 1))\n[slice(0, 10, None)]\n>>> list(gen_even_slices(10, 10))\n[slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)]\n>>> list(gen_even_slices(10, 5))\n[slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)]\n>>> list(gen_even_slices(10, 3))\n[slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)]"
        },
        {
          "name": "get_chunk_n_rows",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "row_bytes",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The expected number of bytes of memory that will be consumed\nduring the processing of each row."
            }
          ],
          "results": [
            {
              "name": "",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Calculates how many rows can be processed within working_memory.\n\nParameters\n----------\nrow_bytes : int\n    The expected number of bytes of memory that will be consumed\n    during the processing of each row.\nmax_n_rows : int, default=None\n    The maximum return value.\nworking_memory : int or float, default=None\n    The number of rows to fit inside this number of MiB will be returned.\n    When None (default), the value of\n    ``sklearn.get_config()['working_memory']`` is used.\n\nReturns\n-------\nint or the value of n_samples\n\nWarns\n-----\nIssues a UserWarning if ``row_bytes`` exceeds ``working_memory`` MiB."
        },
        {
          "name": "indices_to_mask",
          "decorators": [],
          "parameters": [
            {
              "name": "indices",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "List of integers treated as indices."
            },
            {
              "name": "mask_length",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Length of boolean mask to be generated.\nThis parameter must be greater than max(indices)."
            }
          ],
          "results": [
            {
              "name": "mask",
              "type": null,
              "description": "Boolean array that is True where indices are present, else False."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Convert list of indices to boolean mask.\n\nParameters\n----------\nindices : list-like\n    List of integers treated as indices.\nmask_length : int\n    Length of boolean mask to be generated.\n    This parameter must be greater than max(indices).\n\nReturns\n-------\nmask : 1d boolean nd-array\n    Boolean array that is True where indices are present, else False.\n\nExamples\n--------\n>>> from sklearn.utils import indices_to_mask\n>>> indices = [1, 2 , 3, 4]\n>>> indices_to_mask(indices, 5)\narray([False,  True,  True,  True,  True])"
        },
        {
          "name": "is_scalar_nan",
          "decorators": [],
          "parameters": [
            {
              "name": "x",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "",
              "type": "bool",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Tests if x is NaN.\n\nThis function is meant to overcome the issue that np.isnan does not allow\nnon-numerical types as input, and that np.nan is not float('nan').\n\nParameters\n----------\nx : any type\n\nReturns\n-------\nboolean\n\nExamples\n--------\n>>> is_scalar_nan(np.nan)\nTrue\n>>> is_scalar_nan(float(\"nan\"))\nTrue\n>>> is_scalar_nan(None)\nFalse\n>>> is_scalar_nan(\"\")\nFalse\n>>> is_scalar_nan([np.nan])\nFalse"
        },
        {
          "name": "resample",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "resampled_arrays",
              "type": null,
              "description": "Sequence of resampled copies of the collections. The original arrays\nare not impacted."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Resample arrays or sparse matrices in a consistent way.\n\nThe default strategy implements one step of the bootstrapping\nprocedure.\n\nParameters\n----------\n*arrays : sequence of array-like of shape (n_samples,) or             (n_samples, n_outputs)\n    Indexable data-structures can be arrays, lists, dataframes or scipy\n    sparse matrices with consistent first dimension.\n\nreplace : bool, default=True\n    Implements resampling with replacement. If False, this will implement\n    (sliced) random permutations.\n\nn_samples : int, default=None\n    Number of samples to generate. If left to None this is\n    automatically set to the first dimension of the arrays.\n    If replace is False it should not be larger than the length of\n    arrays.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for shuffling\n    the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nstratify : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n    If not None, data is split in a stratified fashion, using this as\n    the class labels.\n\nReturns\n-------\nresampled_arrays : sequence of array-like of shape (n_samples,) or             (n_samples, n_outputs)\n    Sequence of resampled copies of the collections. The original arrays\n    are not impacted.\n\nExamples\n--------\nIt is possible to mix sparse and dense arrays in the same run::\n\n  >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n  >>> y = np.array([0, 1, 2])\n\n  >>> from scipy.sparse import coo_matrix\n  >>> X_sparse = coo_matrix(X)\n\n  >>> from sklearn.utils import resample\n  >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n  >>> X\n  array([[1., 0.],\n         [2., 1.],\n         [1., 0.]])\n\n  >>> X_sparse\n  <3x2 sparse matrix of type '<... 'numpy.float64'>'\n      with 4 stored elements in Compressed Sparse Row format>\n\n  >>> X_sparse.toarray()\n  array([[1., 0.],\n         [2., 1.],\n         [1., 0.]])\n\n  >>> y\n  array([0, 1, 0])\n\n  >>> resample(y, n_samples=2, random_state=0)\n  array([0, 1])\n\nExample using stratification::\n\n  >>> y = [0, 0, 1, 1, 1, 1, 1, 1, 1]\n  >>> resample(y, n_samples=5, replace=False, stratify=y,\n  ...          random_state=0)\n  [1, 1, 1, 0, 1]\n\nSee Also\n--------\nshuffle"
        },
        {
          "name": "safe_mask",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Data on which to apply mask."
            },
            {
              "name": "mask",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Mask to be used on X."
            }
          ],
          "results": [
            {
              "name": "",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Return a mask which is safe to use on X.\n\nParameters\n----------\nX : {array-like, sparse matrix}\n    Data on which to apply mask.\n\nmask : ndarray\n    Mask to be used on X.\n\nReturns\n-------\n    mask"
        },
        {
          "name": "safe_sqr",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "X ** 2",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Element wise squaring of array-likes and sparse matrices.\n\nParameters\n----------\nX : {array-like, ndarray, sparse matrix}\n\ncopy : bool, default=True\n    Whether to create a copy of X and operate on it or to perform\n    inplace computation (default behaviour).\n\nReturns\n-------\nX ** 2 : element wise square"
        },
        {
          "name": "shuffle",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "shuffled_arrays",
              "type": null,
              "description": "Sequence of shuffled copies of the collections. The original arrays\nare not impacted."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Shuffle arrays or sparse matrices in a consistent way.\n\nThis is a convenience alias to ``resample(*arrays, replace=False)`` to do\nrandom permutations of the collections.\n\nParameters\n----------\n*arrays : sequence of indexable data-structures\n    Indexable data-structures can be arrays, lists, dataframes or scipy\n    sparse matrices with consistent first dimension.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for shuffling\n    the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_samples : int, default=None\n    Number of samples to generate. If left to None this is\n    automatically set to the first dimension of the arrays.  It should\n    not be larger than the length of arrays.\n\nReturns\n-------\nshuffled_arrays : sequence of indexable data-structures\n    Sequence of shuffled copies of the collections. The original arrays\n    are not impacted.\n\nExamples\n--------\nIt is possible to mix sparse and dense arrays in the same run::\n\n  >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n  >>> y = np.array([0, 1, 2])\n\n  >>> from scipy.sparse import coo_matrix\n  >>> X_sparse = coo_matrix(X)\n\n  >>> from sklearn.utils import shuffle\n  >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n  >>> X\n  array([[0., 0.],\n         [2., 1.],\n         [1., 0.]])\n\n  >>> X_sparse\n  <3x2 sparse matrix of type '<... 'numpy.float64'>'\n      with 3 stored elements in Compressed Sparse Row format>\n\n  >>> X_sparse.toarray()\n  array([[0., 0.],\n         [2., 1.],\n         [1., 0.]])\n\n  >>> y\n  array([2, 1, 0])\n\n  >>> shuffle(y, n_samples=2, random_state=0)\n  array([0, 1])\n\nSee Also\n--------\nresample"
        },
        {
          "name": "tosequence",
          "decorators": [],
          "parameters": [
            {
              "name": "x",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Cast iterable x to a Sequence, avoiding a copy if possible.\n\nParameters\n----------\nx : iterable"
        }
      ]
    },
    {
      "name": "sklearn.utils._arpack",
      "imports": [],
      "fromImports": [
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_random_state",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.utils._encode",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.utils",
          "declaration": "is_scalar_nan",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "NamedTuple",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "MissingValues",
          "decorators": [],
          "superclasses": [
            "NamedTuple"
          ],
          "methods": [
            {
              "name": "to_list",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Convert tuple to a list where None is always first."
            }
          ],
          "fullDocstring": "Data class for missing data information"
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.utils._estimator_html_repr",
      "imports": [
        {
          "module": "html",
          "alias": null
        },
        {
          "module": "uuid",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "contextlib",
          "declaration": "closing",
          "alias": null
        },
        {
          "module": "contextlib",
          "declaration": "suppress",
          "alias": null
        },
        {
          "module": "io",
          "declaration": "StringIO",
          "alias": null
        },
        {
          "module": "sklearn",
          "declaration": "config_context",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "estimator_html_repr",
          "decorators": [],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The estimator to visualize."
            }
          ],
          "results": [
            {
              "name": "",
              "type": null,
              "description": "HTML representation of estimator."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Build a HTML representation of an estimator.\n\nRead more in the :ref:`User Guide <visualizing_composite_estimators>`.\n\nParameters\n----------\nestimator : estimator object\n    The estimator to visualize.\n\nReturns\n-------\nhtml: str\n    HTML representation of estimator."
        }
      ]
    },
    {
      "name": "sklearn.utils._joblib",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": "_warnings"
        }
      ],
      "fromImports": [
        {
          "module": "joblib",
          "declaration": "Memory",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "Parallel",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "__version__",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "cpu_count",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "delayed",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "dump",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "effective_n_jobs",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "hash",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "load",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "logger",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "parallel_backend",
          "alias": null
        },
        {
          "module": "joblib",
          "declaration": "register_parallel_backend",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.utils._mask",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": "sp"
        },
        {
          "module": "sklearn.utils",
          "declaration": "is_scalar_nan",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_object_dtype_isnan",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.utils._mocking",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClassifierMixin",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "ArraySlicingWrapper",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "Parameters\n----------\narray"
        },
        {
          "name": "CheckingClassifier",
          "decorators": [],
          "superclasses": [
            "ClassifierMixin",
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target relative to X for classification or regression;\nNone for unsupervised learning."
                }
              ],
              "results": [
                {
                  "name": "",
                  "type": null,
                  "description": ""
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fit classifier.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples, n_outputs) or (n_samples,),                 default=None\n    Target relative to X for classification or regression;\n    None for unsupervised learning.\n\n**fit_params : dict of string -> object\n    Parameters passed to the ``fit`` method of the estimator\n\nReturns\n-------\nself"
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                }
              ],
              "results": [
                {
                  "name": "preds",
                  "type": null,
                  "description": "Predictions of the first class seens in `classes_`."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict the first class seen in `classes_`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input data.\n\nReturns\n-------\npreds : ndarray of shape (n_samples,)\n    Predictions of the first class seens in `classes_`."
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                }
              ],
              "results": [
                {
                  "name": "proba",
                  "type": null,
                  "description": "The probabilities for each sample and class."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Predict probabilities for each class.\n\nHere, the dummy classifier will provide a probability of 1 for the\nfirst class of `classes_` and 0 otherwise.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input data.\n\nReturns\n-------\nproba : ndarray of shape (n_samples, n_classes)\n    The probabilities for each sample and class."
            },
            {
              "name": "decision_function",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "The input data."
                }
              ],
              "results": [
                {
                  "name": "decision",
                  "type": null,
                  "description": "Confidence score."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Confidence score.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input data.\n\nReturns\n-------\ndecision : ndarray of shape (n_samples,) if n_classes == 2                else (n_samples, n_classes)\n    Confidence score."
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Input data, where n_samples is the number of samples and\nn_features is the number of features."
                },
                {
                  "name": "Y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": "Target relative to X for classification or regression;\nNone for unsupervised learning."
                }
              ],
              "results": [
                {
                  "name": "score",
                  "type": "float",
                  "description": "Either 0 or 1 depending of `foo_param` (i.e. `foo_param > 1 =>\nscore=1` otherwise `score=0`)."
                }
              ],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": "Fake score.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input data, where n_samples is the number of samples and\n    n_features is the number of features.\n\nY : array-like of shape (n_samples, n_output) or (n_samples,)\n    Target relative to X for classification or regression;\n    None for unsupervised learning.\n\nReturns\n-------\nscore : float\n    Either 0 or 1 depending of `foo_param` (i.e. `foo_param > 1 =>\n    score=1` otherwise `score=0`)."
            }
          ],
          "fullDocstring": "Dummy classifier to test pipelining and meta-estimators.\n\nChecks some property of `X` and `y`in fit / predict.\nThis allows testing whether pipelines / cross-validation or metaestimators\nchanged the input.\n\nCan also be used to check if `fit_params` are passed correctly, and\nto force a certain score to be returned.\n\nParameters\n----------\ncheck_y, check_X : callable, default=None\n    The callable used to validate `X` and `y`. These callable should return\n    a bool where `False` will trigger an `AssertionError`.\n\ncheck_y_params, check_X_params : dict, default=None\n    The optional parameters to pass to `check_X` and `check_y`.\n\nmethods_to_check : \"all\" or list of str, default=\"all\"\n    The methods in which the checks should be applied. By default,\n    all checks will be done on all methods (`fit`, `predict`,\n    `predict_proba`, `decision_function` and `score`).\n\nfoo_param : int, default=0\n    A `foo` param. When `foo > 1`, the output of :meth:`score` will be 1\n    otherwise it is 0.\n\nexpected_fit_params : list of str, default=None\n    A list of the expected parameters given when calling `fit`.\n\nAttributes\n----------\nclasses_ : int\n    The classes seen during `fit`.\n\nn_features_in_ : int\n    The number of features seen during `fit`.\n\nExamples\n--------\n>>> from sklearn.utils._mocking import CheckingClassifier\n\nThis helper allow to assert to specificities regarding `X` or `y`. In this\ncase we expect `check_X` or `check_y` to return a boolean.\n\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = CheckingClassifier(check_X=lambda x: x.shape == (150, 4))\n>>> clf.fit(X, y)\nCheckingClassifier(...)\n\nWe can also provide a check which might raise an error. In this case, we\nexpect `check_X` to return `X` and `check_y` to return `y`.\n\n>>> from sklearn.utils import check_array\n>>> clf = CheckingClassifier(check_X=check_array)\n>>> clf.fit(X, y)\nCheckingClassifier(...)"
        },
        {
          "name": "MockDataFrame",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "Parameters\n----------\narray"
        },
        {
          "name": "NoSampleWeightWrapper",
          "decorators": [],
          "superclasses": [
            "BaseEstimator"
          ],
          "methods": [
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Wrap estimator which will not expose `sample_weight`.\n\nParameters\n----------\nest : estimator, default=None\n    The estimator to wrap."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.utils._pprint",
      "imports": [
        {
          "module": "inspect",
          "alias": null
        },
        {
          "module": "pprint",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections",
          "declaration": "OrderedDict",
          "alias": null
        },
        {
          "module": "sklearn._config",
          "declaration": "get_config",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "is_scalar_nan",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "KeyValTuple",
          "decorators": [],
          "superclasses": [
            "tuple"
          ],
          "methods": [],
          "fullDocstring": "Dummy class for correctly rendering key-value tuples from dicts."
        },
        {
          "name": "KeyValTupleParam",
          "decorators": [],
          "superclasses": [
            "KeyValTuple"
          ],
          "methods": [],
          "fullDocstring": "Dummy class for correctly rendering key-value tuples from parameters."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.utils._show_versions",
      "imports": [
        {
          "module": "importlib",
          "alias": null
        },
        {
          "module": "platform",
          "alias": null
        },
        {
          "module": "sys",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.utils._openmp_helpers",
          "declaration": "_openmp_parallelism_enabled",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "show_versions",
          "decorators": [],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Print useful debugging information\"\n\n.. versionadded:: 0.20"
        }
      ]
    },
    {
      "name": "sklearn.utils._tags",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.utils._testing",
      "imports": [
        {
          "module": "atexit",
          "alias": null
        },
        {
          "module": "contextlib",
          "alias": null
        },
        {
          "module": "functools",
          "alias": null
        },
        {
          "module": "inspect",
          "alias": null
        },
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "os",
          "alias": null
        },
        {
          "module": "os.path",
          "alias": "op"
        },
        {
          "module": "pytest",
          "alias": null
        },
        {
          "module": "re",
          "alias": null
        },
        {
          "module": "scipy",
          "alias": "sp"
        },
        {
          "module": "shutil",
          "alias": null
        },
        {
          "module": "sklearn",
          "alias": null
        },
        {
          "module": "sys",
          "alias": null
        },
        {
          "module": "tempfile",
          "alias": null
        },
        {
          "module": "unittest",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections.abc",
          "declaration": "Iterable",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "wraps",
          "alias": null
        },
        {
          "module": "inspect",
          "declaration": "signature",
          "alias": null
        },
        {
          "module": "numpy.testing",
          "declaration": "assert_allclose",
          "alias": null
        },
        {
          "module": "numpy.testing",
          "declaration": "assert_almost_equal",
          "alias": null
        },
        {
          "module": "numpy.testing",
          "declaration": "assert_approx_equal",
          "alias": null
        },
        {
          "module": "numpy.testing",
          "declaration": "assert_array_almost_equal",
          "alias": null
        },
        {
          "module": "numpy.testing",
          "declaration": "assert_array_equal",
          "alias": null
        },
        {
          "module": "numpy.testing",
          "declaration": "assert_array_less",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "IS_PYPY",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_IS_32BIT",
          "alias": null
        },
        {
          "module": "sklearn.utils.multiclass",
          "declaration": "check_classification_targets",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_X_y",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_is_fitted",
          "alias": null
        },
        {
          "module": "subprocess",
          "declaration": "CalledProcessError",
          "alias": null
        },
        {
          "module": "subprocess",
          "declaration": "STDOUT",
          "alias": null
        },
        {
          "module": "subprocess",
          "declaration": "TimeoutExpired",
          "alias": null
        },
        {
          "module": "subprocess",
          "declaration": "check_output",
          "alias": null
        },
        {
          "module": "unittest",
          "declaration": "TestCase",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "MinimalClassifier",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "set_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "predict_proba",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Minimal classifier implementation with inheriting from BaseEstimator.\n\nThis estimator should be tested with:\n\n* `check_estimator` in `test_estimator_checks.py`;\n* within a `Pipeline` in `test_pipeline.py`;\n* within a `SearchCV` in `test_search.py`."
        },
        {
          "name": "MinimalRegressor",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "set_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "predict",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "score",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Minimal regressor implementation with inheriting from BaseEstimator.\n\nThis estimator should be tested with:\n\n* `check_estimator` in `test_estimator_checks.py`;\n* within a `Pipeline` in `test_pipeline.py`;\n* within a `SearchCV` in `test_search.py`."
        },
        {
          "name": "MinimalTransformer",
          "decorators": [],
          "superclasses": [],
          "methods": [
            {
              "name": "get_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "deep",
                  "type": "Any",
                  "hasDefault": true,
                  "default": "True",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "set_params",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "fit",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            },
            {
              "name": "fit_transform",
              "decorators": [],
              "parameters": [
                {
                  "name": "self",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "X",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                },
                {
                  "name": "y",
                  "type": "Any",
                  "hasDefault": false,
                  "default": "None",
                  "limitation": null,
                  "ignored": false,
                  "description": ""
                }
              ],
              "results": [],
              "hasReturnType": false,
              "returnType": "Any",
              "fullDocstring": ""
            }
          ],
          "fullDocstring": "Minimal transformer implementation with inheriting from\nBaseEstimator.\n\nThis estimator should be tested with:\n\n* `check_estimator` in `test_estimator_checks.py`;\n* within a `Pipeline` in `test_pipeline.py`;\n* within a `SearchCV` in `test_search.py`."
        },
        {
          "name": "TempMemmap",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "Parameters\n----------\ndata\nmmap_mode : str, default='r'"
        }
      ],
      "functions": [
        {
          "name": "assert_allclose_dense_sparse",
          "decorators": [],
          "parameters": [
            {
              "name": "x",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "First array to compare."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Second array to compare."
            },
            {
              "name": "rtol",
              "type": "Any",
              "hasDefault": true,
              "default": "1e-07",
              "limitation": null,
              "ignored": false,
              "description": "relative tolerance; see numpy.allclose."
            },
            {
              "name": "atol",
              "type": "Any",
              "hasDefault": true,
              "default": "1e-09",
              "limitation": null,
              "ignored": false,
              "description": "absolute tolerance; see numpy.allclose. Note that the default here is\nmore tolerant than the default for numpy.testing.assert_allclose, where\natol=0."
            },
            {
              "name": "err_msg",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": "Error message to raise."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Assert allclose for sparse and dense data.\n\nBoth x and y need to be either sparse or dense, they\ncan't be mixed.\n\nParameters\n----------\nx : {array-like, sparse matrix}\n    First array to compare.\n\ny : {array-like, sparse matrix}\n    Second array to compare.\n\nrtol : float, default=1e-07\n    relative tolerance; see numpy.allclose.\n\natol : float, default=1e-9\n    absolute tolerance; see numpy.allclose. Note that the default here is\n    more tolerant than the default for numpy.testing.assert_allclose, where\n    atol=0.\n\nerr_msg : str, default=''\n    Error message to raise."
        },
        {
          "name": "assert_no_warnings",
          "decorators": [],
          "parameters": [
            {
              "name": "func",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Parameters\n----------\nfunc\n*args\n**kw"
        },
        {
          "name": "assert_raise_message",
          "decorators": [],
          "parameters": [
            {
              "name": "exceptions",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "An Exception object."
            },
            {
              "name": "message",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The error message or a substring of the error message."
            },
            {
              "name": "function",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Callable object to raise error."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Helper function to test the message raised in an exception.\n\nGiven an exception, a callable to raise the exception, and\na message string, tests that the correct exception is raised and\nthat the message is a substring of the error thrown. Used to test\nthat the specific message thrown during an exception is correct.\n\nParameters\n----------\nexceptions : exception or tuple of exception\n    An Exception object.\n\nmessage : str\n    The error message or a substring of the error message.\n\nfunction : callable\n    Callable object to raise error.\n\n*args : the positional arguments to `function`.\n\n**kwargs : the keyword arguments to `function`."
        },
        {
          "name": "assert_run_python_script",
          "decorators": [],
          "parameters": [
            {
              "name": "source_code",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The Python source code to execute."
            },
            {
              "name": "timeout",
              "type": "Any",
              "hasDefault": true,
              "default": "60",
              "limitation": null,
              "ignored": false,
              "description": "Time in seconds before timeout."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Utility to check assertions in an independent Python subprocess.\n\nThe script provided in the source code should return 0 and not print\nanything on stderr or stdout.\n\nThis is a port from cloudpickle https://github.com/cloudpipe/cloudpickle\n\nParameters\n----------\nsource_code : str\n    The Python source code to execute.\ntimeout : int, default=60\n    Time in seconds before timeout."
        },
        {
          "name": "assert_warns",
          "decorators": [],
          "parameters": [
            {
              "name": "warning_class",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The class to test for, e.g. UserWarning."
            },
            {
              "name": "func",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Callable object to trigger warnings."
            }
          ],
          "results": [
            {
              "name": "result",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Test that a certain warning occurs.\n\nParameters\n----------\nwarning_class : the warning class\n    The class to test for, e.g. UserWarning.\n\nfunc : callable\n    Callable object to trigger warnings.\n\n*args : the positional arguments to `func`.\n\n**kw : the keyword arguments to `func`\n\nReturns\n-------\nresult : the return value of `func`"
        },
        {
          "name": "assert_warns_div0",
          "decorators": [],
          "parameters": [
            {
              "name": "func",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Assume that numpy's warning for divide by zero is raised.\n\nHandles the case of platforms that do not support warning on divide by\nzero.\n\nParameters\n----------\nfunc\n*args\n**kw"
        },
        {
          "name": "assert_warns_message",
          "decorators": [],
          "parameters": [
            {
              "name": "warning_class",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The class to test for, e.g. UserWarning."
            },
            {
              "name": "message",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The message or a substring of the message to test for. If callable,\nit takes a string as the argument and will trigger an AssertionError\nif the callable returns `False`."
            },
            {
              "name": "func",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Callable object to trigger warnings."
            }
          ],
          "results": [
            {
              "name": "result",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Test that a certain warning occurs and with a certain message.\n\nParameters\n----------\nwarning_class : the warning class\n    The class to test for, e.g. UserWarning.\n\nmessage : str or callable\n    The message or a substring of the message to test for. If callable,\n    it takes a string as the argument and will trigger an AssertionError\n    if the callable returns `False`.\n\nfunc : callable\n    Callable object to trigger warnings.\n\n*args : the positional arguments to `func`.\n\n**kw : the keyword arguments to `func`.\n\nReturns\n-------\nresult : the return value of `func`"
        },
        {
          "name": "check_docstring_parameters",
          "decorators": [],
          "parameters": [
            {
              "name": "func",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The function object to test."
            },
            {
              "name": "doc",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Docstring if it is passed manually to the test."
            },
            {
              "name": "ignore",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Parameters to ignore."
            }
          ],
          "results": [
            {
              "name": "incorrect",
              "type": "List",
              "description": "A list of string describing the incorrect results."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Helper to check docstring.\n\nParameters\n----------\nfunc : callable\n    The function object to test.\ndoc : str, default=None\n    Docstring if it is passed manually to the test.\nignore : list, default=None\n    Parameters to ignore.\n\nReturns\n-------\nincorrect : list\n    A list of string describing the incorrect results."
        },
        {
          "name": "check_skip_network",
          "decorators": [],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "create_memmap_backed_data",
          "decorators": [],
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "mmap_mode",
              "type": "Any",
              "hasDefault": true,
              "default": "r",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "return_folder",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Parameters\n----------\ndata\nmmap_mode : str, default='r'\nreturn_folder :  bool, default=False"
        },
        {
          "name": "ignore_warnings",
          "decorators": [],
          "parameters": [
            {
              "name": "obj",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "callable where you want to ignore the warnings."
            },
            {
              "name": "category",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The category to filter. If Warning, all categories will be muted."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Context manager and decorator to ignore warnings.\n\nNote: Using this (in both variants) will clear all warnings\nfrom all python modules loaded. In case you need to test\ncross-module-warning-logging, this is not your tool of choice.\n\nParameters\n----------\nobj : callable, default=None\n    callable where you want to ignore the warnings.\ncategory : warning class, default=Warning\n    The category to filter. If Warning, all categories will be muted.\n\nExamples\n--------\n>>> with ignore_warnings():\n...     warnings.warn('buhuhuhu')\n\n>>> def nasty_warn():\n...     warnings.warn('buhuhuhu')\n...     print(42)\n\n>>> ignore_warnings(nasty_warn)()\n42"
        },
        {
          "name": "raises",
          "decorators": [],
          "parameters": [
            {
              "name": "expected_exc_type",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "match",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "A regex that the exception message should match. If a list, one of\nthe entries must match. If None, match isn't enforced."
            },
            {
              "name": "may_pass",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "If True, the block is allowed to not raise an exception. Useful in\ncases where some estimators may support a feature but others must\nfail with an appropriate error message. By default, the context\nmanager will raise an exception if the block does not raise an\nexception."
            },
            {
              "name": "err_msg",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If the context manager fails (e.g. the block fails to raise the\nproper exception, or fails to match), then an AssertionError is\nraised with this message. By default, an AssertionError is raised\nwith a default error message (depends on the kind of failure). Use\nthis to indicate how users should fix their estimators to pass the\nchecks."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Context manager to ensure exceptions are raised within a code block.\n\nThis is similar to and inspired from pytest.raises, but supports a few\nother cases.\n\nThis is only intended to be used in estimator_checks.py where we don't\nwant to use pytest. In the rest of the code base, just use pytest.raises\ninstead.\n\nParameters\n----------\nexcepted_exc_type : Exception or list of Exception\n    The exception that should be raised by the block. If a list, the block\n    should raise one of the exceptions.\nmatch : str or list of str, default=None\n    A regex that the exception message should match. If a list, one of\n    the entries must match. If None, match isn't enforced.\nmay_pass : bool, default=False\n    If True, the block is allowed to not raise an exception. Useful in\n    cases where some estimators may support a feature but others must\n    fail with an appropriate error message. By default, the context\n    manager will raise an exception if the block does not raise an\n    exception.\nerr_msg : str, default=None\n    If the context manager fails (e.g. the block fails to raise the\n    proper exception, or fails to match), then an AssertionError is\n    raised with this message. By default, an AssertionError is raised\n    with a default error message (depends on the kind of failure). Use\n    this to indicate how users should fix their estimators to pass the\n    checks.\n\nAttributes\n----------\nraised_and_matched : bool\n    True if an exception was raised and a match was found, False otherwise."
        },
        {
          "name": "set_random_state",
          "decorators": [],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The estimator."
            },
            {
              "name": "random_state",
              "type": "Any",
              "hasDefault": false,
              "default": "0",
              "limitation": null,
              "ignored": false,
              "description": "Pseudo random number generator state.\nPass an int for reproducible results across multiple function calls.\nSee :term:`Glossary <random_state>`."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Set random state of an estimator if it has the `random_state` param.\n\nParameters\n----------\nestimator : object\n    The estimator.\nrandom_state : int, RandomState instance or None, default=0\n    Pseudo random number generator state.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`."
        }
      ]
    },
    {
      "name": "sklearn.utils.class_weight",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "compute_class_weight",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "class_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If 'balanced', class weights will be given by\n``n_samples / (n_classes * np.bincount(y))``.\nIf a dictionary is given, keys are classes and values\nare corresponding class weights.\nIf None is given, the class weights will be uniform."
            }
          ],
          "results": [
            {
              "name": "class_weight_vect",
              "type": null,
              "description": "Array with class_weight_vect[i] the weight for i-th class."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Estimate class weights for unbalanced datasets.\n\nParameters\n----------\nclass_weight : dict, 'balanced' or None\n    If 'balanced', class weights will be given by\n    ``n_samples / (n_classes * np.bincount(y))``.\n    If a dictionary is given, keys are classes and values\n    are corresponding class weights.\n    If None is given, the class weights will be uniform.\n\nclasses : ndarray\n    Array of the classes occurring in the data, as given by\n    ``np.unique(y_org)`` with ``y_org`` the original class labels.\n\ny : array-like of shape (n_samples,)\n    Array of original class labels per sample.\n\nReturns\n-------\nclass_weight_vect : ndarray of shape (n_classes,)\n    Array with class_weight_vect[i] the weight for i-th class.\n\nReferences\n----------\nThe \"balanced\" heuristic is inspired by\nLogistic Regression in Rare Events Data, King, Zen, 2001."
        },
        {
          "name": "compute_sample_weight",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "class_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Weights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data:\n``n_samples / (n_classes * np.bincount(y))``.\n\nFor multi-output, the weights of each column of y will be multiplied."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array of original class labels per sample."
            }
          ],
          "results": [
            {
              "name": "sample_weight_vect",
              "type": null,
              "description": "Array with sample weights as applied to the original y."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Estimate sample weights by class for unbalanced datasets.\n\nParameters\n----------\nclass_weight : dict, list of dicts, \"balanced\", or None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data:\n    ``n_samples / (n_classes * np.bincount(y))``.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Array of original class labels per sample.\n\nindices : array-like of shape (n_subsample,), default=None\n    Array of indices to be used in a subsample. Can be of length less than\n    n_samples in the case of a subsample, or equal to n_samples in the\n    case of a bootstrap subsample with repeated indices. If None, the\n    sample weight will be calculated over the full sample. Only \"balanced\"\n    is supported for class_weight if this is provided.\n\nReturns\n-------\nsample_weight_vect : ndarray of shape (n_samples,)\n    Array with sample weights as applied to the original y."
        }
      ]
    },
    {
      "name": "sklearn.utils.deprecation",
      "imports": [
        {
          "module": "functools",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [],
      "classes": [
        {
          "name": "deprecated",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "Decorator to mark a function or class as deprecated.\n\nIssue a warning when the function is called/the class is instantiated and\nadds a warning to the docstring.\n\nThe optional extra argument will be appended to the deprecation message\nand the docstring. Note: to use this with the default value for extra, put\nin an empty of parentheses:\n\n>>> from sklearn.utils import deprecated\n>>> deprecated()\n<sklearn.utils.deprecation.deprecated object at ...>\n\n>>> @deprecated()\n... def some_function(): pass\n\nParameters\n----------\nextra : str, default=''\n      To be added to the deprecation messages."
        }
      ],
      "functions": []
    },
    {
      "name": "sklearn.utils.estimator_checks",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "pickle",
          "alias": null
        },
        {
          "module": "re",
          "alias": null
        },
        {
          "module": "types",
          "alias": null
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "copy",
          "declaration": "deepcopy",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "partial",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "wraps",
          "alias": null
        },
        {
          "module": "inspect",
          "declaration": "signature",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "scipy.stats",
          "declaration": "rankdata",
          "alias": null
        },
        {
          "module": "sklearn",
          "declaration": "config_context",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "ClusterMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "RegressorMixin",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "_is_pairwise",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "clone",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_classifier",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_outlier_detector",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "is_regressor",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "load_iris",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "make_blobs",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "make_multilabel_classification",
          "alias": null
        },
        {
          "module": "sklearn.datasets",
          "declaration": "make_regression",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "DataConversionWarning",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "SkipTestWarning",
          "alias": null
        },
        {
          "module": "sklearn.feature_selection",
          "declaration": "SelectKBest",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "LogisticRegression",
          "alias": null
        },
        {
          "module": "sklearn.linear_model",
          "declaration": "Ridge",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "accuracy_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "adjusted_rand_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics",
          "declaration": "f1_score",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "linear_kernel",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "pairwise_distances",
          "alias": null
        },
        {
          "module": "sklearn.metrics.pairwise",
          "declaration": "rbf_kernel",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "ShuffleSplit",
          "alias": null
        },
        {
          "module": "sklearn.model_selection",
          "declaration": "train_test_split",
          "alias": null
        },
        {
          "module": "sklearn.model_selection._validation",
          "declaration": "_safe_split",
          "alias": null
        },
        {
          "module": "sklearn.pipeline",
          "declaration": "make_pipeline",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "StandardScaler",
          "alias": null
        },
        {
          "module": "sklearn.preprocessing",
          "declaration": "scale",
          "alias": null
        },
        {
          "module": "sklearn.random_projection",
          "declaration": "BaseRandomProjection",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "IS_PYPY",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "is_scalar_nan",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "shuffle",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_DEFAULT_TAGS",
          "alias": null
        },
        {
          "module": "sklearn.utils._tags",
          "declaration": "_safe_tags",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "SkipTest",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "_get_args",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "assert_allclose",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "assert_allclose_dense_sparse",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "assert_array_almost_equal",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "assert_array_equal",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "assert_raise_message",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "create_memmap_backed_data",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "ignore_warnings",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "raises",
          "alias": null
        },
        {
          "module": "sklearn.utils._testing",
          "declaration": "set_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_num_samples",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "has_fit_parameter",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "check_class_weight_balanced_classifiers",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "classifier_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "X_train",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "y_train",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "X_test",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "y_test",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "weights",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_class_weight_balanced_linear_classifier",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Classifier",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Test class weights with non-contiguous class labels."
        },
        {
          "name": "check_class_weight_classifiers",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "classifier_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_classifier_data_not_an_array",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_classifier_multioutput",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_classifiers_classes",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "classifier_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_classifiers_multilabel_representation_invariance",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "classifier_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_classifiers_one_label",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "classifier_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_classifiers_predictions",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "classifier_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_classifiers_regression_target",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_classifiers_train",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "classifier_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "readonly_memmap",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "X_dtype",
              "type": "Any",
              "hasDefault": true,
              "default": "float64",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_clusterer_compute_labels_predict",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "clusterer_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check that predict is invariant of compute_labels."
        },
        {
          "name": "check_clustering",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "clusterer_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "readonly_memmap",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_complex_data",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_decision_proba_consistency",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_dict_unchanged",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_dont_overwrite_parameters",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_dtype_object",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_estimator",
          "decorators": [],
          "parameters": [
            {
              "name": "Estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimator instance to check.\n\n.. versionchanged:: 0.24\n   Passing a class was deprecated in version 0.23, and support for\n   classes was removed in 0.24."
            },
            {
              "name": "generate_only",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "When `False`, checks are evaluated when `check_estimator` is called.\nWhen `True`, `check_estimator` returns a generator that yields\n(estimator, check) tuples. The check is run by calling\n`check(estimator)`.\n\n.. versionadded:: 0.22"
            }
          ],
          "results": [
            {
              "name": "checks_generator",
              "type": null,
              "description": "Generator that yields (estimator, check) tuples. Returned when\n`generate_only=True`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check if estimator adheres to scikit-learn conventions.\n\nThis estimator will run an extensive test-suite for input validation,\nshapes, etc, making sure that the estimator complies with `scikit-learn`\nconventions as detailed in :ref:`rolling_your_own_estimator`.\nAdditional tests for classifiers, regressors, clustering or transformers\nwill be run if the Estimator class inherits from the corresponding mixin\nfrom sklearn.base.\n\nSetting `generate_only=True` returns a generator that yields (estimator,\ncheck) tuples where the check can be called independently from each\nother, i.e. `check(estimator)`. This allows all checks to be run\nindependently and report the checks that are failing.\n\nscikit-learn provides a pytest specific decorator,\n:func:`~sklearn.utils.parametrize_with_checks`, making it easier to test\nmultiple estimators.\n\nParameters\n----------\nEstimator : estimator object\n    Estimator instance to check.\n\n    .. versionchanged:: 0.24\n       Passing a class was deprecated in version 0.23, and support for\n       classes was removed in 0.24.\n\ngenerate_only : bool, default=False\n    When `False`, checks are evaluated when `check_estimator` is called.\n    When `True`, `check_estimator` returns a generator that yields\n    (estimator, check) tuples. The check is run by calling\n    `check(estimator)`.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\nchecks_generator : generator\n    Generator that yields (estimator, check) tuples. Returned when\n    `generate_only=True`."
        },
        {
          "name": "check_estimator_get_tags_default_keys",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_estimator_sparse_data",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_estimators_data_not_an_array",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "obj_type",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_estimators_dtypes",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_estimators_empty_data_messages",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_estimators_fit_returns_self",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "readonly_memmap",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check if self is returned when calling fit."
        },
        {
          "name": "check_estimators_nan_inf",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_estimators_overwrite_params",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_estimators_partial_fit_n_features",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_estimators_pickle",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Test that we can pickle all estimators."
        },
        {
          "name": "check_estimators_unfitted",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check that predict raises an exception in an unfitted estimator.\n\nUnfitted estimators should raise a NotFittedError."
        },
        {
          "name": "check_fit1d",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_fit2d_1feature",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_fit2d_1sample",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_fit2d_predict1d",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_fit_idempotent",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_fit_non_negative",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_fit_score_takes_y",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_get_params_invariance",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_methods_sample_order_invariance",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_methods_subset_invariance",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_n_features_in",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_n_features_in_after_fitting",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_no_attributes_set_in_init",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check setting during init."
        },
        {
          "name": "check_non_transformer_estimators_n_iter",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_nonsquare_error",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Test that error is thrown when non-square data provided."
        },
        {
          "name": "check_outlier_corruption",
          "decorators": [],
          "parameters": [
            {
              "name": "num_outliers",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "expected_outliers",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "decision",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_outliers_fit_predict",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_outliers_train",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "readonly_memmap",
              "type": "Any",
              "hasDefault": true,
              "default": "True",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_parameters_default_constructible",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "Estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_pipeline_consistency",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_regressor_data_not_an_array",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_regressor_multioutput",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_regressors_int",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "regressor_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_regressors_no_decision_function",
          "decorators": [
            "ignore_warnings"
          ],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "regressor_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_regressors_train",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "regressor_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "readonly_memmap",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "X_dtype",
              "type": "Any",
              "hasDefault": true,
              "default": "<ast.Name object at 0x0000014A7CBA70D0>",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_requires_y_none",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_sample_weights_invariance",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "kind",
              "type": "Any",
              "hasDefault": true,
              "default": "ones",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_sample_weights_list",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_sample_weights_not_an_array",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_sample_weights_pandas_series",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_sample_weights_shape",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_set_params",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_sparsify_coefficients",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_supervised_y_2d",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_supervised_y_no_nan",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_transformer_data_not_an_array",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "transformer",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_transformer_general",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "transformer",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "readonly_memmap",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_transformer_n_iter",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "estimator_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_transformer_preserve_dtypes",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "transformer_orig",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "check_transformers_unfitted",
          "decorators": [],
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "transformer",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        },
        {
          "name": "parametrize_with_checks",
          "decorators": [],
          "parameters": [
            {
              "name": "estimators",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Estimators to generated checks for.\n\n.. versionchanged:: 0.24\n   Passing a class was deprecated in version 0.23, and support for\n   classes was removed in 0.24. Pass an instance instead.\n\n.. versionadded:: 0.24"
            }
          ],
          "results": [
            {
              "name": "decorator",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Pytest specific decorator for parametrizing estimator checks.\n\nThe `id` of each check is set to be a pprint version of the estimator\nand the name of the check with its keyword arguments.\nThis allows to use `pytest -k` to specify which tests to run::\n\n    pytest test_check_estimators.py -k check_estimators_fit_returns_self\n\nParameters\n----------\nestimators : list of estimators instances\n    Estimators to generated checks for.\n\n    .. versionchanged:: 0.24\n       Passing a class was deprecated in version 0.23, and support for\n       classes was removed in 0.24. Pass an instance instead.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ndecorator : `pytest.mark.parametrize`\n\nExamples\n--------\n>>> from sklearn.utils.estimator_checks import parametrize_with_checks\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.tree import DecisionTreeRegressor\n\n>>> @parametrize_with_checks([LogisticRegression(),\n...                           DecisionTreeRegressor()])\n... def test_sklearn_compatible_estimator(estimator, check):\n...     check(estimator)"
        }
      ]
    },
    {
      "name": "sklearn.utils.extmath",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "linalg",
          "alias": null
        },
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils._logistic_sigmoid",
          "declaration": "_log_logistic_sigmoid",
          "alias": null
        },
        {
          "module": "sklearn.utils.sparsefuncs_fast",
          "declaration": "csr_row_norms",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "cartesian",
          "decorators": [],
          "parameters": [
            {
              "name": "arrays",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "1-D arrays to form the cartesian product of."
            },
            {
              "name": "out",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array to place the cartesian product in."
            }
          ],
          "results": [
            {
              "name": "out",
              "type": "NDArray",
              "description": "2-D array of shape (M, len(arrays)) containing cartesian products\nformed of input arrays."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Generate a cartesian product of input arrays.\n\nParameters\n----------\narrays : list of array-like\n    1-D arrays to form the cartesian product of.\nout : ndarray, default=None\n    Array to place the cartesian product in.\n\nReturns\n-------\nout : ndarray\n    2-D array of shape (M, len(arrays)) containing cartesian products\n    formed of input arrays.\n\nExamples\n--------\n>>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\narray([[1, 4, 6],\n       [1, 4, 7],\n       [1, 5, 6],\n       [1, 5, 7],\n       [2, 4, 6],\n       [2, 4, 7],\n       [2, 5, 6],\n       [2, 5, 7],\n       [3, 4, 6],\n       [3, 4, 7],\n       [3, 5, 6],\n       [3, 5, 7]])\n\nNotes\n-----\nThis function may not be used on more than 32 arrays\nbecause the underlying numpy functions do not support it."
        },
        {
          "name": "density",
          "decorators": [],
          "parameters": [
            {
              "name": "w",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The sparse vector."
            }
          ],
          "results": [
            {
              "name": "",
              "type": "float",
              "description": "The density of w, between 0 and 1."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute density of a sparse vector.\n\nParameters\n----------\nw : array-like\n    The sparse vector.\n\nReturns\n-------\nfloat\n    The density of w, between 0 and 1."
        },
        {
          "name": "fast_logdet",
          "decorators": [],
          "parameters": [
            {
              "name": "A",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The matrix."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute log(det(A)) for A symmetric.\n\nEquivalent to : np.log(nl.det(A)) but more robust.\nIt returns -Inf if det(A) is non positive or is not defined.\n\nParameters\n----------\nA : array-like\n    The matrix."
        },
        {
          "name": "log_logistic",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Argument to the logistic function."
            },
            {
              "name": "out",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Preallocated output array."
            }
          ],
          "results": [
            {
              "name": "out",
              "type": null,
              "description": "Log of the logistic function evaluated at every point in x."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.\n\nThis implementation is numerically stable because it splits positive and\nnegative values::\n\n    -log(1 + exp(-x_i))     if x_i > 0\n    x_i - log(1 + exp(x_i)) if x_i <= 0\n\nFor the ordinary logistic function, use ``scipy.special.expit``.\n\nParameters\n----------\nX : array-like of shape (M, N) or (M,)\n    Argument to the logistic function.\n\nout : array-like of shape (M, N) or (M,), default=None\n    Preallocated output array.\n\nReturns\n-------\nout : ndarray of shape (M, N) or (M,)\n    Log of the logistic function evaluated at every point in x.\n\nNotes\n-----\nSee the blog post describing this implementation:\nhttp://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/"
        },
        {
          "name": "make_nonnegative",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The matrix to make non-negative."
            },
            {
              "name": "min_value",
              "type": "Any",
              "hasDefault": false,
              "default": "0",
              "limitation": null,
              "ignored": false,
              "description": "The threshold value."
            }
          ],
          "results": [
            {
              "name": "",
              "type": "ArrayLike",
              "description": "The thresholded array."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Ensure `X.min()` >= `min_value`.\n\nParameters\n----------\nX : array-like\n    The matrix to make non-negative.\nmin_value : float, default=0\n    The threshold value.\n\nReturns\n-------\narray-like\n    The thresholded array.\n\nRaises\n------\nValueError\n    When X is sparse."
        },
        {
          "name": "randomized_range_finder",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "A",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The input data matrix."
            }
          ],
          "results": [
            {
              "name": "Q",
              "type": "NDArray",
              "description": "A (size x size) projection matrix, the range of which\napproximates well the range of the input matrix A."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes an orthonormal matrix whose range approximates the range of A.\n\nParameters\n----------\nA : 2D array\n    The input data matrix.\n\nsize : int\n    Size of the return array.\n\nn_iter : int\n    Number of power iterations used to stabilize the result.\n\npower_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n    Whether the power iterations are normalized with step-by-step\n    QR factorization (the slowest but most accurate), 'none'\n    (the fastest but numerically unstable when `n_iter` is large, e.g.\n    typically 5 or larger), or 'LU' factorization (numerically stable\n    but can lose slightly in accuracy). The 'auto' mode applies no\n    normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n    .. versionadded:: 0.18\n\nrandom_state : int, RandomState instance or None, default=None\n    The seed of the pseudo random number generator to use when shuffling\n    the data, i.e. getting the random vectors to initialize the algorithm.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nQ : ndarray\n    A (size x size) projection matrix, the range of which\n    approximates well the range of the input matrix A.\n\nNotes\n-----\n\nFollows Algorithm 4.3 of\nFinding structure with randomness: Stochastic algorithms for constructing\napproximate matrix decompositions\nHalko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n\nAn implementation of a randomized algorithm for principal component\nanalysis\nA. Szlam et al. 2014"
        },
        {
          "name": "randomized_svd",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "M",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix to decompose."
            },
            {
              "name": "n_components",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Number of singular values and vectors to extract."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Computes a truncated randomized SVD.\n\nParameters\n----------\nM : {ndarray, sparse matrix}\n    Matrix to decompose.\n\nn_components : int\n    Number of singular values and vectors to extract.\n\nn_oversamples : int, default=10\n    Additional number of random vectors to sample the range of M so as\n    to ensure proper conditioning. The total number of random vectors\n    used to find the range of M is n_components + n_oversamples. Smaller\n    number can improve speed but can negatively impact the quality of\n    approximation of singular vectors and singular values.\n\nn_iter : int or 'auto', default='auto'\n    Number of power iterations. It can be used to deal with very noisy\n    problems. When 'auto', it is set to 4, unless `n_components` is small\n    (< .1 * min(X.shape)) `n_iter` in which case is set to 7.\n    This improves precision with few components.\n\n    .. versionchanged:: 0.18\n\npower_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n    Whether the power iterations are normalized with step-by-step\n    QR factorization (the slowest but most accurate), 'none'\n    (the fastest but numerically unstable when `n_iter` is large, e.g.\n    typically 5 or larger), or 'LU' factorization (numerically stable\n    but can lose slightly in accuracy). The 'auto' mode applies no\n    normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n    .. versionadded:: 0.18\n\ntranspose : bool or 'auto', default='auto'\n    Whether the algorithm should be applied to M.T instead of M. The\n    result should approximately be the same. The 'auto' mode will\n    trigger the transposition if M.shape[1] > M.shape[0] since this\n    implementation of randomized SVD tend to be a little faster in that\n    case.\n\n    .. versionchanged:: 0.18\n\nflip_sign : bool, default=True\n    The output of a singular value decomposition is only unique up to a\n    permutation of the signs of the singular vectors. If `flip_sign` is\n    set to `True`, the sign ambiguity is resolved by making the largest\n    loadings for each component in the left singular vectors positive.\n\nrandom_state : int, RandomState instance or None, default=0\n    The seed of the pseudo random number generator to use when shuffling\n    the data, i.e. getting the random vectors to initialize the algorithm.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nNotes\n-----\nThis algorithm finds a (usually very good) approximate truncated\nsingular value decomposition using randomization to speed up the\ncomputations. It is particularly fast on large matrices on which\nyou wish to extract only a small number of components. In order to\nobtain further speed up, `n_iter` can be set <=2 (at the cost of\nloss of precision).\n\nReferences\n----------\n* Finding structure with randomness: Stochastic algorithms for constructing\n  approximate matrix decompositions\n  Halko, et al., 2009 https://arxiv.org/abs/0909.4061\n\n* A randomized algorithm for the decomposition of matrices\n  Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n* An implementation of a randomized algorithm for principal component\n  analysis\n  A. Szlam et al. 2014"
        },
        {
          "name": "row_norms",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The input array."
            },
            {
              "name": "squared",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "If True, return squared norms."
            }
          ],
          "results": [
            {
              "name": "",
              "type": "ArrayLike",
              "description": "The row-wise (squared) Euclidean norm of X."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Row-wise (squared) Euclidean norm of X.\n\nEquivalent to np.sqrt((X * X).sum(axis=1)), but also supports sparse\nmatrices and does not create an X.shape-sized temporary.\n\nPerforms no input validation.\n\nParameters\n----------\nX : array-like\n    The input array.\nsquared : bool, default=False\n    If True, return squared norms.\n\nReturns\n-------\narray-like\n    The row-wise (squared) Euclidean norm of X."
        },
        {
          "name": "safe_sparse_dot",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "a",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "b",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "dot_product",
              "type": null,
              "description": "Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Dot product that handle the sparse matrix case correctly.\n\nParameters\n----------\na : {ndarray, sparse matrix}\nb : {ndarray, sparse matrix}\ndense_output : bool, default=False\n    When False, ``a`` and ``b`` both being sparse will yield sparse output.\n    When True, output will always be a dense array.\n\nReturns\n-------\ndot_product : {ndarray, sparse matrix}\n    Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``."
        },
        {
          "name": "softmax",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Argument to the logistic function."
            },
            {
              "name": "copy",
              "type": "Any",
              "hasDefault": true,
              "default": "True",
              "limitation": null,
              "ignored": false,
              "description": "Copy X or not."
            }
          ],
          "results": [
            {
              "name": "out",
              "type": null,
              "description": "Softmax function evaluated at every point in x."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Calculate the softmax function.\n\nThe softmax function is calculated by\nnp.exp(X) / np.sum(np.exp(X), axis=1)\n\nThis will cause overflow when large values are exponentiated.\nHence the largest value in each row is subtracted from each data\npoint to prevent this.\n\nParameters\n----------\nX : array-like of float of shape (M, N)\n    Argument to the logistic function.\n\ncopy : bool, default=True\n    Copy X or not.\n\nReturns\n-------\nout : ndarray of shape (M, N)\n    Softmax function evaluated at every point in x."
        },
        {
          "name": "squared_norm",
          "decorators": [],
          "parameters": [
            {
              "name": "x",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "",
              "type": "float",
              "description": "The Euclidean norm when x is a vector, the Frobenius norm when x\nis a matrix (2-d array)."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Squared Euclidean or Frobenius norm of x.\n\nFaster than norm(x) ** 2.\n\nParameters\n----------\nx : array-like\n\nReturns\n-------\nfloat\n    The Euclidean norm when x is a vector, the Frobenius norm when x\n    is a matrix (2-d array)."
        },
        {
          "name": "stable_cumsum",
          "decorators": [],
          "parameters": [
            {
              "name": "arr",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "To be cumulatively summed as flat."
            },
            {
              "name": "axis",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Axis along which the cumulative sum is computed.\nThe default (None) is to compute the cumsum over the flattened array."
            },
            {
              "name": "rtol",
              "type": "Any",
              "hasDefault": true,
              "default": "1e-05",
              "limitation": null,
              "ignored": false,
              "description": "Relative tolerance, see ``np.allclose``."
            },
            {
              "name": "atol",
              "type": "Any",
              "hasDefault": true,
              "default": "1e-08",
              "limitation": null,
              "ignored": false,
              "description": "Absolute tolerance, see ``np.allclose``."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Use high precision for cumsum and check that final value matches sum.\n\nParameters\n----------\narr : array-like\n    To be cumulatively summed as flat.\naxis : int, default=None\n    Axis along which the cumulative sum is computed.\n    The default (None) is to compute the cumsum over the flattened array.\nrtol : float, default=1e-05\n    Relative tolerance, see ``np.allclose``.\natol : float, default=1e-08\n    Absolute tolerance, see ``np.allclose``."
        },
        {
          "name": "svd_flip",
          "decorators": [],
          "parameters": [
            {
              "name": "u",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "u and v are the output of `linalg.svd` or\n:func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\ndimensions so one can compute `np.dot(u * s, v)`."
            },
            {
              "name": "v",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "u and v are the output of `linalg.svd` or\n:func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\ndimensions so one can compute `np.dot(u * s, v)`.\nThe input v should really be called vt to be consistent with scipy's\nouput."
            },
            {
              "name": "u_based_decision",
              "type": "Any",
              "hasDefault": true,
              "default": "True",
              "limitation": null,
              "ignored": false,
              "description": "If True, use the columns of u as the basis for sign flipping.\nOtherwise, use the rows of v. The choice of which variable to base the\ndecision on is generally algorithm dependent."
            }
          ],
          "results": [
            {
              "name": "u_adjusted, v_adjusted",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Sign correction to ensure deterministic output from SVD.\n\nAdjusts the columns of u and the rows of v such that the loadings in the\ncolumns in u that are largest in absolute value are always positive.\n\nParameters\n----------\nu : ndarray\n    u and v are the output of `linalg.svd` or\n    :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n    dimensions so one can compute `np.dot(u * s, v)`.\n\nv : ndarray\n    u and v are the output of `linalg.svd` or\n    :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n    dimensions so one can compute `np.dot(u * s, v)`.\n    The input v should really be called vt to be consistent with scipy's\n    ouput.\n\nu_based_decision : bool, default=True\n    If True, use the columns of u as the basis for sign flipping.\n    Otherwise, use the rows of v. The choice of which variable to base the\n    decision on is generally algorithm dependent.\n\n\nReturns\n-------\nu_adjusted, v_adjusted : arrays with the same dimensions as the input."
        },
        {
          "name": "weighted_mode",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "a",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "n-dimensional array of which to find mode(s)."
            },
            {
              "name": "w",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "n-dimensional array of weights for each value."
            }
          ],
          "results": [
            {
              "name": "vals",
              "type": "NDArray",
              "description": "Array of modal values."
            },
            {
              "name": "score",
              "type": "NDArray",
              "description": "Array of weighted counts for each mode."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Returns an array of the weighted modal (most common) value in a.\n\nIf there is more than one such value, only the first is returned.\nThe bin-count for the modal bins is also returned.\n\nThis is an extension of the algorithm in scipy.stats.mode.\n\nParameters\n----------\na : array-like\n    n-dimensional array of which to find mode(s).\nw : array-like\n    n-dimensional array of weights for each value.\naxis : int, default=0\n    Axis along which to operate. Default is 0, i.e. the first axis.\n\nReturns\n-------\nvals : ndarray\n    Array of modal values.\nscore : ndarray\n    Array of weighted counts for each mode.\n\nExamples\n--------\n>>> from sklearn.utils.extmath import weighted_mode\n>>> x = [4, 1, 4, 2, 4, 2]\n>>> weights = [1, 1, 1, 1, 1, 1]\n>>> weighted_mode(x, weights)\n(array([4.]), array([3.]))\n\nThe value 4 appears three times: with uniform weights, the result is\nsimply the mode of the distribution.\n\n>>> weights = [1, 3, 0.5, 1.5, 1, 2]  # deweight the 4's\n>>> weighted_mode(x, weights)\n(array([2.]), array([3.5]))\n\nThe value 2 has the highest score: it appears twice with weights of\n1.5 and 2: the sum of these is 3.5.\n\nSee Also\n--------\nscipy.stats.mode"
        }
      ]
    },
    {
      "name": "sklearn.utils.fixes",
      "imports": [
        {
          "module": "functools",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "scipy.stats",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "distutils.version",
          "declaration": "LooseVersion",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "update_wrapper",
          "alias": null
        },
        {
          "module": "numpy.ma",
          "declaration": "MaskedArray",
          "alias": "_MaskedArray"
        },
        {
          "module": "pkg_resources",
          "declaration": "parse_version",
          "alias": null
        },
        {
          "module": "scipy.sparse.linalg",
          "declaration": "lobpcg",
          "alias": null
        },
        {
          "module": "scipy.sparse.linalg",
          "declaration": "lsqr",
          "alias": "sparse_lsqr"
        },
        {
          "module": "sklearn._config",
          "declaration": "config_context",
          "alias": null
        },
        {
          "module": "sklearn._config",
          "declaration": "get_config",
          "alias": null
        },
        {
          "module": "sklearn.externals._lobpcg",
          "declaration": "lobpcg",
          "alias": null
        },
        {
          "module": "sklearn.utils.deprecation",
          "declaration": "deprecated",
          "alias": null
        }
      ],
      "classes": [
        {
          "name": "MaskedArray",
          "decorators": [],
          "superclasses": [
            "_MaskedArray"
          ],
          "methods": [],
          "fullDocstring": null
        },
        {
          "name": "loguniform",
          "decorators": [],
          "superclasses": [],
          "methods": [],
          "fullDocstring": "A class supporting log-uniform random variables.\n\nParameters\n----------\nlow : float\n    The minimum value\nhigh : float\n    The maximum value\n\nMethods\n-------\nrvs(self, size=None, random_state=None)\n    Generate log-uniform random variables\n\nThe most useful method for Scikit-learn usage is highlighted here.\nFor a full list, see\n`scipy.stats.reciprocal\n<https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html>`_.\nThis list includes all functions of ``scipy.stats`` continuous\ndistributions such as ``pdf``.\n\nNotes\n-----\nThis class generates values between ``low`` and ``high`` or\n\n    low <= loguniform(low, high).rvs() <= high\n\nThe logarithmic probability density function (PDF) is uniform. When\n``x`` is a uniformly distributed random variable between 0 and 1, ``10**x``\nare random variables that are equally likely to be returned.\n\nThis class is an alias to ``scipy.stats.reciprocal``, which uses the\nreciprocal distribution:\nhttps://en.wikipedia.org/wiki/Reciprocal_distribution\n\nExamples\n--------\n\n>>> from sklearn.utils.fixes import loguniform\n>>> rv = loguniform(1e-3, 1e1)\n>>> rvs = rv.rvs(random_state=42, size=1000)\n>>> rvs.min()  # doctest: +SKIP\n0.0010435856341129003\n>>> rvs.max()  # doctest: +SKIP\n9.97403052786026"
        }
      ],
      "functions": [
        {
          "name": "delayed",
          "decorators": [],
          "parameters": [
            {
              "name": "function",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Decorator used to capture the arguments of a function."
        }
      ]
    },
    {
      "name": "sklearn.utils.graph",
      "imports": [],
      "fromImports": [
        {
          "module": "scipy",
          "declaration": "sparse",
          "alias": null
        },
        {
          "module": "sklearn.utils.graph_shortest_path",
          "declaration": "graph_shortest_path",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "single_source_shortest_path_length",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "graph",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Adjacency matrix of the graph. Sparse matrix of format LIL is\npreferred."
            },
            {
              "name": "source",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Starting node for path."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Return the shortest path length from source to all reachable nodes.\n\nReturns a dictionary of shortest path lengths keyed by target.\n\nParameters\n----------\ngraph : {sparse matrix, ndarray} of shape (n, n)\n    Adjacency matrix of the graph. Sparse matrix of format LIL is\n    preferred.\n\nsource : int\n   Starting node for path.\n\ncutoff : int, default=None\n    Depth to stop the search - only paths of length <= cutoff are returned.\n\nExamples\n--------\n>>> from sklearn.utils.graph import single_source_shortest_path_length\n>>> import numpy as np\n>>> graph = np.array([[ 0, 1, 0, 0],\n...                   [ 1, 0, 1, 0],\n...                   [ 0, 1, 0, 1],\n...                   [ 0, 0, 1, 0]])\n>>> list(sorted(single_source_shortest_path_length(graph, 0).items()))\n[(0, 0), (1, 1), (2, 2), (3, 3)]\n>>> graph = np.ones((6, 6))\n>>> list(sorted(single_source_shortest_path_length(graph, 2).items()))\n[(0, 1), (1, 1), (2, 0), (3, 1), (4, 1), (5, 1)]"
        }
      ]
    },
    {
      "name": "sklearn.utils.metaestimators",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "abc",
          "declaration": "ABCMeta",
          "alias": null
        },
        {
          "module": "abc",
          "declaration": "abstractmethod",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "update_wrapper",
          "alias": null
        },
        {
          "module": "operator",
          "declaration": "attrgetter",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "BaseEstimator",
          "alias": null
        },
        {
          "module": "sklearn.base",
          "declaration": "_is_pairwise",
          "alias": null
        },
        {
          "module": "sklearn.utils",
          "declaration": "_safe_indexing",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "Any",
          "alias": null
        },
        {
          "module": "typing",
          "declaration": "List",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "if_delegate_has_method",
          "decorators": [],
          "parameters": [
            {
              "name": "delegate",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Name of the sub-estimator that can be accessed as an attribute of the\nbase object. If a list or a tuple of names are provided, the first\nsub-estimator that is an attribute of the base object will be used."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Create a decorator for methods that are delegated to a sub-estimator\n\nThis enables ducktyping by hasattr returning True according to the\nsub-estimator.\n\nParameters\n----------\ndelegate : string, list of strings or tuple of strings\n    Name of the sub-estimator that can be accessed as an attribute of the\n    base object. If a list or a tuple of names are provided, the first\n    sub-estimator that is an attribute of the base object will be used."
        }
      ]
    },
    {
      "name": "sklearn.utils.multiclass",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "collections.abc",
          "declaration": "Sequence",
          "alias": null
        },
        {
          "module": "itertools",
          "declaration": "chain",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "dok_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "issparse",
          "alias": null
        },
        {
          "module": "scipy.sparse",
          "declaration": "lil_matrix",
          "alias": null
        },
        {
          "module": "scipy.sparse.base",
          "declaration": "spmatrix",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_assert_all_finite",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "check_array",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "check_classification_targets",
          "decorators": [],
          "parameters": [
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Ensure that target y is of a non-regression type.\n\nOnly the following target types (as defined in type_of_target) are allowed:\n    'binary', 'multiclass', 'multiclass-multioutput',\n    'multilabel-indicator', 'multilabel-sequences'\n\nParameters\n----------\ny : array-like"
        },
        {
          "name": "class_distribution",
          "decorators": [],
          "parameters": [
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The labels for each example."
            },
            {
              "name": "sample_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Sample weights."
            }
          ],
          "results": [
            {
              "name": "classes",
              "type": null,
              "description": "List of classes for each column."
            },
            {
              "name": "n_classes",
              "type": null,
              "description": "Number of classes in each column."
            },
            {
              "name": "class_prior",
              "type": null,
              "description": "Class distribution of each column."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute class priors from multioutput-multiclass target data.\n\nParameters\n----------\ny : {array-like, sparse matrix} of size (n_samples, n_outputs)\n    The labels for each example.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nclasses : list of size n_outputs of ndarray of size (n_classes,)\n    List of classes for each column.\n\nn_classes : list of int of size n_outputs\n    Number of classes in each column.\n\nclass_prior : list of size n_outputs of ndarray of size (n_classes,)\n    Class distribution of each column."
        },
        {
          "name": "is_multilabel",
          "decorators": [],
          "parameters": [
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Target values."
            }
          ],
          "results": [
            {
              "name": "out",
              "type": "bool",
              "description": "Return ``True``, if ``y`` is in a multilabel format, else ```False``."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check if ``y`` is in a multilabel format.\n\nParameters\n----------\ny : ndarray of shape (n_samples,)\n    Target values.\n\nReturns\n-------\nout : bool\n    Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.utils.multiclass import is_multilabel\n>>> is_multilabel([0, 1, 0, 1])\nFalse\n>>> is_multilabel([[1], [0, 2], []])\nFalse\n>>> is_multilabel(np.array([[1, 0], [0, 0]]))\nTrue\n>>> is_multilabel(np.array([[1], [0], [0]]))\nFalse\n>>> is_multilabel(np.array([[1, 0, 0]]))\nTrue"
        },
        {
          "name": "type_of_target",
          "decorators": [],
          "parameters": [
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "target_type",
              "type": "str",
              "description": "One of:\n\n* 'continuous': `y` is an array-like of floats that are not all\n  integers, and is 1d or a column vector.\n* 'continuous-multioutput': `y` is a 2d array of floats that are\n  not all integers, and both dimensions are of size > 1.\n* 'binary': `y` contains <= 2 discrete values and is 1d or a column\n  vector.\n* 'multiclass': `y` contains more than two discrete values, is not a\n  sequence of sequences, and is 1d or a column vector.\n* 'multiclass-multioutput': `y` is a 2d array that contains more\n  than two discrete values, is not a sequence of sequences, and both\n  dimensions are of size > 1.\n* 'multilabel-indicator': `y` is a label indicator matrix, an array\n  of two dimensions with at least two columns, and at most 2 unique\n  values.\n* 'unknown': `y` is array-like but none of the above, such as a 3d\n  array, sequence of sequences, or an array of non-sequence objects."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Determine the type of data indicated by the target.\n\nNote that this type is the most specific type that can be inferred.\nFor example:\n\n    * ``binary`` is more specific but compatible with ``multiclass``.\n    * ``multiclass`` of integers is more specific but compatible with\n      ``continuous``.\n    * ``multilabel-indicator`` is more specific but compatible with\n      ``multiclass-multioutput``.\n\nParameters\n----------\ny : array-like\n\nReturns\n-------\ntarget_type : str\n    One of:\n\n    * 'continuous': `y` is an array-like of floats that are not all\n      integers, and is 1d or a column vector.\n    * 'continuous-multioutput': `y` is a 2d array of floats that are\n      not all integers, and both dimensions are of size > 1.\n    * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n      vector.\n    * 'multiclass': `y` contains more than two discrete values, is not a\n      sequence of sequences, and is 1d or a column vector.\n    * 'multiclass-multioutput': `y` is a 2d array that contains more\n      than two discrete values, is not a sequence of sequences, and both\n      dimensions are of size > 1.\n    * 'multilabel-indicator': `y` is a label indicator matrix, an array\n      of two dimensions with at least two columns, and at most 2 unique\n      values.\n    * 'unknown': `y` is array-like but none of the above, such as a 3d\n      array, sequence of sequences, or an array of non-sequence objects.\n\nExamples\n--------\n>>> import numpy as np\n>>> type_of_target([0.1, 0.6])\n'continuous'\n>>> type_of_target([1, -1, -1, 1])\n'binary'\n>>> type_of_target(['a', 'b', 'a'])\n'binary'\n>>> type_of_target([1.0, 2.0])\n'binary'\n>>> type_of_target([1, 0, 2])\n'multiclass'\n>>> type_of_target([1.0, 0.0, 3.0])\n'multiclass'\n>>> type_of_target(['a', 'b', 'c'])\n'multiclass'\n>>> type_of_target(np.array([[1, 2], [3, 1]]))\n'multiclass-multioutput'\n>>> type_of_target([[1, 2]])\n'multilabel-indicator'\n>>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n'continuous-multioutput'\n>>> type_of_target(np.array([[0, 1], [1, 1]]))\n'multilabel-indicator'"
        },
        {
          "name": "unique_labels",
          "decorators": [],
          "parameters": [],
          "results": [
            {
              "name": "out",
              "type": null,
              "description": "An ordered array of unique labels."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Extract an ordered array of unique labels.\n\nWe don't allow:\n    - mix of multilabel and multiclass (single label) targets\n    - mix of label indicator matrix and anything else,\n      because there are no explicit labels)\n    - mix of label indicator matrices of different sizes\n    - mix of string and integer labels\n\nAt the moment, we also don't allow \"multiclass-multioutput\" input type.\n\nParameters\n----------\n*ys : array-likes\n\nReturns\n-------\nout : ndarray of shape (n_unique_labels,)\n    An ordered array of unique labels.\n\nExamples\n--------\n>>> from sklearn.utils.multiclass import unique_labels\n>>> unique_labels([3, 5, 5, 5, 7, 7])\narray([3, 5, 7])\n>>> unique_labels([1, 2, 3, 4], [2, 2, 3, 4])\narray([1, 2, 3, 4])\n>>> unique_labels([1, 2, 10], [5, 11])\narray([ 1,  2,  5, 10, 11])"
        }
      ]
    },
    {
      "name": "sklearn.utils.optimize",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "scipy.optimize.linesearch",
          "declaration": "line_search_wolfe1",
          "alias": null
        },
        {
          "module": "scipy.optimize.linesearch",
          "declaration": "line_search_wolfe2",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "ConvergenceWarning",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.utils.random",
      "imports": [
        {
          "module": "array",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.utils",
          "declaration": "check_random_state",
          "alias": null
        },
        {
          "module": "sklearn.utils._random",
          "declaration": "sample_without_replacement",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.utils.setup",
      "imports": [
        {
          "module": "os",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "numpy.distutils.core",
          "declaration": "setup",
          "alias": null
        },
        {
          "module": "os.path",
          "declaration": "join",
          "alias": null
        },
        {
          "module": "sklearn._build_utils",
          "declaration": "gen_from_templates",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "configuration",
          "decorators": [],
          "parameters": [
            {
              "name": "parent_package",
              "type": "Any",
              "hasDefault": false,
              "default": "",
              "limitation": null,
              "ignored": false,
              "description": ""
            },
            {
              "name": "top_path",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": ""
        }
      ]
    },
    {
      "name": "sklearn.utils.sparsefuncs",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.utils.sparsefuncs_fast",
          "declaration": "csc_mean_variance_axis0",
          "alias": "_csc_mean_var_axis0"
        },
        {
          "module": "sklearn.utils.sparsefuncs_fast",
          "declaration": "csr_mean_variance_axis0",
          "alias": "_csr_mean_var_axis0"
        },
        {
          "module": "sklearn.utils.sparsefuncs_fast",
          "declaration": "incr_mean_variance_axis0",
          "alias": "_incr_mean_var_axis0"
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_check_sample_weight",
          "alias": null
        },
        {
          "module": "sklearn.utils.validation",
          "declaration": "_deprecate_positional_args",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "count_nonzero",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data. It should be of CSR format."
            },
            {
              "name": "axis",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The axis on which the data is aggregated."
            },
            {
              "name": "sample_weight",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Weight for each row of X."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "A variant of X.getnnz() with extension to weighting on axis 0\n\nUseful in efficiently calculating multilabel metrics.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_labels)\n    Input data. It should be of CSR format.\n\naxis : {0, 1}, default=None\n    The axis on which the data is aggregated.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight for each row of X."
        },
        {
          "name": "csc_median_axis_0",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data. It should be of CSC format."
            }
          ],
          "results": [
            {
              "name": "median",
              "type": null,
              "description": "Median."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Find the median across axis 0 of a CSC matrix.\nIt is equivalent to doing np.median(X, axis=0).\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Input data. It should be of CSC format.\n\nReturns\n-------\nmedian : ndarray of shape (n_features,)\n    Median."
        },
        {
          "name": "incr_mean_variance_axis",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data."
            }
          ],
          "results": [
            {
              "name": "means",
              "type": null,
              "description": "Updated feature-wise means if axis = 0 or\nsample-wise means if axis = 1."
            },
            {
              "name": "variances",
              "type": null,
              "description": "Updated feature-wise variances if axis = 0 or\nsample-wise variances if axis = 1."
            },
            {
              "name": "n",
              "type": null,
              "description": "Updated number of seen samples per feature if axis=0\nor number of seen features per sample if axis=1.\n\nIf weights is not None, n is a sum of the weights of the seen\nsamples or features instead of the actual number of seen\nsamples or features."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute incremental mean and variance along an axis on a CSR or\nCSC matrix.\n\nlast_mean, last_var are the statistics computed at the last step by this\nfunction. Both must be initialized to 0-arrays of the proper size, i.e.\nthe number of features in X. last_n is the number of samples encountered\nuntil now.\n\nParameters\n----------\nX : CSR or CSC sparse matrix of shape (n_samples, n_features)\n    Input data.\n\naxis : {0, 1}\n    Axis along which the axis should be computed.\n\nlast_mean : ndarray of shape (n_features,) or (n_samples,), dtype=floating\n    Array of means to update with the new data X.\n    Should be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.\n\nlast_var : ndarray of shape (n_features,) or (n_samples,), dtype=floating\n    Array of variances to update with the new data X.\n    Should be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.\n\nlast_n : float or ndarray of shape (n_features,) or (n_samples,),             dtype=floating\n    Sum of the weights seen so far, excluding the current weights\n    If not float, it should be of shape (n_samples,) if\n    axis=0 or (n_features,) if axis=1. If float it corresponds to\n    having same weights for all samples (or features).\n\nweights : ndarray of shape (n_samples,) or (n_features,), default=None\n    If axis is set to 0 shape is (n_samples,) or\n    if axis is set to 1 shape is (n_features,).\n    If it is set to None, then samples are equally weighted.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nmeans : ndarray of shape (n_features,) or (n_samples,), dtype=floating\n    Updated feature-wise means if axis = 0 or\n    sample-wise means if axis = 1.\n\nvariances : ndarray of shape (n_features,) or (n_samples,), dtype=floating\n    Updated feature-wise variances if axis = 0 or\n    sample-wise variances if axis = 1.\n\nn : ndarray of shape (n_features,) or (n_samples,), dtype=integral\n    Updated number of seen samples per feature if axis=0\n    or number of seen features per sample if axis=1.\n\n    If weights is not None, n is a sum of the weights of the seen\n    samples or features instead of the actual number of seen\n    samples or features.\n\nNotes\n-----\nNaNs are ignored in the algorithm."
        },
        {
          "name": "inplace_column_scale",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix to normalize using the variance of the features. It should be\nof CSC or CSR format."
            },
            {
              "name": "scale",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array of precomputed feature-wise values to use for scaling."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Inplace column scaling of a CSC/CSR matrix.\n\nScale each feature of the data matrix by multiplying with specific scale\nprovided by the caller assuming a (n_samples, n_features) shape.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Matrix to normalize using the variance of the features. It should be\n    of CSC or CSR format.\n\nscale : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n    Array of precomputed feature-wise values to use for scaling."
        },
        {
          "name": "inplace_csr_column_scale",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix to normalize using the variance of the features.\nIt should be of CSR format."
            },
            {
              "name": "scale",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array of precomputed feature-wise values to use for scaling."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Inplace column scaling of a CSR matrix.\n\nScale each feature of the data matrix by multiplying with specific scale\nprovided by the caller assuming a (n_samples, n_features) shape.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Matrix to normalize using the variance of the features.\n    It should be of CSR format.\n\nscale : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n    Array of precomputed feature-wise values to use for scaling."
        },
        {
          "name": "inplace_csr_row_scale",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix to be scaled. It should be of CSR format."
            },
            {
              "name": "scale",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array of precomputed sample-wise values to use for scaling."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Inplace row scaling of a CSR matrix.\n\nScale each sample of the data matrix by multiplying with specific scale\nprovided by the caller assuming a (n_samples, n_features) shape.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Matrix to be scaled. It should be of CSR format.\n\nscale : ndarray of float of shape (n_samples,)\n    Array of precomputed sample-wise values to use for scaling."
        },
        {
          "name": "inplace_row_scale",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix to be scaled. It should be of CSR or CSC format."
            },
            {
              "name": "scale",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Array of precomputed sample-wise values to use for scaling."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Inplace row scaling of a CSR or CSC matrix.\n\nScale each row of the data matrix by multiplying with specific scale\nprovided by the caller assuming a (n_samples, n_features) shape.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Matrix to be scaled. It should be of CSR or CSC format.\n\nscale : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n    Array of precomputed sample-wise values to use for scaling."
        },
        {
          "name": "inplace_swap_column",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix whose two columns are to be swapped. It should be of\nCSR or CSC format."
            },
            {
              "name": "m",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Index of the column of X to be swapped."
            },
            {
              "name": "n",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Index of the column of X to be swapped."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Swaps two columns of a CSC/CSR matrix in-place.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Matrix whose two columns are to be swapped. It should be of\n    CSR or CSC format.\n\nm : int\n    Index of the column of X to be swapped.\n\nn : int\n    Index of the column of X to be swapped."
        },
        {
          "name": "inplace_swap_row",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix whose two rows are to be swapped. It should be of CSR or\nCSC format."
            },
            {
              "name": "m",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Index of the row of X to be swapped."
            },
            {
              "name": "n",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Index of the row of X to be swapped."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Swaps two rows of a CSC/CSR matrix in-place.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Matrix whose two rows are to be swapped. It should be of CSR or\n    CSC format.\n\nm : int\n    Index of the row of X to be swapped.\n\nn : int\n    Index of the row of X to be swapped."
        },
        {
          "name": "inplace_swap_row_csc",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix whose two rows are to be swapped. It should be of\nCSC format."
            },
            {
              "name": "m",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Index of the row of X to be swapped."
            },
            {
              "name": "n",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Index of the row of X to be swapped."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Swaps two rows of a CSC matrix in-place.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Matrix whose two rows are to be swapped. It should be of\n    CSC format.\n\nm : int\n    Index of the row of X to be swapped.\n\nn : int\n    Index of the row of X to be swapped."
        },
        {
          "name": "inplace_swap_row_csr",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Matrix whose two rows are to be swapped. It should be of\nCSR format."
            },
            {
              "name": "m",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Index of the row of X to be swapped."
            },
            {
              "name": "n",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Index of the row of X to be swapped."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Swaps two rows of a CSR matrix in-place.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Matrix whose two rows are to be swapped. It should be of\n    CSR format.\n\nm : int\n    Index of the row of X to be swapped.\n\nn : int\n    Index of the row of X to be swapped."
        },
        {
          "name": "mean_variance_axis",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data. It can be of CSR or CSC format."
            },
            {
              "name": "axis",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Axis along which the axis should be computed."
            },
            {
              "name": "weights",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "if axis is set to 0 shape is (n_samples,) or\nif axis is set to 1 shape is (n_features,).\nIf it is set to None, then samples are equally weighted.\n\n.. versionadded:: 0.24"
            },
            {
              "name": "return_sum_weights",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "If True, returns the sum of weights seen for each feature\nif `axis=0` or each sample if `axis=1`.\n\n.. versionadded:: 0.24"
            }
          ],
          "results": [
            {
              "name": "means",
              "type": null,
              "description": "Feature-wise means."
            },
            {
              "name": "variances",
              "type": null,
              "description": "Feature-wise variances."
            },
            {
              "name": "sum_weights",
              "type": null,
              "description": "Returned if `return_sum_weights` is `True`."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute mean and variance along an axis on a CSR or CSC matrix.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Input data. It can be of CSR or CSC format.\n\naxis : {0, 1}\n    Axis along which the axis should be computed.\n\nweights : ndarray of shape (n_samples,) or (n_features,), default=None\n    if axis is set to 0 shape is (n_samples,) or\n    if axis is set to 1 shape is (n_features,).\n    If it is set to None, then samples are equally weighted.\n\n    .. versionadded:: 0.24\n\nreturn_sum_weights : bool, default=False\n    If True, returns the sum of weights seen for each feature\n    if `axis=0` or each sample if `axis=1`.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\n\nmeans : ndarray of shape (n_features,), dtype=floating\n    Feature-wise means.\n\nvariances : ndarray of shape (n_features,), dtype=floating\n    Feature-wise variances.\n\nsum_weights : ndarray of shape (n_features,), dtype=floating\n    Returned if `return_sum_weights` is `True`."
        },
        {
          "name": "min_max_axis",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data. It should be of CSR or CSC format."
            },
            {
              "name": "axis",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Axis along which the axis should be computed."
            },
            {
              "name": "ignore_nan",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "Ignore or passing through NaN values.\n\n.. versionadded:: 0.20"
            }
          ],
          "results": [
            {
              "name": "mins",
              "type": null,
              "description": "Feature-wise minima."
            },
            {
              "name": "maxs",
              "type": null,
              "description": "Feature-wise maxima."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Compute minimum and maximum along an axis on a CSR or CSC matrix and\noptionally ignore NaN values.\n\nParameters\n----------\nX : sparse matrix of shape (n_samples, n_features)\n    Input data. It should be of CSR or CSC format.\n\naxis : {0, 1}\n    Axis along which the axis should be computed.\n\nignore_nan : bool, default=False\n    Ignore or passing through NaN values.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\n\nmins : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n    Feature-wise minima.\n\nmaxs : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n    Feature-wise maxima."
        }
      ]
    },
    {
      "name": "sklearn.utils.stats",
      "imports": [
        {
          "module": "numpy",
          "alias": "np"
        }
      ],
      "fromImports": [
        {
          "module": "sklearn.utils.extmath",
          "declaration": "stable_cumsum",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_take_along_axis",
          "alias": null
        }
      ],
      "classes": [],
      "functions": []
    },
    {
      "name": "sklearn.utils.validation",
      "imports": [
        {
          "module": "joblib",
          "alias": null
        },
        {
          "module": "numbers",
          "alias": null
        },
        {
          "module": "numpy",
          "alias": "np"
        },
        {
          "module": "scipy.sparse",
          "alias": "sp"
        },
        {
          "module": "warnings",
          "alias": null
        }
      ],
      "fromImports": [
        {
          "module": "contextlib",
          "declaration": "suppress",
          "alias": null
        },
        {
          "module": "functools",
          "declaration": "wraps",
          "alias": null
        },
        {
          "module": "inspect",
          "declaration": "Parameter",
          "alias": null
        },
        {
          "module": "inspect",
          "declaration": "isclass",
          "alias": null
        },
        {
          "module": "inspect",
          "declaration": "signature",
          "alias": null
        },
        {
          "module": "numpy.core.numeric",
          "declaration": "ComplexWarning",
          "alias": null
        },
        {
          "module": "sklearn",
          "declaration": "get_config",
          "alias": "_get_config"
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "DataConversionWarning",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "NotFittedError",
          "alias": null
        },
        {
          "module": "sklearn.exceptions",
          "declaration": "PositiveSpectrumWarning",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "_object_dtype_isnan",
          "alias": null
        },
        {
          "module": "sklearn.utils.fixes",
          "declaration": "parse_version",
          "alias": null
        }
      ],
      "classes": [],
      "functions": [
        {
          "name": "as_float_array",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "XT",
              "type": null,
              "description": "An array of type float."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Converts an array-like to an array of floats.\n\nThe new dtype will be np.float32 or np.float64, depending on the original\ntype. The function can create a copy or modify the argument depending\non the argument copy.\n\nParameters\n----------\nX : {array-like, sparse matrix}\n\ncopy : bool, default=True\n    If True, a copy of X will be created. If False, a copy may still be\n    returned if X's dtype is not a floating point type.\n\nforce_all_finite : bool or 'allow-nan', default=True\n    Whether to raise an error on np.inf, np.nan, pd.NA in X. The\n    possibilities are:\n\n    - True: Force all values of X to be finite.\n    - False: accepts np.inf, np.nan, pd.NA in X.\n    - 'allow-nan': accepts only np.nan and pd.NA values in X. Values cannot\n      be infinite.\n\n    .. versionadded:: 0.20\n       ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n    .. versionchanged:: 0.23\n       Accepts `pd.NA` and converts it into `np.nan`\n\nReturns\n-------\nXT : {ndarray, sparse matrix}\n    An array of type float."
        },
        {
          "name": "assert_all_finite",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Throw a ValueError if X contains NaN or infinity.\n\nParameters\n----------\nX : {ndarray, sparse matrix}\n\nallow_nan : bool, default=False"
        },
        {
          "name": "check_X_y",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data."
            },
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Labels."
            },
            {
              "name": "accept_sparse",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "String[s] representing allowed sparse matrix formats, such as 'csc',\n'csr', etc. If the input is sparse but not in the allowed format,\nit will be converted to the first listed format. True allows the input\nto be any format. False means that a sparse matrix input will\nraise an error."
            }
          ],
          "results": [
            {
              "name": "X_converted",
              "type": null,
              "description": "The converted and validated X."
            },
            {
              "name": "y_converted",
              "type": null,
              "description": "The converted and validated y."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Input validation for standard estimators.\n\nChecks X and y for consistent length, enforces X to be 2D and y 1D. By\ndefault, X is checked to be non-empty and containing only finite values.\nStandard input checks are also applied to y, such as checking that y\ndoes not have np.nan or np.inf targets. For multi-label y, set\nmulti_output=True to allow 2D and sparse y. If the dtype of X is\nobject, attempt converting to float, raising on failure.\n\nParameters\n----------\nX : {ndarray, list, sparse matrix}\n    Input data.\n\ny : {ndarray, list, sparse matrix}\n    Labels.\n\naccept_sparse : str, bool or list of str, default=False\n    String[s] representing allowed sparse matrix formats, such as 'csc',\n    'csr', etc. If the input is sparse but not in the allowed format,\n    it will be converted to the first listed format. True allows the input\n    to be any format. False means that a sparse matrix input will\n    raise an error.\n\naccept_large_sparse : bool, default=True\n    If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n    accept_sparse, accept_large_sparse will cause it to be accepted only\n    if its indices are stored with a 32-bit dtype.\n\n    .. versionadded:: 0.20\n\ndtype : 'numeric', type, list of type or None, default='numeric'\n    Data type of result. If None, the dtype of the input is preserved.\n    If \"numeric\", dtype is preserved unless array.dtype is object.\n    If dtype is a list of types, conversion on the first type is only\n    performed if the dtype of the input is not in the list.\n\norder : {'F', 'C'}, default=None\n    Whether an array will be forced to be fortran or c-style.\n\ncopy : bool, default=False\n    Whether a forced copy will be triggered. If copy=False, a copy might\n    be triggered by a conversion.\n\nforce_all_finite : bool or 'allow-nan', default=True\n    Whether to raise an error on np.inf, np.nan, pd.NA in X. This parameter\n    does not influence whether y can have np.inf, np.nan, pd.NA values.\n    The possibilities are:\n\n    - True: Force all values of X to be finite.\n    - False: accepts np.inf, np.nan, pd.NA in X.\n    - 'allow-nan': accepts only np.nan or pd.NA values in X. Values cannot\n      be infinite.\n\n    .. versionadded:: 0.20\n       ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n    .. versionchanged:: 0.23\n       Accepts `pd.NA` and converts it into `np.nan`\n\nensure_2d : bool, default=True\n    Whether to raise a value error if X is not 2D.\n\nallow_nd : bool, default=False\n    Whether to allow X.ndim > 2.\n\nmulti_output : bool, default=False\n    Whether to allow 2D y (array or sparse matrix). If false, y will be\n    validated as a vector. y cannot have np.nan or np.inf values if\n    multi_output=True.\n\nensure_min_samples : int, default=1\n    Make sure that X has a minimum number of samples in its first\n    axis (rows for a 2D array).\n\nensure_min_features : int, default=1\n    Make sure that the 2D array has some minimum number of features\n    (columns). The default value of 1 rejects empty datasets.\n    This check is only enforced when X has effectively 2 dimensions or\n    is originally 1D and ``ensure_2d`` is True. Setting to 0 disables\n    this check.\n\ny_numeric : bool, default=False\n    Whether to ensure that y has a numeric type. If dtype of y is object,\n    it is converted to float64. Should only be used for regression\n    algorithms.\n\nestimator : str or estimator instance, default=None\n    If passed, include the name of the estimator in warning messages.\n\nReturns\n-------\nX_converted : object\n    The converted and validated X.\n\ny_converted : object\n    The converted and validated y."
        },
        {
          "name": "check_array",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "array",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input object to check / convert."
            },
            {
              "name": "accept_sparse",
              "type": "Any",
              "hasDefault": false,
              "default": "False",
              "limitation": null,
              "ignored": false,
              "description": "String[s] representing allowed sparse matrix formats, such as 'csc',\n'csr', etc. If the input is sparse but not in the allowed format,\nit will be converted to the first listed format. True allows the input\nto be any format. False means that a sparse matrix input will\nraise an error."
            }
          ],
          "results": [
            {
              "name": "array_converted",
              "type": null,
              "description": "The converted and validated array."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Input validation on an array, list, sparse matrix or similar.\n\nBy default, the input is checked to be a non-empty 2D array containing\nonly finite values. If the dtype of the array is object, attempt\nconverting to float, raising on failure.\n\nParameters\n----------\narray : object\n    Input object to check / convert.\n\naccept_sparse : str, bool or list/tuple of str, default=False\n    String[s] representing allowed sparse matrix formats, such as 'csc',\n    'csr', etc. If the input is sparse but not in the allowed format,\n    it will be converted to the first listed format. True allows the input\n    to be any format. False means that a sparse matrix input will\n    raise an error.\n\naccept_large_sparse : bool, default=True\n    If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n    accept_sparse, accept_large_sparse=False will cause it to be accepted\n    only if its indices are stored with a 32-bit dtype.\n\n    .. versionadded:: 0.20\n\ndtype : 'numeric', type, list of type or None, default='numeric'\n    Data type of result. If None, the dtype of the input is preserved.\n    If \"numeric\", dtype is preserved unless array.dtype is object.\n    If dtype is a list of types, conversion on the first type is only\n    performed if the dtype of the input is not in the list.\n\norder : {'F', 'C'} or None, default=None\n    Whether an array will be forced to be fortran or c-style.\n    When order is None (default), then if copy=False, nothing is ensured\n    about the memory layout of the output array; otherwise (copy=True)\n    the memory layout of the returned array is kept as close as possible\n    to the original array.\n\ncopy : bool, default=False\n    Whether a forced copy will be triggered. If copy=False, a copy might\n    be triggered by a conversion.\n\nforce_all_finite : bool or 'allow-nan', default=True\n    Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n    possibilities are:\n\n    - True: Force all values of array to be finite.\n    - False: accepts np.inf, np.nan, pd.NA in array.\n    - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n      cannot be infinite.\n\n    .. versionadded:: 0.20\n       ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n    .. versionchanged:: 0.23\n       Accepts `pd.NA` and converts it into `np.nan`\n\nensure_2d : bool, default=True\n    Whether to raise a value error if array is not 2D.\n\nallow_nd : bool, default=False\n    Whether to allow array.ndim > 2.\n\nensure_min_samples : int, default=1\n    Make sure that the array has a minimum number of samples in its first\n    axis (rows for a 2D array). Setting to 0 disables this check.\n\nensure_min_features : int, default=1\n    Make sure that the 2D array has some minimum number of features\n    (columns). The default value of 1 rejects empty datasets.\n    This check is only enforced when the input data has effectively 2\n    dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n    disables this check.\n\nestimator : str or estimator instance, default=None\n    If passed, include the name of the estimator in warning messages.\n\nReturns\n-------\narray_converted : object\n    The converted and validated array."
        },
        {
          "name": "check_consistent_length",
          "decorators": [],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check that all arrays have consistent first dimensions.\n\nChecks whether all objects in arrays have the same shape or length.\n\nParameters\n----------\n*arrays : list or tuple of input objects.\n    Objects that will be checked for consistent length."
        },
        {
          "name": "check_is_fitted",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "estimator instance for which the check is performed."
            },
            {
              "name": "attributes",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Attribute name(s) given as string or a list/tuple of strings\nEg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\nIf `None`, `estimator` is considered fitted if there exist an\nattribute that ends with a underscore and does not start with double\nunderscore."
            }
          ],
          "results": [
            {
              "name": "",
              "type": "...",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Perform is_fitted validation for estimator.\n\nChecks if the estimator is fitted by verifying the presence of\nfitted attributes (ending with a trailing underscore) and otherwise\nraises a NotFittedError with the given message.\n\nThis utility is meant to be used internally by estimators themselves,\ntypically in their own predict / transform methods.\n\nParameters\n----------\nestimator : estimator instance\n    estimator instance for which the check is performed.\n\nattributes : str, list or tuple of str, default=None\n    Attribute name(s) given as string or a list/tuple of strings\n    Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n    If `None`, `estimator` is considered fitted if there exist an\n    attribute that ends with a underscore and does not start with double\n    underscore.\n\nmsg : str, default=None\n    The default error message is, \"This %(name)s instance is not fitted\n    yet. Call 'fit' with appropriate arguments before using this\n    estimator.\"\n\n    For custom messages if \"%(name)s\" is present in the message string,\n    it is substituted for the estimator name.\n\n    Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\nall_or_any : callable, {all, any}, default=all\n    Specify whether all or any of the given attributes must exist.\n\nReturns\n-------\nNone\n\nRaises\n------\nNotFittedError\n    If the attributes are not found."
        },
        {
          "name": "check_memory",
          "decorators": [],
          "parameters": [
            {
              "name": "memory",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "memory",
              "type": null,
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check that ``memory`` is joblib.Memory-like.\n\njoblib.Memory-like means that ``memory`` can be converted into a\njoblib.Memory instance (typically a str denoting the ``location``)\nor has the same interface (has a ``cache`` method).\n\nParameters\n----------\nmemory : None, str or object with the joblib.Memory interface\n\nReturns\n-------\nmemory : object with the joblib.Memory interface\n\nRaises\n------\nValueError\n    If ``memory`` is not joblib.Memory-like."
        },
        {
          "name": "check_non_negative",
          "decorators": [],
          "parameters": [
            {
              "name": "X",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input data."
            },
            {
              "name": "whom",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Who passed X to this function."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Check if there is any negative value in an array.\n\nParameters\n----------\nX : {array-like, sparse matrix}\n    Input data.\n\nwhom : str\n    Who passed X to this function."
        },
        {
          "name": "check_random_state",
          "decorators": [],
          "parameters": [
            {
              "name": "seed",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "If seed is None, return the RandomState singleton used by np.random.\nIf seed is an int, return a new RandomState instance seeded with seed.\nIf seed is already a RandomState instance, return it.\nOtherwise raise ValueError."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Turn seed into a np.random.RandomState instance\n\nParameters\n----------\nseed : None, int or instance of RandomState\n    If seed is None, return the RandomState singleton used by np.random.\n    If seed is an int, return a new RandomState instance seeded with seed.\n    If seed is already a RandomState instance, return it.\n    Otherwise raise ValueError."
        },
        {
          "name": "check_scalar",
          "decorators": [],
          "parameters": [
            {
              "name": "x",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The scalar parameter to validate."
            },
            {
              "name": "name",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The name of the parameter to be printed in error messages."
            },
            {
              "name": "target_type",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Acceptable data types for the parameter."
            }
          ],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Validate scalar parameters type and value.\n\nParameters\n----------\nx : object\n    The scalar parameter to validate.\n\nname : str\n    The name of the parameter to be printed in error messages.\n\ntarget_type : type or tuple\n    Acceptable data types for the parameter.\n\nmin_val : float or int, default=None\n    The minimum valid value the parameter can take. If None (default) it\n    is implied that the parameter does not have a lower bound.\n\nmax_val : float or int, default=None\n    The maximum valid value the parameter can take. If None (default) it\n    is implied that the parameter does not have an upper bound.\n\nRaises\n-------\nTypeError\n    If the parameter's type does not match the desired type.\n\nValueError\n    If the parameter's value violates the given bounds."
        },
        {
          "name": "check_symmetric",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "array",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "Input object to check / convert. Must be two-dimensional and square,\notherwise a ValueError will be raised."
            }
          ],
          "results": [
            {
              "name": "array_sym",
              "type": null,
              "description": "Symmetrized version of the input array, i.e. the average of array\nand array.transpose(). If sparse, then duplicate entries are first\nsummed and zeros are eliminated."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Make sure that array is 2D, square and symmetric.\n\nIf the array is not symmetric, then a symmetrized version is returned.\nOptionally, a warning or exception is raised if the matrix is not\nsymmetric.\n\nParameters\n----------\narray : {ndarray, sparse matrix}\n    Input object to check / convert. Must be two-dimensional and square,\n    otherwise a ValueError will be raised.\n\ntol : float, default=1e-10\n    Absolute tolerance for equivalence of arrays. Default = 1E-10.\n\nraise_warning : bool, default=True\n    If True then raise a warning if conversion is required.\n\nraise_exception : bool, default=False\n    If True then raise an exception if array is not symmetric.\n\nReturns\n-------\narray_sym : {ndarray, sparse matrix}\n    Symmetrized version of the input array, i.e. the average of array\n    and array.transpose(). If sparse, then duplicate entries are first\n    summed and zeros are eliminated."
        },
        {
          "name": "column_or_1d",
          "decorators": [
            "_deprecate_positional_args"
          ],
          "parameters": [
            {
              "name": "y",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": ""
            }
          ],
          "results": [
            {
              "name": "y",
              "type": "NDArray",
              "description": ""
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Ravel column or 1d numpy array, else raises an error.\n\nParameters\n----------\ny : array-like\n\nwarn : bool, default=False\n   To control display of warnings.\n\nReturns\n-------\ny : ndarray"
        },
        {
          "name": "has_fit_parameter",
          "decorators": [],
          "parameters": [
            {
              "name": "estimator",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "An estimator to inspect."
            },
            {
              "name": "parameter",
              "type": "Any",
              "hasDefault": false,
              "default": "None",
              "limitation": null,
              "ignored": false,
              "description": "The searched parameter."
            }
          ],
          "results": [
            {
              "name": "",
              "type": null,
              "description": "Whether the parameter was found to be a named parameter of the\nestimator's fit method."
            }
          ],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Checks whether the estimator's fit method supports the given parameter.\n\nParameters\n----------\nestimator : object\n    An estimator to inspect.\n\nparameter : str\n    The searched parameter.\n\nReturns\n-------\nis_parameter: bool\n    Whether the parameter was found to be a named parameter of the\n    estimator's fit method.\n\nExamples\n--------\n>>> from sklearn.svm import SVC\n>>> has_fit_parameter(SVC(), \"sample_weight\")\nTrue"
        },
        {
          "name": "indexable",
          "decorators": [],
          "parameters": [],
          "results": [],
          "hasReturnType": false,
          "returnType": "Any",
          "fullDocstring": "Make arrays indexable for cross-validation.\n\nChecks consistent length, passes through None, and ensures that everything\ncan be indexed by converting sparse matrices to csr and converting\nnon-interable objects to arrays.\n\nParameters\n----------\n*iterables : {lists, dataframes, ndarrays, sparse matrices}\n    List of objects to ensure sliceability."
        }
      ]
    }
  ]
}